{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1660856217200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "llm = Llama(model_path=\".\\models\\llama-2-7b-chat.Q4_K_S.gguf\")\n",
    "# output = llm(\"Q: Name the planets in the solar system? A: \", max_tokens=64, stop=[\"Q:\", \"\\n\"], echo=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"main_data.csv\")\n",
    "\n",
    "df_html = df[\"html\"][219:396]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_html.to_csv('pandas.txt', header=None, index=None, sep=' ', mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.document_loaders import CSVLoader\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.llms import LlamaCpp\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import LlamaCppEmbeddings\n",
    "# split uusing token\n",
    "from transformers import GPT2TokenizerFast\n",
    "from langchain.text_splitter import CharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "252"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = TextLoader(\"pandas.txt\")\n",
    "documents = loader.load_and_split()\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "474"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = CSVLoader(file_path=\"main_data.csv\", csv_args={\n",
    "                'delimiter': ','})\n",
    "documents = loader.load()\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1409"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1024, # changing this increased answer size ()\n",
    "    chunk_overlap=20 \n",
    ")\n",
    "texts = text_splitter.split_documents(documents)\n",
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[73], line 6\u001b[0m\n\u001b[0;32m      2\u001b[0m tokenizer \u001b[39m=\u001b[39m GPT2TokenizerFast\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mgpt2\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m text_splitter \u001b[39m=\u001b[39m CharacterTextSplitter\u001b[39m.\u001b[39mfrom_huggingface_tokenizer(\n\u001b[0;32m      4\u001b[0m     tokenizer, chunk_size\u001b[39m=\u001b[39m\u001b[39m512\u001b[39m, chunk_overlap\u001b[39m=\u001b[39m\u001b[39m20\u001b[39m\n\u001b[0;32m      5\u001b[0m )\n\u001b[1;32m----> 6\u001b[0m texts \u001b[39m=\u001b[39m text_splitter\u001b[39m.\u001b[39;49msplit_text(documents)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\langchain\\text_splitter.py:298\u001b[0m, in \u001b[0;36mCharacterTextSplitter.split_text\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    294\u001b[0m \u001b[39m# First we naively split the large input into a bunch of smaller ones.\u001b[39;00m\n\u001b[0;32m    295\u001b[0m separator \u001b[39m=\u001b[39m (\n\u001b[0;32m    296\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_separator \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_separator_regex \u001b[39melse\u001b[39;00m re\u001b[39m.\u001b[39mescape(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_separator)\n\u001b[0;32m    297\u001b[0m )\n\u001b[1;32m--> 298\u001b[0m splits \u001b[39m=\u001b[39m _split_text_with_regex(text, separator, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_keep_separator)\n\u001b[0;32m    299\u001b[0m _separator \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_keep_separator \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_separator\n\u001b[0;32m    300\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_merge_splits(splits, _separator)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\langchain\\text_splitter.py:87\u001b[0m, in \u001b[0;36m_split_text_with_regex\u001b[1;34m(text, separator, keep_separator)\u001b[0m\n\u001b[0;32m     85\u001b[0m         splits \u001b[39m=\u001b[39m [_splits[\u001b[39m0\u001b[39m]] \u001b[39m+\u001b[39m splits\n\u001b[0;32m     86\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 87\u001b[0m         splits \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39;49msplit(separator, text)\n\u001b[0;32m     88\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     89\u001b[0m     splits \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(text)\n",
      "File \u001b[1;32mc:\\Users\\Ujjwal\\miniconda3\\envs\\chatbot\\lib\\re.py:231\u001b[0m, in \u001b[0;36msplit\u001b[1;34m(pattern, string, maxsplit, flags)\u001b[0m\n\u001b[0;32m    223\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msplit\u001b[39m(pattern, string, maxsplit\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, flags\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m):\n\u001b[0;32m    224\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Split the source string by the occurrences of the pattern,\u001b[39;00m\n\u001b[0;32m    225\u001b[0m \u001b[39m    returning a list containing the resulting substrings.  If\u001b[39;00m\n\u001b[0;32m    226\u001b[0m \u001b[39m    capturing parentheses are used in pattern, then the text of all\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    229\u001b[0m \u001b[39m    and the remainder of the string is returned as the final element\u001b[39;00m\n\u001b[0;32m    230\u001b[0m \u001b[39m    of the list.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 231\u001b[0m     \u001b[39mreturn\u001b[39;00m _compile(pattern, flags)\u001b[39m.\u001b[39;49msplit(string, maxsplit)\n",
      "\u001b[1;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "# using a different tokenizer\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "text_splitter = CharacterTextSplitter.from_huggingface_tokenizer(\n",
    "    tokenizer, chunk_size=512, chunk_overlap=20\n",
    ")\n",
    "texts = text_splitter.split_text(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")\n",
    "# embeddings = LlamaCppEmbeddings(model_path=\"./models/llama-2-7b-chat.Q4_K_S.gguf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = Chroma.from_documents(texts, embeddings, persist_directory=\"db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2617282214176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "llm = LlamaCpp(\n",
    "    model_path=\"./models/vicuna-13b-v1.5.Q4_K_M.gguf\",\n",
    "    n_ctx=2048,\n",
    "    # verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=db.as_retriever(search_kwargs={\"k\": 3}),\n",
    "    return_source_documents=True,\n",
    "    verbose=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    To join CCI's new student server, you can follow these steps:\n",
      "    1. Go to the \"Internal Servers\" page for current students and click on the appropriate server.\n",
      "    2. You will be prompted to log in with your Drexel username and password, as well as complete multi-factor authentication (MFA).\n",
      "    3. Once you have successfully logged in, you should be able to join the new student server.\n"
     ]
    }
   ],
   "source": [
    "res = qa(f\"\"\"\n",
    "    How to join CCI's new student server?\n",
    "\"\"\")\n",
    "print(res[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'streamlit' has no attribute '_is_running_with_streamlit'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[46], line 13\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mvectorstores\u001b[39;00m \u001b[39mimport\u001b[39;00m Chroma\n\u001b[0;32m     12\u001b[0m \u001b[39m# Customize the layout\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m st\u001b[39m.\u001b[39;49mset_page_config(page_title\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtest\u001b[39;49m\u001b[39m\"\u001b[39;49m, layout\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mwide\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m     14\u001b[0m st\u001b[39m.\u001b[39mmarkdown(\u001b[39mf\u001b[39m\u001b[39m\"\"\"\u001b[39m\n\u001b[0;32m     15\u001b[0m \u001b[39m            <style>\u001b[39m\n\u001b[0;32m     16\u001b[0m \u001b[39m            .stApp \u001b[39m\u001b[39m{{\u001b[39;00m\u001b[39mbackground-image: url(\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mhttps://images.unsplash.com/photo-1509537257950-20f875b03669?ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D&auto=format&fit=crop&w=1469&q=80\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m);\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[39m         </style>\u001b[39m\n\u001b[0;32m     20\u001b[0m \u001b[39m         \u001b[39m\u001b[39m\"\"\"\u001b[39m, unsafe_allow_html\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     22\u001b[0m \u001b[39m# function for writing uploaded file in temp\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Ujjwal\\miniconda3\\envs\\chatbot\\lib\\site-packages\\streamlit\\commands\\page_config.py:160\u001b[0m, in \u001b[0;36mset_page_config\u001b[1;34m(page_title, page_icon, layout, initial_sidebar_state, menu_items)\u001b[0m\n\u001b[0;32m    157\u001b[0m     menu_items_proto \u001b[39m=\u001b[39m msg\u001b[39m.\u001b[39mpage_config_changed\u001b[39m.\u001b[39mmenu_items\n\u001b[0;32m    158\u001b[0m     set_menu_items_proto(lowercase_menu_items, menu_items_proto)\n\u001b[1;32m--> 160\u001b[0m ctx \u001b[39m=\u001b[39m get_script_run_ctx()\n\u001b[0;32m    161\u001b[0m \u001b[39mif\u001b[39;00m ctx \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    162\u001b[0m     \u001b[39mreturn\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Ujjwal\\miniconda3\\envs\\chatbot\\lib\\site-packages\\streamlit\\runtime\\scriptrunner\\script_run_context.py:145\u001b[0m, in \u001b[0;36mget_script_run_ctx\u001b[1;34m()\u001b[0m\n\u001b[0;32m    141\u001b[0m thread \u001b[39m=\u001b[39m threading\u001b[39m.\u001b[39mcurrent_thread()\n\u001b[0;32m    142\u001b[0m ctx: Optional[ScriptRunContext] \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(\n\u001b[0;32m    143\u001b[0m     thread, SCRIPT_RUN_CONTEXT_ATTR_NAME, \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    144\u001b[0m )\n\u001b[1;32m--> 145\u001b[0m \u001b[39mif\u001b[39;00m ctx \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m streamlit\u001b[39m.\u001b[39;49m_is_running_with_streamlit:\n\u001b[0;32m    146\u001b[0m     \u001b[39m# Only warn about a missing ScriptRunContext if we were started\u001b[39;00m\n\u001b[0;32m    147\u001b[0m     \u001b[39m# via `streamlit run`. Otherwise, the user is likely running a\u001b[39;00m\n\u001b[0;32m    148\u001b[0m     \u001b[39m# script \"bare\", and doesn't need to be warned about streamlit\u001b[39;00m\n\u001b[0;32m    149\u001b[0m     \u001b[39m# bits that are irrelevant when not connected to a session.\u001b[39;00m\n\u001b[0;32m    150\u001b[0m     LOGGER\u001b[39m.\u001b[39mwarning(\u001b[39m\"\u001b[39m\u001b[39mThread \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m: missing ScriptRunContext\u001b[39m\u001b[39m\"\u001b[39m, thread\u001b[39m.\u001b[39mname)\n\u001b[0;32m    152\u001b[0m \u001b[39mreturn\u001b[39;00m ctx\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'streamlit' has no attribute '_is_running_with_streamlit'"
     ]
    }
   ],
   "source": [
    "# Bring in deps\n",
    "import streamlit as st\n",
    "from langchain.llms import LlamaCpp\n",
    "from langchain.embeddings import LlamaCppEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "\n",
    "# Customize the layout\n",
    "st.set_page_config(page_title=\"test\", layout=\"wide\")\n",
    "st.markdown(f\"\"\"\n",
    "            <style>\n",
    "            .stApp {{background-image: url(\"https://images.unsplash.com/photo-1509537257950-20f875b03669?ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D&auto=format&fit=crop&w=1469&q=80\");\n",
    "                     background-attachment: fixed;\n",
    "                     background-size: cover}}\n",
    "         </style>\n",
    "         \"\"\", unsafe_allow_html=True)\n",
    "\n",
    "# function for writing uploaded file in temp\n",
    "def write_text_file(content, file_path):\n",
    "    try:\n",
    "        with open(file_path, 'w') as file:\n",
    "            file.write(content)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while writing the file: {e}\")\n",
    "        return False\n",
    "\n",
    "# set prompt template\n",
    "# prompt_template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "# {context}\n",
    "# Question: {question}\n",
    "# Answer:\"\"\"\n",
    "# prompt = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
    "\n",
    "# initialize hte LLM & Embeddings\n",
    "llm = LlamaCpp(model_path=\"./models/llama-7b.ggmlv3.q4_0.bin\")\n",
    "embeddings = LlamaCppEmbeddings(model_path=\"models/llama-7b.ggmlv3.q4_0.bin\")\n",
    "# llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
    "llm_chain = LLMChain(llm=llm)\n",
    "\n",
    "st.title(\"Document Conversation\")\n",
    "uploaded_file = st.file_uploader(\"Upload an article\", type=\"txt\")\n",
    "\n",
    "if uploaded_file is not None:\n",
    "    content = uploaded_file.read().decode('utf-8')\n",
    "    # st.write(content)\n",
    "    file_path = \"temp/file.txt\"\n",
    "    write_text_file(content, file_path)\n",
    "\n",
    "    loader = TextLoader(file_path)\n",
    "    docs = loader.load()\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=100, chunk_overlap=0)\n",
    "    texts = text_splitter.split_documents(docs)\n",
    "    db = Chroma.from_documents(texts, embeddings)\n",
    "    st.success(\"File Loaded Successfully!!\")\n",
    "\n",
    "    # Query through LLM\n",
    "    question = st.text_input(\"Ask something from the file\", placeholder=\"Find something similar to: ....this.... in the text?\", disabled=not uploaded_file,)\n",
    "    if question:\n",
    "        similar_doc = db.similarity_search(question, k=1)\n",
    "        context = similar_doc[0].page_content\n",
    "        # query_llm = LLMChain(llm=llm, prompt=prompt)\n",
    "        query_llm = LLMChain(llm=llm)\n",
    "        response = query_llm.run({\"context\": context, \"question\": question})\n",
    "        st.write(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
