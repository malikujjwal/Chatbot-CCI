{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>If Zoom Asks for a Password or Login</th>\n",
       "      <th>Kaltura Drexel Streams</th>\n",
       "      <th>Zoom -- Recover Deleted Meeting or Recording</th>\n",
       "      <th>Camtasia</th>\n",
       "      <th>Zoom -- Recording</th>\n",
       "      <th>Accessing files in Microsoft Teams with Non-Drexel email</th>\n",
       "      <th>echo360 -- Q and A Feature</th>\n",
       "      <th>echo360 -- Muting the Microphone during a Recording or Broadcast</th>\n",
       "      <th>Kaltura Drexel Streams -- Share Audio or Video with Non-Drexel Users or Sharing with Anyone</th>\n",
       "      <th>Zoom -- Add Videos to Bb Learn using Kaltura Drexel Streams</th>\n",
       "      <th>...</th>\n",
       "      <th>Single &amp; Double-Sided Printing in Safari</th>\n",
       "      <th>Faxing on Papercut at Drexel CCI</th>\n",
       "      <th>CCI Commons</th>\n",
       "      <th>Printing Single and Double-sided in browser</th>\n",
       "      <th>Single &amp; Double-Sided Printing in Chrome</th>\n",
       "      <th>Printing with PaperCut</th>\n",
       "      <th>Poster Printing</th>\n",
       "      <th>Scanning without a Scanner</th>\n",
       "      <th>Single &amp; Double-Sided Printing in Firefox</th>\n",
       "      <th>Single &amp; Double-Sided Printing in Microsoft Edge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://support.cci.drexel.edu/remote-conferen...</td>\n",
       "      <td>https://support.cci.drexel.edu/remote-conferen...</td>\n",
       "      <td>https://support.cci.drexel.edu/remote-conferen...</td>\n",
       "      <td>https://support.cci.drexel.edu/remote-conferen...</td>\n",
       "      <td>https://support.cci.drexel.edu/remote-conferen...</td>\n",
       "      <td>https://support.cci.drexel.edu/remote-conferen...</td>\n",
       "      <td>https://support.cci.drexel.edu/remote-conferen...</td>\n",
       "      <td>https://support.cci.drexel.edu/remote-conferen...</td>\n",
       "      <td>https://support.cci.drexel.edu/remote-conferen...</td>\n",
       "      <td>https://support.cci.drexel.edu/remote-conferen...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>https://support.cci.drexel.edu/remote-conferen...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://support.cci.drexel.edu/remote-conferen...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://support.cci.drexel.edu/remote-conferen...</td>\n",
       "      <td>https://support.cci.drexel.edu/remote-conferen...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>https://support.cci.drexel.edu/printing/cci-co...</td>\n",
       "      <td>https://support.cci.drexel.edu/printing/cci-co...</td>\n",
       "      <td>https://support.cci.drexel.edu/printing/printi...</td>\n",
       "      <td>https://support.cci.drexel.edu/printing/cci-co...</td>\n",
       "      <td>https://support.cci.drexel.edu/printing/cci-co...</td>\n",
       "      <td>https://support.cci.drexel.edu/printing/cci-co...</td>\n",
       "      <td>https://support.cci.drexel.edu/printing/poster...</td>\n",
       "      <td>https://support.cci.drexel.edu/printing/scanni...</td>\n",
       "      <td>https://support.cci.drexel.edu/printing/cci-co...</td>\n",
       "      <td>https://support.cci.drexel.edu/printing/cci-co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9 rows Ã— 702 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                If Zoom Asks for a Password or Login  \\\n",
       "0  https://support.cci.drexel.edu/remote-conferen...   \n",
       "1                                                NaN   \n",
       "2                                                NaN   \n",
       "3                                                NaN   \n",
       "4                                                NaN   \n",
       "5                                                NaN   \n",
       "6                                                NaN   \n",
       "7                                                NaN   \n",
       "8                                                NaN   \n",
       "\n",
       "                              Kaltura Drexel Streams  \\\n",
       "0  https://support.cci.drexel.edu/remote-conferen...   \n",
       "1  https://support.cci.drexel.edu/remote-conferen...   \n",
       "2                                                NaN   \n",
       "3                                                NaN   \n",
       "4                                                NaN   \n",
       "5                                                NaN   \n",
       "6                                                NaN   \n",
       "7                                                NaN   \n",
       "8                                                NaN   \n",
       "\n",
       "        Zoom -- Recover Deleted Meeting or Recording  \\\n",
       "0  https://support.cci.drexel.edu/remote-conferen...   \n",
       "1                                                NaN   \n",
       "2                                                NaN   \n",
       "3                                                NaN   \n",
       "4                                                NaN   \n",
       "5                                                NaN   \n",
       "6                                                NaN   \n",
       "7                                                NaN   \n",
       "8                                                NaN   \n",
       "\n",
       "                                            Camtasia  \\\n",
       "0  https://support.cci.drexel.edu/remote-conferen...   \n",
       "1  https://support.cci.drexel.edu/remote-conferen...   \n",
       "2                                                NaN   \n",
       "3                                                NaN   \n",
       "4                                                NaN   \n",
       "5                                                NaN   \n",
       "6                                                NaN   \n",
       "7                                                NaN   \n",
       "8                                                NaN   \n",
       "\n",
       "                                   Zoom -- Recording  \\\n",
       "0  https://support.cci.drexel.edu/remote-conferen...   \n",
       "1                                                NaN   \n",
       "2                                                NaN   \n",
       "3                                                NaN   \n",
       "4                                                NaN   \n",
       "5                                                NaN   \n",
       "6                                                NaN   \n",
       "7                                                NaN   \n",
       "8                                                NaN   \n",
       "\n",
       "  Accessing files in Microsoft Teams with Non-Drexel email  \\\n",
       "0  https://support.cci.drexel.edu/remote-conferen...         \n",
       "1                                                NaN         \n",
       "2                                                NaN         \n",
       "3                                                NaN         \n",
       "4                                                NaN         \n",
       "5                                                NaN         \n",
       "6                                                NaN         \n",
       "7                                                NaN         \n",
       "8                                                NaN         \n",
       "\n",
       "                          echo360 -- Q and A Feature  \\\n",
       "0  https://support.cci.drexel.edu/remote-conferen...   \n",
       "1                                                NaN   \n",
       "2                                                NaN   \n",
       "3                                                NaN   \n",
       "4                                                NaN   \n",
       "5                                                NaN   \n",
       "6                                                NaN   \n",
       "7                                                NaN   \n",
       "8                                                NaN   \n",
       "\n",
       "  echo360 -- Muting the Microphone during a Recording or Broadcast  \\\n",
       "0  https://support.cci.drexel.edu/remote-conferen...                 \n",
       "1                                                NaN                 \n",
       "2                                                NaN                 \n",
       "3                                                NaN                 \n",
       "4                                                NaN                 \n",
       "5                                                NaN                 \n",
       "6                                                NaN                 \n",
       "7                                                NaN                 \n",
       "8                                                NaN                 \n",
       "\n",
       "  Kaltura Drexel Streams -- Share Audio or Video with Non-Drexel Users or Sharing with Anyone  \\\n",
       "0  https://support.cci.drexel.edu/remote-conferen...                                            \n",
       "1  https://support.cci.drexel.edu/remote-conferen...                                            \n",
       "2                                                NaN                                            \n",
       "3                                                NaN                                            \n",
       "4                                                NaN                                            \n",
       "5                                                NaN                                            \n",
       "6                                                NaN                                            \n",
       "7                                                NaN                                            \n",
       "8                                                NaN                                            \n",
       "\n",
       "  Zoom -- Add Videos to Bb Learn using Kaltura Drexel Streams  ...  \\\n",
       "0  https://support.cci.drexel.edu/remote-conferen...           ...   \n",
       "1  https://support.cci.drexel.edu/remote-conferen...           ...   \n",
       "2                                                NaN           ...   \n",
       "3                                                NaN           ...   \n",
       "4                                                NaN           ...   \n",
       "5                                                NaN           ...   \n",
       "6                                                NaN           ...   \n",
       "7                                                NaN           ...   \n",
       "8                                                NaN           ...   \n",
       "\n",
       "            Single & Double-Sided Printing in Safari  \\\n",
       "0                                                NaN   \n",
       "1                                                NaN   \n",
       "2                                                NaN   \n",
       "3                                                NaN   \n",
       "4                                                NaN   \n",
       "5                                                NaN   \n",
       "6                                                NaN   \n",
       "7  https://support.cci.drexel.edu/printing/cci-co...   \n",
       "8                                                NaN   \n",
       "\n",
       "                    Faxing on Papercut at Drexel CCI  \\\n",
       "0                                                NaN   \n",
       "1                                                NaN   \n",
       "2                                                NaN   \n",
       "3                                                NaN   \n",
       "4                                                NaN   \n",
       "5                                                NaN   \n",
       "6                                                NaN   \n",
       "7  https://support.cci.drexel.edu/printing/cci-co...   \n",
       "8                                                NaN   \n",
       "\n",
       "                                         CCI Commons  \\\n",
       "0                                                NaN   \n",
       "1                                                NaN   \n",
       "2                                                NaN   \n",
       "3                                                NaN   \n",
       "4                                                NaN   \n",
       "5                                                NaN   \n",
       "6                                                NaN   \n",
       "7  https://support.cci.drexel.edu/printing/printi...   \n",
       "8                                                NaN   \n",
       "\n",
       "         Printing Single and Double-sided in browser  \\\n",
       "0                                                NaN   \n",
       "1                                                NaN   \n",
       "2                                                NaN   \n",
       "3                                                NaN   \n",
       "4                                                NaN   \n",
       "5                                                NaN   \n",
       "6                                                NaN   \n",
       "7  https://support.cci.drexel.edu/printing/cci-co...   \n",
       "8                                                NaN   \n",
       "\n",
       "            Single & Double-Sided Printing in Chrome  \\\n",
       "0                                                NaN   \n",
       "1                                                NaN   \n",
       "2                                                NaN   \n",
       "3                                                NaN   \n",
       "4                                                NaN   \n",
       "5                                                NaN   \n",
       "6                                                NaN   \n",
       "7  https://support.cci.drexel.edu/printing/cci-co...   \n",
       "8                                                NaN   \n",
       "\n",
       "                              Printing with PaperCut  \\\n",
       "0                                                NaN   \n",
       "1                                                NaN   \n",
       "2                                                NaN   \n",
       "3                                                NaN   \n",
       "4                                                NaN   \n",
       "5                                                NaN   \n",
       "6                                                NaN   \n",
       "7  https://support.cci.drexel.edu/printing/cci-co...   \n",
       "8                                                NaN   \n",
       "\n",
       "                                     Poster Printing  \\\n",
       "0                                                NaN   \n",
       "1                                                NaN   \n",
       "2                                                NaN   \n",
       "3                                                NaN   \n",
       "4                                                NaN   \n",
       "5                                                NaN   \n",
       "6                                                NaN   \n",
       "7  https://support.cci.drexel.edu/printing/poster...   \n",
       "8                                                NaN   \n",
       "\n",
       "                          Scanning without a Scanner  \\\n",
       "0                                                NaN   \n",
       "1                                                NaN   \n",
       "2                                                NaN   \n",
       "3                                                NaN   \n",
       "4                                                NaN   \n",
       "5                                                NaN   \n",
       "6                                                NaN   \n",
       "7  https://support.cci.drexel.edu/printing/scanni...   \n",
       "8                                                NaN   \n",
       "\n",
       "           Single & Double-Sided Printing in Firefox  \\\n",
       "0                                                NaN   \n",
       "1                                                NaN   \n",
       "2                                                NaN   \n",
       "3                                                NaN   \n",
       "4                                                NaN   \n",
       "5                                                NaN   \n",
       "6                                                NaN   \n",
       "7  https://support.cci.drexel.edu/printing/cci-co...   \n",
       "8                                                NaN   \n",
       "\n",
       "    Single & Double-Sided Printing in Microsoft Edge  \n",
       "0                                                NaN  \n",
       "1                                                NaN  \n",
       "2                                                NaN  \n",
       "3                                                NaN  \n",
       "4                                                NaN  \n",
       "5                                                NaN  \n",
       "6                                                NaN  \n",
       "7  https://support.cci.drexel.edu/printing/cci-co...  \n",
       "8                                                NaN  \n",
       "\n",
       "[9 rows x 702 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_json(\"main_data.json\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ujjwal\\miniconda3\\envs\\pytorch2\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (â€¦)lve/main/config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 571/571 [00:00<00:00, 421kB/s]\n",
      "c:\\Users\\Ujjwal\\miniconda3\\envs\\pytorch2\\lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Ujjwal\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Downloading pytorch_model.bin: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 496M/496M [00:33<00:00, 14.9MB/s] \n",
      "Downloading (â€¦)okenizer_config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79.0/79.0 [00:00<00:00, 79.0kB/s]\n",
      "Downloading (â€¦)olve/main/vocab.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 899k/899k [00:00<00:00, 18.2MB/s]\n",
      "Downloading (â€¦)olve/main/merges.txt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 456k/456k [00:00<00:00, 11.5MB/s]\n",
      "Downloading (â€¦)cial_tokens_map.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 772/772 [00:00<?, ?B/s] \n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n",
    "\n",
    "model_name = \"deepset/roberta-base-squad2\"\n",
    "\n",
    "# a) Get predictions\n",
    "nlp = pipeline('question-answering', model=model_name, tokenizer=model_name)\n",
    "QA_input = {\n",
    "    'question': 'Why is model conversion important?',\n",
    "    'context': 'The option to convert models between FARM and transformers gives freedom to the user and let people easily switch between frameworks.'\n",
    "}\n",
    "res = nlp(QA_input)\n",
    "\n",
    "# b) Load model & tokenizer\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: <s>\n"
     ]
    }
   ],
   "source": [
    "# Define the model and tokenizer names\n",
    "model_name = \"deepset/roberta-base-squad2\"\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Define the question and context\n",
    "question = \"Zoom is asking for a password\"\n",
    "context = \"Zoom Asks for a Password: https://support.cci.drexel.edu/remote-conferencing-and-class-capture/zoom/if-zoom-asks-password-or-login/\"\n",
    "\n",
    "# Tokenize the question and context\n",
    "inputs = tokenizer(question, context, return_tensors=\"pt\")\n",
    "\n",
    "# Perform question answering\n",
    "outputs = model(**inputs)\n",
    "\n",
    "start_scores = outputs.start_logits\n",
    "end_scores = outputs.end_logits\n",
    "\n",
    "# Convert the token indices to answer text\n",
    "answer_start = torch.argmax(start_scores)\n",
    "answer_end = torch.argmax(end_scores) + 1\n",
    "answer = tokenizer.decode(inputs[\"input_ids\"][0][answer_start:answer_end])\n",
    "\n",
    "# Print the answer\n",
    "print(\"Answer:\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score': 0.40735939145088196, 'start': 22, 'end': 23, 'answer': '1'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "qa_model = pipeline(\"question-answering\")\n",
    "question = \"Zoom is asking me for a login\"\n",
    "context = \"The login for zoom is 1. The password for zoom is 1\"\n",
    "qa_model(question = question, context = context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Contact The CCI Helpdesk\\nWord Accessibility Review\\nTagged with: word accessibility\\nFollow these instructions for accessibility to Word:\\nOpen the word document\\n\\n2. Download the file\\n\\n3. Select \"Review\"\\n\\n4. Select \"Check accessibility\". It shows if the document can be scanned using the accessibility checker or not.\\n\\n5. If the document cannot be scanned it will give you the. option of converting the document. The \"convert document\" function can be found in files.\\n\\n6. Select \"Ok\"\\n\\n7. This will highlight the inspect and highlight the errors in your document.\\n\\n8.You can then makes changes and adjust the document accordingly.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"main_data.csv\")\n",
    "data[\"html\"][200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "supportDoc = {}\n",
    "for dirpath, dirs, files in os.walk('C:\\Drexel\\SA\\Chatbot\\support-page\\html'): \n",
    "  for filename in files:\n",
    "    fname = os.path.join(dirpath,filename)\n",
    "    if fname.endswith('.html'):\n",
    "      dirName = dirpath.split('\\\\')[-1]\n",
    "      supportDoc[dirName.replace(\"-\", \" \") if dirName.replace(\"-\", \" \") != \"html\" else \"zoom\"] = fname\n",
    "\n",
    "queries = [query for query in supportDoc]\n",
    "documents = [supportDoc[query] for query in supportDoc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Ujjwal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Ujjwal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "from llama_index import download_loader, VectorStoreIndex, ServiceContext, StorageContext, load_index_from_storage\n",
    "from pathlib import Path\n",
    "\n",
    "UnstructuredReader = download_loader(\"UnstructuredReader\", refresh_cache=True)\n",
    "\n",
    "loader = UnstructuredReader()\n",
    "doc_set = {}\n",
    "all_docs = []\n",
    "for query in supportDoc:\n",
    "    html_docs = loader.load_data(file=Path(supportDoc[query]), split_documents=False)\n",
    "    # insert metadata\n",
    "    for d in html_docs:\n",
    "        d.metadata = {\"query\": query}\n",
    "    doc_set[query] = html_docs\n",
    "    all_docs.extend(html_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******\n",
      "Could not load OpenAI model. Using default LlamaCPP=llama2-13b-chat. If you intended to use OpenAI, please check your OPENAI_API_KEY.\n",
      "Original error:\n",
      "No API key found for OpenAI.\n",
      "Please set either the OPENAI_API_KEY environment variable or openai.api_key prior to initialization.\n",
      "API keys can be found or created at https://platform.openai.com/account/api-keys\n",
      "\n",
      "******\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******\n",
      "Could not load OpenAIEmbedding. Using HuggingFaceBgeEmbeddings with model_name=BAAI/bge-small-en. If you intended to use OpenAI, please check your OPENAI_API_KEY.\n",
      "Original error:\n",
      "No API key found for OpenAI.\n",
      "Please set either the OPENAI_API_KEY environment variable or openai.api_key prior to initialization.\n",
      "API keys can be found or created at https://platform.openai.com/account/api-keys\n",
      "\n",
      "******\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "service_context = ServiceContext.from_defaults(chunk_size=512)\n",
    "index_set = {}\n",
    "for title in doc_set:\n",
    "    storage_context = StorageContext.from_defaults()\n",
    "    cur_index = VectorStoreIndex.from_documents(\n",
    "        doc_set[title],\n",
    "        service_context=service_context,\n",
    "        storage_context=storage_context,\n",
    "    )\n",
    "    index_set[title] = cur_index\n",
    "    storage_context.persist(persist_dir=f'./storage/{title}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package langchain.llms in langchain:\n",
      "\n",
      "NAME\n",
      "    langchain.llms\n",
      "\n",
      "DESCRIPTION\n",
      "    **LLM** classes provide\n",
      "    access to the large language model (**LLM**) APIs and services.\n",
      "    \n",
      "    **Class hierarchy:**\n",
      "    \n",
      "    .. code-block::\n",
      "    \n",
      "        BaseLanguageModel --> BaseLLM --> LLM --> <name>  # Examples: AI21, HuggingFaceHub, OpenAI\n",
      "    \n",
      "    **Main helpers:**\n",
      "    \n",
      "    .. code-block::\n",
      "    \n",
      "        LLMResult, PromptValue,\n",
      "        CallbackManagerForLLMRun, AsyncCallbackManagerForLLMRun,\n",
      "        CallbackManager, AsyncCallbackManager,\n",
      "        AIMessage, BaseMessage\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    ai21\n",
      "    aleph_alpha\n",
      "    amazon_api_gateway\n",
      "    anthropic\n",
      "    anyscale\n",
      "    aviary\n",
      "    azureml_endpoint\n",
      "    bananadev\n",
      "    base\n",
      "    baseten\n",
      "    beam\n",
      "    bedrock\n",
      "    bittensor\n",
      "    cerebriumai\n",
      "    chatglm\n",
      "    clarifai\n",
      "    cohere\n",
      "    ctransformers\n",
      "    ctranslate2\n",
      "    databricks\n",
      "    deepinfra\n",
      "    deepsparse\n",
      "    edenai\n",
      "    fake\n",
      "    fireworks\n",
      "    forefrontai\n",
      "    google_palm\n",
      "    gooseai\n",
      "    gpt4all\n",
      "    huggingface_endpoint\n",
      "    huggingface_hub\n",
      "    huggingface_pipeline\n",
      "    huggingface_text_gen_inference\n",
      "    human\n",
      "    koboldai\n",
      "    llamacpp\n",
      "    loading\n",
      "    manifest\n",
      "    minimax\n",
      "    mlflow_ai_gateway\n",
      "    modal\n",
      "    mosaicml\n",
      "    nlpcloud\n",
      "    octoai_endpoint\n",
      "    ollama\n",
      "    opaqueprompts\n",
      "    openai\n",
      "    openllm\n",
      "    openlm\n",
      "    petals\n",
      "    pipelineai\n",
      "    predibase\n",
      "    predictionguard\n",
      "    promptlayer_openai\n",
      "    replicate\n",
      "    rwkv\n",
      "    sagemaker_endpoint\n",
      "    self_hosted\n",
      "    self_hosted_hugging_face\n",
      "    stochasticai\n",
      "    symblai_nebula\n",
      "    textgen\n",
      "    titan_takeoff\n",
      "    tongyi\n",
      "    utils\n",
      "    vertexai\n",
      "    vllm\n",
      "    writer\n",
      "    xinference\n",
      "\n",
      "CLASSES\n",
      "    langchain.llms.anthropic._AnthropicCommon(langchain.schema.language_model.BaseLanguageModel)\n",
      "        langchain.llms.anthropic.Anthropic(langchain.llms.base.LLM, langchain.llms.anthropic._AnthropicCommon)\n",
      "    langchain.llms.base.BaseLLM(langchain.schema.language_model.BaseLanguageModel, abc.ABC)\n",
      "        langchain.llms.ctranslate2.CTranslate2\n",
      "        langchain.llms.fireworks.FireworksChat\n",
      "        langchain.llms.google_palm.GooglePalm(langchain.llms.base.BaseLLM, pydantic.main.BaseModel)\n",
      "        langchain.llms.ollama.Ollama(langchain.llms.base.BaseLLM, langchain.llms.ollama._OllamaCommon)\n",
      "        langchain.llms.openai.OpenAIChat\n",
      "            langchain.llms.promptlayer_openai.PromptLayerOpenAIChat\n",
      "        langchain.llms.vllm.VLLM\n",
      "    langchain.llms.base.LLM(langchain.llms.base.BaseLLM)\n",
      "        langchain.llms.ai21.AI21\n",
      "        langchain.llms.aleph_alpha.AlephAlpha\n",
      "        langchain.llms.amazon_api_gateway.AmazonAPIGateway\n",
      "        langchain.llms.anthropic.Anthropic(langchain.llms.base.LLM, langchain.llms.anthropic._AnthropicCommon)\n",
      "        langchain.llms.anyscale.Anyscale\n",
      "        langchain.llms.aviary.Aviary\n",
      "        langchain.llms.azureml_endpoint.AzureMLOnlineEndpoint(langchain.llms.base.LLM, pydantic.main.BaseModel)\n",
      "        langchain.llms.bananadev.Banana\n",
      "        langchain.llms.baseten.Baseten\n",
      "        langchain.llms.beam.Beam\n",
      "        langchain.llms.bedrock.Bedrock(langchain.llms.base.LLM, langchain.llms.bedrock.BedrockBase)\n",
      "        langchain.llms.bittensor.NIBittensorLLM\n",
      "        langchain.llms.cerebriumai.CerebriumAI\n",
      "        langchain.llms.chatglm.ChatGLM\n",
      "        langchain.llms.clarifai.Clarifai\n",
      "        langchain.llms.cohere.Cohere\n",
      "        langchain.llms.ctransformers.CTransformers\n",
      "        langchain.llms.databricks.Databricks\n",
      "        langchain.llms.deepinfra.DeepInfra\n",
      "        langchain.llms.deepsparse.DeepSparse\n",
      "        langchain.llms.edenai.EdenAI\n",
      "        langchain.llms.fake.FakeListLLM\n",
      "        langchain.llms.forefrontai.ForefrontAI\n",
      "        langchain.llms.gooseai.GooseAI\n",
      "        langchain.llms.gpt4all.GPT4All\n",
      "        langchain.llms.huggingface_endpoint.HuggingFaceEndpoint\n",
      "        langchain.llms.huggingface_hub.HuggingFaceHub\n",
      "        langchain.llms.huggingface_pipeline.HuggingFacePipeline\n",
      "        langchain.llms.huggingface_text_gen_inference.HuggingFaceTextGenInference\n",
      "        langchain.llms.human.HumanInputLLM\n",
      "        langchain.llms.koboldai.KoboldApiLLM\n",
      "        langchain.llms.llamacpp.LlamaCpp\n",
      "        langchain.llms.manifest.ManifestWrapper\n",
      "        langchain.llms.minimax.Minimax\n",
      "        langchain.llms.mlflow_ai_gateway.MlflowAIGateway\n",
      "        langchain.llms.modal.Modal\n",
      "        langchain.llms.mosaicml.MosaicML\n",
      "        langchain.llms.nlpcloud.NLPCloud\n",
      "        langchain.llms.octoai_endpoint.OctoAIEndpoint\n",
      "        langchain.llms.opaqueprompts.OpaquePrompts\n",
      "        langchain.llms.openllm.OpenLLM\n",
      "        langchain.llms.petals.Petals\n",
      "        langchain.llms.pipelineai.PipelineAI(langchain.llms.base.LLM, pydantic.main.BaseModel)\n",
      "        langchain.llms.predibase.Predibase\n",
      "        langchain.llms.predictionguard.PredictionGuard\n",
      "        langchain.llms.replicate.Replicate\n",
      "        langchain.llms.rwkv.RWKV(langchain.llms.base.LLM, pydantic.main.BaseModel)\n",
      "        langchain.llms.sagemaker_endpoint.SagemakerEndpoint\n",
      "        langchain.llms.self_hosted.SelfHostedPipeline\n",
      "            langchain.llms.self_hosted_hugging_face.SelfHostedHuggingFaceLLM\n",
      "        langchain.llms.stochasticai.StochasticAI\n",
      "        langchain.llms.symblai_nebula.Nebula\n",
      "        langchain.llms.textgen.TextGen\n",
      "        langchain.llms.titan_takeoff.TitanTakeoff\n",
      "        langchain.llms.tongyi.Tongyi\n",
      "        langchain.llms.vertexai.VertexAI(langchain.llms.vertexai._VertexAICommon, langchain.llms.base.LLM)\n",
      "        langchain.llms.vertexai.VertexAIModelGarden(langchain.llms.vertexai._VertexAIBase, langchain.llms.base.LLM)\n",
      "        langchain.llms.writer.Writer\n",
      "        langchain.llms.xinference.Xinference\n",
      "    langchain.llms.bedrock.BedrockBase(pydantic.main.BaseModel, abc.ABC)\n",
      "        langchain.llms.bedrock.Bedrock(langchain.llms.base.LLM, langchain.llms.bedrock.BedrockBase)\n",
      "    langchain.llms.fireworks.BaseFireworks(langchain.llms.base.BaseLLM)\n",
      "        langchain.llms.fireworks.Fireworks\n",
      "    langchain.llms.ollama._OllamaCommon(langchain.schema.language_model.BaseLanguageModel)\n",
      "        langchain.llms.ollama.Ollama(langchain.llms.base.BaseLLM, langchain.llms.ollama._OllamaCommon)\n",
      "    langchain.llms.openai.BaseOpenAI(langchain.llms.base.BaseLLM)\n",
      "        langchain.llms.openai.AzureOpenAI\n",
      "        langchain.llms.openai.OpenAI\n",
      "            langchain.llms.promptlayer_openai.PromptLayerOpenAI\n",
      "        langchain.llms.openlm.OpenLM\n",
      "        langchain.llms.vllm.VLLMOpenAI\n",
      "    langchain.llms.vertexai._VertexAIBase(pydantic.main.BaseModel)\n",
      "        langchain.llms.vertexai.VertexAIModelGarden(langchain.llms.vertexai._VertexAIBase, langchain.llms.base.LLM)\n",
      "    langchain.llms.vertexai._VertexAICommon(langchain.llms.vertexai._VertexAIBase)\n",
      "        langchain.llms.vertexai.VertexAI(langchain.llms.vertexai._VertexAICommon, langchain.llms.base.LLM)\n",
      "    pydantic.main.BaseModel(pydantic.utils.Representation)\n",
      "        langchain.llms.azureml_endpoint.AzureMLOnlineEndpoint(langchain.llms.base.LLM, pydantic.main.BaseModel)\n",
      "        langchain.llms.google_palm.GooglePalm(langchain.llms.base.BaseLLM, pydantic.main.BaseModel)\n",
      "        langchain.llms.pipelineai.PipelineAI(langchain.llms.base.LLM, pydantic.main.BaseModel)\n",
      "        langchain.llms.rwkv.RWKV(langchain.llms.base.LLM, pydantic.main.BaseModel)\n",
      "    \n",
      "    class AI21(langchain.llms.base.LLM)\n",
      "     |  AI21(*, cache: Optional[bool] = None, verbose: bool = None, callbacks: Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager, NoneType] = None, callback_manager: Optional[langchain.callbacks.base.BaseCallbackManager] = None, tags: Optional[List[str]] = None, metadata: Optional[Dict[str, Any]] = None, model: str = 'j2-jumbo-instruct', temperature: float = 0.7, maxTokens: int = 256, minTokens: int = 0, topP: float = 1.0, presencePenalty: langchain.llms.ai21.AI21PenaltyData = AI21PenaltyData(scale=0, applyToWhitespaces=True, applyToPunctuations=True, applyToNumbers=True, applyToStopwords=True, applyToEmojis=True), countPenalty: langchain.llms.ai21.AI21PenaltyData = AI21PenaltyData(scale=0, applyToWhitespaces=True, applyToPunctuations=True, applyToNumbers=True, applyToStopwords=True, applyToEmojis=True), frequencyPenalty: langchain.llms.ai21.AI21PenaltyData = AI21PenaltyData(scale=0, applyToWhitespaces=True, applyToPunctuations=True, applyToNumbers=True, applyToStopwords=True, applyToEmojis=True), numResults: int = 1, logitBias: Optional[Dict[str, float]] = None, ai21_api_key: Optional[str] = None, stop: Optional[List[str]] = None, base_url: Optional[str] = None) -> None\n",
      "     |  \n",
      "     |  AI21 large language models.\n",
      "     |  \n",
      "     |  To use, you should have the environment variable ``AI21_API_KEY``\n",
      "     |  set with your API key.\n",
      "     |  \n",
      "     |  Example:\n",
      "     |      .. code-block:: python\n",
      "     |  \n",
      "     |          from langchain.llms import AI21\n",
      "     |          ai21 = AI21(model=\"j2-jumbo-instruct\")\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      AI21\n",
      "     |      langchain.llms.base.LLM\n",
      "     |      langchain.llms.base.BaseLLM\n",
      "     |      langchain.schema.language_model.BaseLanguageModel\n",
      "     |      langchain.load.serializable.Serializable\n",
      "     |      pydantic.main.BaseModel\n",
      "     |      pydantic.utils.Representation\n",
      "     |      langchain.schema.runnable.base.Runnable\n",
      "     |      typing.Generic\n",
      "     |      abc.ABC\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  validate_environment(values: Dict) -> Dict from pydantic.main.ModelMetaclass\n",
      "     |      Validate that api key exists in environment.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __json_encoder__ = pydantic_encoder(obj: Any) -> Any\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  Config = <class 'langchain.llms.ai21.AI21.Config'>\n",
      "     |      Configuration for this pydantic object.\n",
      "     |  \n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {'ai21_api_key': typing.Optional[str], 'base_url': t...\n",
      "     |  \n",
      "     |  __class_vars__ = set()\n",
      "     |  \n",
      "     |  __config__ = <class 'langchain.llms.ai21.Config'>\n",
      "     |  \n",
      "     |  __custom_root_type__ = False\n",
      "     |  \n",
      "     |  __exclude_fields__ = {'callback_manager': True, 'callbacks': True, 'me...\n",
      "     |  \n",
      "     |  __fields__ = {'ai21_api_key': ModelField(name='ai21_api_key', type=Opt...\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "     |  \n",
      "     |  __include_fields__ = None\n",
      "     |  \n",
      "     |  __parameters__ = ()\n",
      "     |  \n",
      "     |  __post_root_validators__ = [(False, <function BaseLLM.raise_deprecatio...\n",
      "     |  \n",
      "     |  __pre_root_validators__ = []\n",
      "     |  \n",
      "     |  __private_attributes__ = {'_lc_kwargs': ModelPrivateAttr(default=Pydan...\n",
      "     |  \n",
      "     |  __schema_cache__ = {}\n",
      "     |  \n",
      "     |  __signature__ = <Signature (*, cache: Optional[bool] = None, ver... = ...\n",
      "     |  \n",
      "     |  __validators__ = {'verbose': [<pydantic.class_validators.Validator obj...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  __call__(self, prompt: 'str', stop: 'Optional[List[str]]' = None, callbacks: 'Callbacks' = None, *, tags: 'Optional[List[str]]' = None, metadata: 'Optional[Dict[str, Any]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Check Cache and run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  __str__(self) -> 'str'\n",
      "     |      Get a string representation of the object for printing.\n",
      "     |  \n",
      "     |  async abatch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      "     |      Default implementation of abatch, which calls ainvoke N times.\n",
      "     |      Subclasses should override this method if they can batch more efficiently.\n",
      "     |  \n",
      "     |  async agenerate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  async agenerate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Asynchronously pass a sequence of prompts and return model generations.\n",
      "     |      \n",
      "     |      This method should make use of batched calls for models that expose a batched\n",
      "     |      API.\n",
      "     |      \n",
      "     |      Use this method when you want to:\n",
      "     |          1. take advantage of batched calls,\n",
      "     |          2. need more output from the model than just the top generated value,\n",
      "     |          3. are building chains that are agnostic to the underlying language model\n",
      "     |              type (e.g., pure text completion models vs chat models).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      "     |              converted to match the format of any language model (string for pure\n",
      "     |              text generation models and BaseMessages for chat models).\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          callbacks: Callbacks to pass through. Used for executing additional\n",
      "     |              functionality, such as logging or streaming, throughout generation.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          An LLMResult, which contains a list of candidate Generations for each input\n",
      "     |              prompt and additional model provider-specific output.\n",
      "     |  \n",
      "     |  async ainvoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Default implementation of ainvoke, which calls invoke in a thread pool.\n",
      "     |      Subclasses should override this method if they can run asynchronously.\n",
      "     |  \n",
      "     |  async apredict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Asynchronously pass a string to the model and return a string prediction.\n",
      "     |      \n",
      "     |      Use this method when calling pure text generation models and only the top\n",
      "     |          candidate generation is needed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: String input to pass to the model.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a string.\n",
      "     |  \n",
      "     |  async apredict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      "     |      Asynchronously pass messages to the model and return a message prediction.\n",
      "     |      \n",
      "     |      Use this method when calling chat models and only the top\n",
      "     |          candidate generation is needed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: A sequence of chat messages corresponding to a single model input.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a message.\n",
      "     |  \n",
      "     |  async astream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'AsyncIterator[str]'\n",
      "     |      Default implementation of astream, which calls ainvoke.\n",
      "     |      Subclasses should override this method if they support streaming output.\n",
      "     |  \n",
      "     |  batch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      "     |      Default implementation of batch, which calls invoke N times.\n",
      "     |      Subclasses should override this method if they can batch more efficiently.\n",
      "     |  \n",
      "     |  dict(self, **kwargs: 'Any') -> 'Dict'\n",
      "     |      Return a dictionary of the LLM.\n",
      "     |  \n",
      "     |  generate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  generate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Pass a sequence of prompts to the model and return model generations.\n",
      "     |      \n",
      "     |      This method should make use of batched calls for models that expose a batched\n",
      "     |      API.\n",
      "     |      \n",
      "     |      Use this method when you want to:\n",
      "     |          1. take advantage of batched calls,\n",
      "     |          2. need more output from the model than just the top generated value,\n",
      "     |          3. are building chains that are agnostic to the underlying language model\n",
      "     |              type (e.g., pure text completion models vs chat models).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      "     |              converted to match the format of any language model (string for pure\n",
      "     |              text generation models and BaseMessages for chat models).\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          callbacks: Callbacks to pass through. Used for executing additional\n",
      "     |              functionality, such as logging or streaming, throughout generation.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          An LLMResult, which contains a list of candidate Generations for each input\n",
      "     |              prompt and additional model provider-specific output.\n",
      "     |  \n",
      "     |  invoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |  \n",
      "     |  predict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Pass a single string input to the model and return a string prediction.\n",
      "     |      \n",
      "     |       Use this method when passing in raw text. If you want to pass in specific\n",
      "     |          types of chat messages, use predict_messages.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: String input to pass to the model.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a string.\n",
      "     |  \n",
      "     |  predict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      "     |      Pass a message sequence to the model and return a message prediction.\n",
      "     |      \n",
      "     |      Use this method when passing in chat messages. If you want to pass in raw text,\n",
      "     |          use predict.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: A sequence of chat messages corresponding to a single model input.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a message.\n",
      "     |  \n",
      "     |  save(self, file_path: 'Union[Path, str]') -> 'None'\n",
      "     |      Save the LLM.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          file_path: Path to file to save the LLM to.\n",
      "     |      \n",
      "     |      Example:\n",
      "     |      .. code-block:: python\n",
      "     |      \n",
      "     |          llm.save(file_path=\"path/llm.yaml\")\n",
      "     |  \n",
      "     |  stream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'Iterator[str]'\n",
      "     |      Default implementation of stream, which calls invoke.\n",
      "     |      Subclasses should override this method if they support streaming output.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  raise_deprecation(values: 'Dict') -> 'Dict' from pydantic.main.ModelMetaclass\n",
      "     |      Raise deprecation warning if callback_manager is used.\n",
      "     |  \n",
      "     |  set_verbose(verbose: 'Optional[bool]') -> 'bool' from pydantic.main.ModelMetaclass\n",
      "     |      If verbose is None, set it.\n",
      "     |      \n",
      "     |      This allows users to pass in None as verbose to access the global setting.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  __orig_bases__ = (langchain.schema.language_model.BaseLanguageModel[st...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.schema.language_model.BaseLanguageModel:\n",
      "     |  \n",
      "     |  get_num_tokens(self, text: 'str') -> 'int'\n",
      "     |      Get the number of tokens present in the text.\n",
      "     |      \n",
      "     |      Useful for checking if an input will fit in a model's context window.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: The string input to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The integer number of tokens in the text.\n",
      "     |  \n",
      "     |  get_num_tokens_from_messages(self, messages: 'List[BaseMessage]') -> 'int'\n",
      "     |      Get the number of tokens in the messages.\n",
      "     |      \n",
      "     |      Useful for checking if an input will fit in a model's context window.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: The message inputs to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The sum of the number of tokens across the messages.\n",
      "     |  \n",
      "     |  get_token_ids(self, text: 'str') -> 'List[int]'\n",
      "     |      Return the ordered ids of the tokens in a text.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: The string input to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          A list of ids corresponding to the tokens in the text, in order they occur\n",
      "     |              in the text.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from langchain.schema.language_model.BaseLanguageModel:\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.load.serializable.Serializable:\n",
      "     |  \n",
      "     |  __init__(self, **kwargs: Any) -> None\n",
      "     |      Create a new model by parsing and validating input data from keyword arguments.\n",
      "     |      \n",
      "     |      Raises ValidationError if the input data cannot be parsed to form a valid model.\n",
      "     |  \n",
      "     |  to_json(self) -> Union[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\n",
      "     |  \n",
      "     |  to_json_not_implemented(self) -> langchain.load.serializable.SerializedNotImplemented\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from langchain.load.serializable.Serializable:\n",
      "     |  \n",
      "     |  lc_attributes\n",
      "     |      Return a list of attribute names that should be included in the\n",
      "     |      serialized kwargs. These attributes must be accepted by the\n",
      "     |      constructor.\n",
      "     |  \n",
      "     |  lc_namespace\n",
      "     |      Return the namespace of the langchain object.\n",
      "     |      eg. [\"langchain\", \"llms\", \"openai\"]\n",
      "     |  \n",
      "     |  lc_secrets\n",
      "     |      Return a map of constructor argument names to secret ids.\n",
      "     |      eg. {\"openai_api_key\": \"OPENAI_API_KEY\"}\n",
      "     |  \n",
      "     |  lc_serializable\n",
      "     |      Return whether or not the class is serializable.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __eq__(self, other: Any) -> bool\n",
      "     |  \n",
      "     |  __getstate__(self) -> 'DictAny'\n",
      "     |  \n",
      "     |  __iter__(self) -> 'TupleGenerator'\n",
      "     |      so `dict(model)` works\n",
      "     |  \n",
      "     |  __repr_args__(self) -> 'ReprArgs'\n",
      "     |  \n",
      "     |  __setattr__(self, name, value)\n",
      "     |  \n",
      "     |  __setstate__(self, state: 'DictAny') -> None\n",
      "     |  \n",
      "     |  copy(self: 'Model', *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, update: Optional[ForwardRef('DictStrAny')] = None, deep: bool = False) -> 'Model'\n",
      "     |      Duplicate a model, optionally choose which fields to include, exclude and change.\n",
      "     |      \n",
      "     |      :param include: fields to include in new model\n",
      "     |      :param exclude: fields to exclude from new model, as with values this takes precedence over include\n",
      "     |      :param update: values to change/add in the new model. Note: the data is not validated before creating\n",
      "     |          the new model: you should trust this data\n",
      "     |      :param deep: set to `True` to make a deep copy of the model\n",
      "     |      :return: new model instance\n",
      "     |  \n",
      "     |  json(self, *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) -> 'unicode'\n",
      "     |      Generate a JSON representation of the model, `include` and `exclude` arguments as per `dict()`.\n",
      "     |      \n",
      "     |      `encoder` is an optional function to supply as `default` to json.dumps(), other arguments as per `json.dumps()`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __get_validators__() -> 'CallableGenerator' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  __try_update_forward_refs__(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      "     |      Same as update_forward_refs but will not raise exception\n",
      "     |      when forward references are not defined.\n",
      "     |  \n",
      "     |  construct(_fields_set: Optional[ForwardRef('SetStr')] = None, **values: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |      Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data.\n",
      "     |      Default values are respected, but no other validation is performed.\n",
      "     |      Behaves as if `Config.extra = 'allow'` was set since it adds all passed values\n",
      "     |  \n",
      "     |  from_orm(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_file(path: Union[str, pathlib.Path], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_obj(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_raw(b: Union[str, bytes], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema(by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}') -> 'DictStrAny' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema_json(*, by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}', **dumps_kwargs: Any) -> 'unicode' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  update_forward_refs(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      "     |      Try to update ForwardRefs on fields based on this Model, globalns and localns.\n",
      "     |  \n",
      "     |  validate(value: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __fields_set__\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.utils.Representation:\n",
      "     |  \n",
      "     |  __pretty__(self, fmt: Callable[[Any], Any], **kwargs: Any) -> Generator[Any, NoneType, NoneType]\n",
      "     |      Used by devtools (https://python-devtools.helpmanual.io/) to provide a human readable representations of objects\n",
      "     |  \n",
      "     |  __repr__(self) -> 'unicode'\n",
      "     |  \n",
      "     |  __repr_name__(self) -> 'unicode'\n",
      "     |      Name of the instance's class, used in __repr__.\n",
      "     |  \n",
      "     |  __repr_str__(self, join_str: 'unicode') -> 'unicode'\n",
      "     |  \n",
      "     |  __rich_repr__(self) -> 'RichReprResult'\n",
      "     |      Get fields for Rich library\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.schema.runnable.base.Runnable:\n",
      "     |  \n",
      "     |  __or__(self, other: 'Union[Runnable[Any, Other], Callable[[Any], Other], Mapping[str, Union[Runnable[Any, Other], Callable[[Any], Other], Any]]]') -> 'RunnableSequence[Input, Other]'\n",
      "     |      Return self|value.\n",
      "     |  \n",
      "     |  __ror__(self, other: 'Union[Runnable[Other, Any], Callable[[Any], Other], Mapping[str, Union[Runnable[Other, Any], Callable[[Other], Any], Any]]]') -> 'RunnableSequence[Other, Output]'\n",
      "     |      Return value|self.\n",
      "     |  \n",
      "     |  async atransform(self, input: 'AsyncIterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'AsyncIterator[Output]'\n",
      "     |      Default implementation of atransform, which buffers input and calls astream.\n",
      "     |      Subclasses should override this method if they can start producing output while\n",
      "     |      input is still being generated.\n",
      "     |  \n",
      "     |  bind(self, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      "     |      Bind arguments to a Runnable, returning a new Runnable.\n",
      "     |  \n",
      "     |  map(self) -> 'Runnable[List[Input], List[Output]]'\n",
      "     |      Return a new Runnable that maps a list of inputs to a list of outputs,\n",
      "     |      by calling invoke() with each input.\n",
      "     |  \n",
      "     |  transform(self, input: 'Iterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'Iterator[Output]'\n",
      "     |      Default implementation of transform, which buffers input and then calls stream.\n",
      "     |      Subclasses should override this method if they can start producing output while\n",
      "     |      input is still being generated.\n",
      "     |  \n",
      "     |  with_config(self, config: 'Optional[RunnableConfig]' = None, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      "     |      Bind config to a Runnable, returning a new Runnable.\n",
      "     |  \n",
      "     |  with_fallbacks(self, fallbacks: 'Sequence[Runnable[Input, Output]]', *, exceptions_to_handle: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,)) -> 'RunnableWithFallbacks[Input, Output]'\n",
      "     |  \n",
      "     |  with_retry(self, *, retry_if_exception_type: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,), wait_exponential_jitter: 'bool' = True, stop_after_attempt: 'int' = 3) -> 'Runnable[Input, Output]'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __class_getitem__(params) from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  __init_subclass__(*args, **kwargs) from pydantic.main.ModelMetaclass\n",
      "     |      This method is called when a class is subclassed.\n",
      "     |      \n",
      "     |      The default implementation does nothing. It may be\n",
      "     |      overridden to extend subclasses.\n",
      "    \n",
      "    class AlephAlpha(langchain.llms.base.LLM)\n",
      "     |  AlephAlpha(*, cache: Optional[bool] = None, verbose: bool = None, callbacks: Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager, NoneType] = None, callback_manager: Optional[langchain.callbacks.base.BaseCallbackManager] = None, tags: Optional[List[str]] = None, metadata: Optional[Dict[str, Any]] = None, client: Any = None, model: Optional[str] = 'luminous-base', maximum_tokens: int = 64, temperature: float = 0.0, top_k: int = 0, top_p: float = 0.0, presence_penalty: float = 0.0, frequency_penalty: float = 0.0, repetition_penalties_include_prompt: Optional[bool] = False, use_multiplicative_presence_penalty: Optional[bool] = False, penalty_bias: Optional[str] = None, penalty_exceptions: Optional[List[str]] = None, penalty_exceptions_include_stop_sequences: Optional[bool] = None, best_of: Optional[int] = None, n: int = 1, logit_bias: Optional[Dict[int, float]] = None, log_probs: Optional[int] = None, tokens: Optional[bool] = False, disable_optimizations: Optional[bool] = False, minimum_tokens: Optional[int] = 0, echo: bool = False, use_multiplicative_frequency_penalty: bool = False, sequence_penalty: float = 0.0, sequence_penalty_min_length: int = 2, use_multiplicative_sequence_penalty: bool = False, completion_bias_inclusion: Optional[Sequence[str]] = None, completion_bias_inclusion_first_token_only: bool = False, completion_bias_exclusion: Optional[Sequence[str]] = None, completion_bias_exclusion_first_token_only: bool = False, contextual_control_threshold: Optional[float] = None, control_log_additive: Optional[bool] = True, repetition_penalties_include_completion: bool = True, raw_completion: bool = False, stop_sequences: Optional[List[str]] = None, aleph_alpha_api_key: Optional[str] = None, host: str = 'https://api.aleph-alpha.com', hosting: Optional[str] = None, request_timeout_seconds: int = 305, total_retries: int = 8, nice: bool = False) -> None\n",
      "     |  \n",
      "     |  Aleph Alpha large language models.\n",
      "     |  \n",
      "     |  To use, you should have the ``aleph_alpha_client`` python package installed, and the\n",
      "     |  environment variable ``ALEPH_ALPHA_API_KEY`` set with your API key, or pass\n",
      "     |  it as a named parameter to the constructor.\n",
      "     |  \n",
      "     |  Parameters are explained more in depth here:\n",
      "     |  https://github.com/Aleph-Alpha/aleph-alpha-client/blob/c14b7dd2b4325c7da0d6a119f6e76385800e097b/aleph_alpha_client/completion.py#L10\n",
      "     |  \n",
      "     |  Example:\n",
      "     |      .. code-block:: python\n",
      "     |  \n",
      "     |          from langchain.llms import AlephAlpha\n",
      "     |          aleph_alpha = AlephAlpha(aleph_alpha_api_key=\"my-api-key\")\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      AlephAlpha\n",
      "     |      langchain.llms.base.LLM\n",
      "     |      langchain.llms.base.BaseLLM\n",
      "     |      langchain.schema.language_model.BaseLanguageModel\n",
      "     |      langchain.load.serializable.Serializable\n",
      "     |      pydantic.main.BaseModel\n",
      "     |      pydantic.utils.Representation\n",
      "     |      langchain.schema.runnable.base.Runnable\n",
      "     |      typing.Generic\n",
      "     |      abc.ABC\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  validate_environment(values: Dict) -> Dict from pydantic.main.ModelMetaclass\n",
      "     |      Validate that api key and python package exists in environment.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __json_encoder__ = pydantic_encoder(obj: Any) -> Any\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  Config = <class 'langchain.llms.aleph_alpha.AlephAlpha.Config'>\n",
      "     |      Configuration for this pydantic object.\n",
      "     |  \n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {'aleph_alpha_api_key': typing.Optional[str], 'best_...\n",
      "     |  \n",
      "     |  __class_vars__ = set()\n",
      "     |  \n",
      "     |  __config__ = <class 'langchain.llms.aleph_alpha.Config'>\n",
      "     |  \n",
      "     |  __custom_root_type__ = False\n",
      "     |  \n",
      "     |  __exclude_fields__ = {'callback_manager': True, 'callbacks': True, 'me...\n",
      "     |  \n",
      "     |  __fields__ = {'aleph_alpha_api_key': ModelField(name='aleph_alpha_api_...\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "     |  \n",
      "     |  __include_fields__ = None\n",
      "     |  \n",
      "     |  __parameters__ = ()\n",
      "     |  \n",
      "     |  __post_root_validators__ = [(False, <function BaseLLM.raise_deprecatio...\n",
      "     |  \n",
      "     |  __pre_root_validators__ = []\n",
      "     |  \n",
      "     |  __private_attributes__ = {'_lc_kwargs': ModelPrivateAttr(default=Pydan...\n",
      "     |  \n",
      "     |  __schema_cache__ = {}\n",
      "     |  \n",
      "     |  __signature__ = <Signature (*, cache: Optional[bool] = None, ver...al_...\n",
      "     |  \n",
      "     |  __validators__ = {'verbose': [<pydantic.class_validators.Validator obj...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  __call__(self, prompt: 'str', stop: 'Optional[List[str]]' = None, callbacks: 'Callbacks' = None, *, tags: 'Optional[List[str]]' = None, metadata: 'Optional[Dict[str, Any]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Check Cache and run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  __str__(self) -> 'str'\n",
      "     |      Get a string representation of the object for printing.\n",
      "     |  \n",
      "     |  async abatch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      "     |      Default implementation of abatch, which calls ainvoke N times.\n",
      "     |      Subclasses should override this method if they can batch more efficiently.\n",
      "     |  \n",
      "     |  async agenerate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  async agenerate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Asynchronously pass a sequence of prompts and return model generations.\n",
      "     |      \n",
      "     |      This method should make use of batched calls for models that expose a batched\n",
      "     |      API.\n",
      "     |      \n",
      "     |      Use this method when you want to:\n",
      "     |          1. take advantage of batched calls,\n",
      "     |          2. need more output from the model than just the top generated value,\n",
      "     |          3. are building chains that are agnostic to the underlying language model\n",
      "     |              type (e.g., pure text completion models vs chat models).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      "     |              converted to match the format of any language model (string for pure\n",
      "     |              text generation models and BaseMessages for chat models).\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          callbacks: Callbacks to pass through. Used for executing additional\n",
      "     |              functionality, such as logging or streaming, throughout generation.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          An LLMResult, which contains a list of candidate Generations for each input\n",
      "     |              prompt and additional model provider-specific output.\n",
      "     |  \n",
      "     |  async ainvoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Default implementation of ainvoke, which calls invoke in a thread pool.\n",
      "     |      Subclasses should override this method if they can run asynchronously.\n",
      "     |  \n",
      "     |  async apredict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Asynchronously pass a string to the model and return a string prediction.\n",
      "     |      \n",
      "     |      Use this method when calling pure text generation models and only the top\n",
      "     |          candidate generation is needed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: String input to pass to the model.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a string.\n",
      "     |  \n",
      "     |  async apredict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      "     |      Asynchronously pass messages to the model and return a message prediction.\n",
      "     |      \n",
      "     |      Use this method when calling chat models and only the top\n",
      "     |          candidate generation is needed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: A sequence of chat messages corresponding to a single model input.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a message.\n",
      "     |  \n",
      "     |  async astream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'AsyncIterator[str]'\n",
      "     |      Default implementation of astream, which calls ainvoke.\n",
      "     |      Subclasses should override this method if they support streaming output.\n",
      "     |  \n",
      "     |  batch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      "     |      Default implementation of batch, which calls invoke N times.\n",
      "     |      Subclasses should override this method if they can batch more efficiently.\n",
      "     |  \n",
      "     |  dict(self, **kwargs: 'Any') -> 'Dict'\n",
      "     |      Return a dictionary of the LLM.\n",
      "     |  \n",
      "     |  generate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  generate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Pass a sequence of prompts to the model and return model generations.\n",
      "     |      \n",
      "     |      This method should make use of batched calls for models that expose a batched\n",
      "     |      API.\n",
      "     |      \n",
      "     |      Use this method when you want to:\n",
      "     |          1. take advantage of batched calls,\n",
      "     |          2. need more output from the model than just the top generated value,\n",
      "     |          3. are building chains that are agnostic to the underlying language model\n",
      "     |              type (e.g., pure text completion models vs chat models).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      "     |              converted to match the format of any language model (string for pure\n",
      "     |              text generation models and BaseMessages for chat models).\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          callbacks: Callbacks to pass through. Used for executing additional\n",
      "     |              functionality, such as logging or streaming, throughout generation.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          An LLMResult, which contains a list of candidate Generations for each input\n",
      "     |              prompt and additional model provider-specific output.\n",
      "     |  \n",
      "     |  invoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |  \n",
      "     |  predict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Pass a single string input to the model and return a string prediction.\n",
      "     |      \n",
      "     |       Use this method when passing in raw text. If you want to pass in specific\n",
      "     |          types of chat messages, use predict_messages.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: String input to pass to the model.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a string.\n",
      "     |  \n",
      "     |  predict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      "     |      Pass a message sequence to the model and return a message prediction.\n",
      "     |      \n",
      "     |      Use this method when passing in chat messages. If you want to pass in raw text,\n",
      "     |          use predict.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: A sequence of chat messages corresponding to a single model input.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a message.\n",
      "     |  \n",
      "     |  save(self, file_path: 'Union[Path, str]') -> 'None'\n",
      "     |      Save the LLM.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          file_path: Path to file to save the LLM to.\n",
      "     |      \n",
      "     |      Example:\n",
      "     |      .. code-block:: python\n",
      "     |      \n",
      "     |          llm.save(file_path=\"path/llm.yaml\")\n",
      "     |  \n",
      "     |  stream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'Iterator[str]'\n",
      "     |      Default implementation of stream, which calls invoke.\n",
      "     |      Subclasses should override this method if they support streaming output.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  raise_deprecation(values: 'Dict') -> 'Dict' from pydantic.main.ModelMetaclass\n",
      "     |      Raise deprecation warning if callback_manager is used.\n",
      "     |  \n",
      "     |  set_verbose(verbose: 'Optional[bool]') -> 'bool' from pydantic.main.ModelMetaclass\n",
      "     |      If verbose is None, set it.\n",
      "     |      \n",
      "     |      This allows users to pass in None as verbose to access the global setting.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  __orig_bases__ = (langchain.schema.language_model.BaseLanguageModel[st...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.schema.language_model.BaseLanguageModel:\n",
      "     |  \n",
      "     |  get_num_tokens(self, text: 'str') -> 'int'\n",
      "     |      Get the number of tokens present in the text.\n",
      "     |      \n",
      "     |      Useful for checking if an input will fit in a model's context window.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: The string input to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The integer number of tokens in the text.\n",
      "     |  \n",
      "     |  get_num_tokens_from_messages(self, messages: 'List[BaseMessage]') -> 'int'\n",
      "     |      Get the number of tokens in the messages.\n",
      "     |      \n",
      "     |      Useful for checking if an input will fit in a model's context window.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: The message inputs to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The sum of the number of tokens across the messages.\n",
      "     |  \n",
      "     |  get_token_ids(self, text: 'str') -> 'List[int]'\n",
      "     |      Return the ordered ids of the tokens in a text.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: The string input to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          A list of ids corresponding to the tokens in the text, in order they occur\n",
      "     |              in the text.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from langchain.schema.language_model.BaseLanguageModel:\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.load.serializable.Serializable:\n",
      "     |  \n",
      "     |  __init__(self, **kwargs: Any) -> None\n",
      "     |      Create a new model by parsing and validating input data from keyword arguments.\n",
      "     |      \n",
      "     |      Raises ValidationError if the input data cannot be parsed to form a valid model.\n",
      "     |  \n",
      "     |  to_json(self) -> Union[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\n",
      "     |  \n",
      "     |  to_json_not_implemented(self) -> langchain.load.serializable.SerializedNotImplemented\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from langchain.load.serializable.Serializable:\n",
      "     |  \n",
      "     |  lc_attributes\n",
      "     |      Return a list of attribute names that should be included in the\n",
      "     |      serialized kwargs. These attributes must be accepted by the\n",
      "     |      constructor.\n",
      "     |  \n",
      "     |  lc_namespace\n",
      "     |      Return the namespace of the langchain object.\n",
      "     |      eg. [\"langchain\", \"llms\", \"openai\"]\n",
      "     |  \n",
      "     |  lc_secrets\n",
      "     |      Return a map of constructor argument names to secret ids.\n",
      "     |      eg. {\"openai_api_key\": \"OPENAI_API_KEY\"}\n",
      "     |  \n",
      "     |  lc_serializable\n",
      "     |      Return whether or not the class is serializable.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __eq__(self, other: Any) -> bool\n",
      "     |  \n",
      "     |  __getstate__(self) -> 'DictAny'\n",
      "     |  \n",
      "     |  __iter__(self) -> 'TupleGenerator'\n",
      "     |      so `dict(model)` works\n",
      "     |  \n",
      "     |  __repr_args__(self) -> 'ReprArgs'\n",
      "     |  \n",
      "     |  __setattr__(self, name, value)\n",
      "     |  \n",
      "     |  __setstate__(self, state: 'DictAny') -> None\n",
      "     |  \n",
      "     |  copy(self: 'Model', *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, update: Optional[ForwardRef('DictStrAny')] = None, deep: bool = False) -> 'Model'\n",
      "     |      Duplicate a model, optionally choose which fields to include, exclude and change.\n",
      "     |      \n",
      "     |      :param include: fields to include in new model\n",
      "     |      :param exclude: fields to exclude from new model, as with values this takes precedence over include\n",
      "     |      :param update: values to change/add in the new model. Note: the data is not validated before creating\n",
      "     |          the new model: you should trust this data\n",
      "     |      :param deep: set to `True` to make a deep copy of the model\n",
      "     |      :return: new model instance\n",
      "     |  \n",
      "     |  json(self, *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) -> 'unicode'\n",
      "     |      Generate a JSON representation of the model, `include` and `exclude` arguments as per `dict()`.\n",
      "     |      \n",
      "     |      `encoder` is an optional function to supply as `default` to json.dumps(), other arguments as per `json.dumps()`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __get_validators__() -> 'CallableGenerator' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  __try_update_forward_refs__(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      "     |      Same as update_forward_refs but will not raise exception\n",
      "     |      when forward references are not defined.\n",
      "     |  \n",
      "     |  construct(_fields_set: Optional[ForwardRef('SetStr')] = None, **values: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |      Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data.\n",
      "     |      Default values are respected, but no other validation is performed.\n",
      "     |      Behaves as if `Config.extra = 'allow'` was set since it adds all passed values\n",
      "     |  \n",
      "     |  from_orm(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_file(path: Union[str, pathlib.Path], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_obj(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_raw(b: Union[str, bytes], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema(by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}') -> 'DictStrAny' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema_json(*, by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}', **dumps_kwargs: Any) -> 'unicode' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  update_forward_refs(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      "     |      Try to update ForwardRefs on fields based on this Model, globalns and localns.\n",
      "     |  \n",
      "     |  validate(value: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __fields_set__\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.utils.Representation:\n",
      "     |  \n",
      "     |  __pretty__(self, fmt: Callable[[Any], Any], **kwargs: Any) -> Generator[Any, NoneType, NoneType]\n",
      "     |      Used by devtools (https://python-devtools.helpmanual.io/) to provide a human readable representations of objects\n",
      "     |  \n",
      "     |  __repr__(self) -> 'unicode'\n",
      "     |  \n",
      "     |  __repr_name__(self) -> 'unicode'\n",
      "     |      Name of the instance's class, used in __repr__.\n",
      "     |  \n",
      "     |  __repr_str__(self, join_str: 'unicode') -> 'unicode'\n",
      "     |  \n",
      "     |  __rich_repr__(self) -> 'RichReprResult'\n",
      "     |      Get fields for Rich library\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.schema.runnable.base.Runnable:\n",
      "     |  \n",
      "     |  __or__(self, other: 'Union[Runnable[Any, Other], Callable[[Any], Other], Mapping[str, Union[Runnable[Any, Other], Callable[[Any], Other], Any]]]') -> 'RunnableSequence[Input, Other]'\n",
      "     |      Return self|value.\n",
      "     |  \n",
      "     |  __ror__(self, other: 'Union[Runnable[Other, Any], Callable[[Any], Other], Mapping[str, Union[Runnable[Other, Any], Callable[[Other], Any], Any]]]') -> 'RunnableSequence[Other, Output]'\n",
      "     |      Return value|self.\n",
      "     |  \n",
      "     |  async atransform(self, input: 'AsyncIterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'AsyncIterator[Output]'\n",
      "     |      Default implementation of atransform, which buffers input and calls astream.\n",
      "     |      Subclasses should override this method if they can start producing output while\n",
      "     |      input is still being generated.\n",
      "     |  \n",
      "     |  bind(self, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      "     |      Bind arguments to a Runnable, returning a new Runnable.\n",
      "     |  \n",
      "     |  map(self) -> 'Runnable[List[Input], List[Output]]'\n",
      "     |      Return a new Runnable that maps a list of inputs to a list of outputs,\n",
      "     |      by calling invoke() with each input.\n",
      "     |  \n",
      "     |  transform(self, input: 'Iterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'Iterator[Output]'\n",
      "     |      Default implementation of transform, which buffers input and then calls stream.\n",
      "     |      Subclasses should override this method if they can start producing output while\n",
      "     |      input is still being generated.\n",
      "     |  \n",
      "     |  with_config(self, config: 'Optional[RunnableConfig]' = None, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      "     |      Bind config to a Runnable, returning a new Runnable.\n",
      "     |  \n",
      "     |  with_fallbacks(self, fallbacks: 'Sequence[Runnable[Input, Output]]', *, exceptions_to_handle: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,)) -> 'RunnableWithFallbacks[Input, Output]'\n",
      "     |  \n",
      "     |  with_retry(self, *, retry_if_exception_type: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,), wait_exponential_jitter: 'bool' = True, stop_after_attempt: 'int' = 3) -> 'Runnable[Input, Output]'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __class_getitem__(params) from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  __init_subclass__(*args, **kwargs) from pydantic.main.ModelMetaclass\n",
      "     |      This method is called when a class is subclassed.\n",
      "     |      \n",
      "     |      The default implementation does nothing. It may be\n",
      "     |      overridden to extend subclasses.\n",
      "    \n",
      "    class AmazonAPIGateway(langchain.llms.base.LLM)\n",
      "     |  AmazonAPIGateway(*, cache: Optional[bool] = None, verbose: bool = None, callbacks: Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager, NoneType] = None, callback_manager: Optional[langchain.callbacks.base.BaseCallbackManager] = None, tags: Optional[List[str]] = None, metadata: Optional[Dict[str, Any]] = None, api_url: str, headers: Optional[Dict] = None, model_kwargs: Optional[Dict] = None, content_handler: langchain.llms.amazon_api_gateway.ContentHandlerAmazonAPIGateway = <langchain.llms.amazon_api_gateway.ContentHandlerAmazonAPIGateway object at 0x00000227AEA943D0>) -> None\n",
      "     |  \n",
      "     |  Amazon API Gateway to access LLM models hosted on AWS.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      AmazonAPIGateway\n",
      "     |      langchain.llms.base.LLM\n",
      "     |      langchain.llms.base.BaseLLM\n",
      "     |      langchain.schema.language_model.BaseLanguageModel\n",
      "     |      langchain.load.serializable.Serializable\n",
      "     |      pydantic.main.BaseModel\n",
      "     |      pydantic.utils.Representation\n",
      "     |      langchain.schema.runnable.base.Runnable\n",
      "     |      typing.Generic\n",
      "     |      abc.ABC\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __json_encoder__ = pydantic_encoder(obj: Any) -> Any\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  Config = <class 'langchain.llms.amazon_api_gateway.AmazonAPIGateway.Co...\n",
      "     |      Configuration for this pydantic object.\n",
      "     |  \n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {'api_url': <class 'str'>, 'content_handler': <class...\n",
      "     |  \n",
      "     |  __class_vars__ = set()\n",
      "     |  \n",
      "     |  __config__ = <class 'langchain.llms.amazon_api_gateway.Config'>\n",
      "     |  \n",
      "     |  __custom_root_type__ = False\n",
      "     |  \n",
      "     |  __exclude_fields__ = {'callback_manager': True, 'callbacks': True, 'me...\n",
      "     |  \n",
      "     |  __fields__ = {'api_url': ModelField(name='api_url', type=str, required...\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "     |  \n",
      "     |  __include_fields__ = None\n",
      "     |  \n",
      "     |  __parameters__ = ()\n",
      "     |  \n",
      "     |  __post_root_validators__ = [(False, <function BaseLLM.raise_deprecatio...\n",
      "     |  \n",
      "     |  __pre_root_validators__ = []\n",
      "     |  \n",
      "     |  __private_attributes__ = {'_lc_kwargs': ModelPrivateAttr(default=Pydan...\n",
      "     |  \n",
      "     |  __schema_cache__ = {}\n",
      "     |  \n",
      "     |  __signature__ = <Signature (*, cache: Optional[bool] = None, ver...PIG...\n",
      "     |  \n",
      "     |  __validators__ = {'verbose': [<pydantic.class_validators.Validator obj...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  __call__(self, prompt: 'str', stop: 'Optional[List[str]]' = None, callbacks: 'Callbacks' = None, *, tags: 'Optional[List[str]]' = None, metadata: 'Optional[Dict[str, Any]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Check Cache and run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  __str__(self) -> 'str'\n",
      "     |      Get a string representation of the object for printing.\n",
      "     |  \n",
      "     |  async abatch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      "     |      Default implementation of abatch, which calls ainvoke N times.\n",
      "     |      Subclasses should override this method if they can batch more efficiently.\n",
      "     |  \n",
      "     |  async agenerate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  async agenerate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Asynchronously pass a sequence of prompts and return model generations.\n",
      "     |      \n",
      "     |      This method should make use of batched calls for models that expose a batched\n",
      "     |      API.\n",
      "     |      \n",
      "     |      Use this method when you want to:\n",
      "     |          1. take advantage of batched calls,\n",
      "     |          2. need more output from the model than just the top generated value,\n",
      "     |          3. are building chains that are agnostic to the underlying language model\n",
      "     |              type (e.g., pure text completion models vs chat models).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      "     |              converted to match the format of any language model (string for pure\n",
      "     |              text generation models and BaseMessages for chat models).\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          callbacks: Callbacks to pass through. Used for executing additional\n",
      "     |              functionality, such as logging or streaming, throughout generation.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          An LLMResult, which contains a list of candidate Generations for each input\n",
      "     |              prompt and additional model provider-specific output.\n",
      "     |  \n",
      "     |  async ainvoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Default implementation of ainvoke, which calls invoke in a thread pool.\n",
      "     |      Subclasses should override this method if they can run asynchronously.\n",
      "     |  \n",
      "     |  async apredict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Asynchronously pass a string to the model and return a string prediction.\n",
      "     |      \n",
      "     |      Use this method when calling pure text generation models and only the top\n",
      "     |          candidate generation is needed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: String input to pass to the model.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a string.\n",
      "     |  \n",
      "     |  async apredict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      "     |      Asynchronously pass messages to the model and return a message prediction.\n",
      "     |      \n",
      "     |      Use this method when calling chat models and only the top\n",
      "     |          candidate generation is needed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: A sequence of chat messages corresponding to a single model input.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a message.\n",
      "     |  \n",
      "     |  async astream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'AsyncIterator[str]'\n",
      "     |      Default implementation of astream, which calls ainvoke.\n",
      "     |      Subclasses should override this method if they support streaming output.\n",
      "     |  \n",
      "     |  batch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      "     |      Default implementation of batch, which calls invoke N times.\n",
      "     |      Subclasses should override this method if they can batch more efficiently.\n",
      "     |  \n",
      "     |  dict(self, **kwargs: 'Any') -> 'Dict'\n",
      "     |      Return a dictionary of the LLM.\n",
      "     |  \n",
      "     |  generate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  generate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Pass a sequence of prompts to the model and return model generations.\n",
      "     |      \n",
      "     |      This method should make use of batched calls for models that expose a batched\n",
      "     |      API.\n",
      "     |      \n",
      "     |      Use this method when you want to:\n",
      "     |          1. take advantage of batched calls,\n",
      "     |          2. need more output from the model than just the top generated value,\n",
      "     |          3. are building chains that are agnostic to the underlying language model\n",
      "     |              type (e.g., pure text completion models vs chat models).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      "     |              converted to match the format of any language model (string for pure\n",
      "     |              text generation models and BaseMessages for chat models).\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          callbacks: Callbacks to pass through. Used for executing additional\n",
      "     |              functionality, such as logging or streaming, throughout generation.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          An LLMResult, which contains a list of candidate Generations for each input\n",
      "     |              prompt and additional model provider-specific output.\n",
      "     |  \n",
      "     |  invoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |  \n",
      "     |  predict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Pass a single string input to the model and return a string prediction.\n",
      "     |      \n",
      "     |       Use this method when passing in raw text. If you want to pass in specific\n",
      "     |          types of chat messages, use predict_messages.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: String input to pass to the model.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a string.\n",
      "     |  \n",
      "     |  predict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      "     |      Pass a message sequence to the model and return a message prediction.\n",
      "     |      \n",
      "     |      Use this method when passing in chat messages. If you want to pass in raw text,\n",
      "     |          use predict.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: A sequence of chat messages corresponding to a single model input.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a message.\n",
      "     |  \n",
      "     |  save(self, file_path: 'Union[Path, str]') -> 'None'\n",
      "     |      Save the LLM.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          file_path: Path to file to save the LLM to.\n",
      "     |      \n",
      "     |      Example:\n",
      "     |      .. code-block:: python\n",
      "     |      \n",
      "     |          llm.save(file_path=\"path/llm.yaml\")\n",
      "     |  \n",
      "     |  stream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'Iterator[str]'\n",
      "     |      Default implementation of stream, which calls invoke.\n",
      "     |      Subclasses should override this method if they support streaming output.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  raise_deprecation(values: 'Dict') -> 'Dict' from pydantic.main.ModelMetaclass\n",
      "     |      Raise deprecation warning if callback_manager is used.\n",
      "     |  \n",
      "     |  set_verbose(verbose: 'Optional[bool]') -> 'bool' from pydantic.main.ModelMetaclass\n",
      "     |      If verbose is None, set it.\n",
      "     |      \n",
      "     |      This allows users to pass in None as verbose to access the global setting.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  __orig_bases__ = (langchain.schema.language_model.BaseLanguageModel[st...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.schema.language_model.BaseLanguageModel:\n",
      "     |  \n",
      "     |  get_num_tokens(self, text: 'str') -> 'int'\n",
      "     |      Get the number of tokens present in the text.\n",
      "     |      \n",
      "     |      Useful for checking if an input will fit in a model's context window.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: The string input to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The integer number of tokens in the text.\n",
      "     |  \n",
      "     |  get_num_tokens_from_messages(self, messages: 'List[BaseMessage]') -> 'int'\n",
      "     |      Get the number of tokens in the messages.\n",
      "     |      \n",
      "     |      Useful for checking if an input will fit in a model's context window.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: The message inputs to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The sum of the number of tokens across the messages.\n",
      "     |  \n",
      "     |  get_token_ids(self, text: 'str') -> 'List[int]'\n",
      "     |      Return the ordered ids of the tokens in a text.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: The string input to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          A list of ids corresponding to the tokens in the text, in order they occur\n",
      "     |              in the text.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from langchain.schema.language_model.BaseLanguageModel:\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.load.serializable.Serializable:\n",
      "     |  \n",
      "     |  __init__(self, **kwargs: Any) -> None\n",
      "     |      Create a new model by parsing and validating input data from keyword arguments.\n",
      "     |      \n",
      "     |      Raises ValidationError if the input data cannot be parsed to form a valid model.\n",
      "     |  \n",
      "     |  to_json(self) -> Union[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\n",
      "     |  \n",
      "     |  to_json_not_implemented(self) -> langchain.load.serializable.SerializedNotImplemented\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from langchain.load.serializable.Serializable:\n",
      "     |  \n",
      "     |  lc_attributes\n",
      "     |      Return a list of attribute names that should be included in the\n",
      "     |      serialized kwargs. These attributes must be accepted by the\n",
      "     |      constructor.\n",
      "     |  \n",
      "     |  lc_namespace\n",
      "     |      Return the namespace of the langchain object.\n",
      "     |      eg. [\"langchain\", \"llms\", \"openai\"]\n",
      "     |  \n",
      "     |  lc_secrets\n",
      "     |      Return a map of constructor argument names to secret ids.\n",
      "     |      eg. {\"openai_api_key\": \"OPENAI_API_KEY\"}\n",
      "     |  \n",
      "     |  lc_serializable\n",
      "     |      Return whether or not the class is serializable.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __eq__(self, other: Any) -> bool\n",
      "     |  \n",
      "     |  __getstate__(self) -> 'DictAny'\n",
      "     |  \n",
      "     |  __iter__(self) -> 'TupleGenerator'\n",
      "     |      so `dict(model)` works\n",
      "     |  \n",
      "     |  __repr_args__(self) -> 'ReprArgs'\n",
      "     |  \n",
      "     |  __setattr__(self, name, value)\n",
      "     |  \n",
      "     |  __setstate__(self, state: 'DictAny') -> None\n",
      "     |  \n",
      "     |  copy(self: 'Model', *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, update: Optional[ForwardRef('DictStrAny')] = None, deep: bool = False) -> 'Model'\n",
      "     |      Duplicate a model, optionally choose which fields to include, exclude and change.\n",
      "     |      \n",
      "     |      :param include: fields to include in new model\n",
      "     |      :param exclude: fields to exclude from new model, as with values this takes precedence over include\n",
      "     |      :param update: values to change/add in the new model. Note: the data is not validated before creating\n",
      "     |          the new model: you should trust this data\n",
      "     |      :param deep: set to `True` to make a deep copy of the model\n",
      "     |      :return: new model instance\n",
      "     |  \n",
      "     |  json(self, *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) -> 'unicode'\n",
      "     |      Generate a JSON representation of the model, `include` and `exclude` arguments as per `dict()`.\n",
      "     |      \n",
      "     |      `encoder` is an optional function to supply as `default` to json.dumps(), other arguments as per `json.dumps()`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __get_validators__() -> 'CallableGenerator' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  __try_update_forward_refs__(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      "     |      Same as update_forward_refs but will not raise exception\n",
      "     |      when forward references are not defined.\n",
      "     |  \n",
      "     |  construct(_fields_set: Optional[ForwardRef('SetStr')] = None, **values: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |      Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data.\n",
      "     |      Default values are respected, but no other validation is performed.\n",
      "     |      Behaves as if `Config.extra = 'allow'` was set since it adds all passed values\n",
      "     |  \n",
      "     |  from_orm(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_file(path: Union[str, pathlib.Path], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_obj(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_raw(b: Union[str, bytes], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema(by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}') -> 'DictStrAny' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema_json(*, by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}', **dumps_kwargs: Any) -> 'unicode' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  update_forward_refs(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      "     |      Try to update ForwardRefs on fields based on this Model, globalns and localns.\n",
      "     |  \n",
      "     |  validate(value: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __fields_set__\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.utils.Representation:\n",
      "     |  \n",
      "     |  __pretty__(self, fmt: Callable[[Any], Any], **kwargs: Any) -> Generator[Any, NoneType, NoneType]\n",
      "     |      Used by devtools (https://python-devtools.helpmanual.io/) to provide a human readable representations of objects\n",
      "     |  \n",
      "     |  __repr__(self) -> 'unicode'\n",
      "     |  \n",
      "     |  __repr_name__(self) -> 'unicode'\n",
      "     |      Name of the instance's class, used in __repr__.\n",
      "     |  \n",
      "     |  __repr_str__(self, join_str: 'unicode') -> 'unicode'\n",
      "     |  \n",
      "     |  __rich_repr__(self) -> 'RichReprResult'\n",
      "     |      Get fields for Rich library\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.schema.runnable.base.Runnable:\n",
      "     |  \n",
      "     |  __or__(self, other: 'Union[Runnable[Any, Other], Callable[[Any], Other], Mapping[str, Union[Runnable[Any, Other], Callable[[Any], Other], Any]]]') -> 'RunnableSequence[Input, Other]'\n",
      "     |      Return self|value.\n",
      "     |  \n",
      "     |  __ror__(self, other: 'Union[Runnable[Other, Any], Callable[[Any], Other], Mapping[str, Union[Runnable[Other, Any], Callable[[Other], Any], Any]]]') -> 'RunnableSequence[Other, Output]'\n",
      "     |      Return value|self.\n",
      "     |  \n",
      "     |  async atransform(self, input: 'AsyncIterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'AsyncIterator[Output]'\n",
      "     |      Default implementation of atransform, which buffers input and calls astream.\n",
      "     |      Subclasses should override this method if they can start producing output while\n",
      "     |      input is still being generated.\n",
      "     |  \n",
      "     |  bind(self, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      "     |      Bind arguments to a Runnable, returning a new Runnable.\n",
      "     |  \n",
      "     |  map(self) -> 'Runnable[List[Input], List[Output]]'\n",
      "     |      Return a new Runnable that maps a list of inputs to a list of outputs,\n",
      "     |      by calling invoke() with each input.\n",
      "     |  \n",
      "     |  transform(self, input: 'Iterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'Iterator[Output]'\n",
      "     |      Default implementation of transform, which buffers input and then calls stream.\n",
      "     |      Subclasses should override this method if they can start producing output while\n",
      "     |      input is still being generated.\n",
      "     |  \n",
      "     |  with_config(self, config: 'Optional[RunnableConfig]' = None, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      "     |      Bind config to a Runnable, returning a new Runnable.\n",
      "     |  \n",
      "     |  with_fallbacks(self, fallbacks: 'Sequence[Runnable[Input, Output]]', *, exceptions_to_handle: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,)) -> 'RunnableWithFallbacks[Input, Output]'\n",
      "     |  \n",
      "     |  with_retry(self, *, retry_if_exception_type: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,), wait_exponential_jitter: 'bool' = True, stop_after_attempt: 'int' = 3) -> 'Runnable[Input, Output]'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __class_getitem__(params) from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  __init_subclass__(*args, **kwargs) from pydantic.main.ModelMetaclass\n",
      "     |      This method is called when a class is subclassed.\n",
      "     |      \n",
      "     |      The default implementation does nothing. It may be\n",
      "     |      overridden to extend subclasses.\n",
      "    \n",
      "    class Anthropic(langchain.llms.base.LLM, _AnthropicCommon)\n",
      "     |  Anthropic(*, client: Any = None, async_client: Any = None, model_name: str = 'claude-2', max_tokens: int = 256, temperature: Optional[float] = None, top_k: Optional[int] = None, top_p: Optional[float] = None, streaming: bool = False, default_request_timeout: Optional[float] = None, anthropic_api_url: Optional[str] = None, anthropic_api_key: Optional[str] = None, HUMAN_PROMPT: Optional[str] = None, AI_PROMPT: Optional[str] = None, count_tokens: Optional[Callable[[str], int]] = None, model_kwargs: Dict[str, Any] = None, cache: Optional[bool] = None, verbose: bool = None, callbacks: Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager, NoneType] = None, callback_manager: Optional[langchain.callbacks.base.BaseCallbackManager] = None, tags: Optional[List[str]] = None, metadata: Optional[Dict[str, Any]] = None) -> None\n",
      "     |  \n",
      "     |  Anthropic large language models.\n",
      "     |  \n",
      "     |  To use, you should have the ``anthropic`` python package installed, and the\n",
      "     |  environment variable ``ANTHROPIC_API_KEY`` set with your API key, or pass\n",
      "     |  it as a named parameter to the constructor.\n",
      "     |  \n",
      "     |  Example:\n",
      "     |      .. code-block:: python\n",
      "     |  \n",
      "     |          import anthropic\n",
      "     |          from langchain.llms import Anthropic\n",
      "     |  \n",
      "     |          model = Anthropic(model=\"<model_name>\", anthropic_api_key=\"my-api-key\")\n",
      "     |  \n",
      "     |          # Simplest invocation, automatically wrapped with HUMAN_PROMPT\n",
      "     |          # and AI_PROMPT.\n",
      "     |          response = model(\"What are the biggest risks facing humanity?\")\n",
      "     |  \n",
      "     |          # Or if you want to use the chat mode, build a few-shot-prompt, or\n",
      "     |          # put words in the Assistant's mouth, use HUMAN_PROMPT and AI_PROMPT:\n",
      "     |          raw_prompt = \"What are the biggest risks facing humanity?\"\n",
      "     |          prompt = f\"{anthropic.HUMAN_PROMPT} {prompt}{anthropic.AI_PROMPT}\"\n",
      "     |          response = model(prompt)\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Anthropic\n",
      "     |      langchain.llms.base.LLM\n",
      "     |      langchain.llms.base.BaseLLM\n",
      "     |      _AnthropicCommon\n",
      "     |      langchain.schema.language_model.BaseLanguageModel\n",
      "     |      langchain.load.serializable.Serializable\n",
      "     |      pydantic.main.BaseModel\n",
      "     |      pydantic.utils.Representation\n",
      "     |      langchain.schema.runnable.base.Runnable\n",
      "     |      typing.Generic\n",
      "     |      abc.ABC\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  get_num_tokens(self, text: str) -> int\n",
      "     |      Calculate number of tokens.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  raise_warning(values: Dict) -> Dict from pydantic.main.ModelMetaclass\n",
      "     |      Raise warning that this class is deprecated.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __json_encoder__ = pydantic_encoder(obj: Any) -> Any\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  Config = <class 'langchain.llms.anthropic.Anthropic.Config'>\n",
      "     |      Configuration for this pydantic object.\n",
      "     |  \n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {}\n",
      "     |  \n",
      "     |  __class_vars__ = set()\n",
      "     |  \n",
      "     |  __config__ = <class 'langchain.llms.anthropic.Config'>\n",
      "     |  \n",
      "     |  __custom_root_type__ = False\n",
      "     |  \n",
      "     |  __exclude_fields__ = {'callback_manager': True, 'callbacks': True, 'me...\n",
      "     |  \n",
      "     |  __fields__ = {'AI_PROMPT': ModelField(name='AI_PROMPT', type=Optional[...\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "     |  \n",
      "     |  __include_fields__ = None\n",
      "     |  \n",
      "     |  __parameters__ = ()\n",
      "     |  \n",
      "     |  __post_root_validators__ = [(False, <function _AnthropicCommon.validat...\n",
      "     |  \n",
      "     |  __pre_root_validators__ = [<function _AnthropicCommon.build_extra>]\n",
      "     |  \n",
      "     |  __private_attributes__ = {'_lc_kwargs': ModelPrivateAttr(default=Pydan...\n",
      "     |  \n",
      "     |  __schema_cache__ = {}\n",
      "     |  \n",
      "     |  __signature__ = <Signature (*, client: Any = None, async_client:...tad...\n",
      "     |  \n",
      "     |  __validators__ = {'verbose': [<pydantic.class_validators.Validator obj...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  __call__(self, prompt: 'str', stop: 'Optional[List[str]]' = None, callbacks: 'Callbacks' = None, *, tags: 'Optional[List[str]]' = None, metadata: 'Optional[Dict[str, Any]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Check Cache and run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  __str__(self) -> 'str'\n",
      "     |      Get a string representation of the object for printing.\n",
      "     |  \n",
      "     |  async abatch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      "     |      Default implementation of abatch, which calls ainvoke N times.\n",
      "     |      Subclasses should override this method if they can batch more efficiently.\n",
      "     |  \n",
      "     |  async agenerate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  async agenerate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Asynchronously pass a sequence of prompts and return model generations.\n",
      "     |      \n",
      "     |      This method should make use of batched calls for models that expose a batched\n",
      "     |      API.\n",
      "     |      \n",
      "     |      Use this method when you want to:\n",
      "     |          1. take advantage of batched calls,\n",
      "     |          2. need more output from the model than just the top generated value,\n",
      "     |          3. are building chains that are agnostic to the underlying language model\n",
      "     |              type (e.g., pure text completion models vs chat models).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      "     |              converted to match the format of any language model (string for pure\n",
      "     |              text generation models and BaseMessages for chat models).\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          callbacks: Callbacks to pass through. Used for executing additional\n",
      "     |              functionality, such as logging or streaming, throughout generation.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          An LLMResult, which contains a list of candidate Generations for each input\n",
      "     |              prompt and additional model provider-specific output.\n",
      "     |  \n",
      "     |  async ainvoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Default implementation of ainvoke, which calls invoke in a thread pool.\n",
      "     |      Subclasses should override this method if they can run asynchronously.\n",
      "     |  \n",
      "     |  async apredict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Asynchronously pass a string to the model and return a string prediction.\n",
      "     |      \n",
      "     |      Use this method when calling pure text generation models and only the top\n",
      "     |          candidate generation is needed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: String input to pass to the model.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a string.\n",
      "     |  \n",
      "     |  async apredict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      "     |      Asynchronously pass messages to the model and return a message prediction.\n",
      "     |      \n",
      "     |      Use this method when calling chat models and only the top\n",
      "     |          candidate generation is needed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: A sequence of chat messages corresponding to a single model input.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a message.\n",
      "     |  \n",
      "     |  async astream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'AsyncIterator[str]'\n",
      "     |      Default implementation of astream, which calls ainvoke.\n",
      "     |      Subclasses should override this method if they support streaming output.\n",
      "     |  \n",
      "     |  batch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      "     |      Default implementation of batch, which calls invoke N times.\n",
      "     |      Subclasses should override this method if they can batch more efficiently.\n",
      "     |  \n",
      "     |  dict(self, **kwargs: 'Any') -> 'Dict'\n",
      "     |      Return a dictionary of the LLM.\n",
      "     |  \n",
      "     |  generate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  generate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Pass a sequence of prompts to the model and return model generations.\n",
      "     |      \n",
      "     |      This method should make use of batched calls for models that expose a batched\n",
      "     |      API.\n",
      "     |      \n",
      "     |      Use this method when you want to:\n",
      "     |          1. take advantage of batched calls,\n",
      "     |          2. need more output from the model than just the top generated value,\n",
      "     |          3. are building chains that are agnostic to the underlying language model\n",
      "     |              type (e.g., pure text completion models vs chat models).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      "     |              converted to match the format of any language model (string for pure\n",
      "     |              text generation models and BaseMessages for chat models).\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          callbacks: Callbacks to pass through. Used for executing additional\n",
      "     |              functionality, such as logging or streaming, throughout generation.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          An LLMResult, which contains a list of candidate Generations for each input\n",
      "     |              prompt and additional model provider-specific output.\n",
      "     |  \n",
      "     |  invoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |  \n",
      "     |  predict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Pass a single string input to the model and return a string prediction.\n",
      "     |      \n",
      "     |       Use this method when passing in raw text. If you want to pass in specific\n",
      "     |          types of chat messages, use predict_messages.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: String input to pass to the model.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a string.\n",
      "     |  \n",
      "     |  predict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      "     |      Pass a message sequence to the model and return a message prediction.\n",
      "     |      \n",
      "     |      Use this method when passing in chat messages. If you want to pass in raw text,\n",
      "     |          use predict.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: A sequence of chat messages corresponding to a single model input.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a message.\n",
      "     |  \n",
      "     |  save(self, file_path: 'Union[Path, str]') -> 'None'\n",
      "     |      Save the LLM.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          file_path: Path to file to save the LLM to.\n",
      "     |      \n",
      "     |      Example:\n",
      "     |      .. code-block:: python\n",
      "     |      \n",
      "     |          llm.save(file_path=\"path/llm.yaml\")\n",
      "     |  \n",
      "     |  stream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'Iterator[str]'\n",
      "     |      Default implementation of stream, which calls invoke.\n",
      "     |      Subclasses should override this method if they support streaming output.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  raise_deprecation(values: 'Dict') -> 'Dict' from pydantic.main.ModelMetaclass\n",
      "     |      Raise deprecation warning if callback_manager is used.\n",
      "     |  \n",
      "     |  set_verbose(verbose: 'Optional[bool]') -> 'bool' from pydantic.main.ModelMetaclass\n",
      "     |      If verbose is None, set it.\n",
      "     |      \n",
      "     |      This allows users to pass in None as verbose to access the global setting.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  __orig_bases__ = (langchain.schema.language_model.BaseLanguageModel[st...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from _AnthropicCommon:\n",
      "     |  \n",
      "     |  build_extra(values: Dict) -> Dict from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  validate_environment(values: Dict) -> Dict from pydantic.main.ModelMetaclass\n",
      "     |      Validate that api key and python package exists in environment.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.schema.language_model.BaseLanguageModel:\n",
      "     |  \n",
      "     |  get_num_tokens_from_messages(self, messages: 'List[BaseMessage]') -> 'int'\n",
      "     |      Get the number of tokens in the messages.\n",
      "     |      \n",
      "     |      Useful for checking if an input will fit in a model's context window.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: The message inputs to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The sum of the number of tokens across the messages.\n",
      "     |  \n",
      "     |  get_token_ids(self, text: 'str') -> 'List[int]'\n",
      "     |      Return the ordered ids of the tokens in a text.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: The string input to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          A list of ids corresponding to the tokens in the text, in order they occur\n",
      "     |              in the text.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from langchain.schema.language_model.BaseLanguageModel:\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.load.serializable.Serializable:\n",
      "     |  \n",
      "     |  __init__(self, **kwargs: Any) -> None\n",
      "     |      Create a new model by parsing and validating input data from keyword arguments.\n",
      "     |      \n",
      "     |      Raises ValidationError if the input data cannot be parsed to form a valid model.\n",
      "     |  \n",
      "     |  to_json(self) -> Union[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\n",
      "     |  \n",
      "     |  to_json_not_implemented(self) -> langchain.load.serializable.SerializedNotImplemented\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from langchain.load.serializable.Serializable:\n",
      "     |  \n",
      "     |  lc_attributes\n",
      "     |      Return a list of attribute names that should be included in the\n",
      "     |      serialized kwargs. These attributes must be accepted by the\n",
      "     |      constructor.\n",
      "     |  \n",
      "     |  lc_namespace\n",
      "     |      Return the namespace of the langchain object.\n",
      "     |      eg. [\"langchain\", \"llms\", \"openai\"]\n",
      "     |  \n",
      "     |  lc_secrets\n",
      "     |      Return a map of constructor argument names to secret ids.\n",
      "     |      eg. {\"openai_api_key\": \"OPENAI_API_KEY\"}\n",
      "     |  \n",
      "     |  lc_serializable\n",
      "     |      Return whether or not the class is serializable.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __eq__(self, other: Any) -> bool\n",
      "     |  \n",
      "     |  __getstate__(self) -> 'DictAny'\n",
      "     |  \n",
      "     |  __iter__(self) -> 'TupleGenerator'\n",
      "     |      so `dict(model)` works\n",
      "     |  \n",
      "     |  __repr_args__(self) -> 'ReprArgs'\n",
      "     |  \n",
      "     |  __setattr__(self, name, value)\n",
      "     |  \n",
      "     |  __setstate__(self, state: 'DictAny') -> None\n",
      "     |  \n",
      "     |  copy(self: 'Model', *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, update: Optional[ForwardRef('DictStrAny')] = None, deep: bool = False) -> 'Model'\n",
      "     |      Duplicate a model, optionally choose which fields to include, exclude and change.\n",
      "     |      \n",
      "     |      :param include: fields to include in new model\n",
      "     |      :param exclude: fields to exclude from new model, as with values this takes precedence over include\n",
      "     |      :param update: values to change/add in the new model. Note: the data is not validated before creating\n",
      "     |          the new model: you should trust this data\n",
      "     |      :param deep: set to `True` to make a deep copy of the model\n",
      "     |      :return: new model instance\n",
      "     |  \n",
      "     |  json(self, *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) -> 'unicode'\n",
      "     |      Generate a JSON representation of the model, `include` and `exclude` arguments as per `dict()`.\n",
      "     |      \n",
      "     |      `encoder` is an optional function to supply as `default` to json.dumps(), other arguments as per `json.dumps()`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __get_validators__() -> 'CallableGenerator' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  __try_update_forward_refs__(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      "     |      Same as update_forward_refs but will not raise exception\n",
      "     |      when forward references are not defined.\n",
      "     |  \n",
      "     |  construct(_fields_set: Optional[ForwardRef('SetStr')] = None, **values: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |      Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data.\n",
      "     |      Default values are respected, but no other validation is performed.\n",
      "     |      Behaves as if `Config.extra = 'allow'` was set since it adds all passed values\n",
      "     |  \n",
      "     |  from_orm(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_file(path: Union[str, pathlib.Path], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_obj(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_raw(b: Union[str, bytes], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema(by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}') -> 'DictStrAny' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema_json(*, by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}', **dumps_kwargs: Any) -> 'unicode' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  update_forward_refs(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      "     |      Try to update ForwardRefs on fields based on this Model, globalns and localns.\n",
      "     |  \n",
      "     |  validate(value: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __fields_set__\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.utils.Representation:\n",
      "     |  \n",
      "     |  __pretty__(self, fmt: Callable[[Any], Any], **kwargs: Any) -> Generator[Any, NoneType, NoneType]\n",
      "     |      Used by devtools (https://python-devtools.helpmanual.io/) to provide a human readable representations of objects\n",
      "     |  \n",
      "     |  __repr__(self) -> 'unicode'\n",
      "     |  \n",
      "     |  __repr_name__(self) -> 'unicode'\n",
      "     |      Name of the instance's class, used in __repr__.\n",
      "     |  \n",
      "     |  __repr_str__(self, join_str: 'unicode') -> 'unicode'\n",
      "     |  \n",
      "     |  __rich_repr__(self) -> 'RichReprResult'\n",
      "     |      Get fields for Rich library\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.schema.runnable.base.Runnable:\n",
      "     |  \n",
      "     |  __or__(self, other: 'Union[Runnable[Any, Other], Callable[[Any], Other], Mapping[str, Union[Runnable[Any, Other], Callable[[Any], Other], Any]]]') -> 'RunnableSequence[Input, Other]'\n",
      "     |      Return self|value.\n",
      "     |  \n",
      "     |  __ror__(self, other: 'Union[Runnable[Other, Any], Callable[[Any], Other], Mapping[str, Union[Runnable[Other, Any], Callable[[Other], Any], Any]]]') -> 'RunnableSequence[Other, Output]'\n",
      "     |      Return value|self.\n",
      "     |  \n",
      "     |  async atransform(self, input: 'AsyncIterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'AsyncIterator[Output]'\n",
      "     |      Default implementation of atransform, which buffers input and calls astream.\n",
      "     |      Subclasses should override this method if they can start producing output while\n",
      "     |      input is still being generated.\n",
      "     |  \n",
      "     |  bind(self, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      "     |      Bind arguments to a Runnable, returning a new Runnable.\n",
      "     |  \n",
      "     |  map(self) -> 'Runnable[List[Input], List[Output]]'\n",
      "     |      Return a new Runnable that maps a list of inputs to a list of outputs,\n",
      "     |      by calling invoke() with each input.\n",
      "     |  \n",
      "     |  transform(self, input: 'Iterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'Iterator[Output]'\n",
      "     |      Default implementation of transform, which buffers input and then calls stream.\n",
      "     |      Subclasses should override this method if they can start producing output while\n",
      "     |      input is still being generated.\n",
      "     |  \n",
      "     |  with_config(self, config: 'Optional[RunnableConfig]' = None, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      "     |      Bind config to a Runnable, returning a new Runnable.\n",
      "     |  \n",
      "     |  with_fallbacks(self, fallbacks: 'Sequence[Runnable[Input, Output]]', *, exceptions_to_handle: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,)) -> 'RunnableWithFallbacks[Input, Output]'\n",
      "     |  \n",
      "     |  with_retry(self, *, retry_if_exception_type: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,), wait_exponential_jitter: 'bool' = True, stop_after_attempt: 'int' = 3) -> 'Runnable[Input, Output]'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __class_getitem__(params) from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  __init_subclass__(*args, **kwargs) from pydantic.main.ModelMetaclass\n",
      "     |      This method is called when a class is subclassed.\n",
      "     |      \n",
      "     |      The default implementation does nothing. It may be\n",
      "     |      overridden to extend subclasses.\n",
      "    \n",
      "    class Anyscale(langchain.llms.base.LLM)\n",
      "     |  Anyscale(*, cache: Optional[bool] = None, verbose: bool = None, callbacks: Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager, NoneType] = None, callback_manager: Optional[langchain.callbacks.base.BaseCallbackManager] = None, tags: Optional[List[str]] = None, metadata: Optional[Dict[str, Any]] = None, model_kwargs: Optional[dict] = None, anyscale_service_url: Optional[str] = None, anyscale_service_route: Optional[str] = None, anyscale_service_token: Optional[str] = None) -> None\n",
      "     |  \n",
      "     |  Anyscale Service models.\n",
      "     |  \n",
      "     |  To use, you should have the environment variable ``ANYSCALE_SERVICE_URL``,\n",
      "     |  ``ANYSCALE_SERVICE_ROUTE`` and ``ANYSCALE_SERVICE_TOKEN`` set with your Anyscale\n",
      "     |  Service, or pass it as a named parameter to the constructor.\n",
      "     |  \n",
      "     |  Example:\n",
      "     |      .. code-block:: python\n",
      "     |  \n",
      "     |          from langchain.llms import Anyscale\n",
      "     |          anyscale = Anyscale(anyscale_service_url=\"SERVICE_URL\",\n",
      "     |                              anyscale_service_route=\"SERVICE_ROUTE\",\n",
      "     |                              anyscale_service_token=\"SERVICE_TOKEN\")\n",
      "     |  \n",
      "     |          # Use Ray for distributed processing\n",
      "     |          import ray\n",
      "     |          prompt_list=[]\n",
      "     |          @ray.remote\n",
      "     |          def send_query(llm, prompt):\n",
      "     |              resp = llm(prompt)\n",
      "     |              return resp\n",
      "     |          futures = [send_query.remote(anyscale, prompt) for prompt in prompt_list]\n",
      "     |          results = ray.get(futures)\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Anyscale\n",
      "     |      langchain.llms.base.LLM\n",
      "     |      langchain.llms.base.BaseLLM\n",
      "     |      langchain.schema.language_model.BaseLanguageModel\n",
      "     |      langchain.load.serializable.Serializable\n",
      "     |      pydantic.main.BaseModel\n",
      "     |      pydantic.utils.Representation\n",
      "     |      langchain.schema.runnable.base.Runnable\n",
      "     |      typing.Generic\n",
      "     |      abc.ABC\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  validate_environment(values: Dict) -> Dict from pydantic.main.ModelMetaclass\n",
      "     |      Validate that api key and python package exists in environment.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __json_encoder__ = pydantic_encoder(obj: Any) -> Any\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  Config = <class 'langchain.llms.anyscale.Anyscale.Config'>\n",
      "     |      Configuration for this pydantic object.\n",
      "     |  \n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {'anyscale_service_route': typing.Optional[str], 'an...\n",
      "     |  \n",
      "     |  __class_vars__ = set()\n",
      "     |  \n",
      "     |  __config__ = <class 'langchain.llms.anyscale.Config'>\n",
      "     |  \n",
      "     |  __custom_root_type__ = False\n",
      "     |  \n",
      "     |  __exclude_fields__ = {'callback_manager': True, 'callbacks': True, 'me...\n",
      "     |  \n",
      "     |  __fields__ = {'anyscale_service_route': ModelField(name='anyscale_serv...\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "     |  \n",
      "     |  __include_fields__ = None\n",
      "     |  \n",
      "     |  __parameters__ = ()\n",
      "     |  \n",
      "     |  __post_root_validators__ = [(False, <function BaseLLM.raise_deprecatio...\n",
      "     |  \n",
      "     |  __pre_root_validators__ = []\n",
      "     |  \n",
      "     |  __private_attributes__ = {'_lc_kwargs': ModelPrivateAttr(default=Pydan...\n",
      "     |  \n",
      "     |  __schema_cache__ = {}\n",
      "     |  \n",
      "     |  __signature__ = <Signature (*, cache: Optional[bool] = None, ver...ale...\n",
      "     |  \n",
      "     |  __validators__ = {'verbose': [<pydantic.class_validators.Validator obj...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  __call__(self, prompt: 'str', stop: 'Optional[List[str]]' = None, callbacks: 'Callbacks' = None, *, tags: 'Optional[List[str]]' = None, metadata: 'Optional[Dict[str, Any]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Check Cache and run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  __str__(self) -> 'str'\n",
      "     |      Get a string representation of the object for printing.\n",
      "     |  \n",
      "     |  async abatch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      "     |      Default implementation of abatch, which calls ainvoke N times.\n",
      "     |      Subclasses should override this method if they can batch more efficiently.\n",
      "     |  \n",
      "     |  async agenerate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  async agenerate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Asynchronously pass a sequence of prompts and return model generations.\n",
      "     |      \n",
      "     |      This method should make use of batched calls for models that expose a batched\n",
      "     |      API.\n",
      "     |      \n",
      "     |      Use this method when you want to:\n",
      "     |          1. take advantage of batched calls,\n",
      "     |          2. need more output from the model than just the top generated value,\n",
      "     |          3. are building chains that are agnostic to the underlying language model\n",
      "     |              type (e.g., pure text completion models vs chat models).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      "     |              converted to match the format of any language model (string for pure\n",
      "     |              text generation models and BaseMessages for chat models).\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          callbacks: Callbacks to pass through. Used for executing additional\n",
      "     |              functionality, such as logging or streaming, throughout generation.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          An LLMResult, which contains a list of candidate Generations for each input\n",
      "     |              prompt and additional model provider-specific output.\n",
      "     |  \n",
      "     |  async ainvoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Default implementation of ainvoke, which calls invoke in a thread pool.\n",
      "     |      Subclasses should override this method if they can run asynchronously.\n",
      "     |  \n",
      "     |  async apredict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Asynchronously pass a string to the model and return a string prediction.\n",
      "     |      \n",
      "     |      Use this method when calling pure text generation models and only the top\n",
      "     |          candidate generation is needed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: String input to pass to the model.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a string.\n",
      "     |  \n",
      "     |  async apredict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      "     |      Asynchronously pass messages to the model and return a message prediction.\n",
      "     |      \n",
      "     |      Use this method when calling chat models and only the top\n",
      "     |          candidate generation is needed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: A sequence of chat messages corresponding to a single model input.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a message.\n",
      "     |  \n",
      "     |  async astream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'AsyncIterator[str]'\n",
      "     |      Default implementation of astream, which calls ainvoke.\n",
      "     |      Subclasses should override this method if they support streaming output.\n",
      "     |  \n",
      "     |  batch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      "     |      Default implementation of batch, which calls invoke N times.\n",
      "     |      Subclasses should override this method if they can batch more efficiently.\n",
      "     |  \n",
      "     |  dict(self, **kwargs: 'Any') -> 'Dict'\n",
      "     |      Return a dictionary of the LLM.\n",
      "     |  \n",
      "     |  generate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  generate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Pass a sequence of prompts to the model and return model generations.\n",
      "     |      \n",
      "     |      This method should make use of batched calls for models that expose a batched\n",
      "     |      API.\n",
      "     |      \n",
      "     |      Use this method when you want to:\n",
      "     |          1. take advantage of batched calls,\n",
      "     |          2. need more output from the model than just the top generated value,\n",
      "     |          3. are building chains that are agnostic to the underlying language model\n",
      "     |              type (e.g., pure text completion models vs chat models).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      "     |              converted to match the format of any language model (string for pure\n",
      "     |              text generation models and BaseMessages for chat models).\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          callbacks: Callbacks to pass through. Used for executing additional\n",
      "     |              functionality, such as logging or streaming, throughout generation.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          An LLMResult, which contains a list of candidate Generations for each input\n",
      "     |              prompt and additional model provider-specific output.\n",
      "     |  \n",
      "     |  invoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |  \n",
      "     |  predict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Pass a single string input to the model and return a string prediction.\n",
      "     |      \n",
      "     |       Use this method when passing in raw text. If you want to pass in specific\n",
      "     |          types of chat messages, use predict_messages.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: String input to pass to the model.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a string.\n",
      "     |  \n",
      "     |  predict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      "     |      Pass a message sequence to the model and return a message prediction.\n",
      "     |      \n",
      "     |      Use this method when passing in chat messages. If you want to pass in raw text,\n",
      "     |          use predict.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: A sequence of chat messages corresponding to a single model input.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a message.\n",
      "     |  \n",
      "     |  save(self, file_path: 'Union[Path, str]') -> 'None'\n",
      "     |      Save the LLM.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          file_path: Path to file to save the LLM to.\n",
      "     |      \n",
      "     |      Example:\n",
      "     |      .. code-block:: python\n",
      "     |      \n",
      "     |          llm.save(file_path=\"path/llm.yaml\")\n",
      "     |  \n",
      "     |  stream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'Iterator[str]'\n",
      "     |      Default implementation of stream, which calls invoke.\n",
      "     |      Subclasses should override this method if they support streaming output.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  raise_deprecation(values: 'Dict') -> 'Dict' from pydantic.main.ModelMetaclass\n",
      "     |      Raise deprecation warning if callback_manager is used.\n",
      "     |  \n",
      "     |  set_verbose(verbose: 'Optional[bool]') -> 'bool' from pydantic.main.ModelMetaclass\n",
      "     |      If verbose is None, set it.\n",
      "     |      \n",
      "     |      This allows users to pass in None as verbose to access the global setting.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  __orig_bases__ = (langchain.schema.language_model.BaseLanguageModel[st...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.schema.language_model.BaseLanguageModel:\n",
      "     |  \n",
      "     |  get_num_tokens(self, text: 'str') -> 'int'\n",
      "     |      Get the number of tokens present in the text.\n",
      "     |      \n",
      "     |      Useful for checking if an input will fit in a model's context window.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: The string input to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The integer number of tokens in the text.\n",
      "     |  \n",
      "     |  get_num_tokens_from_messages(self, messages: 'List[BaseMessage]') -> 'int'\n",
      "     |      Get the number of tokens in the messages.\n",
      "     |      \n",
      "     |      Useful for checking if an input will fit in a model's context window.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: The message inputs to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The sum of the number of tokens across the messages.\n",
      "     |  \n",
      "     |  get_token_ids(self, text: 'str') -> 'List[int]'\n",
      "     |      Return the ordered ids of the tokens in a text.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: The string input to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          A list of ids corresponding to the tokens in the text, in order they occur\n",
      "     |              in the text.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from langchain.schema.language_model.BaseLanguageModel:\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.load.serializable.Serializable:\n",
      "     |  \n",
      "     |  __init__(self, **kwargs: Any) -> None\n",
      "     |      Create a new model by parsing and validating input data from keyword arguments.\n",
      "     |      \n",
      "     |      Raises ValidationError if the input data cannot be parsed to form a valid model.\n",
      "     |  \n",
      "     |  to_json(self) -> Union[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\n",
      "     |  \n",
      "     |  to_json_not_implemented(self) -> langchain.load.serializable.SerializedNotImplemented\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from langchain.load.serializable.Serializable:\n",
      "     |  \n",
      "     |  lc_attributes\n",
      "     |      Return a list of attribute names that should be included in the\n",
      "     |      serialized kwargs. These attributes must be accepted by the\n",
      "     |      constructor.\n",
      "     |  \n",
      "     |  lc_namespace\n",
      "     |      Return the namespace of the langchain object.\n",
      "     |      eg. [\"langchain\", \"llms\", \"openai\"]\n",
      "     |  \n",
      "     |  lc_secrets\n",
      "     |      Return a map of constructor argument names to secret ids.\n",
      "     |      eg. {\"openai_api_key\": \"OPENAI_API_KEY\"}\n",
      "     |  \n",
      "     |  lc_serializable\n",
      "     |      Return whether or not the class is serializable.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __eq__(self, other: Any) -> bool\n",
      "     |  \n",
      "     |  __getstate__(self) -> 'DictAny'\n",
      "     |  \n",
      "     |  __iter__(self) -> 'TupleGenerator'\n",
      "     |      so `dict(model)` works\n",
      "     |  \n",
      "     |  __repr_args__(self) -> 'ReprArgs'\n",
      "     |  \n",
      "     |  __setattr__(self, name, value)\n",
      "     |  \n",
      "     |  __setstate__(self, state: 'DictAny') -> None\n",
      "     |  \n",
      "     |  copy(self: 'Model', *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, update: Optional[ForwardRef('DictStrAny')] = None, deep: bool = False) -> 'Model'\n",
      "     |      Duplicate a model, optionally choose which fields to include, exclude and change.\n",
      "     |      \n",
      "     |      :param include: fields to include in new model\n",
      "     |      :param exclude: fields to exclude from new model, as with values this takes precedence over include\n",
      "     |      :param update: values to change/add in the new model. Note: the data is not validated before creating\n",
      "     |          the new model: you should trust this data\n",
      "     |      :param deep: set to `True` to make a deep copy of the model\n",
      "     |      :return: new model instance\n",
      "     |  \n",
      "     |  json(self, *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) -> 'unicode'\n",
      "     |      Generate a JSON representation of the model, `include` and `exclude` arguments as per `dict()`.\n",
      "     |      \n",
      "     |      `encoder` is an optional function to supply as `default` to json.dumps(), other arguments as per `json.dumps()`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __get_validators__() -> 'CallableGenerator' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  __try_update_forward_refs__(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      "     |      Same as update_forward_refs but will not raise exception\n",
      "     |      when forward references are not defined.\n",
      "     |  \n",
      "     |  construct(_fields_set: Optional[ForwardRef('SetStr')] = None, **values: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |      Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data.\n",
      "     |      Default values are respected, but no other validation is performed.\n",
      "     |      Behaves as if `Config.extra = 'allow'` was set since it adds all passed values\n",
      "     |  \n",
      "     |  from_orm(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_file(path: Union[str, pathlib.Path], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_obj(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_raw(b: Union[str, bytes], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema(by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}') -> 'DictStrAny' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema_json(*, by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}', **dumps_kwargs: Any) -> 'unicode' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  update_forward_refs(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      "     |      Try to update ForwardRefs on fields based on this Model, globalns and localns.\n",
      "     |  \n",
      "     |  validate(value: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __fields_set__\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.utils.Representation:\n",
      "     |  \n",
      "     |  __pretty__(self, fmt: Callable[[Any], Any], **kwargs: Any) -> Generator[Any, NoneType, NoneType]\n",
      "     |      Used by devtools (https://python-devtools.helpmanual.io/) to provide a human readable representations of objects\n",
      "     |  \n",
      "     |  __repr__(self) -> 'unicode'\n",
      "     |  \n",
      "     |  __repr_name__(self) -> 'unicode'\n",
      "     |      Name of the instance's class, used in __repr__.\n",
      "     |  \n",
      "     |  __repr_str__(self, join_str: 'unicode') -> 'unicode'\n",
      "     |  \n",
      "     |  __rich_repr__(self) -> 'RichReprResult'\n",
      "     |      Get fields for Rich library\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.schema.runnable.base.Runnable:\n",
      "     |  \n",
      "     |  __or__(self, other: 'Union[Runnable[Any, Other], Callable[[Any], Other], Mapping[str, Union[Runnable[Any, Other], Callable[[Any], Other], Any]]]') -> 'RunnableSequence[Input, Other]'\n",
      "     |      Return self|value.\n",
      "     |  \n",
      "     |  __ror__(self, other: 'Union[Runnable[Other, Any], Callable[[Any], Other], Mapping[str, Union[Runnable[Other, Any], Callable[[Other], Any], Any]]]') -> 'RunnableSequence[Other, Output]'\n",
      "     |      Return value|self.\n",
      "     |  \n",
      "     |  async atransform(self, input: 'AsyncIterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'AsyncIterator[Output]'\n",
      "     |      Default implementation of atransform, which buffers input and calls astream.\n",
      "     |      Subclasses should override this method if they can start producing output while\n",
      "     |      input is still being generated.\n",
      "     |  \n",
      "     |  bind(self, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      "     |      Bind arguments to a Runnable, returning a new Runnable.\n",
      "     |  \n",
      "     |  map(self) -> 'Runnable[List[Input], List[Output]]'\n",
      "     |      Return a new Runnable that maps a list of inputs to a list of outputs,\n",
      "     |      by calling invoke() with each input.\n",
      "     |  \n",
      "     |  transform(self, input: 'Iterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'Iterator[Output]'\n",
      "     |      Default implementation of transform, which buffers input and then calls stream.\n",
      "     |      Subclasses should override this method if they can start producing output while\n",
      "     |      input is still being generated.\n",
      "     |  \n",
      "     |  with_config(self, config: 'Optional[RunnableConfig]' = None, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      "     |      Bind config to a Runnable, returning a new Runnable.\n",
      "     |  \n",
      "     |  with_fallbacks(self, fallbacks: 'Sequence[Runnable[Input, Output]]', *, exceptions_to_handle: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,)) -> 'RunnableWithFallbacks[Input, Output]'\n",
      "     |  \n",
      "     |  with_retry(self, *, retry_if_exception_type: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,), wait_exponential_jitter: 'bool' = True, stop_after_attempt: 'int' = 3) -> 'Runnable[Input, Output]'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __class_getitem__(params) from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  __init_subclass__(*args, **kwargs) from pydantic.main.ModelMetaclass\n",
      "     |      This method is called when a class is subclassed.\n",
      "     |      \n",
      "     |      The default implementation does nothing. It may be\n",
      "     |      overridden to extend subclasses.\n",
      "    \n",
      "    class Aviary(langchain.llms.base.LLM)\n",
      "     |  Aviary(*, cache: Optional[bool] = None, verbose: bool = None, callbacks: Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager, NoneType] = None, callback_manager: Optional[langchain.callbacks.base.BaseCallbackManager] = None, tags: Optional[List[str]] = None, metadata: Optional[Dict[str, Any]] = None, model: str = 'amazon/LightGPT', aviary_url: Optional[str] = None, aviary_token: Optional[str] = None, use_prompt_format: bool = True, version: Optional[str] = None) -> None\n",
      "     |  \n",
      "     |  Aviary hosted models.\n",
      "     |  \n",
      "     |  Aviary is a backend for hosted models. You can\n",
      "     |  find out more about aviary at\n",
      "     |  http://github.com/ray-project/aviary\n",
      "     |  \n",
      "     |  To get a list of the models supported on an\n",
      "     |  aviary, follow the instructions on the website to\n",
      "     |  install the aviary CLI and then use:\n",
      "     |  `aviary models`\n",
      "     |  \n",
      "     |  AVIARY_URL and AVIARY_TOKEN environment variables must be set.\n",
      "     |  \n",
      "     |  Attributes:\n",
      "     |      model: The name of the model to use. Defaults to \"amazon/LightGPT\".\n",
      "     |      aviary_url: The URL for the Aviary backend. Defaults to None.\n",
      "     |      aviary_token: The bearer token for the Aviary backend. Defaults to None.\n",
      "     |      use_prompt_format: If True, the prompt template for the model will be ignored.\n",
      "     |          Defaults to True.\n",
      "     |      version: API version to use for Aviary. Defaults to None.\n",
      "     |  \n",
      "     |  Example:\n",
      "     |      .. code-block:: python\n",
      "     |  \n",
      "     |          from langchain.llms import Aviary\n",
      "     |          os.environ[\"AVIARY_URL\"] = \"<URL>\"\n",
      "     |          os.environ[\"AVIARY_TOKEN\"] = \"<TOKEN>\"\n",
      "     |          light = Aviary(model='amazon/LightGPT')\n",
      "     |          output = light('How do you make fried rice?')\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Aviary\n",
      "     |      langchain.llms.base.LLM\n",
      "     |      langchain.llms.base.BaseLLM\n",
      "     |      langchain.schema.language_model.BaseLanguageModel\n",
      "     |      langchain.load.serializable.Serializable\n",
      "     |      pydantic.main.BaseModel\n",
      "     |      pydantic.utils.Representation\n",
      "     |      langchain.schema.runnable.base.Runnable\n",
      "     |      typing.Generic\n",
      "     |      abc.ABC\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  validate_environment(values: Dict) -> Dict from pydantic.main.ModelMetaclass\n",
      "     |      Validate that api key and python package exists in environment.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __json_encoder__ = pydantic_encoder(obj: Any) -> Any\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  Config = <class 'langchain.llms.aviary.Aviary.Config'>\n",
      "     |      Configuration for this pydantic object.\n",
      "     |  \n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {'aviary_token': typing.Optional[str], 'aviary_url':...\n",
      "     |  \n",
      "     |  __class_vars__ = set()\n",
      "     |  \n",
      "     |  __config__ = <class 'langchain.llms.aviary.Config'>\n",
      "     |  \n",
      "     |  __custom_root_type__ = False\n",
      "     |  \n",
      "     |  __exclude_fields__ = {'callback_manager': True, 'callbacks': True, 'me...\n",
      "     |  \n",
      "     |  __fields__ = {'aviary_token': ModelField(name='aviary_token', type=Opt...\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "     |  \n",
      "     |  __include_fields__ = None\n",
      "     |  \n",
      "     |  __parameters__ = ()\n",
      "     |  \n",
      "     |  __post_root_validators__ = [(False, <function BaseLLM.raise_deprecatio...\n",
      "     |  \n",
      "     |  __pre_root_validators__ = [<function Aviary.validate_environment>]\n",
      "     |  \n",
      "     |  __private_attributes__ = {'_lc_kwargs': ModelPrivateAttr(default=Pydan...\n",
      "     |  \n",
      "     |  __schema_cache__ = {}\n",
      "     |  \n",
      "     |  __signature__ = <Signature (*, cache: Optional[bool] = None, ver...l =...\n",
      "     |  \n",
      "     |  __validators__ = {'verbose': [<pydantic.class_validators.Validator obj...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  __call__(self, prompt: 'str', stop: 'Optional[List[str]]' = None, callbacks: 'Callbacks' = None, *, tags: 'Optional[List[str]]' = None, metadata: 'Optional[Dict[str, Any]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Check Cache and run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  __str__(self) -> 'str'\n",
      "     |      Get a string representation of the object for printing.\n",
      "     |  \n",
      "     |  async abatch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      "     |      Default implementation of abatch, which calls ainvoke N times.\n",
      "     |      Subclasses should override this method if they can batch more efficiently.\n",
      "     |  \n",
      "     |  async agenerate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  async agenerate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Asynchronously pass a sequence of prompts and return model generations.\n",
      "     |      \n",
      "     |      This method should make use of batched calls for models that expose a batched\n",
      "     |      API.\n",
      "     |      \n",
      "     |      Use this method when you want to:\n",
      "     |          1. take advantage of batched calls,\n",
      "     |          2. need more output from the model than just the top generated value,\n",
      "     |          3. are building chains that are agnostic to the underlying language model\n",
      "     |              type (e.g., pure text completion models vs chat models).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      "     |              converted to match the format of any language model (string for pure\n",
      "     |              text generation models and BaseMessages for chat models).\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          callbacks: Callbacks to pass through. Used for executing additional\n",
      "     |              functionality, such as logging or streaming, throughout generation.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          An LLMResult, which contains a list of candidate Generations for each input\n",
      "     |              prompt and additional model provider-specific output.\n",
      "     |  \n",
      "     |  async ainvoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Default implementation of ainvoke, which calls invoke in a thread pool.\n",
      "     |      Subclasses should override this method if they can run asynchronously.\n",
      "     |  \n",
      "     |  async apredict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Asynchronously pass a string to the model and return a string prediction.\n",
      "     |      \n",
      "     |      Use this method when calling pure text generation models and only the top\n",
      "     |          candidate generation is needed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: String input to pass to the model.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a string.\n",
      "     |  \n",
      "     |  async apredict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      "     |      Asynchronously pass messages to the model and return a message prediction.\n",
      "     |      \n",
      "     |      Use this method when calling chat models and only the top\n",
      "     |          candidate generation is needed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: A sequence of chat messages corresponding to a single model input.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a message.\n",
      "     |  \n",
      "     |  async astream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'AsyncIterator[str]'\n",
      "     |      Default implementation of astream, which calls ainvoke.\n",
      "     |      Subclasses should override this method if they support streaming output.\n",
      "     |  \n",
      "     |  batch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      "     |      Default implementation of batch, which calls invoke N times.\n",
      "     |      Subclasses should override this method if they can batch more efficiently.\n",
      "     |  \n",
      "     |  dict(self, **kwargs: 'Any') -> 'Dict'\n",
      "     |      Return a dictionary of the LLM.\n",
      "     |  \n",
      "     |  generate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  generate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Pass a sequence of prompts to the model and return model generations.\n",
      "     |      \n",
      "     |      This method should make use of batched calls for models that expose a batched\n",
      "     |      API.\n",
      "     |      \n",
      "     |      Use this method when you want to:\n",
      "     |          1. take advantage of batched calls,\n",
      "     |          2. need more output from the model than just the top generated value,\n",
      "     |          3. are building chains that are agnostic to the underlying language model\n",
      "     |              type (e.g., pure text completion models vs chat models).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      "     |              converted to match the format of any language model (string for pure\n",
      "     |              text generation models and BaseMessages for chat models).\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          callbacks: Callbacks to pass through. Used for executing additional\n",
      "     |              functionality, such as logging or streaming, throughout generation.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          An LLMResult, which contains a list of candidate Generations for each input\n",
      "     |              prompt and additional model provider-specific output.\n",
      "     |  \n",
      "     |  invoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |  \n",
      "     |  predict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Pass a single string input to the model and return a string prediction.\n",
      "     |      \n",
      "     |       Use this method when passing in raw text. If you want to pass in specific\n",
      "     |          types of chat messages, use predict_messages.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: String input to pass to the model.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a string.\n",
      "     |  \n",
      "     |  predict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      "     |      Pass a message sequence to the model and return a message prediction.\n",
      "     |      \n",
      "     |      Use this method when passing in chat messages. If you want to pass in raw text,\n",
      "     |          use predict.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: A sequence of chat messages corresponding to a single model input.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a message.\n",
      "     |  \n",
      "     |  save(self, file_path: 'Union[Path, str]') -> 'None'\n",
      "     |      Save the LLM.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          file_path: Path to file to save the LLM to.\n",
      "     |      \n",
      "     |      Example:\n",
      "     |      .. code-block:: python\n",
      "     |      \n",
      "     |          llm.save(file_path=\"path/llm.yaml\")\n",
      "     |  \n",
      "     |  stream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'Iterator[str]'\n",
      "     |      Default implementation of stream, which calls invoke.\n",
      "     |      Subclasses should override this method if they support streaming output.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  raise_deprecation(values: 'Dict') -> 'Dict' from pydantic.main.ModelMetaclass\n",
      "     |      Raise deprecation warning if callback_manager is used.\n",
      "     |  \n",
      "     |  set_verbose(verbose: 'Optional[bool]') -> 'bool' from pydantic.main.ModelMetaclass\n",
      "     |      If verbose is None, set it.\n",
      "     |      \n",
      "     |      This allows users to pass in None as verbose to access the global setting.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  __orig_bases__ = (langchain.schema.language_model.BaseLanguageModel[st...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.schema.language_model.BaseLanguageModel:\n",
      "     |  \n",
      "     |  get_num_tokens(self, text: 'str') -> 'int'\n",
      "     |      Get the number of tokens present in the text.\n",
      "     |      \n",
      "     |      Useful for checking if an input will fit in a model's context window.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: The string input to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The integer number of tokens in the text.\n",
      "     |  \n",
      "     |  get_num_tokens_from_messages(self, messages: 'List[BaseMessage]') -> 'int'\n",
      "     |      Get the number of tokens in the messages.\n",
      "     |      \n",
      "     |      Useful for checking if an input will fit in a model's context window.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: The message inputs to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The sum of the number of tokens across the messages.\n",
      "     |  \n",
      "     |  get_token_ids(self, text: 'str') -> 'List[int]'\n",
      "     |      Return the ordered ids of the tokens in a text.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: The string input to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          A list of ids corresponding to the tokens in the text, in order they occur\n",
      "     |              in the text.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from langchain.schema.language_model.BaseLanguageModel:\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.load.serializable.Serializable:\n",
      "     |  \n",
      "     |  __init__(self, **kwargs: Any) -> None\n",
      "     |      Create a new model by parsing and validating input data from keyword arguments.\n",
      "     |      \n",
      "     |      Raises ValidationError if the input data cannot be parsed to form a valid model.\n",
      "     |  \n",
      "     |  to_json(self) -> Union[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\n",
      "     |  \n",
      "     |  to_json_not_implemented(self) -> langchain.load.serializable.SerializedNotImplemented\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from langchain.load.serializable.Serializable:\n",
      "     |  \n",
      "     |  lc_attributes\n",
      "     |      Return a list of attribute names that should be included in the\n",
      "     |      serialized kwargs. These attributes must be accepted by the\n",
      "     |      constructor.\n",
      "     |  \n",
      "     |  lc_namespace\n",
      "     |      Return the namespace of the langchain object.\n",
      "     |      eg. [\"langchain\", \"llms\", \"openai\"]\n",
      "     |  \n",
      "     |  lc_secrets\n",
      "     |      Return a map of constructor argument names to secret ids.\n",
      "     |      eg. {\"openai_api_key\": \"OPENAI_API_KEY\"}\n",
      "     |  \n",
      "     |  lc_serializable\n",
      "     |      Return whether or not the class is serializable.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __eq__(self, other: Any) -> bool\n",
      "     |  \n",
      "     |  __getstate__(self) -> 'DictAny'\n",
      "     |  \n",
      "     |  __iter__(self) -> 'TupleGenerator'\n",
      "     |      so `dict(model)` works\n",
      "     |  \n",
      "     |  __repr_args__(self) -> 'ReprArgs'\n",
      "     |  \n",
      "     |  __setattr__(self, name, value)\n",
      "     |  \n",
      "     |  __setstate__(self, state: 'DictAny') -> None\n",
      "     |  \n",
      "     |  copy(self: 'Model', *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, update: Optional[ForwardRef('DictStrAny')] = None, deep: bool = False) -> 'Model'\n",
      "     |      Duplicate a model, optionally choose which fields to include, exclude and change.\n",
      "     |      \n",
      "     |      :param include: fields to include in new model\n",
      "     |      :param exclude: fields to exclude from new model, as with values this takes precedence over include\n",
      "     |      :param update: values to change/add in the new model. Note: the data is not validated before creating\n",
      "     |          the new model: you should trust this data\n",
      "     |      :param deep: set to `True` to make a deep copy of the model\n",
      "     |      :return: new model instance\n",
      "     |  \n",
      "     |  json(self, *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) -> 'unicode'\n",
      "     |      Generate a JSON representation of the model, `include` and `exclude` arguments as per `dict()`.\n",
      "     |      \n",
      "     |      `encoder` is an optional function to supply as `default` to json.dumps(), other arguments as per `json.dumps()`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __get_validators__() -> 'CallableGenerator' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  __try_update_forward_refs__(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      "     |      Same as update_forward_refs but will not raise exception\n",
      "     |      when forward references are not defined.\n",
      "     |  \n",
      "     |  construct(_fields_set: Optional[ForwardRef('SetStr')] = None, **values: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |      Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data.\n",
      "     |      Default values are respected, but no other validation is performed.\n",
      "     |      Behaves as if `Config.extra = 'allow'` was set since it adds all passed values\n",
      "     |  \n",
      "     |  from_orm(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_file(path: Union[str, pathlib.Path], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_obj(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_raw(b: Union[str, bytes], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema(by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}') -> 'DictStrAny' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema_json(*, by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}', **dumps_kwargs: Any) -> 'unicode' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  update_forward_refs(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      "     |      Try to update ForwardRefs on fields based on this Model, globalns and localns.\n",
      "     |  \n",
      "     |  validate(value: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __fields_set__\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.utils.Representation:\n",
      "     |  \n",
      "     |  __pretty__(self, fmt: Callable[[Any], Any], **kwargs: Any) -> Generator[Any, NoneType, NoneType]\n",
      "     |      Used by devtools (https://python-devtools.helpmanual.io/) to provide a human readable representations of objects\n",
      "     |  \n",
      "     |  __repr__(self) -> 'unicode'\n",
      "     |  \n",
      "     |  __repr_name__(self) -> 'unicode'\n",
      "     |      Name of the instance's class, used in __repr__.\n",
      "     |  \n",
      "     |  __repr_str__(self, join_str: 'unicode') -> 'unicode'\n",
      "     |  \n",
      "     |  __rich_repr__(self) -> 'RichReprResult'\n",
      "     |      Get fields for Rich library\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.schema.runnable.base.Runnable:\n",
      "     |  \n",
      "     |  __or__(self, other: 'Union[Runnable[Any, Other], Callable[[Any], Other], Mapping[str, Union[Runnable[Any, Other], Callable[[Any], Other], Any]]]') -> 'RunnableSequence[Input, Other]'\n",
      "     |      Return self|value.\n",
      "     |  \n",
      "     |  __ror__(self, other: 'Union[Runnable[Other, Any], Callable[[Any], Other], Mapping[str, Union[Runnable[Other, Any], Callable[[Other], Any], Any]]]') -> 'RunnableSequence[Other, Output]'\n",
      "     |      Return value|self.\n",
      "     |  \n",
      "     |  async atransform(self, input: 'AsyncIterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'AsyncIterator[Output]'\n",
      "     |      Default implementation of atransform, which buffers input and calls astream.\n",
      "     |      Subclasses should override this method if they can start producing output while\n",
      "     |      input is still being generated.\n",
      "     |  \n",
      "     |  bind(self, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      "     |      Bind arguments to a Runnable, returning a new Runnable.\n",
      "     |  \n",
      "     |  map(self) -> 'Runnable[List[Input], List[Output]]'\n",
      "     |      Return a new Runnable that maps a list of inputs to a list of outputs,\n",
      "     |      by calling invoke() with each input.\n",
      "     |  \n",
      "     |  transform(self, input: 'Iterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'Iterator[Output]'\n",
      "     |      Default implementation of transform, which buffers input and then calls stream.\n",
      "     |      Subclasses should override this method if they can start producing output while\n",
      "     |      input is still being generated.\n",
      "     |  \n",
      "     |  with_config(self, config: 'Optional[RunnableConfig]' = None, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      "     |      Bind config to a Runnable, returning a new Runnable.\n",
      "     |  \n",
      "     |  with_fallbacks(self, fallbacks: 'Sequence[Runnable[Input, Output]]', *, exceptions_to_handle: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,)) -> 'RunnableWithFallbacks[Input, Output]'\n",
      "     |  \n",
      "     |  with_retry(self, *, retry_if_exception_type: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,), wait_exponential_jitter: 'bool' = True, stop_after_attempt: 'int' = 3) -> 'Runnable[Input, Output]'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __class_getitem__(params) from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  __init_subclass__(*args, **kwargs) from pydantic.main.ModelMetaclass\n",
      "     |      This method is called when a class is subclassed.\n",
      "     |      \n",
      "     |      The default implementation does nothing. It may be\n",
      "     |      overridden to extend subclasses.\n",
      "    \n",
      "    class AzureMLOnlineEndpoint(langchain.llms.base.LLM, pydantic.main.BaseModel)\n",
      "     |  AzureMLOnlineEndpoint(*, cache: Optional[bool] = None, verbose: bool = None, callbacks: Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager, NoneType] = None, callback_manager: Optional[langchain.callbacks.base.BaseCallbackManager] = None, tags: Optional[List[str]] = None, metadata: Optional[Dict[str, Any]] = None, endpoint_url: str = '', endpoint_api_key: str = '', deployment_name: str = '', http_client: Any = None, content_formatter: Any = None, model_kwargs: Optional[dict] = None) -> None\n",
      "     |  \n",
      "     |  Azure ML Online Endpoint models.\n",
      "     |  \n",
      "     |  Example:\n",
      "     |      .. code-block:: python\n",
      "     |  \n",
      "     |          azure_llm = AzureMLOnlineEndpoint(\n",
      "     |              endpoint_url=\"https://<your-endpoint>.<your_region>.inference.ml.azure.com/score\",\n",
      "     |              endpoint_api_key=\"my-api-key\",\n",
      "     |              content_formatter=content_formatter,\n",
      "     |          )\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      AzureMLOnlineEndpoint\n",
      "     |      langchain.llms.base.LLM\n",
      "     |      langchain.llms.base.BaseLLM\n",
      "     |      langchain.schema.language_model.BaseLanguageModel\n",
      "     |      langchain.load.serializable.Serializable\n",
      "     |      pydantic.main.BaseModel\n",
      "     |      pydantic.utils.Representation\n",
      "     |      langchain.schema.runnable.base.Runnable\n",
      "     |      typing.Generic\n",
      "     |      abc.ABC\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  validate_client(field_value: Any, values: Dict) -> langchain.llms.azureml_endpoint.AzureMLEndpointClient from pydantic.main.ModelMetaclass\n",
      "     |      Validate that api key and python package exists in environment.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __json_encoder__ = pydantic_encoder(obj: Any) -> Any\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {'content_formatter': typing.Any, 'deployment_name':...\n",
      "     |  \n",
      "     |  __class_vars__ = set()\n",
      "     |  \n",
      "     |  __config__ = <class 'langchain.llms.azureml_endpoint.Config'>\n",
      "     |  \n",
      "     |  __custom_root_type__ = False\n",
      "     |  \n",
      "     |  __exclude_fields__ = {'callback_manager': True, 'callbacks': True, 'me...\n",
      "     |  \n",
      "     |  __fields__ = {'cache': ModelField(name='cache', type=Optional[bool], r...\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "     |  \n",
      "     |  __include_fields__ = None\n",
      "     |  \n",
      "     |  __parameters__ = ()\n",
      "     |  \n",
      "     |  __post_root_validators__ = [(False, <function BaseLLM.raise_deprecatio...\n",
      "     |  \n",
      "     |  __pre_root_validators__ = []\n",
      "     |  \n",
      "     |  __private_attributes__ = {'_lc_kwargs': ModelPrivateAttr(default=Pydan...\n",
      "     |  \n",
      "     |  __schema_cache__ = {}\n",
      "     |  \n",
      "     |  __signature__ = <Signature (*, cache: Optional[bool] = None, ver...ne,...\n",
      "     |  \n",
      "     |  __validators__ = {'http_client': [<pydantic.class_validators.Validator...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  __call__(self, prompt: 'str', stop: 'Optional[List[str]]' = None, callbacks: 'Callbacks' = None, *, tags: 'Optional[List[str]]' = None, metadata: 'Optional[Dict[str, Any]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Check Cache and run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  __str__(self) -> 'str'\n",
      "     |      Get a string representation of the object for printing.\n",
      "     |  \n",
      "     |  async abatch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      "     |      Default implementation of abatch, which calls ainvoke N times.\n",
      "     |      Subclasses should override this method if they can batch more efficiently.\n",
      "     |  \n",
      "     |  async agenerate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  async agenerate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Asynchronously pass a sequence of prompts and return model generations.\n",
      "     |      \n",
      "     |      This method should make use of batched calls for models that expose a batched\n",
      "     |      API.\n",
      "     |      \n",
      "     |      Use this method when you want to:\n",
      "     |          1. take advantage of batched calls,\n",
      "     |          2. need more output from the model than just the top generated value,\n",
      "     |          3. are building chains that are agnostic to the underlying language model\n",
      "     |              type (e.g., pure text completion models vs chat models).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      "     |              converted to match the format of any language model (string for pure\n",
      "     |              text generation models and BaseMessages for chat models).\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          callbacks: Callbacks to pass through. Used for executing additional\n",
      "     |              functionality, such as logging or streaming, throughout generation.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          An LLMResult, which contains a list of candidate Generations for each input\n",
      "     |              prompt and additional model provider-specific output.\n",
      "     |  \n",
      "     |  async ainvoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Default implementation of ainvoke, which calls invoke in a thread pool.\n",
      "     |      Subclasses should override this method if they can run asynchronously.\n",
      "     |  \n",
      "     |  async apredict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Asynchronously pass a string to the model and return a string prediction.\n",
      "     |      \n",
      "     |      Use this method when calling pure text generation models and only the top\n",
      "     |          candidate generation is needed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: String input to pass to the model.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a string.\n",
      "     |  \n",
      "     |  async apredict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      "     |      Asynchronously pass messages to the model and return a message prediction.\n",
      "     |      \n",
      "     |      Use this method when calling chat models and only the top\n",
      "     |          candidate generation is needed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: A sequence of chat messages corresponding to a single model input.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a message.\n",
      "     |  \n",
      "     |  async astream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'AsyncIterator[str]'\n",
      "     |      Default implementation of astream, which calls ainvoke.\n",
      "     |      Subclasses should override this method if they support streaming output.\n",
      "     |  \n",
      "     |  batch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      "     |      Default implementation of batch, which calls invoke N times.\n",
      "     |      Subclasses should override this method if they can batch more efficiently.\n",
      "     |  \n",
      "     |  dict(self, **kwargs: 'Any') -> 'Dict'\n",
      "     |      Return a dictionary of the LLM.\n",
      "     |  \n",
      "     |  generate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  generate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Pass a sequence of prompts to the model and return model generations.\n",
      "     |      \n",
      "     |      This method should make use of batched calls for models that expose a batched\n",
      "     |      API.\n",
      "     |      \n",
      "     |      Use this method when you want to:\n",
      "     |          1. take advantage of batched calls,\n",
      "     |          2. need more output from the model than just the top generated value,\n",
      "     |          3. are building chains that are agnostic to the underlying language model\n",
      "     |              type (e.g., pure text completion models vs chat models).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      "     |              converted to match the format of any language model (string for pure\n",
      "     |              text generation models and BaseMessages for chat models).\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          callbacks: Callbacks to pass through. Used for executing additional\n",
      "     |              functionality, such as logging or streaming, throughout generation.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          An LLMResult, which contains a list of candidate Generations for each input\n",
      "     |              prompt and additional model provider-specific output.\n",
      "     |  \n",
      "     |  invoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |  \n",
      "     |  predict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Pass a single string input to the model and return a string prediction.\n",
      "     |      \n",
      "     |       Use this method when passing in raw text. If you want to pass in specific\n",
      "     |          types of chat messages, use predict_messages.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: String input to pass to the model.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a string.\n",
      "     |  \n",
      "     |  predict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      "     |      Pass a message sequence to the model and return a message prediction.\n",
      "     |      \n",
      "     |      Use this method when passing in chat messages. If you want to pass in raw text,\n",
      "     |          use predict.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: A sequence of chat messages corresponding to a single model input.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a message.\n",
      "     |  \n",
      "     |  save(self, file_path: 'Union[Path, str]') -> 'None'\n",
      "     |      Save the LLM.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          file_path: Path to file to save the LLM to.\n",
      "     |      \n",
      "     |      Example:\n",
      "     |      .. code-block:: python\n",
      "     |      \n",
      "     |          llm.save(file_path=\"path/llm.yaml\")\n",
      "     |  \n",
      "     |  stream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'Iterator[str]'\n",
      "     |      Default implementation of stream, which calls invoke.\n",
      "     |      Subclasses should override this method if they support streaming output.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  raise_deprecation(values: 'Dict') -> 'Dict' from pydantic.main.ModelMetaclass\n",
      "     |      Raise deprecation warning if callback_manager is used.\n",
      "     |  \n",
      "     |  set_verbose(verbose: 'Optional[bool]') -> 'bool' from pydantic.main.ModelMetaclass\n",
      "     |      If verbose is None, set it.\n",
      "     |      \n",
      "     |      This allows users to pass in None as verbose to access the global setting.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  Config = <class 'langchain.llms.base.BaseLLM.Config'>\n",
      "     |      Configuration for this pydantic object.\n",
      "     |  \n",
      "     |  \n",
      "     |  __orig_bases__ = (langchain.schema.language_model.BaseLanguageModel[st...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.schema.language_model.BaseLanguageModel:\n",
      "     |  \n",
      "     |  get_num_tokens(self, text: 'str') -> 'int'\n",
      "     |      Get the number of tokens present in the text.\n",
      "     |      \n",
      "     |      Useful for checking if an input will fit in a model's context window.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: The string input to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The integer number of tokens in the text.\n",
      "     |  \n",
      "     |  get_num_tokens_from_messages(self, messages: 'List[BaseMessage]') -> 'int'\n",
      "     |      Get the number of tokens in the messages.\n",
      "     |      \n",
      "     |      Useful for checking if an input will fit in a model's context window.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: The message inputs to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The sum of the number of tokens across the messages.\n",
      "     |  \n",
      "     |  get_token_ids(self, text: 'str') -> 'List[int]'\n",
      "     |      Return the ordered ids of the tokens in a text.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: The string input to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          A list of ids corresponding to the tokens in the text, in order they occur\n",
      "     |              in the text.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from langchain.schema.language_model.BaseLanguageModel:\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.load.serializable.Serializable:\n",
      "     |  \n",
      "     |  __init__(self, **kwargs: Any) -> None\n",
      "     |      Create a new model by parsing and validating input data from keyword arguments.\n",
      "     |      \n",
      "     |      Raises ValidationError if the input data cannot be parsed to form a valid model.\n",
      "     |  \n",
      "     |  to_json(self) -> Union[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\n",
      "     |  \n",
      "     |  to_json_not_implemented(self) -> langchain.load.serializable.SerializedNotImplemented\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from langchain.load.serializable.Serializable:\n",
      "     |  \n",
      "     |  lc_attributes\n",
      "     |      Return a list of attribute names that should be included in the\n",
      "     |      serialized kwargs. These attributes must be accepted by the\n",
      "     |      constructor.\n",
      "     |  \n",
      "     |  lc_namespace\n",
      "     |      Return the namespace of the langchain object.\n",
      "     |      eg. [\"langchain\", \"llms\", \"openai\"]\n",
      "     |  \n",
      "     |  lc_secrets\n",
      "     |      Return a map of constructor argument names to secret ids.\n",
      "     |      eg. {\"openai_api_key\": \"OPENAI_API_KEY\"}\n",
      "     |  \n",
      "     |  lc_serializable\n",
      "     |      Return whether or not the class is serializable.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __eq__(self, other: Any) -> bool\n",
      "     |  \n",
      "     |  __getstate__(self) -> 'DictAny'\n",
      "     |  \n",
      "     |  __iter__(self) -> 'TupleGenerator'\n",
      "     |      so `dict(model)` works\n",
      "     |  \n",
      "     |  __repr_args__(self) -> 'ReprArgs'\n",
      "     |  \n",
      "     |  __setattr__(self, name, value)\n",
      "     |  \n",
      "     |  __setstate__(self, state: 'DictAny') -> None\n",
      "     |  \n",
      "     |  copy(self: 'Model', *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, update: Optional[ForwardRef('DictStrAny')] = None, deep: bool = False) -> 'Model'\n",
      "     |      Duplicate a model, optionally choose which fields to include, exclude and change.\n",
      "     |      \n",
      "     |      :param include: fields to include in new model\n",
      "     |      :param exclude: fields to exclude from new model, as with values this takes precedence over include\n",
      "     |      :param update: values to change/add in the new model. Note: the data is not validated before creating\n",
      "     |          the new model: you should trust this data\n",
      "     |      :param deep: set to `True` to make a deep copy of the model\n",
      "     |      :return: new model instance\n",
      "     |  \n",
      "     |  json(self, *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) -> 'unicode'\n",
      "     |      Generate a JSON representation of the model, `include` and `exclude` arguments as per `dict()`.\n",
      "     |      \n",
      "     |      `encoder` is an optional function to supply as `default` to json.dumps(), other arguments as per `json.dumps()`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __get_validators__() -> 'CallableGenerator' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  __try_update_forward_refs__(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      "     |      Same as update_forward_refs but will not raise exception\n",
      "     |      when forward references are not defined.\n",
      "     |  \n",
      "     |  construct(_fields_set: Optional[ForwardRef('SetStr')] = None, **values: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |      Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data.\n",
      "     |      Default values are respected, but no other validation is performed.\n",
      "     |      Behaves as if `Config.extra = 'allow'` was set since it adds all passed values\n",
      "     |  \n",
      "     |  from_orm(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_file(path: Union[str, pathlib.Path], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_obj(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_raw(b: Union[str, bytes], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema(by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}') -> 'DictStrAny' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema_json(*, by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}', **dumps_kwargs: Any) -> 'unicode' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  update_forward_refs(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      "     |      Try to update ForwardRefs on fields based on this Model, globalns and localns.\n",
      "     |  \n",
      "     |  validate(value: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __fields_set__\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.utils.Representation:\n",
      "     |  \n",
      "     |  __pretty__(self, fmt: Callable[[Any], Any], **kwargs: Any) -> Generator[Any, NoneType, NoneType]\n",
      "     |      Used by devtools (https://python-devtools.helpmanual.io/) to provide a human readable representations of objects\n",
      "     |  \n",
      "     |  __repr__(self) -> 'unicode'\n",
      "     |  \n",
      "     |  __repr_name__(self) -> 'unicode'\n",
      "     |      Name of the instance's class, used in __repr__.\n",
      "     |  \n",
      "     |  __repr_str__(self, join_str: 'unicode') -> 'unicode'\n",
      "     |  \n",
      "     |  __rich_repr__(self) -> 'RichReprResult'\n",
      "     |      Get fields for Rich library\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.schema.runnable.base.Runnable:\n",
      "     |  \n",
      "     |  __or__(self, other: 'Union[Runnable[Any, Other], Callable[[Any], Other], Mapping[str, Union[Runnable[Any, Other], Callable[[Any], Other], Any]]]') -> 'RunnableSequence[Input, Other]'\n",
      "     |      Return self|value.\n",
      "     |  \n",
      "     |  __ror__(self, other: 'Union[Runnable[Other, Any], Callable[[Any], Other], Mapping[str, Union[Runnable[Other, Any], Callable[[Other], Any], Any]]]') -> 'RunnableSequence[Other, Output]'\n",
      "     |      Return value|self.\n",
      "     |  \n",
      "     |  async atransform(self, input: 'AsyncIterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'AsyncIterator[Output]'\n",
      "     |      Default implementation of atransform, which buffers input and calls astream.\n",
      "     |      Subclasses should override this method if they can start producing output while\n",
      "     |      input is still being generated.\n",
      "     |  \n",
      "     |  bind(self, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      "     |      Bind arguments to a Runnable, returning a new Runnable.\n",
      "     |  \n",
      "     |  map(self) -> 'Runnable[List[Input], List[Output]]'\n",
      "     |      Return a new Runnable that maps a list of inputs to a list of outputs,\n",
      "     |      by calling invoke() with each input.\n",
      "     |  \n",
      "     |  transform(self, input: 'Iterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'Iterator[Output]'\n",
      "     |      Default implementation of transform, which buffers input and then calls stream.\n",
      "     |      Subclasses should override this method if they can start producing output while\n",
      "     |      input is still being generated.\n",
      "     |  \n",
      "     |  with_config(self, config: 'Optional[RunnableConfig]' = None, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      "     |      Bind config to a Runnable, returning a new Runnable.\n",
      "     |  \n",
      "     |  with_fallbacks(self, fallbacks: 'Sequence[Runnable[Input, Output]]', *, exceptions_to_handle: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,)) -> 'RunnableWithFallbacks[Input, Output]'\n",
      "     |  \n",
      "     |  with_retry(self, *, retry_if_exception_type: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,), wait_exponential_jitter: 'bool' = True, stop_after_attempt: 'int' = 3) -> 'Runnable[Input, Output]'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __class_getitem__(params) from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  __init_subclass__(*args, **kwargs) from pydantic.main.ModelMetaclass\n",
      "     |      This method is called when a class is subclassed.\n",
      "     |      \n",
      "     |      The default implementation does nothing. It may be\n",
      "     |      overridden to extend subclasses.\n",
      "    \n",
      "    class AzureOpenAI(BaseOpenAI)\n",
      "     |  AzureOpenAI(*, cache: Optional[bool] = None, verbose: bool = None, callbacks: Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager, NoneType] = None, callback_manager: Optional[langchain.callbacks.base.BaseCallbackManager] = None, tags: Optional[List[str]] = None, metadata: Optional[Dict[str, Any]] = None, client: Any = None, model: str = 'text-davinci-003', temperature: float = 0.7, max_tokens: int = 256, top_p: float = 1, frequency_penalty: float = 0, presence_penalty: float = 0, n: int = 1, best_of: int = 1, model_kwargs: Dict[str, Any] = None, openai_api_key: Optional[str] = None, openai_api_base: Optional[str] = None, openai_organization: Optional[str] = None, openai_proxy: Optional[str] = None, batch_size: int = 20, request_timeout: Union[float, Tuple[float, float], NoneType] = None, logit_bias: Optional[Dict[str, float]] = None, max_retries: int = 6, streaming: bool = False, allowed_special: Union[Literal['all'], AbstractSet[str]] = set(), disallowed_special: Union[Literal['all'], Collection[str]] = 'all', tiktoken_model_name: Optional[str] = None, deployment_name: str = '', openai_api_type: str = '', openai_api_version: str = '') -> None\n",
      "     |  \n",
      "     |  Azure-specific OpenAI large language models.\n",
      "     |  \n",
      "     |  To use, you should have the ``openai`` python package installed, and the\n",
      "     |  environment variable ``OPENAI_API_KEY`` set with your API key.\n",
      "     |  \n",
      "     |  Any parameters that are valid to be passed to the openai.create call can be passed\n",
      "     |  in, even if not explicitly saved on this class.\n",
      "     |  \n",
      "     |  Example:\n",
      "     |      .. code-block:: python\n",
      "     |  \n",
      "     |          from langchain.llms import AzureOpenAI\n",
      "     |          openai = AzureOpenAI(model_name=\"text-davinci-003\")\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      AzureOpenAI\n",
      "     |      BaseOpenAI\n",
      "     |      langchain.llms.base.BaseLLM\n",
      "     |      langchain.schema.language_model.BaseLanguageModel\n",
      "     |      langchain.load.serializable.Serializable\n",
      "     |      pydantic.main.BaseModel\n",
      "     |      pydantic.utils.Representation\n",
      "     |      langchain.schema.runnable.base.Runnable\n",
      "     |      typing.Generic\n",
      "     |      abc.ABC\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  validate_azure_settings(values: 'Dict') -> 'Dict' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __json_encoder__ = pydantic_encoder(obj: Any) -> Any\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {'deployment_name': 'str', 'openai_api_type': 'str',...\n",
      "     |  \n",
      "     |  __class_vars__ = set()\n",
      "     |  \n",
      "     |  __config__ = <class 'langchain.llms.openai.Config'>\n",
      "     |  \n",
      "     |  __custom_root_type__ = False\n",
      "     |  \n",
      "     |  __exclude_fields__ = {'callback_manager': True, 'callbacks': True, 'me...\n",
      "     |  \n",
      "     |  __fields__ = {'allowed_special': ModelField(name='allowed_special', ty...\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "     |  \n",
      "     |  __include_fields__ = None\n",
      "     |  \n",
      "     |  __parameters__ = ()\n",
      "     |  \n",
      "     |  __post_root_validators__ = [(False, <function BaseLLM.raise_deprecatio...\n",
      "     |  \n",
      "     |  __pre_root_validators__ = [<function BaseOpenAI.build_extra>]\n",
      "     |  \n",
      "     |  __private_attributes__ = {'_lc_kwargs': ModelPrivateAttr(default=Pydan...\n",
      "     |  \n",
      "     |  __schema_cache__ = {}\n",
      "     |  \n",
      "     |  __signature__ = <Signature (*, cache: Optional[bool] = None, ver... st...\n",
      "     |  \n",
      "     |  __validators__ = {'verbose': [<pydantic.class_validators.Validator obj...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BaseOpenAI:\n",
      "     |  \n",
      "     |  create_llm_result(self, choices: 'Any', prompts: 'List[str]', token_usage: 'Dict[str, int]') -> 'LLMResult'\n",
      "     |      Create the LLMResult from the choices and prompts.\n",
      "     |  \n",
      "     |  get_sub_prompts(self, params: 'Dict[str, Any]', prompts: 'List[str]', stop: 'Optional[List[str]]' = None) -> 'List[List[str]]'\n",
      "     |      Get the sub prompts for llm call.\n",
      "     |  \n",
      "     |  get_token_ids(self, text: 'str') -> 'List[int]'\n",
      "     |      Get the token IDs using the tiktoken package.\n",
      "     |  \n",
      "     |  max_tokens_for_prompt(self, prompt: 'str') -> 'int'\n",
      "     |      Calculate the maximum number of tokens possible to generate for a prompt.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompt: The prompt to pass into the model.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The maximum number of tokens to generate for a prompt.\n",
      "     |      \n",
      "     |      Example:\n",
      "     |          .. code-block:: python\n",
      "     |      \n",
      "     |              max_tokens = openai.max_token_for_prompt(\"Tell me a joke.\")\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from BaseOpenAI:\n",
      "     |  \n",
      "     |  build_extra(values: 'Dict[str, Any]') -> 'Dict[str, Any]' from pydantic.main.ModelMetaclass\n",
      "     |      Build extra kwargs from additional params that were passed in.\n",
      "     |  \n",
      "     |  validate_environment(values: 'Dict') -> 'Dict' from pydantic.main.ModelMetaclass\n",
      "     |      Validate that api key and python package exists in environment.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from BaseOpenAI:\n",
      "     |  \n",
      "     |  __new__(cls, **data: 'Any') -> 'Union[OpenAIChat, BaseOpenAI]'\n",
      "     |      Initialize the OpenAI object.\n",
      "     |  \n",
      "     |  modelname_to_contextsize(modelname: 'str') -> 'int'\n",
      "     |      Calculate the maximum number of tokens possible to generate for a model.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          modelname: The modelname we want to know the context size for.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The maximum context size\n",
      "     |      \n",
      "     |      Example:\n",
      "     |          .. code-block:: python\n",
      "     |      \n",
      "     |              max_tokens = openai.modelname_to_contextsize(\"text-davinci-003\")\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from BaseOpenAI:\n",
      "     |  \n",
      "     |  lc_secrets\n",
      "     |      Return a map of constructor argument names to secret ids.\n",
      "     |      eg. {\"openai_api_key\": \"OPENAI_API_KEY\"}\n",
      "     |  \n",
      "     |  lc_serializable\n",
      "     |      Return whether or not the class is serializable.\n",
      "     |  \n",
      "     |  max_context_size\n",
      "     |      Get max context size for this model.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from BaseOpenAI:\n",
      "     |  \n",
      "     |  Config = <class 'langchain.llms.openai.BaseOpenAI.Config'>\n",
      "     |      Configuration for this pydantic object.\n",
      "     |  \n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  __call__(self, prompt: 'str', stop: 'Optional[List[str]]' = None, callbacks: 'Callbacks' = None, *, tags: 'Optional[List[str]]' = None, metadata: 'Optional[Dict[str, Any]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Check Cache and run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  __str__(self) -> 'str'\n",
      "     |      Get a string representation of the object for printing.\n",
      "     |  \n",
      "     |  async abatch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      "     |      Default implementation of abatch, which calls ainvoke N times.\n",
      "     |      Subclasses should override this method if they can batch more efficiently.\n",
      "     |  \n",
      "     |  async agenerate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  async agenerate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Asynchronously pass a sequence of prompts and return model generations.\n",
      "     |      \n",
      "     |      This method should make use of batched calls for models that expose a batched\n",
      "     |      API.\n",
      "     |      \n",
      "     |      Use this method when you want to:\n",
      "     |          1. take advantage of batched calls,\n",
      "     |          2. need more output from the model than just the top generated value,\n",
      "     |          3. are building chains that are agnostic to the underlying language model\n",
      "     |              type (e.g., pure text completion models vs chat models).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      "     |              converted to match the format of any language model (string for pure\n",
      "     |              text generation models and BaseMessages for chat models).\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          callbacks: Callbacks to pass through. Used for executing additional\n",
      "     |              functionality, such as logging or streaming, throughout generation.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          An LLMResult, which contains a list of candidate Generations for each input\n",
      "     |              prompt and additional model provider-specific output.\n",
      "     |  \n",
      "     |  async ainvoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Default implementation of ainvoke, which calls invoke in a thread pool.\n",
      "     |      Subclasses should override this method if they can run asynchronously.\n",
      "     |  \n",
      "     |  async apredict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Asynchronously pass a string to the model and return a string prediction.\n",
      "     |      \n",
      "     |      Use this method when calling pure text generation models and only the top\n",
      "     |          candidate generation is needed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: String input to pass to the model.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a string.\n",
      "     |  \n",
      "     |  async apredict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      "     |      Asynchronously pass messages to the model and return a message prediction.\n",
      "     |      \n",
      "     |      Use this method when calling chat models and only the top\n",
      "     |          candidate generation is needed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: A sequence of chat messages corresponding to a single model input.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a message.\n",
      "     |  \n",
      "     |  async astream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'AsyncIterator[str]'\n",
      "     |      Default implementation of astream, which calls ainvoke.\n",
      "     |      Subclasses should override this method if they support streaming output.\n",
      "     |  \n",
      "     |  batch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      "     |      Default implementation of batch, which calls invoke N times.\n",
      "     |      Subclasses should override this method if they can batch more efficiently.\n",
      "     |  \n",
      "     |  dict(self, **kwargs: 'Any') -> 'Dict'\n",
      "     |      Return a dictionary of the LLM.\n",
      "     |  \n",
      "     |  generate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  generate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Pass a sequence of prompts to the model and return model generations.\n",
      "     |      \n",
      "     |      This method should make use of batched calls for models that expose a batched\n",
      "     |      API.\n",
      "     |      \n",
      "     |      Use this method when you want to:\n",
      "     |          1. take advantage of batched calls,\n",
      "     |          2. need more output from the model than just the top generated value,\n",
      "     |          3. are building chains that are agnostic to the underlying language model\n",
      "     |              type (e.g., pure text completion models vs chat models).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      "     |              converted to match the format of any language model (string for pure\n",
      "     |              text generation models and BaseMessages for chat models).\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          callbacks: Callbacks to pass through. Used for executing additional\n",
      "     |              functionality, such as logging or streaming, throughout generation.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          An LLMResult, which contains a list of candidate Generations for each input\n",
      "     |              prompt and additional model provider-specific output.\n",
      "     |  \n",
      "     |  invoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |  \n",
      "     |  predict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Pass a single string input to the model and return a string prediction.\n",
      "     |      \n",
      "     |       Use this method when passing in raw text. If you want to pass in specific\n",
      "     |          types of chat messages, use predict_messages.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: String input to pass to the model.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a string.\n",
      "     |  \n",
      "     |  predict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      "     |      Pass a message sequence to the model and return a message prediction.\n",
      "     |      \n",
      "     |      Use this method when passing in chat messages. If you want to pass in raw text,\n",
      "     |          use predict.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: A sequence of chat messages corresponding to a single model input.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a message.\n",
      "     |  \n",
      "     |  save(self, file_path: 'Union[Path, str]') -> 'None'\n",
      "     |      Save the LLM.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          file_path: Path to file to save the LLM to.\n",
      "     |      \n",
      "     |      Example:\n",
      "     |      .. code-block:: python\n",
      "     |      \n",
      "     |          llm.save(file_path=\"path/llm.yaml\")\n",
      "     |  \n",
      "     |  stream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'Iterator[str]'\n",
      "     |      Default implementation of stream, which calls invoke.\n",
      "     |      Subclasses should override this method if they support streaming output.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  raise_deprecation(values: 'Dict') -> 'Dict' from pydantic.main.ModelMetaclass\n",
      "     |      Raise deprecation warning if callback_manager is used.\n",
      "     |  \n",
      "     |  set_verbose(verbose: 'Optional[bool]') -> 'bool' from pydantic.main.ModelMetaclass\n",
      "     |      If verbose is None, set it.\n",
      "     |      \n",
      "     |      This allows users to pass in None as verbose to access the global setting.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  __orig_bases__ = (langchain.schema.language_model.BaseLanguageModel[st...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.schema.language_model.BaseLanguageModel:\n",
      "     |  \n",
      "     |  get_num_tokens(self, text: 'str') -> 'int'\n",
      "     |      Get the number of tokens present in the text.\n",
      "     |      \n",
      "     |      Useful for checking if an input will fit in a model's context window.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: The string input to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The integer number of tokens in the text.\n",
      "     |  \n",
      "     |  get_num_tokens_from_messages(self, messages: 'List[BaseMessage]') -> 'int'\n",
      "     |      Get the number of tokens in the messages.\n",
      "     |      \n",
      "     |      Useful for checking if an input will fit in a model's context window.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: The message inputs to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The sum of the number of tokens across the messages.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from langchain.schema.language_model.BaseLanguageModel:\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.load.serializable.Serializable:\n",
      "     |  \n",
      "     |  __init__(self, **kwargs: Any) -> None\n",
      "     |      Create a new model by parsing and validating input data from keyword arguments.\n",
      "     |      \n",
      "     |      Raises ValidationError if the input data cannot be parsed to form a valid model.\n",
      "     |  \n",
      "     |  to_json(self) -> Union[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\n",
      "     |  \n",
      "     |  to_json_not_implemented(self) -> langchain.load.serializable.SerializedNotImplemented\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from langchain.load.serializable.Serializable:\n",
      "     |  \n",
      "     |  lc_attributes\n",
      "     |      Return a list of attribute names that should be included in the\n",
      "     |      serialized kwargs. These attributes must be accepted by the\n",
      "     |      constructor.\n",
      "     |  \n",
      "     |  lc_namespace\n",
      "     |      Return the namespace of the langchain object.\n",
      "     |      eg. [\"langchain\", \"llms\", \"openai\"]\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __eq__(self, other: Any) -> bool\n",
      "     |  \n",
      "     |  __getstate__(self) -> 'DictAny'\n",
      "     |  \n",
      "     |  __iter__(self) -> 'TupleGenerator'\n",
      "     |      so `dict(model)` works\n",
      "     |  \n",
      "     |  __repr_args__(self) -> 'ReprArgs'\n",
      "     |  \n",
      "     |  __setattr__(self, name, value)\n",
      "     |  \n",
      "     |  __setstate__(self, state: 'DictAny') -> None\n",
      "     |  \n",
      "     |  copy(self: 'Model', *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, update: Optional[ForwardRef('DictStrAny')] = None, deep: bool = False) -> 'Model'\n",
      "     |      Duplicate a model, optionally choose which fields to include, exclude and change.\n",
      "     |      \n",
      "     |      :param include: fields to include in new model\n",
      "     |      :param exclude: fields to exclude from new model, as with values this takes precedence over include\n",
      "     |      :param update: values to change/add in the new model. Note: the data is not validated before creating\n",
      "     |          the new model: you should trust this data\n",
      "     |      :param deep: set to `True` to make a deep copy of the model\n",
      "     |      :return: new model instance\n",
      "     |  \n",
      "     |  json(self, *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) -> 'unicode'\n",
      "     |      Generate a JSON representation of the model, `include` and `exclude` arguments as per `dict()`.\n",
      "     |      \n",
      "     |      `encoder` is an optional function to supply as `default` to json.dumps(), other arguments as per `json.dumps()`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __get_validators__() -> 'CallableGenerator' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  __try_update_forward_refs__(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      "     |      Same as update_forward_refs but will not raise exception\n",
      "     |      when forward references are not defined.\n",
      "     |  \n",
      "     |  construct(_fields_set: Optional[ForwardRef('SetStr')] = None, **values: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |      Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data.\n",
      "     |      Default values are respected, but no other validation is performed.\n",
      "     |      Behaves as if `Config.extra = 'allow'` was set since it adds all passed values\n",
      "     |  \n",
      "     |  from_orm(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_file(path: Union[str, pathlib.Path], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_obj(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_raw(b: Union[str, bytes], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema(by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}') -> 'DictStrAny' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema_json(*, by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}', **dumps_kwargs: Any) -> 'unicode' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  update_forward_refs(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      "     |      Try to update ForwardRefs on fields based on this Model, globalns and localns.\n",
      "     |  \n",
      "     |  validate(value: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __fields_set__\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.utils.Representation:\n",
      "     |  \n",
      "     |  __pretty__(self, fmt: Callable[[Any], Any], **kwargs: Any) -> Generator[Any, NoneType, NoneType]\n",
      "     |      Used by devtools (https://python-devtools.helpmanual.io/) to provide a human readable representations of objects\n",
      "     |  \n",
      "     |  __repr__(self) -> 'unicode'\n",
      "     |  \n",
      "     |  __repr_name__(self) -> 'unicode'\n",
      "     |      Name of the instance's class, used in __repr__.\n",
      "     |  \n",
      "     |  __repr_str__(self, join_str: 'unicode') -> 'unicode'\n",
      "     |  \n",
      "     |  __rich_repr__(self) -> 'RichReprResult'\n",
      "     |      Get fields for Rich library\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.schema.runnable.base.Runnable:\n",
      "     |  \n",
      "     |  __or__(self, other: 'Union[Runnable[Any, Other], Callable[[Any], Other], Mapping[str, Union[Runnable[Any, Other], Callable[[Any], Other], Any]]]') -> 'RunnableSequence[Input, Other]'\n",
      "     |      Return self|value.\n",
      "     |  \n",
      "     |  __ror__(self, other: 'Union[Runnable[Other, Any], Callable[[Any], Other], Mapping[str, Union[Runnable[Other, Any], Callable[[Other], Any], Any]]]') -> 'RunnableSequence[Other, Output]'\n",
      "     |      Return value|self.\n",
      "     |  \n",
      "     |  async atransform(self, input: 'AsyncIterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'AsyncIterator[Output]'\n",
      "     |      Default implementation of atransform, which buffers input and calls astream.\n",
      "     |      Subclasses should override this method if they can start producing output while\n",
      "     |      input is still being generated.\n",
      "     |  \n",
      "     |  bind(self, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      "     |      Bind arguments to a Runnable, returning a new Runnable.\n",
      "     |  \n",
      "     |  map(self) -> 'Runnable[List[Input], List[Output]]'\n",
      "     |      Return a new Runnable that maps a list of inputs to a list of outputs,\n",
      "     |      by calling invoke() with each input.\n",
      "     |  \n",
      "     |  transform(self, input: 'Iterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'Iterator[Output]'\n",
      "     |      Default implementation of transform, which buffers input and then calls stream.\n",
      "     |      Subclasses should override this method if they can start producing output while\n",
      "     |      input is still being generated.\n",
      "     |  \n",
      "     |  with_config(self, config: 'Optional[RunnableConfig]' = None, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      "     |      Bind config to a Runnable, returning a new Runnable.\n",
      "     |  \n",
      "     |  with_fallbacks(self, fallbacks: 'Sequence[Runnable[Input, Output]]', *, exceptions_to_handle: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,)) -> 'RunnableWithFallbacks[Input, Output]'\n",
      "     |  \n",
      "     |  with_retry(self, *, retry_if_exception_type: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,), wait_exponential_jitter: 'bool' = True, stop_after_attempt: 'int' = 3) -> 'Runnable[Input, Output]'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __class_getitem__(params) from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  __init_subclass__(*args, **kwargs) from pydantic.main.ModelMetaclass\n",
      "     |      This method is called when a class is subclassed.\n",
      "     |      \n",
      "     |      The default implementation does nothing. It may be\n",
      "     |      overridden to extend subclasses.\n",
      "    \n",
      "    class Banana(langchain.llms.base.LLM)\n",
      "     |  Banana(*, cache: Optional[bool] = None, verbose: bool = None, callbacks: Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager, NoneType] = None, callback_manager: Optional[langchain.callbacks.base.BaseCallbackManager] = None, tags: Optional[List[str]] = None, metadata: Optional[Dict[str, Any]] = None, model_key: str = '', model_url_slug: str = '', model_kwargs: Dict[str, Any] = None, banana_api_key: Optional[str] = None) -> None\n",
      "     |  \n",
      "     |  Banana large language models.\n",
      "     |  \n",
      "     |  To use, you should have the ``banana-dev`` python package installed,\n",
      "     |  and the environment variable ``BANANA_API_KEY`` set with your API key.\n",
      "     |  This is the team API key available in the Banana dashboard.\n",
      "     |  \n",
      "     |  Any parameters that are valid to be passed to the call can be passed\n",
      "     |  in, even if not explicitly saved on this class.\n",
      "     |  \n",
      "     |  Example:\n",
      "     |      .. code-block:: python\n",
      "     |  \n",
      "     |          from langchain.llms import Banana\n",
      "     |          banana = Banana(model_key=\"\", model_url_slug=\"\")\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Banana\n",
      "     |      langchain.llms.base.LLM\n",
      "     |      langchain.llms.base.BaseLLM\n",
      "     |      langchain.schema.language_model.BaseLanguageModel\n",
      "     |      langchain.load.serializable.Serializable\n",
      "     |      pydantic.main.BaseModel\n",
      "     |      pydantic.utils.Representation\n",
      "     |      langchain.schema.runnable.base.Runnable\n",
      "     |      typing.Generic\n",
      "     |      abc.ABC\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  build_extra(values: Dict[str, Any]) -> Dict[str, Any] from pydantic.main.ModelMetaclass\n",
      "     |      Build extra kwargs from additional params that were passed in.\n",
      "     |  \n",
      "     |  validate_environment(values: Dict) -> Dict from pydantic.main.ModelMetaclass\n",
      "     |      Validate that api key and python package exists in environment.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __json_encoder__ = pydantic_encoder(obj: Any) -> Any\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  Config = <class 'langchain.llms.bananadev.Banana.Config'>\n",
      "     |      Configuration for this pydantic config.\n",
      "     |  \n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {'banana_api_key': typing.Optional[str], 'model_key'...\n",
      "     |  \n",
      "     |  __class_vars__ = set()\n",
      "     |  \n",
      "     |  __config__ = <class 'langchain.llms.bananadev.Config'>\n",
      "     |  \n",
      "     |  __custom_root_type__ = False\n",
      "     |  \n",
      "     |  __exclude_fields__ = {'callback_manager': True, 'callbacks': True, 'me...\n",
      "     |  \n",
      "     |  __fields__ = {'banana_api_key': ModelField(name='banana_api_key', type...\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "     |  \n",
      "     |  __include_fields__ = None\n",
      "     |  \n",
      "     |  __parameters__ = ()\n",
      "     |  \n",
      "     |  __post_root_validators__ = [(False, <function BaseLLM.raise_deprecatio...\n",
      "     |  \n",
      "     |  __pre_root_validators__ = [<function Banana.build_extra>]\n",
      "     |  \n",
      "     |  __private_attributes__ = {'_lc_kwargs': ModelPrivateAttr(default=Pydan...\n",
      "     |  \n",
      "     |  __schema_cache__ = {}\n",
      "     |  \n",
      "     |  __signature__ = <Signature (*, cache: Optional[bool] = None, ver...e, ...\n",
      "     |  \n",
      "     |  __validators__ = {'verbose': [<pydantic.class_validators.Validator obj...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  __call__(self, prompt: 'str', stop: 'Optional[List[str]]' = None, callbacks: 'Callbacks' = None, *, tags: 'Optional[List[str]]' = None, metadata: 'Optional[Dict[str, Any]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Check Cache and run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  __str__(self) -> 'str'\n",
      "     |      Get a string representation of the object for printing.\n",
      "     |  \n",
      "     |  async abatch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      "     |      Default implementation of abatch, which calls ainvoke N times.\n",
      "     |      Subclasses should override this method if they can batch more efficiently.\n",
      "     |  \n",
      "     |  async agenerate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  async agenerate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Asynchronously pass a sequence of prompts and return model generations.\n",
      "     |      \n",
      "     |      This method should make use of batched calls for models that expose a batched\n",
      "     |      API.\n",
      "     |      \n",
      "     |      Use this method when you want to:\n",
      "     |          1. take advantage of batched calls,\n",
      "     |          2. need more output from the model than just the top generated value,\n",
      "     |          3. are building chains that are agnostic to the underlying language model\n",
      "     |              type (e.g., pure text completion models vs chat models).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      "     |              converted to match the format of any language model (string for pure\n",
      "     |              text generation models and BaseMessages for chat models).\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          callbacks: Callbacks to pass through. Used for executing additional\n",
      "     |              functionality, such as logging or streaming, throughout generation.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          An LLMResult, which contains a list of candidate Generations for each input\n",
      "     |              prompt and additional model provider-specific output.\n",
      "     |  \n",
      "     |  async ainvoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Default implementation of ainvoke, which calls invoke in a thread pool.\n",
      "     |      Subclasses should override this method if they can run asynchronously.\n",
      "     |  \n",
      "     |  async apredict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Asynchronously pass a string to the model and return a string prediction.\n",
      "     |      \n",
      "     |      Use this method when calling pure text generation models and only the top\n",
      "     |          candidate generation is needed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: String input to pass to the model.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a string.\n",
      "     |  \n",
      "     |  async apredict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      "     |      Asynchronously pass messages to the model and return a message prediction.\n",
      "     |      \n",
      "     |      Use this method when calling chat models and only the top\n",
      "     |          candidate generation is needed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: A sequence of chat messages corresponding to a single model input.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a message.\n",
      "     |  \n",
      "     |  async astream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'AsyncIterator[str]'\n",
      "     |      Default implementation of astream, which calls ainvoke.\n",
      "     |      Subclasses should override this method if they support streaming output.\n",
      "     |  \n",
      "     |  batch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      "     |      Default implementation of batch, which calls invoke N times.\n",
      "     |      Subclasses should override this method if they can batch more efficiently.\n",
      "     |  \n",
      "     |  dict(self, **kwargs: 'Any') -> 'Dict'\n",
      "     |      Return a dictionary of the LLM.\n",
      "     |  \n",
      "     |  generate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  generate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Pass a sequence of prompts to the model and return model generations.\n",
      "     |      \n",
      "     |      This method should make use of batched calls for models that expose a batched\n",
      "     |      API.\n",
      "     |      \n",
      "     |      Use this method when you want to:\n",
      "     |          1. take advantage of batched calls,\n",
      "     |          2. need more output from the model than just the top generated value,\n",
      "     |          3. are building chains that are agnostic to the underlying language model\n",
      "     |              type (e.g., pure text completion models vs chat models).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      "     |              converted to match the format of any language model (string for pure\n",
      "     |              text generation models and BaseMessages for chat models).\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          callbacks: Callbacks to pass through. Used for executing additional\n",
      "     |              functionality, such as logging or streaming, throughout generation.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          An LLMResult, which contains a list of candidate Generations for each input\n",
      "     |              prompt and additional model provider-specific output.\n",
      "     |  \n",
      "     |  invoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |  \n",
      "     |  predict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Pass a single string input to the model and return a string prediction.\n",
      "     |      \n",
      "     |       Use this method when passing in raw text. If you want to pass in specific\n",
      "     |          types of chat messages, use predict_messages.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: String input to pass to the model.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a string.\n",
      "     |  \n",
      "     |  predict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      "     |      Pass a message sequence to the model and return a message prediction.\n",
      "     |      \n",
      "     |      Use this method when passing in chat messages. If you want to pass in raw text,\n",
      "     |          use predict.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: A sequence of chat messages corresponding to a single model input.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a message.\n",
      "     |  \n",
      "     |  save(self, file_path: 'Union[Path, str]') -> 'None'\n",
      "     |      Save the LLM.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          file_path: Path to file to save the LLM to.\n",
      "     |      \n",
      "     |      Example:\n",
      "     |      .. code-block:: python\n",
      "     |      \n",
      "     |          llm.save(file_path=\"path/llm.yaml\")\n",
      "     |  \n",
      "     |  stream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'Iterator[str]'\n",
      "     |      Default implementation of stream, which calls invoke.\n",
      "     |      Subclasses should override this method if they support streaming output.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  raise_deprecation(values: 'Dict') -> 'Dict' from pydantic.main.ModelMetaclass\n",
      "     |      Raise deprecation warning if callback_manager is used.\n",
      "     |  \n",
      "     |  set_verbose(verbose: 'Optional[bool]') -> 'bool' from pydantic.main.ModelMetaclass\n",
      "     |      If verbose is None, set it.\n",
      "     |      \n",
      "     |      This allows users to pass in None as verbose to access the global setting.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  __orig_bases__ = (langchain.schema.language_model.BaseLanguageModel[st...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.schema.language_model.BaseLanguageModel:\n",
      "     |  \n",
      "     |  get_num_tokens(self, text: 'str') -> 'int'\n",
      "     |      Get the number of tokens present in the text.\n",
      "     |      \n",
      "     |      Useful for checking if an input will fit in a model's context window.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: The string input to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The integer number of tokens in the text.\n",
      "     |  \n",
      "     |  get_num_tokens_from_messages(self, messages: 'List[BaseMessage]') -> 'int'\n",
      "     |      Get the number of tokens in the messages.\n",
      "     |      \n",
      "     |      Useful for checking if an input will fit in a model's context window.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: The message inputs to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The sum of the number of tokens across the messages.\n",
      "     |  \n",
      "     |  get_token_ids(self, text: 'str') -> 'List[int]'\n",
      "     |      Return the ordered ids of the tokens in a text.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: The string input to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          A list of ids corresponding to the tokens in the text, in order they occur\n",
      "     |              in the text.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from langchain.schema.language_model.BaseLanguageModel:\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.load.serializable.Serializable:\n",
      "     |  \n",
      "     |  __init__(self, **kwargs: Any) -> None\n",
      "     |      Create a new model by parsing and validating input data from keyword arguments.\n",
      "     |      \n",
      "     |      Raises ValidationError if the input data cannot be parsed to form a valid model.\n",
      "     |  \n",
      "     |  to_json(self) -> Union[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\n",
      "     |  \n",
      "     |  to_json_not_implemented(self) -> langchain.load.serializable.SerializedNotImplemented\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from langchain.load.serializable.Serializable:\n",
      "     |  \n",
      "     |  lc_attributes\n",
      "     |      Return a list of attribute names that should be included in the\n",
      "     |      serialized kwargs. These attributes must be accepted by the\n",
      "     |      constructor.\n",
      "     |  \n",
      "     |  lc_namespace\n",
      "     |      Return the namespace of the langchain object.\n",
      "     |      eg. [\"langchain\", \"llms\", \"openai\"]\n",
      "     |  \n",
      "     |  lc_secrets\n",
      "     |      Return a map of constructor argument names to secret ids.\n",
      "     |      eg. {\"openai_api_key\": \"OPENAI_API_KEY\"}\n",
      "     |  \n",
      "     |  lc_serializable\n",
      "     |      Return whether or not the class is serializable.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __eq__(self, other: Any) -> bool\n",
      "     |  \n",
      "     |  __getstate__(self) -> 'DictAny'\n",
      "     |  \n",
      "     |  __iter__(self) -> 'TupleGenerator'\n",
      "     |      so `dict(model)` works\n",
      "     |  \n",
      "     |  __repr_args__(self) -> 'ReprArgs'\n",
      "     |  \n",
      "     |  __setattr__(self, name, value)\n",
      "     |  \n",
      "     |  __setstate__(self, state: 'DictAny') -> None\n",
      "     |  \n",
      "     |  copy(self: 'Model', *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, update: Optional[ForwardRef('DictStrAny')] = None, deep: bool = False) -> 'Model'\n",
      "     |      Duplicate a model, optionally choose which fields to include, exclude and change.\n",
      "     |      \n",
      "     |      :param include: fields to include in new model\n",
      "     |      :param exclude: fields to exclude from new model, as with values this takes precedence over include\n",
      "     |      :param update: values to change/add in the new model. Note: the data is not validated before creating\n",
      "     |          the new model: you should trust this data\n",
      "     |      :param deep: set to `True` to make a deep copy of the model\n",
      "     |      :return: new model instance\n",
      "     |  \n",
      "     |  json(self, *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) -> 'unicode'\n",
      "     |      Generate a JSON representation of the model, `include` and `exclude` arguments as per `dict()`.\n",
      "     |      \n",
      "     |      `encoder` is an optional function to supply as `default` to json.dumps(), other arguments as per `json.dumps()`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __get_validators__() -> 'CallableGenerator' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  __try_update_forward_refs__(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      "     |      Same as update_forward_refs but will not raise exception\n",
      "     |      when forward references are not defined.\n",
      "     |  \n",
      "     |  construct(_fields_set: Optional[ForwardRef('SetStr')] = None, **values: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |      Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data.\n",
      "     |      Default values are respected, but no other validation is performed.\n",
      "     |      Behaves as if `Config.extra = 'allow'` was set since it adds all passed values\n",
      "     |  \n",
      "     |  from_orm(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_file(path: Union[str, pathlib.Path], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_obj(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_raw(b: Union[str, bytes], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema(by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}') -> 'DictStrAny' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema_json(*, by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}', **dumps_kwargs: Any) -> 'unicode' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  update_forward_refs(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      "     |      Try to update ForwardRefs on fields based on this Model, globalns and localns.\n",
      "     |  \n",
      "     |  validate(value: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __fields_set__\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.utils.Representation:\n",
      "     |  \n",
      "     |  __pretty__(self, fmt: Callable[[Any], Any], **kwargs: Any) -> Generator[Any, NoneType, NoneType]\n",
      "     |      Used by devtools (https://python-devtools.helpmanual.io/) to provide a human readable representations of objects\n",
      "     |  \n",
      "     |  __repr__(self) -> 'unicode'\n",
      "     |  \n",
      "     |  __repr_name__(self) -> 'unicode'\n",
      "     |      Name of the instance's class, used in __repr__.\n",
      "     |  \n",
      "     |  __repr_str__(self, join_str: 'unicode') -> 'unicode'\n",
      "     |  \n",
      "     |  __rich_repr__(self) -> 'RichReprResult'\n",
      "     |      Get fields for Rich library\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.schema.runnable.base.Runnable:\n",
      "     |  \n",
      "     |  __or__(self, other: 'Union[Runnable[Any, Other], Callable[[Any], Other], Mapping[str, Union[Runnable[Any, Other], Callable[[Any], Other], Any]]]') -> 'RunnableSequence[Input, Other]'\n",
      "     |      Return self|value.\n",
      "     |  \n",
      "     |  __ror__(self, other: 'Union[Runnable[Other, Any], Callable[[Any], Other], Mapping[str, Union[Runnable[Other, Any], Callable[[Other], Any], Any]]]') -> 'RunnableSequence[Other, Output]'\n",
      "     |      Return value|self.\n",
      "     |  \n",
      "     |  async atransform(self, input: 'AsyncIterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'AsyncIterator[Output]'\n",
      "     |      Default implementation of atransform, which buffers input and calls astream.\n",
      "     |      Subclasses should override this method if they can start producing output while\n",
      "     |      input is still being generated.\n",
      "     |  \n",
      "     |  bind(self, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      "     |      Bind arguments to a Runnable, returning a new Runnable.\n",
      "     |  \n",
      "     |  map(self) -> 'Runnable[List[Input], List[Output]]'\n",
      "     |      Return a new Runnable that maps a list of inputs to a list of outputs,\n",
      "     |      by calling invoke() with each input.\n",
      "     |  \n",
      "     |  transform(self, input: 'Iterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'Iterator[Output]'\n",
      "     |      Default implementation of transform, which buffers input and then calls stream.\n",
      "     |      Subclasses should override this method if they can start producing output while\n",
      "     |      input is still being generated.\n",
      "     |  \n",
      "     |  with_config(self, config: 'Optional[RunnableConfig]' = None, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      "     |      Bind config to a Runnable, returning a new Runnable.\n",
      "     |  \n",
      "     |  with_fallbacks(self, fallbacks: 'Sequence[Runnable[Input, Output]]', *, exceptions_to_handle: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,)) -> 'RunnableWithFallbacks[Input, Output]'\n",
      "     |  \n",
      "     |  with_retry(self, *, retry_if_exception_type: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,), wait_exponential_jitter: 'bool' = True, stop_after_attempt: 'int' = 3) -> 'Runnable[Input, Output]'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __class_getitem__(params) from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  __init_subclass__(*args, **kwargs) from pydantic.main.ModelMetaclass\n",
      "     |      This method is called when a class is subclassed.\n",
      "     |      \n",
      "     |      The default implementation does nothing. It may be\n",
      "     |      overridden to extend subclasses.\n",
      "    \n",
      "    class Baseten(langchain.llms.base.LLM)\n",
      "     |  Baseten(*, cache: Optional[bool] = None, verbose: bool = None, callbacks: Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager, NoneType] = None, callback_manager: Optional[langchain.callbacks.base.BaseCallbackManager] = None, tags: Optional[List[str]] = None, metadata: Optional[Dict[str, Any]] = None, model: str, input: Dict[str, Any] = None, model_kwargs: Dict[str, Any] = None) -> None\n",
      "     |  \n",
      "     |  Baseten models.\n",
      "     |  \n",
      "     |  To use, you should have the ``baseten`` python package installed,\n",
      "     |  and run ``baseten.login()`` with your Baseten API key.\n",
      "     |  \n",
      "     |  The required ``model`` param can be either a model id or model\n",
      "     |  version id. Using a model version ID will result in\n",
      "     |  slightly faster invocation.\n",
      "     |  Any other model parameters can also\n",
      "     |  be passed in with the format input={model_param: value, ...}\n",
      "     |  \n",
      "     |  The Baseten model must accept a dictionary of input with the key\n",
      "     |  \"prompt\" and return a dictionary with a key \"data\" which maps\n",
      "     |  to a list of response strings.\n",
      "     |  \n",
      "     |  Example:\n",
      "     |      .. code-block:: python\n",
      "     |          from langchain.llms import Baseten\n",
      "     |          my_model = Baseten(model=\"MODEL_ID\")\n",
      "     |          output = my_model(\"prompt\")\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Baseten\n",
      "     |      langchain.llms.base.LLM\n",
      "     |      langchain.llms.base.BaseLLM\n",
      "     |      langchain.schema.language_model.BaseLanguageModel\n",
      "     |      langchain.load.serializable.Serializable\n",
      "     |      pydantic.main.BaseModel\n",
      "     |      pydantic.utils.Representation\n",
      "     |      langchain.schema.runnable.base.Runnable\n",
      "     |      typing.Generic\n",
      "     |      abc.ABC\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __json_encoder__ = pydantic_encoder(obj: Any) -> Any\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {'input': typing.Dict[str, typing.Any], 'model': <cl...\n",
      "     |  \n",
      "     |  __class_vars__ = set()\n",
      "     |  \n",
      "     |  __config__ = <class 'langchain.llms.baseten.Config'>\n",
      "     |  \n",
      "     |  __custom_root_type__ = False\n",
      "     |  \n",
      "     |  __exclude_fields__ = {'callback_manager': True, 'callbacks': True, 'me...\n",
      "     |  \n",
      "     |  __fields__ = {'cache': ModelField(name='cache', type=Optional[bool], r...\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "     |  \n",
      "     |  __include_fields__ = None\n",
      "     |  \n",
      "     |  __parameters__ = ()\n",
      "     |  \n",
      "     |  __post_root_validators__ = [(False, <function BaseLLM.raise_deprecatio...\n",
      "     |  \n",
      "     |  __pre_root_validators__ = []\n",
      "     |  \n",
      "     |  __private_attributes__ = {'_lc_kwargs': ModelPrivateAttr(default=Pydan...\n",
      "     |  \n",
      "     |  __schema_cache__ = {}\n",
      "     |  \n",
      "     |  __signature__ = <Signature (*, cache: Optional[bool] = None, ver...ne,...\n",
      "     |  \n",
      "     |  __validators__ = {'verbose': [<pydantic.class_validators.Validator obj...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  __call__(self, prompt: 'str', stop: 'Optional[List[str]]' = None, callbacks: 'Callbacks' = None, *, tags: 'Optional[List[str]]' = None, metadata: 'Optional[Dict[str, Any]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Check Cache and run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  __str__(self) -> 'str'\n",
      "     |      Get a string representation of the object for printing.\n",
      "     |  \n",
      "     |  async abatch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      "     |      Default implementation of abatch, which calls ainvoke N times.\n",
      "     |      Subclasses should override this method if they can batch more efficiently.\n",
      "     |  \n",
      "     |  async agenerate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  async agenerate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Asynchronously pass a sequence of prompts and return model generations.\n",
      "     |      \n",
      "     |      This method should make use of batched calls for models that expose a batched\n",
      "     |      API.\n",
      "     |      \n",
      "     |      Use this method when you want to:\n",
      "     |          1. take advantage of batched calls,\n",
      "     |          2. need more output from the model than just the top generated value,\n",
      "     |          3. are building chains that are agnostic to the underlying language model\n",
      "     |              type (e.g., pure text completion models vs chat models).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      "     |              converted to match the format of any language model (string for pure\n",
      "     |              text generation models and BaseMessages for chat models).\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          callbacks: Callbacks to pass through. Used for executing additional\n",
      "     |              functionality, such as logging or streaming, throughout generation.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          An LLMResult, which contains a list of candidate Generations for each input\n",
      "     |              prompt and additional model provider-specific output.\n",
      "     |  \n",
      "     |  async ainvoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Default implementation of ainvoke, which calls invoke in a thread pool.\n",
      "     |      Subclasses should override this method if they can run asynchronously.\n",
      "     |  \n",
      "     |  async apredict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Asynchronously pass a string to the model and return a string prediction.\n",
      "     |      \n",
      "     |      Use this method when calling pure text generation models and only the top\n",
      "     |          candidate generation is needed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: String input to pass to the model.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a string.\n",
      "     |  \n",
      "     |  async apredict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      "     |      Asynchronously pass messages to the model and return a message prediction.\n",
      "     |      \n",
      "     |      Use this method when calling chat models and only the top\n",
      "     |          candidate generation is needed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: A sequence of chat messages corresponding to a single model input.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a message.\n",
      "     |  \n",
      "     |  async astream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'AsyncIterator[str]'\n",
      "     |      Default implementation of astream, which calls ainvoke.\n",
      "     |      Subclasses should override this method if they support streaming output.\n",
      "     |  \n",
      "     |  batch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      "     |      Default implementation of batch, which calls invoke N times.\n",
      "     |      Subclasses should override this method if they can batch more efficiently.\n",
      "     |  \n",
      "     |  dict(self, **kwargs: 'Any') -> 'Dict'\n",
      "     |      Return a dictionary of the LLM.\n",
      "     |  \n",
      "     |  generate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  generate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Pass a sequence of prompts to the model and return model generations.\n",
      "     |      \n",
      "     |      This method should make use of batched calls for models that expose a batched\n",
      "     |      API.\n",
      "     |      \n",
      "     |      Use this method when you want to:\n",
      "     |          1. take advantage of batched calls,\n",
      "     |          2. need more output from the model than just the top generated value,\n",
      "     |          3. are building chains that are agnostic to the underlying language model\n",
      "     |              type (e.g., pure text completion models vs chat models).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      "     |              converted to match the format of any language model (string for pure\n",
      "     |              text generation models and BaseMessages for chat models).\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          callbacks: Callbacks to pass through. Used for executing additional\n",
      "     |              functionality, such as logging or streaming, throughout generation.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          An LLMResult, which contains a list of candidate Generations for each input\n",
      "     |              prompt and additional model provider-specific output.\n",
      "     |  \n",
      "     |  invoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |  \n",
      "     |  predict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Pass a single string input to the model and return a string prediction.\n",
      "     |      \n",
      "     |       Use this method when passing in raw text. If you want to pass in specific\n",
      "     |          types of chat messages, use predict_messages.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: String input to pass to the model.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a string.\n",
      "     |  \n",
      "     |  predict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      "     |      Pass a message sequence to the model and return a message prediction.\n",
      "     |      \n",
      "     |      Use this method when passing in chat messages. If you want to pass in raw text,\n",
      "     |          use predict.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: A sequence of chat messages corresponding to a single model input.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a message.\n",
      "     |  \n",
      "     |  save(self, file_path: 'Union[Path, str]') -> 'None'\n",
      "     |      Save the LLM.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          file_path: Path to file to save the LLM to.\n",
      "     |      \n",
      "     |      Example:\n",
      "     |      .. code-block:: python\n",
      "     |      \n",
      "     |          llm.save(file_path=\"path/llm.yaml\")\n",
      "     |  \n",
      "     |  stream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'Iterator[str]'\n",
      "     |      Default implementation of stream, which calls invoke.\n",
      "     |      Subclasses should override this method if they support streaming output.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  raise_deprecation(values: 'Dict') -> 'Dict' from pydantic.main.ModelMetaclass\n",
      "     |      Raise deprecation warning if callback_manager is used.\n",
      "     |  \n",
      "     |  set_verbose(verbose: 'Optional[bool]') -> 'bool' from pydantic.main.ModelMetaclass\n",
      "     |      If verbose is None, set it.\n",
      "     |      \n",
      "     |      This allows users to pass in None as verbose to access the global setting.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  Config = <class 'langchain.llms.base.BaseLLM.Config'>\n",
      "     |      Configuration for this pydantic object.\n",
      "     |  \n",
      "     |  \n",
      "     |  __orig_bases__ = (langchain.schema.language_model.BaseLanguageModel[st...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.schema.language_model.BaseLanguageModel:\n",
      "     |  \n",
      "     |  get_num_tokens(self, text: 'str') -> 'int'\n",
      "     |      Get the number of tokens present in the text.\n",
      "     |      \n",
      "     |      Useful for checking if an input will fit in a model's context window.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: The string input to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The integer number of tokens in the text.\n",
      "     |  \n",
      "     |  get_num_tokens_from_messages(self, messages: 'List[BaseMessage]') -> 'int'\n",
      "     |      Get the number of tokens in the messages.\n",
      "     |      \n",
      "     |      Useful for checking if an input will fit in a model's context window.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: The message inputs to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The sum of the number of tokens across the messages.\n",
      "     |  \n",
      "     |  get_token_ids(self, text: 'str') -> 'List[int]'\n",
      "     |      Return the ordered ids of the tokens in a text.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: The string input to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          A list of ids corresponding to the tokens in the text, in order they occur\n",
      "     |              in the text.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from langchain.schema.language_model.BaseLanguageModel:\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.load.serializable.Serializable:\n",
      "     |  \n",
      "     |  __init__(self, **kwargs: Any) -> None\n",
      "     |      Create a new model by parsing and validating input data from keyword arguments.\n",
      "     |      \n",
      "     |      Raises ValidationError if the input data cannot be parsed to form a valid model.\n",
      "     |  \n",
      "     |  to_json(self) -> Union[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\n",
      "     |  \n",
      "     |  to_json_not_implemented(self) -> langchain.load.serializable.SerializedNotImplemented\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from langchain.load.serializable.Serializable:\n",
      "     |  \n",
      "     |  lc_attributes\n",
      "     |      Return a list of attribute names that should be included in the\n",
      "     |      serialized kwargs. These attributes must be accepted by the\n",
      "     |      constructor.\n",
      "     |  \n",
      "     |  lc_namespace\n",
      "     |      Return the namespace of the langchain object.\n",
      "     |      eg. [\"langchain\", \"llms\", \"openai\"]\n",
      "     |  \n",
      "     |  lc_secrets\n",
      "     |      Return a map of constructor argument names to secret ids.\n",
      "     |      eg. {\"openai_api_key\": \"OPENAI_API_KEY\"}\n",
      "     |  \n",
      "     |  lc_serializable\n",
      "     |      Return whether or not the class is serializable.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __eq__(self, other: Any) -> bool\n",
      "     |  \n",
      "     |  __getstate__(self) -> 'DictAny'\n",
      "     |  \n",
      "     |  __iter__(self) -> 'TupleGenerator'\n",
      "     |      so `dict(model)` works\n",
      "     |  \n",
      "     |  __repr_args__(self) -> 'ReprArgs'\n",
      "     |  \n",
      "     |  __setattr__(self, name, value)\n",
      "     |  \n",
      "     |  __setstate__(self, state: 'DictAny') -> None\n",
      "     |  \n",
      "     |  copy(self: 'Model', *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, update: Optional[ForwardRef('DictStrAny')] = None, deep: bool = False) -> 'Model'\n",
      "     |      Duplicate a model, optionally choose which fields to include, exclude and change.\n",
      "     |      \n",
      "     |      :param include: fields to include in new model\n",
      "     |      :param exclude: fields to exclude from new model, as with values this takes precedence over include\n",
      "     |      :param update: values to change/add in the new model. Note: the data is not validated before creating\n",
      "     |          the new model: you should trust this data\n",
      "     |      :param deep: set to `True` to make a deep copy of the model\n",
      "     |      :return: new model instance\n",
      "     |  \n",
      "     |  json(self, *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) -> 'unicode'\n",
      "     |      Generate a JSON representation of the model, `include` and `exclude` arguments as per `dict()`.\n",
      "     |      \n",
      "     |      `encoder` is an optional function to supply as `default` to json.dumps(), other arguments as per `json.dumps()`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __get_validators__() -> 'CallableGenerator' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  __try_update_forward_refs__(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      "     |      Same as update_forward_refs but will not raise exception\n",
      "     |      when forward references are not defined.\n",
      "     |  \n",
      "     |  construct(_fields_set: Optional[ForwardRef('SetStr')] = None, **values: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |      Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data.\n",
      "     |      Default values are respected, but no other validation is performed.\n",
      "     |      Behaves as if `Config.extra = 'allow'` was set since it adds all passed values\n",
      "     |  \n",
      "     |  from_orm(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_file(path: Union[str, pathlib.Path], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_obj(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_raw(b: Union[str, bytes], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema(by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}') -> 'DictStrAny' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema_json(*, by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}', **dumps_kwargs: Any) -> 'unicode' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  update_forward_refs(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      "     |      Try to update ForwardRefs on fields based on this Model, globalns and localns.\n",
      "     |  \n",
      "     |  validate(value: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __fields_set__\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.utils.Representation:\n",
      "     |  \n",
      "     |  __pretty__(self, fmt: Callable[[Any], Any], **kwargs: Any) -> Generator[Any, NoneType, NoneType]\n",
      "     |      Used by devtools (https://python-devtools.helpmanual.io/) to provide a human readable representations of objects\n",
      "     |  \n",
      "     |  __repr__(self) -> 'unicode'\n",
      "     |  \n",
      "     |  __repr_name__(self) -> 'unicode'\n",
      "     |      Name of the instance's class, used in __repr__.\n",
      "     |  \n",
      "     |  __repr_str__(self, join_str: 'unicode') -> 'unicode'\n",
      "     |  \n",
      "     |  __rich_repr__(self) -> 'RichReprResult'\n",
      "     |      Get fields for Rich library\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.schema.runnable.base.Runnable:\n",
      "     |  \n",
      "     |  __or__(self, other: 'Union[Runnable[Any, Other], Callable[[Any], Other], Mapping[str, Union[Runnable[Any, Other], Callable[[Any], Other], Any]]]') -> 'RunnableSequence[Input, Other]'\n",
      "     |      Return self|value.\n",
      "     |  \n",
      "     |  __ror__(self, other: 'Union[Runnable[Other, Any], Callable[[Any], Other], Mapping[str, Union[Runnable[Other, Any], Callable[[Other], Any], Any]]]') -> 'RunnableSequence[Other, Output]'\n",
      "     |      Return value|self.\n",
      "     |  \n",
      "     |  async atransform(self, input: 'AsyncIterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'AsyncIterator[Output]'\n",
      "     |      Default implementation of atransform, which buffers input and calls astream.\n",
      "     |      Subclasses should override this method if they can start producing output while\n",
      "     |      input is still being generated.\n",
      "     |  \n",
      "     |  bind(self, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      "     |      Bind arguments to a Runnable, returning a new Runnable.\n",
      "     |  \n",
      "     |  map(self) -> 'Runnable[List[Input], List[Output]]'\n",
      "     |      Return a new Runnable that maps a list of inputs to a list of outputs,\n",
      "     |      by calling invoke() with each input.\n",
      "     |  \n",
      "     |  transform(self, input: 'Iterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'Iterator[Output]'\n",
      "     |      Default implementation of transform, which buffers input and then calls stream.\n",
      "     |      Subclasses should override this method if they can start producing output while\n",
      "     |      input is still being generated.\n",
      "     |  \n",
      "     |  with_config(self, config: 'Optional[RunnableConfig]' = None, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      "     |      Bind config to a Runnable, returning a new Runnable.\n",
      "     |  \n",
      "     |  with_fallbacks(self, fallbacks: 'Sequence[Runnable[Input, Output]]', *, exceptions_to_handle: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,)) -> 'RunnableWithFallbacks[Input, Output]'\n",
      "     |  \n",
      "     |  with_retry(self, *, retry_if_exception_type: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,), wait_exponential_jitter: 'bool' = True, stop_after_attempt: 'int' = 3) -> 'Runnable[Input, Output]'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __class_getitem__(params) from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  __init_subclass__(*args, **kwargs) from pydantic.main.ModelMetaclass\n",
      "     |      This method is called when a class is subclassed.\n",
      "     |      \n",
      "     |      The default implementation does nothing. It may be\n",
      "     |      overridden to extend subclasses.\n",
      "    \n",
      "    class Beam(langchain.llms.base.LLM)\n",
      "     |  Beam(*, cache: Optional[bool] = None, verbose: bool = None, callbacks: Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager, NoneType] = None, callback_manager: Optional[langchain.callbacks.base.BaseCallbackManager] = None, tags: Optional[List[str]] = None, metadata: Optional[Dict[str, Any]] = None, model_name: str = '', name: str = '', cpu: str = '', memory: str = '', gpu: str = '', python_version: str = '', python_packages: List[str] = [], max_length: str = '', url: str = '', model_kwargs: Dict[str, Any] = None, beam_client_id: str = '', beam_client_secret: str = '', app_id: Optional[str] = None) -> None\n",
      "     |  \n",
      "     |  Beam API for gpt2 large language model.\n",
      "     |  \n",
      "     |  To use, you should have the ``beam-sdk`` python package installed,\n",
      "     |  and the environment variable ``BEAM_CLIENT_ID`` set with your client id\n",
      "     |  and ``BEAM_CLIENT_SECRET`` set with your client secret. Information on how\n",
      "     |  to get this is available here: https://docs.beam.cloud/account/api-keys.\n",
      "     |  \n",
      "     |  The wrapper can then be called as follows, where the name, cpu, memory, gpu,\n",
      "     |  python version, and python packages can be updated accordingly. Once deployed,\n",
      "     |  the instance can be called.\n",
      "     |  \n",
      "     |  Example:\n",
      "     |      .. code-block:: python\n",
      "     |  \n",
      "     |          llm = Beam(model_name=\"gpt2\",\n",
      "     |              name=\"langchain-gpt2\",\n",
      "     |              cpu=8,\n",
      "     |              memory=\"32Gi\",\n",
      "     |              gpu=\"A10G\",\n",
      "     |              python_version=\"python3.8\",\n",
      "     |              python_packages=[\n",
      "     |                  \"diffusers[torch]>=0.10\",\n",
      "     |                  \"transformers\",\n",
      "     |                  \"torch\",\n",
      "     |                  \"pillow\",\n",
      "     |                  \"accelerate\",\n",
      "     |                  \"safetensors\",\n",
      "     |                  \"xformers\",],\n",
      "     |              max_length=50)\n",
      "     |          llm._deploy()\n",
      "     |          call_result = llm._call(input)\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Beam\n",
      "     |      langchain.llms.base.LLM\n",
      "     |      langchain.llms.base.BaseLLM\n",
      "     |      langchain.schema.language_model.BaseLanguageModel\n",
      "     |      langchain.load.serializable.Serializable\n",
      "     |      pydantic.main.BaseModel\n",
      "     |      pydantic.utils.Representation\n",
      "     |      langchain.schema.runnable.base.Runnable\n",
      "     |      typing.Generic\n",
      "     |      abc.ABC\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  app_creation(self) -> None\n",
      "     |      Creates a Python file which will contain your Beam app definition.\n",
      "     |  \n",
      "     |  run_creation(self) -> None\n",
      "     |      Creates a Python file which will be deployed on beam.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  build_extra(values: Dict[str, Any]) -> Dict[str, Any] from pydantic.main.ModelMetaclass\n",
      "     |      Build extra kwargs from additional params that were passed in.\n",
      "     |  \n",
      "     |  validate_environment(values: Dict) -> Dict from pydantic.main.ModelMetaclass\n",
      "     |      Validate that api key and python package exists in environment.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __json_encoder__ = pydantic_encoder(obj: Any) -> Any\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  authorization\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  Config = <class 'langchain.llms.beam.Beam.Config'>\n",
      "     |      Configuration for this pydantic config.\n",
      "     |  \n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {'app_id': typing.Optional[str], 'beam_client_id': <...\n",
      "     |  \n",
      "     |  __class_vars__ = set()\n",
      "     |  \n",
      "     |  __config__ = <class 'langchain.llms.beam.Config'>\n",
      "     |  \n",
      "     |  __custom_root_type__ = False\n",
      "     |  \n",
      "     |  __exclude_fields__ = {'callback_manager': True, 'callbacks': True, 'me...\n",
      "     |  \n",
      "     |  __fields__ = {'app_id': ModelField(name='app_id', type=Optional[str], ...\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "     |  \n",
      "     |  __include_fields__ = None\n",
      "     |  \n",
      "     |  __parameters__ = ()\n",
      "     |  \n",
      "     |  __post_root_validators__ = [(False, <function BaseLLM.raise_deprecatio...\n",
      "     |  \n",
      "     |  __pre_root_validators__ = [<function Beam.build_extra>]\n",
      "     |  \n",
      "     |  __private_attributes__ = {'_lc_kwargs': ModelPrivateAttr(default=Pydan...\n",
      "     |  \n",
      "     |  __schema_cache__ = {}\n",
      "     |  \n",
      "     |  __signature__ = <Signature (*, cache: Optional[bool] = None, ver... st...\n",
      "     |  \n",
      "     |  __validators__ = {'verbose': [<pydantic.class_validators.Validator obj...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  __call__(self, prompt: 'str', stop: 'Optional[List[str]]' = None, callbacks: 'Callbacks' = None, *, tags: 'Optional[List[str]]' = None, metadata: 'Optional[Dict[str, Any]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Check Cache and run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  __str__(self) -> 'str'\n",
      "     |      Get a string representation of the object for printing.\n",
      "     |  \n",
      "     |  async abatch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      "     |      Default implementation of abatch, which calls ainvoke N times.\n",
      "     |      Subclasses should override this method if they can batch more efficiently.\n",
      "     |  \n",
      "     |  async agenerate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  async agenerate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Asynchronously pass a sequence of prompts and return model generations.\n",
      "     |      \n",
      "     |      This method should make use of batched calls for models that expose a batched\n",
      "     |      API.\n",
      "     |      \n",
      "     |      Use this method when you want to:\n",
      "     |          1. take advantage of batched calls,\n",
      "     |          2. need more output from the model than just the top generated value,\n",
      "     |          3. are building chains that are agnostic to the underlying language model\n",
      "     |              type (e.g., pure text completion models vs chat models).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      "     |              converted to match the format of any language model (string for pure\n",
      "     |              text generation models and BaseMessages for chat models).\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          callbacks: Callbacks to pass through. Used for executing additional\n",
      "     |              functionality, such as logging or streaming, throughout generation.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          An LLMResult, which contains a list of candidate Generations for each input\n",
      "     |              prompt and additional model provider-specific output.\n",
      "     |  \n",
      "     |  async ainvoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Default implementation of ainvoke, which calls invoke in a thread pool.\n",
      "     |      Subclasses should override this method if they can run asynchronously.\n",
      "     |  \n",
      "     |  async apredict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Asynchronously pass a string to the model and return a string prediction.\n",
      "     |      \n",
      "     |      Use this method when calling pure text generation models and only the top\n",
      "     |          candidate generation is needed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: String input to pass to the model.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a string.\n",
      "     |  \n",
      "     |  async apredict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      "     |      Asynchronously pass messages to the model and return a message prediction.\n",
      "     |      \n",
      "     |      Use this method when calling chat models and only the top\n",
      "     |          candidate generation is needed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: A sequence of chat messages corresponding to a single model input.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a message.\n",
      "     |  \n",
      "     |  async astream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'AsyncIterator[str]'\n",
      "     |      Default implementation of astream, which calls ainvoke.\n",
      "     |      Subclasses should override this method if they support streaming output.\n",
      "     |  \n",
      "     |  batch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      "     |      Default implementation of batch, which calls invoke N times.\n",
      "     |      Subclasses should override this method if they can batch more efficiently.\n",
      "     |  \n",
      "     |  dict(self, **kwargs: 'Any') -> 'Dict'\n",
      "     |      Return a dictionary of the LLM.\n",
      "     |  \n",
      "     |  generate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  generate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Pass a sequence of prompts to the model and return model generations.\n",
      "     |      \n",
      "     |      This method should make use of batched calls for models that expose a batched\n",
      "     |      API.\n",
      "     |      \n",
      "     |      Use this method when you want to:\n",
      "     |          1. take advantage of batched calls,\n",
      "     |          2. need more output from the model than just the top generated value,\n",
      "     |          3. are building chains that are agnostic to the underlying language model\n",
      "     |              type (e.g., pure text completion models vs chat models).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      "     |              converted to match the format of any language model (string for pure\n",
      "     |              text generation models and BaseMessages for chat models).\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          callbacks: Callbacks to pass through. Used for executing additional\n",
      "     |              functionality, such as logging or streaming, throughout generation.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          An LLMResult, which contains a list of candidate Generations for each input\n",
      "     |              prompt and additional model provider-specific output.\n",
      "     |  \n",
      "     |  invoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |  \n",
      "     |  predict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Pass a single string input to the model and return a string prediction.\n",
      "     |      \n",
      "     |       Use this method when passing in raw text. If you want to pass in specific\n",
      "     |          types of chat messages, use predict_messages.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: String input to pass to the model.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a string.\n",
      "     |  \n",
      "     |  predict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      "     |      Pass a message sequence to the model and return a message prediction.\n",
      "     |      \n",
      "     |      Use this method when passing in chat messages. If you want to pass in raw text,\n",
      "     |          use predict.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: A sequence of chat messages corresponding to a single model input.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a message.\n",
      "     |  \n",
      "     |  save(self, file_path: 'Union[Path, str]') -> 'None'\n",
      "     |      Save the LLM.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          file_path: Path to file to save the LLM to.\n",
      "     |      \n",
      "     |      Example:\n",
      "     |      .. code-block:: python\n",
      "     |      \n",
      "     |          llm.save(file_path=\"path/llm.yaml\")\n",
      "     |  \n",
      "     |  stream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'Iterator[str]'\n",
      "     |      Default implementation of stream, which calls invoke.\n",
      "     |      Subclasses should override this method if they support streaming output.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  raise_deprecation(values: 'Dict') -> 'Dict' from pydantic.main.ModelMetaclass\n",
      "     |      Raise deprecation warning if callback_manager is used.\n",
      "     |  \n",
      "     |  set_verbose(verbose: 'Optional[bool]') -> 'bool' from pydantic.main.ModelMetaclass\n",
      "     |      If verbose is None, set it.\n",
      "     |      \n",
      "     |      This allows users to pass in None as verbose to access the global setting.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  __orig_bases__ = (langchain.schema.language_model.BaseLanguageModel[st...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.schema.language_model.BaseLanguageModel:\n",
      "     |  \n",
      "     |  get_num_tokens(self, text: 'str') -> 'int'\n",
      "     |      Get the number of tokens present in the text.\n",
      "     |      \n",
      "     |      Useful for checking if an input will fit in a model's context window.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: The string input to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The integer number of tokens in the text.\n",
      "     |  \n",
      "     |  get_num_tokens_from_messages(self, messages: 'List[BaseMessage]') -> 'int'\n",
      "     |      Get the number of tokens in the messages.\n",
      "     |      \n",
      "     |      Useful for checking if an input will fit in a model's context window.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: The message inputs to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The sum of the number of tokens across the messages.\n",
      "     |  \n",
      "     |  get_token_ids(self, text: 'str') -> 'List[int]'\n",
      "     |      Return the ordered ids of the tokens in a text.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: The string input to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          A list of ids corresponding to the tokens in the text, in order they occur\n",
      "     |              in the text.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from langchain.schema.language_model.BaseLanguageModel:\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.load.serializable.Serializable:\n",
      "     |  \n",
      "     |  __init__(self, **kwargs: Any) -> None\n",
      "     |      Create a new model by parsing and validating input data from keyword arguments.\n",
      "     |      \n",
      "     |      Raises ValidationError if the input data cannot be parsed to form a valid model.\n",
      "     |  \n",
      "     |  to_json(self) -> Union[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\n",
      "     |  \n",
      "     |  to_json_not_implemented(self) -> langchain.load.serializable.SerializedNotImplemented\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from langchain.load.serializable.Serializable:\n",
      "     |  \n",
      "     |  lc_attributes\n",
      "     |      Return a list of attribute names that should be included in the\n",
      "     |      serialized kwargs. These attributes must be accepted by the\n",
      "     |      constructor.\n",
      "     |  \n",
      "     |  lc_namespace\n",
      "     |      Return the namespace of the langchain object.\n",
      "     |      eg. [\"langchain\", \"llms\", \"openai\"]\n",
      "     |  \n",
      "     |  lc_secrets\n",
      "     |      Return a map of constructor argument names to secret ids.\n",
      "     |      eg. {\"openai_api_key\": \"OPENAI_API_KEY\"}\n",
      "     |  \n",
      "     |  lc_serializable\n",
      "     |      Return whether or not the class is serializable.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __eq__(self, other: Any) -> bool\n",
      "     |  \n",
      "     |  __getstate__(self) -> 'DictAny'\n",
      "     |  \n",
      "     |  __iter__(self) -> 'TupleGenerator'\n",
      "     |      so `dict(model)` works\n",
      "     |  \n",
      "     |  __repr_args__(self) -> 'ReprArgs'\n",
      "     |  \n",
      "     |  __setattr__(self, name, value)\n",
      "     |  \n",
      "     |  __setstate__(self, state: 'DictAny') -> None\n",
      "     |  \n",
      "     |  copy(self: 'Model', *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, update: Optional[ForwardRef('DictStrAny')] = None, deep: bool = False) -> 'Model'\n",
      "     |      Duplicate a model, optionally choose which fields to include, exclude and change.\n",
      "     |      \n",
      "     |      :param include: fields to include in new model\n",
      "     |      :param exclude: fields to exclude from new model, as with values this takes precedence over include\n",
      "     |      :param update: values to change/add in the new model. Note: the data is not validated before creating\n",
      "     |          the new model: you should trust this data\n",
      "     |      :param deep: set to `True` to make a deep copy of the model\n",
      "     |      :return: new model instance\n",
      "     |  \n",
      "     |  json(self, *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) -> 'unicode'\n",
      "     |      Generate a JSON representation of the model, `include` and `exclude` arguments as per `dict()`.\n",
      "     |      \n",
      "     |      `encoder` is an optional function to supply as `default` to json.dumps(), other arguments as per `json.dumps()`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __get_validators__() -> 'CallableGenerator' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  __try_update_forward_refs__(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      "     |      Same as update_forward_refs but will not raise exception\n",
      "     |      when forward references are not defined.\n",
      "     |  \n",
      "     |  construct(_fields_set: Optional[ForwardRef('SetStr')] = None, **values: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |      Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data.\n",
      "     |      Default values are respected, but no other validation is performed.\n",
      "     |      Behaves as if `Config.extra = 'allow'` was set since it adds all passed values\n",
      "     |  \n",
      "     |  from_orm(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_file(path: Union[str, pathlib.Path], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_obj(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_raw(b: Union[str, bytes], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema(by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}') -> 'DictStrAny' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema_json(*, by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}', **dumps_kwargs: Any) -> 'unicode' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  update_forward_refs(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      "     |      Try to update ForwardRefs on fields based on this Model, globalns and localns.\n",
      "     |  \n",
      "     |  validate(value: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __fields_set__\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.utils.Representation:\n",
      "     |  \n",
      "     |  __pretty__(self, fmt: Callable[[Any], Any], **kwargs: Any) -> Generator[Any, NoneType, NoneType]\n",
      "     |      Used by devtools (https://python-devtools.helpmanual.io/) to provide a human readable representations of objects\n",
      "     |  \n",
      "     |  __repr__(self) -> 'unicode'\n",
      "     |  \n",
      "     |  __repr_name__(self) -> 'unicode'\n",
      "     |      Name of the instance's class, used in __repr__.\n",
      "     |  \n",
      "     |  __repr_str__(self, join_str: 'unicode') -> 'unicode'\n",
      "     |  \n",
      "     |  __rich_repr__(self) -> 'RichReprResult'\n",
      "     |      Get fields for Rich library\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.schema.runnable.base.Runnable:\n",
      "     |  \n",
      "     |  __or__(self, other: 'Union[Runnable[Any, Other], Callable[[Any], Other], Mapping[str, Union[Runnable[Any, Other], Callable[[Any], Other], Any]]]') -> 'RunnableSequence[Input, Other]'\n",
      "     |      Return self|value.\n",
      "     |  \n",
      "     |  __ror__(self, other: 'Union[Runnable[Other, Any], Callable[[Any], Other], Mapping[str, Union[Runnable[Other, Any], Callable[[Other], Any], Any]]]') -> 'RunnableSequence[Other, Output]'\n",
      "     |      Return value|self.\n",
      "     |  \n",
      "     |  async atransform(self, input: 'AsyncIterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'AsyncIterator[Output]'\n",
      "     |      Default implementation of atransform, which buffers input and calls astream.\n",
      "     |      Subclasses should override this method if they can start producing output while\n",
      "     |      input is still being generated.\n",
      "     |  \n",
      "     |  bind(self, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      "     |      Bind arguments to a Runnable, returning a new Runnable.\n",
      "     |  \n",
      "     |  map(self) -> 'Runnable[List[Input], List[Output]]'\n",
      "     |      Return a new Runnable that maps a list of inputs to a list of outputs,\n",
      "     |      by calling invoke() with each input.\n",
      "     |  \n",
      "     |  transform(self, input: 'Iterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'Iterator[Output]'\n",
      "     |      Default implementation of transform, which buffers input and then calls stream.\n",
      "     |      Subclasses should override this method if they can start producing output while\n",
      "     |      input is still being generated.\n",
      "     |  \n",
      "     |  with_config(self, config: 'Optional[RunnableConfig]' = None, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      "     |      Bind config to a Runnable, returning a new Runnable.\n",
      "     |  \n",
      "     |  with_fallbacks(self, fallbacks: 'Sequence[Runnable[Input, Output]]', *, exceptions_to_handle: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,)) -> 'RunnableWithFallbacks[Input, Output]'\n",
      "     |  \n",
      "     |  with_retry(self, *, retry_if_exception_type: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,), wait_exponential_jitter: 'bool' = True, stop_after_attempt: 'int' = 3) -> 'Runnable[Input, Output]'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __class_getitem__(params) from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  __init_subclass__(*args, **kwargs) from pydantic.main.ModelMetaclass\n",
      "     |      This method is called when a class is subclassed.\n",
      "     |      \n",
      "     |      The default implementation does nothing. It may be\n",
      "     |      overridden to extend subclasses.\n",
      "    \n",
      "    class Bedrock(langchain.llms.base.LLM, BedrockBase)\n",
      "     |  Bedrock(*, client: Any = None, region_name: Optional[str] = None, credentials_profile_name: Optional[str] = None, model_id: str, model_kwargs: Optional[Dict] = None, endpoint_url: Optional[str] = None, cache: Optional[bool] = None, verbose: bool = None, callbacks: Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager, NoneType] = None, callback_manager: Optional[langchain.callbacks.base.BaseCallbackManager] = None, tags: Optional[List[str]] = None, metadata: Optional[Dict[str, Any]] = None) -> None\n",
      "     |  \n",
      "     |  Bedrock models.\n",
      "     |  \n",
      "     |  To authenticate, the AWS client uses the following methods to\n",
      "     |  automatically load credentials:\n",
      "     |  https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html\n",
      "     |  \n",
      "     |  If a specific credential profile should be used, you must pass\n",
      "     |  the name of the profile from the ~/.aws/credentials file that is to be used.\n",
      "     |  \n",
      "     |  Make sure the credentials / roles used have the required policies to\n",
      "     |  access the Bedrock service.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Bedrock\n",
      "     |      langchain.llms.base.LLM\n",
      "     |      langchain.llms.base.BaseLLM\n",
      "     |      langchain.schema.language_model.BaseLanguageModel\n",
      "     |      langchain.load.serializable.Serializable\n",
      "     |      BedrockBase\n",
      "     |      pydantic.main.BaseModel\n",
      "     |      pydantic.utils.Representation\n",
      "     |      langchain.schema.runnable.base.Runnable\n",
      "     |      typing.Generic\n",
      "     |      abc.ABC\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __json_encoder__ = pydantic_encoder(obj: Any) -> Any\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  Config = <class 'langchain.llms.bedrock.Bedrock.Config'>\n",
      "     |      Configuration for this pydantic object.\n",
      "     |  \n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {}\n",
      "     |  \n",
      "     |  __class_vars__ = set()\n",
      "     |  \n",
      "     |  __config__ = <class 'langchain.llms.bedrock.Config'>\n",
      "     |  \n",
      "     |  __custom_root_type__ = False\n",
      "     |  \n",
      "     |  __exclude_fields__ = {'callback_manager': True, 'callbacks': True, 'me...\n",
      "     |  \n",
      "     |  __fields__ = {'cache': ModelField(name='cache', type=Optional[bool], r...\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "     |  \n",
      "     |  __include_fields__ = None\n",
      "     |  \n",
      "     |  __parameters__ = ()\n",
      "     |  \n",
      "     |  __post_root_validators__ = [(False, <function BedrockBase.validate_env...\n",
      "     |  \n",
      "     |  __pre_root_validators__ = []\n",
      "     |  \n",
      "     |  __private_attributes__ = {'_lc_kwargs': ModelPrivateAttr(default=Pydan...\n",
      "     |  \n",
      "     |  __schema_cache__ = {}\n",
      "     |  \n",
      "     |  __signature__ = <Signature (*, client: Any = None, region_name: ...tad...\n",
      "     |  \n",
      "     |  __validators__ = {'verbose': [<pydantic.class_validators.Validator obj...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  __call__(self, prompt: 'str', stop: 'Optional[List[str]]' = None, callbacks: 'Callbacks' = None, *, tags: 'Optional[List[str]]' = None, metadata: 'Optional[Dict[str, Any]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Check Cache and run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  __str__(self) -> 'str'\n",
      "     |      Get a string representation of the object for printing.\n",
      "     |  \n",
      "     |  async abatch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      "     |      Default implementation of abatch, which calls ainvoke N times.\n",
      "     |      Subclasses should override this method if they can batch more efficiently.\n",
      "     |  \n",
      "     |  async agenerate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  async agenerate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Asynchronously pass a sequence of prompts and return model generations.\n",
      "     |      \n",
      "     |      This method should make use of batched calls for models that expose a batched\n",
      "     |      API.\n",
      "     |      \n",
      "     |      Use this method when you want to:\n",
      "     |          1. take advantage of batched calls,\n",
      "     |          2. need more output from the model than just the top generated value,\n",
      "     |          3. are building chains that are agnostic to the underlying language model\n",
      "     |              type (e.g., pure text completion models vs chat models).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      "     |              converted to match the format of any language model (string for pure\n",
      "     |              text generation models and BaseMessages for chat models).\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          callbacks: Callbacks to pass through. Used for executing additional\n",
      "     |              functionality, such as logging or streaming, throughout generation.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          An LLMResult, which contains a list of candidate Generations for each input\n",
      "     |              prompt and additional model provider-specific output.\n",
      "     |  \n",
      "     |  async ainvoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Default implementation of ainvoke, which calls invoke in a thread pool.\n",
      "     |      Subclasses should override this method if they can run asynchronously.\n",
      "     |  \n",
      "     |  async apredict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Asynchronously pass a string to the model and return a string prediction.\n",
      "     |      \n",
      "     |      Use this method when calling pure text generation models and only the top\n",
      "     |          candidate generation is needed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: String input to pass to the model.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a string.\n",
      "     |  \n",
      "     |  async apredict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      "     |      Asynchronously pass messages to the model and return a message prediction.\n",
      "     |      \n",
      "     |      Use this method when calling chat models and only the top\n",
      "     |          candidate generation is needed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: A sequence of chat messages corresponding to a single model input.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a message.\n",
      "     |  \n",
      "     |  async astream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'AsyncIterator[str]'\n",
      "     |      Default implementation of astream, which calls ainvoke.\n",
      "     |      Subclasses should override this method if they support streaming output.\n",
      "     |  \n",
      "     |  batch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      "     |      Default implementation of batch, which calls invoke N times.\n",
      "     |      Subclasses should override this method if they can batch more efficiently.\n",
      "     |  \n",
      "     |  dict(self, **kwargs: 'Any') -> 'Dict'\n",
      "     |      Return a dictionary of the LLM.\n",
      "     |  \n",
      "     |  generate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  generate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Pass a sequence of prompts to the model and return model generations.\n",
      "     |      \n",
      "     |      This method should make use of batched calls for models that expose a batched\n",
      "     |      API.\n",
      "     |      \n",
      "     |      Use this method when you want to:\n",
      "     |          1. take advantage of batched calls,\n",
      "     |          2. need more output from the model than just the top generated value,\n",
      "     |          3. are building chains that are agnostic to the underlying language model\n",
      "     |              type (e.g., pure text completion models vs chat models).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      "     |              converted to match the format of any language model (string for pure\n",
      "     |              text generation models and BaseMessages for chat models).\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          callbacks: Callbacks to pass through. Used for executing additional\n",
      "     |              functionality, such as logging or streaming, throughout generation.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          An LLMResult, which contains a list of candidate Generations for each input\n",
      "     |              prompt and additional model provider-specific output.\n",
      "     |  \n",
      "     |  invoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |  \n",
      "     |  predict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Pass a single string input to the model and return a string prediction.\n",
      "     |      \n",
      "     |       Use this method when passing in raw text. If you want to pass in specific\n",
      "     |          types of chat messages, use predict_messages.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: String input to pass to the model.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a string.\n",
      "     |  \n",
      "     |  predict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      "     |      Pass a message sequence to the model and return a message prediction.\n",
      "     |      \n",
      "     |      Use this method when passing in chat messages. If you want to pass in raw text,\n",
      "     |          use predict.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: A sequence of chat messages corresponding to a single model input.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a message.\n",
      "     |  \n",
      "     |  save(self, file_path: 'Union[Path, str]') -> 'None'\n",
      "     |      Save the LLM.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          file_path: Path to file to save the LLM to.\n",
      "     |      \n",
      "     |      Example:\n",
      "     |      .. code-block:: python\n",
      "     |      \n",
      "     |          llm.save(file_path=\"path/llm.yaml\")\n",
      "     |  \n",
      "     |  stream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'Iterator[str]'\n",
      "     |      Default implementation of stream, which calls invoke.\n",
      "     |      Subclasses should override this method if they support streaming output.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  raise_deprecation(values: 'Dict') -> 'Dict' from pydantic.main.ModelMetaclass\n",
      "     |      Raise deprecation warning if callback_manager is used.\n",
      "     |  \n",
      "     |  set_verbose(verbose: 'Optional[bool]') -> 'bool' from pydantic.main.ModelMetaclass\n",
      "     |      If verbose is None, set it.\n",
      "     |      \n",
      "     |      This allows users to pass in None as verbose to access the global setting.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  __orig_bases__ = (langchain.schema.language_model.BaseLanguageModel[st...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.schema.language_model.BaseLanguageModel:\n",
      "     |  \n",
      "     |  get_num_tokens(self, text: 'str') -> 'int'\n",
      "     |      Get the number of tokens present in the text.\n",
      "     |      \n",
      "     |      Useful for checking if an input will fit in a model's context window.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: The string input to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The integer number of tokens in the text.\n",
      "     |  \n",
      "     |  get_num_tokens_from_messages(self, messages: 'List[BaseMessage]') -> 'int'\n",
      "     |      Get the number of tokens in the messages.\n",
      "     |      \n",
      "     |      Useful for checking if an input will fit in a model's context window.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: The message inputs to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The sum of the number of tokens across the messages.\n",
      "     |  \n",
      "     |  get_token_ids(self, text: 'str') -> 'List[int]'\n",
      "     |      Return the ordered ids of the tokens in a text.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: The string input to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          A list of ids corresponding to the tokens in the text, in order they occur\n",
      "     |              in the text.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from langchain.schema.language_model.BaseLanguageModel:\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.load.serializable.Serializable:\n",
      "     |  \n",
      "     |  __init__(self, **kwargs: Any) -> None\n",
      "     |      Create a new model by parsing and validating input data from keyword arguments.\n",
      "     |      \n",
      "     |      Raises ValidationError if the input data cannot be parsed to form a valid model.\n",
      "     |  \n",
      "     |  to_json(self) -> Union[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\n",
      "     |  \n",
      "     |  to_json_not_implemented(self) -> langchain.load.serializable.SerializedNotImplemented\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from langchain.load.serializable.Serializable:\n",
      "     |  \n",
      "     |  lc_attributes\n",
      "     |      Return a list of attribute names that should be included in the\n",
      "     |      serialized kwargs. These attributes must be accepted by the\n",
      "     |      constructor.\n",
      "     |  \n",
      "     |  lc_namespace\n",
      "     |      Return the namespace of the langchain object.\n",
      "     |      eg. [\"langchain\", \"llms\", \"openai\"]\n",
      "     |  \n",
      "     |  lc_secrets\n",
      "     |      Return a map of constructor argument names to secret ids.\n",
      "     |      eg. {\"openai_api_key\": \"OPENAI_API_KEY\"}\n",
      "     |  \n",
      "     |  lc_serializable\n",
      "     |      Return whether or not the class is serializable.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from BedrockBase:\n",
      "     |  \n",
      "     |  validate_environment(values: Dict) -> Dict from pydantic.main.ModelMetaclass\n",
      "     |      Validate that AWS credentials to and python package exists in environment.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __eq__(self, other: Any) -> bool\n",
      "     |  \n",
      "     |  __getstate__(self) -> 'DictAny'\n",
      "     |  \n",
      "     |  __iter__(self) -> 'TupleGenerator'\n",
      "     |      so `dict(model)` works\n",
      "     |  \n",
      "     |  __repr_args__(self) -> 'ReprArgs'\n",
      "     |  \n",
      "     |  __setattr__(self, name, value)\n",
      "     |  \n",
      "     |  __setstate__(self, state: 'DictAny') -> None\n",
      "     |  \n",
      "     |  copy(self: 'Model', *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, update: Optional[ForwardRef('DictStrAny')] = None, deep: bool = False) -> 'Model'\n",
      "     |      Duplicate a model, optionally choose which fields to include, exclude and change.\n",
      "     |      \n",
      "     |      :param include: fields to include in new model\n",
      "     |      :param exclude: fields to exclude from new model, as with values this takes precedence over include\n",
      "     |      :param update: values to change/add in the new model. Note: the data is not validated before creating\n",
      "     |          the new model: you should trust this data\n",
      "     |      :param deep: set to `True` to make a deep copy of the model\n",
      "     |      :return: new model instance\n",
      "     |  \n",
      "     |  json(self, *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) -> 'unicode'\n",
      "     |      Generate a JSON representation of the model, `include` and `exclude` arguments as per `dict()`.\n",
      "     |      \n",
      "     |      `encoder` is an optional function to supply as `default` to json.dumps(), other arguments as per `json.dumps()`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __get_validators__() -> 'CallableGenerator' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  __try_update_forward_refs__(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      "     |      Same as update_forward_refs but will not raise exception\n",
      "     |      when forward references are not defined.\n",
      "     |  \n",
      "     |  construct(_fields_set: Optional[ForwardRef('SetStr')] = None, **values: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |      Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data.\n",
      "     |      Default values are respected, but no other validation is performed.\n",
      "     |      Behaves as if `Config.extra = 'allow'` was set since it adds all passed values\n",
      "     |  \n",
      "     |  from_orm(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_file(path: Union[str, pathlib.Path], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_obj(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_raw(b: Union[str, bytes], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema(by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}') -> 'DictStrAny' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema_json(*, by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}', **dumps_kwargs: Any) -> 'unicode' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  update_forward_refs(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      "     |      Try to update ForwardRefs on fields based on this Model, globalns and localns.\n",
      "     |  \n",
      "     |  validate(value: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __fields_set__\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.utils.Representation:\n",
      "     |  \n",
      "     |  __pretty__(self, fmt: Callable[[Any], Any], **kwargs: Any) -> Generator[Any, NoneType, NoneType]\n",
      "     |      Used by devtools (https://python-devtools.helpmanual.io/) to provide a human readable representations of objects\n",
      "     |  \n",
      "     |  __repr__(self) -> 'unicode'\n",
      "     |  \n",
      "     |  __repr_name__(self) -> 'unicode'\n",
      "     |      Name of the instance's class, used in __repr__.\n",
      "     |  \n",
      "     |  __repr_str__(self, join_str: 'unicode') -> 'unicode'\n",
      "     |  \n",
      "     |  __rich_repr__(self) -> 'RichReprResult'\n",
      "     |      Get fields for Rich library\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.schema.runnable.base.Runnable:\n",
      "     |  \n",
      "     |  __or__(self, other: 'Union[Runnable[Any, Other], Callable[[Any], Other], Mapping[str, Union[Runnable[Any, Other], Callable[[Any], Other], Any]]]') -> 'RunnableSequence[Input, Other]'\n",
      "     |      Return self|value.\n",
      "     |  \n",
      "     |  __ror__(self, other: 'Union[Runnable[Other, Any], Callable[[Any], Other], Mapping[str, Union[Runnable[Other, Any], Callable[[Other], Any], Any]]]') -> 'RunnableSequence[Other, Output]'\n",
      "     |      Return value|self.\n",
      "     |  \n",
      "     |  async atransform(self, input: 'AsyncIterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'AsyncIterator[Output]'\n",
      "     |      Default implementation of atransform, which buffers input and calls astream.\n",
      "     |      Subclasses should override this method if they can start producing output while\n",
      "     |      input is still being generated.\n",
      "     |  \n",
      "     |  bind(self, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      "     |      Bind arguments to a Runnable, returning a new Runnable.\n",
      "     |  \n",
      "     |  map(self) -> 'Runnable[List[Input], List[Output]]'\n",
      "     |      Return a new Runnable that maps a list of inputs to a list of outputs,\n",
      "     |      by calling invoke() with each input.\n",
      "     |  \n",
      "     |  transform(self, input: 'Iterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'Iterator[Output]'\n",
      "     |      Default implementation of transform, which buffers input and then calls stream.\n",
      "     |      Subclasses should override this method if they can start producing output while\n",
      "     |      input is still being generated.\n",
      "     |  \n",
      "     |  with_config(self, config: 'Optional[RunnableConfig]' = None, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      "     |      Bind config to a Runnable, returning a new Runnable.\n",
      "     |  \n",
      "     |  with_fallbacks(self, fallbacks: 'Sequence[Runnable[Input, Output]]', *, exceptions_to_handle: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,)) -> 'RunnableWithFallbacks[Input, Output]'\n",
      "     |  \n",
      "     |  with_retry(self, *, retry_if_exception_type: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,), wait_exponential_jitter: 'bool' = True, stop_after_attempt: 'int' = 3) -> 'Runnable[Input, Output]'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __class_getitem__(params) from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  __init_subclass__(*args, **kwargs) from pydantic.main.ModelMetaclass\n",
      "     |      This method is called when a class is subclassed.\n",
      "     |      \n",
      "     |      The default implementation does nothing. It may be\n",
      "     |      overridden to extend subclasses.\n",
      "    \n",
      "    class CTransformers(langchain.llms.base.LLM)\n",
      "     |  CTransformers(*, cache: Optional[bool] = None, verbose: bool = None, callbacks: Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager, NoneType] = None, callback_manager: Optional[langchain.callbacks.base.BaseCallbackManager] = None, tags: Optional[List[str]] = None, metadata: Optional[Dict[str, Any]] = None, client: Any = None, model: str, model_type: Optional[str] = None, model_file: Optional[str] = None, config: Optional[Dict[str, Any]] = None, lib: Optional[str] = None) -> None\n",
      "     |  \n",
      "     |  C Transformers LLM models.\n",
      "     |  \n",
      "     |  To use, you should have the ``ctransformers`` python package installed.\n",
      "     |  See https://github.com/marella/ctransformers\n",
      "     |  \n",
      "     |  Example:\n",
      "     |      .. code-block:: python\n",
      "     |  \n",
      "     |          from langchain.llms import CTransformers\n",
      "     |  \n",
      "     |          llm = CTransformers(model=\"/path/to/ggml-gpt-2.bin\", model_type=\"gpt2\")\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      CTransformers\n",
      "     |      langchain.llms.base.LLM\n",
      "     |      langchain.llms.base.BaseLLM\n",
      "     |      langchain.schema.language_model.BaseLanguageModel\n",
      "     |      langchain.load.serializable.Serializable\n",
      "     |      pydantic.main.BaseModel\n",
      "     |      pydantic.utils.Representation\n",
      "     |      langchain.schema.runnable.base.Runnable\n",
      "     |      typing.Generic\n",
      "     |      abc.ABC\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  validate_environment(values: Dict) -> Dict from pydantic.main.ModelMetaclass\n",
      "     |      Validate that ``ctransformers`` package is installed.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __json_encoder__ = pydantic_encoder(obj: Any) -> Any\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {'client': typing.Any, 'config': typing.Optional[typ...\n",
      "     |  \n",
      "     |  __class_vars__ = set()\n",
      "     |  \n",
      "     |  __config__ = <class 'langchain.llms.ctransformers.Config'>\n",
      "     |  \n",
      "     |  __custom_root_type__ = False\n",
      "     |  \n",
      "     |  __exclude_fields__ = {'callback_manager': True, 'callbacks': True, 'me...\n",
      "     |  \n",
      "     |  __fields__ = {'cache': ModelField(name='cache', type=Optional[bool], r...\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "     |  \n",
      "     |  __include_fields__ = None\n",
      "     |  \n",
      "     |  __parameters__ = ()\n",
      "     |  \n",
      "     |  __post_root_validators__ = [(False, <function BaseLLM.raise_deprecatio...\n",
      "     |  \n",
      "     |  __pre_root_validators__ = []\n",
      "     |  \n",
      "     |  __private_attributes__ = {'_lc_kwargs': ModelPrivateAttr(default=Pydan...\n",
      "     |  \n",
      "     |  __schema_cache__ = {}\n",
      "     |  \n",
      "     |  __signature__ = <Signature (*, cache: Optional[bool] = None, ver...Any...\n",
      "     |  \n",
      "     |  __validators__ = {'verbose': [<pydantic.class_validators.Validator obj...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  __call__(self, prompt: 'str', stop: 'Optional[List[str]]' = None, callbacks: 'Callbacks' = None, *, tags: 'Optional[List[str]]' = None, metadata: 'Optional[Dict[str, Any]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Check Cache and run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  __str__(self) -> 'str'\n",
      "     |      Get a string representation of the object for printing.\n",
      "     |  \n",
      "     |  async abatch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      "     |      Default implementation of abatch, which calls ainvoke N times.\n",
      "     |      Subclasses should override this method if they can batch more efficiently.\n",
      "     |  \n",
      "     |  async agenerate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  async agenerate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Asynchronously pass a sequence of prompts and return model generations.\n",
      "     |      \n",
      "     |      This method should make use of batched calls for models that expose a batched\n",
      "     |      API.\n",
      "     |      \n",
      "     |      Use this method when you want to:\n",
      "     |          1. take advantage of batched calls,\n",
      "     |          2. need more output from the model than just the top generated value,\n",
      "     |          3. are building chains that are agnostic to the underlying language model\n",
      "     |              type (e.g., pure text completion models vs chat models).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      "     |              converted to match the format of any language model (string for pure\n",
      "     |              text generation models and BaseMessages for chat models).\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          callbacks: Callbacks to pass through. Used for executing additional\n",
      "     |              functionality, such as logging or streaming, throughout generation.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          An LLMResult, which contains a list of candidate Generations for each input\n",
      "     |              prompt and additional model provider-specific output.\n",
      "     |  \n",
      "     |  async ainvoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Default implementation of ainvoke, which calls invoke in a thread pool.\n",
      "     |      Subclasses should override this method if they can run asynchronously.\n",
      "     |  \n",
      "     |  async apredict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Asynchronously pass a string to the model and return a string prediction.\n",
      "     |      \n",
      "     |      Use this method when calling pure text generation models and only the top\n",
      "     |          candidate generation is needed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: String input to pass to the model.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a string.\n",
      "     |  \n",
      "     |  async apredict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      "     |      Asynchronously pass messages to the model and return a message prediction.\n",
      "     |      \n",
      "     |      Use this method when calling chat models and only the top\n",
      "     |          candidate generation is needed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: A sequence of chat messages corresponding to a single model input.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a message.\n",
      "     |  \n",
      "     |  async astream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'AsyncIterator[str]'\n",
      "     |      Default implementation of astream, which calls ainvoke.\n",
      "     |      Subclasses should override this method if they support streaming output.\n",
      "     |  \n",
      "     |  batch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      "     |      Default implementation of batch, which calls invoke N times.\n",
      "     |      Subclasses should override this method if they can batch more efficiently.\n",
      "     |  \n",
      "     |  dict(self, **kwargs: 'Any') -> 'Dict'\n",
      "     |      Return a dictionary of the LLM.\n",
      "     |  \n",
      "     |  generate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  generate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Pass a sequence of prompts to the model and return model generations.\n",
      "     |      \n",
      "     |      This method should make use of batched calls for models that expose a batched\n",
      "     |      API.\n",
      "     |      \n",
      "     |      Use this method when you want to:\n",
      "     |          1. take advantage of batched calls,\n",
      "     |          2. need more output from the model than just the top generated value,\n",
      "     |          3. are building chains that are agnostic to the underlying language model\n",
      "     |              type (e.g., pure text completion models vs chat models).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      "     |              converted to match the format of any language model (string for pure\n",
      "     |              text generation models and BaseMessages for chat models).\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          callbacks: Callbacks to pass through. Used for executing additional\n",
      "     |              functionality, such as logging or streaming, throughout generation.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          An LLMResult, which contains a list of candidate Generations for each input\n",
      "     |              prompt and additional model provider-specific output.\n",
      "     |  \n",
      "     |  invoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |  \n",
      "     |  predict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Pass a single string input to the model and return a string prediction.\n",
      "     |      \n",
      "     |       Use this method when passing in raw text. If you want to pass in specific\n",
      "     |          types of chat messages, use predict_messages.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: String input to pass to the model.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a string.\n",
      "     |  \n",
      "     |  predict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      "     |      Pass a message sequence to the model and return a message prediction.\n",
      "     |      \n",
      "     |      Use this method when passing in chat messages. If you want to pass in raw text,\n",
      "     |          use predict.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: A sequence of chat messages corresponding to a single model input.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a message.\n",
      "     |  \n",
      "     |  save(self, file_path: 'Union[Path, str]') -> 'None'\n",
      "     |      Save the LLM.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          file_path: Path to file to save the LLM to.\n",
      "     |      \n",
      "     |      Example:\n",
      "     |      .. code-block:: python\n",
      "     |      \n",
      "     |          llm.save(file_path=\"path/llm.yaml\")\n",
      "     |  \n",
      "     |  stream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'Iterator[str]'\n",
      "     |      Default implementation of stream, which calls invoke.\n",
      "     |      Subclasses should override this method if they support streaming output.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  raise_deprecation(values: 'Dict') -> 'Dict' from pydantic.main.ModelMetaclass\n",
      "     |      Raise deprecation warning if callback_manager is used.\n",
      "     |  \n",
      "     |  set_verbose(verbose: 'Optional[bool]') -> 'bool' from pydantic.main.ModelMetaclass\n",
      "     |      If verbose is None, set it.\n",
      "     |      \n",
      "     |      This allows users to pass in None as verbose to access the global setting.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  Config = <class 'langchain.llms.base.BaseLLM.Config'>\n",
      "     |      Configuration for this pydantic object.\n",
      "     |  \n",
      "     |  \n",
      "     |  __orig_bases__ = (langchain.schema.language_model.BaseLanguageModel[st...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.schema.language_model.BaseLanguageModel:\n",
      "     |  \n",
      "     |  get_num_tokens(self, text: 'str') -> 'int'\n",
      "     |      Get the number of tokens present in the text.\n",
      "     |      \n",
      "     |      Useful for checking if an input will fit in a model's context window.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: The string input to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The integer number of tokens in the text.\n",
      "     |  \n",
      "     |  get_num_tokens_from_messages(self, messages: 'List[BaseMessage]') -> 'int'\n",
      "     |      Get the number of tokens in the messages.\n",
      "     |      \n",
      "     |      Useful for checking if an input will fit in a model's context window.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: The message inputs to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The sum of the number of tokens across the messages.\n",
      "     |  \n",
      "     |  get_token_ids(self, text: 'str') -> 'List[int]'\n",
      "     |      Return the ordered ids of the tokens in a text.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: The string input to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          A list of ids corresponding to the tokens in the text, in order they occur\n",
      "     |              in the text.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from langchain.schema.language_model.BaseLanguageModel:\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.load.serializable.Serializable:\n",
      "     |  \n",
      "     |  __init__(self, **kwargs: Any) -> None\n",
      "     |      Create a new model by parsing and validating input data from keyword arguments.\n",
      "     |      \n",
      "     |      Raises ValidationError if the input data cannot be parsed to form a valid model.\n",
      "     |  \n",
      "     |  to_json(self) -> Union[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\n",
      "     |  \n",
      "     |  to_json_not_implemented(self) -> langchain.load.serializable.SerializedNotImplemented\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from langchain.load.serializable.Serializable:\n",
      "     |  \n",
      "     |  lc_attributes\n",
      "     |      Return a list of attribute names that should be included in the\n",
      "     |      serialized kwargs. These attributes must be accepted by the\n",
      "     |      constructor.\n",
      "     |  \n",
      "     |  lc_namespace\n",
      "     |      Return the namespace of the langchain object.\n",
      "     |      eg. [\"langchain\", \"llms\", \"openai\"]\n",
      "     |  \n",
      "     |  lc_secrets\n",
      "     |      Return a map of constructor argument names to secret ids.\n",
      "     |      eg. {\"openai_api_key\": \"OPENAI_API_KEY\"}\n",
      "     |  \n",
      "     |  lc_serializable\n",
      "     |      Return whether or not the class is serializable.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __eq__(self, other: Any) -> bool\n",
      "     |  \n",
      "     |  __getstate__(self) -> 'DictAny'\n",
      "     |  \n",
      "     |  __iter__(self) -> 'TupleGenerator'\n",
      "     |      so `dict(model)` works\n",
      "     |  \n",
      "     |  __repr_args__(self) -> 'ReprArgs'\n",
      "     |  \n",
      "     |  __setattr__(self, name, value)\n",
      "     |  \n",
      "     |  __setstate__(self, state: 'DictAny') -> None\n",
      "     |  \n",
      "     |  copy(self: 'Model', *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, update: Optional[ForwardRef('DictStrAny')] = None, deep: bool = False) -> 'Model'\n",
      "     |      Duplicate a model, optionally choose which fields to include, exclude and change.\n",
      "     |      \n",
      "     |      :param include: fields to include in new model\n",
      "     |      :param exclude: fields to exclude from new model, as with values this takes precedence over include\n",
      "     |      :param update: values to change/add in the new model. Note: the data is not validated before creating\n",
      "     |          the new model: you should trust this data\n",
      "     |      :param deep: set to `True` to make a deep copy of the model\n",
      "     |      :return: new model instance\n",
      "     |  \n",
      "     |  json(self, *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) -> 'unicode'\n",
      "     |      Generate a JSON representation of the model, `include` and `exclude` arguments as per `dict()`.\n",
      "     |      \n",
      "     |      `encoder` is an optional function to supply as `default` to json.dumps(), other arguments as per `json.dumps()`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __get_validators__() -> 'CallableGenerator' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  __try_update_forward_refs__(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      "     |      Same as update_forward_refs but will not raise exception\n",
      "     |      when forward references are not defined.\n",
      "     |  \n",
      "     |  construct(_fields_set: Optional[ForwardRef('SetStr')] = None, **values: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |      Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data.\n",
      "     |      Default values are respected, but no other validation is performed.\n",
      "     |      Behaves as if `Config.extra = 'allow'` was set since it adds all passed values\n",
      "     |  \n",
      "     |  from_orm(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_file(path: Union[str, pathlib.Path], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_obj(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_raw(b: Union[str, bytes], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema(by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}') -> 'DictStrAny' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema_json(*, by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}', **dumps_kwargs: Any) -> 'unicode' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  update_forward_refs(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      "     |      Try to update ForwardRefs on fields based on this Model, globalns and localns.\n",
      "     |  \n",
      "     |  validate(value: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __fields_set__\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.utils.Representation:\n",
      "     |  \n",
      "     |  __pretty__(self, fmt: Callable[[Any], Any], **kwargs: Any) -> Generator[Any, NoneType, NoneType]\n",
      "     |      Used by devtools (https://python-devtools.helpmanual.io/) to provide a human readable representations of objects\n",
      "     |  \n",
      "     |  __repr__(self) -> 'unicode'\n",
      "     |  \n",
      "     |  __repr_name__(self) -> 'unicode'\n",
      "     |      Name of the instance's class, used in __repr__.\n",
      "     |  \n",
      "     |  __repr_str__(self, join_str: 'unicode') -> 'unicode'\n",
      "     |  \n",
      "     |  __rich_repr__(self) -> 'RichReprResult'\n",
      "     |      Get fields for Rich library\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.schema.runnable.base.Runnable:\n",
      "     |  \n",
      "     |  __or__(self, other: 'Union[Runnable[Any, Other], Callable[[Any], Other], Mapping[str, Union[Runnable[Any, Other], Callable[[Any], Other], Any]]]') -> 'RunnableSequence[Input, Other]'\n",
      "     |      Return self|value.\n",
      "     |  \n",
      "     |  __ror__(self, other: 'Union[Runnable[Other, Any], Callable[[Any], Other], Mapping[str, Union[Runnable[Other, Any], Callable[[Other], Any], Any]]]') -> 'RunnableSequence[Other, Output]'\n",
      "     |      Return value|self.\n",
      "     |  \n",
      "     |  async atransform(self, input: 'AsyncIterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'AsyncIterator[Output]'\n",
      "     |      Default implementation of atransform, which buffers input and calls astream.\n",
      "     |      Subclasses should override this method if they can start producing output while\n",
      "     |      input is still being generated.\n",
      "     |  \n",
      "     |  bind(self, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      "     |      Bind arguments to a Runnable, returning a new Runnable.\n",
      "     |  \n",
      "     |  map(self) -> 'Runnable[List[Input], List[Output]]'\n",
      "     |      Return a new Runnable that maps a list of inputs to a list of outputs,\n",
      "     |      by calling invoke() with each input.\n",
      "     |  \n",
      "     |  transform(self, input: 'Iterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'Iterator[Output]'\n",
      "     |      Default implementation of transform, which buffers input and then calls stream.\n",
      "     |      Subclasses should override this method if they can start producing output while\n",
      "     |      input is still being generated.\n",
      "     |  \n",
      "     |  with_config(self, config: 'Optional[RunnableConfig]' = None, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      "     |      Bind config to a Runnable, returning a new Runnable.\n",
      "     |  \n",
      "     |  with_fallbacks(self, fallbacks: 'Sequence[Runnable[Input, Output]]', *, exceptions_to_handle: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,)) -> 'RunnableWithFallbacks[Input, Output]'\n",
      "     |  \n",
      "     |  with_retry(self, *, retry_if_exception_type: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,), wait_exponential_jitter: 'bool' = True, stop_after_attempt: 'int' = 3) -> 'Runnable[Input, Output]'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __class_getitem__(params) from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  __init_subclass__(*args, **kwargs) from pydantic.main.ModelMetaclass\n",
      "     |      This method is called when a class is subclassed.\n",
      "     |      \n",
      "     |      The default implementation does nothing. It may be\n",
      "     |      overridden to extend subclasses.\n",
      "    \n",
      "    class CTranslate2(langchain.llms.base.BaseLLM)\n",
      "     |  CTranslate2(*, cache: Optional[bool] = None, verbose: bool = None, callbacks: Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager, NoneType] = None, callback_manager: Optional[langchain.callbacks.base.BaseCallbackManager] = None, tags: Optional[List[str]] = None, metadata: Optional[Dict[str, Any]] = None, model_path: str = '', tokenizer_name: str = '', device: str = 'cpu', device_index: Union[int, List[int]] = 0, compute_type: Union[str, Dict[str, str]] = 'default', max_length: int = 512, sampling_topk: int = 1, sampling_topp: float = 1, sampling_temperature: float = 1, client: Any = None, tokenizer: Any = None, ctranslate2_kwargs: Dict[str, Any] = None) -> None\n",
      "     |  \n",
      "     |  CTranslate2 language model.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      CTranslate2\n",
      "     |      langchain.llms.base.BaseLLM\n",
      "     |      langchain.schema.language_model.BaseLanguageModel\n",
      "     |      langchain.load.serializable.Serializable\n",
      "     |      pydantic.main.BaseModel\n",
      "     |      pydantic.utils.Representation\n",
      "     |      langchain.schema.runnable.base.Runnable\n",
      "     |      typing.Generic\n",
      "     |      abc.ABC\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  validate_environment(values: Dict) -> Dict from pydantic.main.ModelMetaclass\n",
      "     |      Validate that python package exists in environment.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __json_encoder__ = pydantic_encoder(obj: Any) -> Any\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {'client': typing.Any, 'compute_type': typing.Union[...\n",
      "     |  \n",
      "     |  __class_vars__ = set()\n",
      "     |  \n",
      "     |  __config__ = <class 'langchain.llms.ctranslate2.Config'>\n",
      "     |  \n",
      "     |  __custom_root_type__ = False\n",
      "     |  \n",
      "     |  __exclude_fields__ = {'callback_manager': True, 'callbacks': True, 'me...\n",
      "     |  \n",
      "     |  __fields__ = {'cache': ModelField(name='cache', type=Optional[bool], r...\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "     |  \n",
      "     |  __include_fields__ = None\n",
      "     |  \n",
      "     |  __parameters__ = ()\n",
      "     |  \n",
      "     |  __post_root_validators__ = [(False, <function BaseLLM.raise_deprecatio...\n",
      "     |  \n",
      "     |  __pre_root_validators__ = []\n",
      "     |  \n",
      "     |  __private_attributes__ = {'_lc_kwargs': ModelPrivateAttr(default=Pydan...\n",
      "     |  \n",
      "     |  __schema_cache__ = {}\n",
      "     |  \n",
      "     |  __signature__ = <Signature (*, cache: Optional[bool] = None, ver...ran...\n",
      "     |  \n",
      "     |  __validators__ = {'verbose': [<pydantic.class_validators.Validator obj...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  __call__(self, prompt: 'str', stop: 'Optional[List[str]]' = None, callbacks: 'Callbacks' = None, *, tags: 'Optional[List[str]]' = None, metadata: 'Optional[Dict[str, Any]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Check Cache and run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  __str__(self) -> 'str'\n",
      "     |      Get a string representation of the object for printing.\n",
      "     |  \n",
      "     |  async abatch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      "     |      Default implementation of abatch, which calls ainvoke N times.\n",
      "     |      Subclasses should override this method if they can batch more efficiently.\n",
      "     |  \n",
      "     |  async agenerate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  async agenerate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Asynchronously pass a sequence of prompts and return model generations.\n",
      "     |      \n",
      "     |      This method should make use of batched calls for models that expose a batched\n",
      "     |      API.\n",
      "     |      \n",
      "     |      Use this method when you want to:\n",
      "     |          1. take advantage of batched calls,\n",
      "     |          2. need more output from the model than just the top generated value,\n",
      "     |          3. are building chains that are agnostic to the underlying language model\n",
      "     |              type (e.g., pure text completion models vs chat models).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      "     |              converted to match the format of any language model (string for pure\n",
      "     |              text generation models and BaseMessages for chat models).\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          callbacks: Callbacks to pass through. Used for executing additional\n",
      "     |              functionality, such as logging or streaming, throughout generation.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          An LLMResult, which contains a list of candidate Generations for each input\n",
      "     |              prompt and additional model provider-specific output.\n",
      "     |  \n",
      "     |  async ainvoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Default implementation of ainvoke, which calls invoke in a thread pool.\n",
      "     |      Subclasses should override this method if they can run asynchronously.\n",
      "     |  \n",
      "     |  async apredict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Asynchronously pass a string to the model and return a string prediction.\n",
      "     |      \n",
      "     |      Use this method when calling pure text generation models and only the top\n",
      "     |          candidate generation is needed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: String input to pass to the model.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a string.\n",
      "     |  \n",
      "     |  async apredict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      "     |      Asynchronously pass messages to the model and return a message prediction.\n",
      "     |      \n",
      "     |      Use this method when calling chat models and only the top\n",
      "     |          candidate generation is needed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: A sequence of chat messages corresponding to a single model input.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a message.\n",
      "     |  \n",
      "     |  async astream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'AsyncIterator[str]'\n",
      "     |      Default implementation of astream, which calls ainvoke.\n",
      "     |      Subclasses should override this method if they support streaming output.\n",
      "     |  \n",
      "     |  batch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      "     |      Default implementation of batch, which calls invoke N times.\n",
      "     |      Subclasses should override this method if they can batch more efficiently.\n",
      "     |  \n",
      "     |  dict(self, **kwargs: 'Any') -> 'Dict'\n",
      "     |      Return a dictionary of the LLM.\n",
      "     |  \n",
      "     |  generate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  generate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Pass a sequence of prompts to the model and return model generations.\n",
      "     |      \n",
      "     |      This method should make use of batched calls for models that expose a batched\n",
      "     |      API.\n",
      "     |      \n",
      "     |      Use this method when you want to:\n",
      "     |          1. take advantage of batched calls,\n",
      "     |          2. need more output from the model than just the top generated value,\n",
      "     |          3. are building chains that are agnostic to the underlying language model\n",
      "     |              type (e.g., pure text completion models vs chat models).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      "     |              converted to match the format of any language model (string for pure\n",
      "     |              text generation models and BaseMessages for chat models).\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          callbacks: Callbacks to pass through. Used for executing additional\n",
      "     |              functionality, such as logging or streaming, throughout generation.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          An LLMResult, which contains a list of candidate Generations for each input\n",
      "     |              prompt and additional model provider-specific output.\n",
      "     |  \n",
      "     |  invoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |  \n",
      "     |  predict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Pass a single string input to the model and return a string prediction.\n",
      "     |      \n",
      "     |       Use this method when passing in raw text. If you want to pass in specific\n",
      "     |          types of chat messages, use predict_messages.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: String input to pass to the model.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a string.\n",
      "     |  \n",
      "     |  predict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      "     |      Pass a message sequence to the model and return a message prediction.\n",
      "     |      \n",
      "     |      Use this method when passing in chat messages. If you want to pass in raw text,\n",
      "     |          use predict.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: A sequence of chat messages corresponding to a single model input.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a message.\n",
      "     |  \n",
      "     |  save(self, file_path: 'Union[Path, str]') -> 'None'\n",
      "     |      Save the LLM.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          file_path: Path to file to save the LLM to.\n",
      "     |      \n",
      "     |      Example:\n",
      "     |      .. code-block:: python\n",
      "     |      \n",
      "     |          llm.save(file_path=\"path/llm.yaml\")\n",
      "     |  \n",
      "     |  stream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'Iterator[str]'\n",
      "     |      Default implementation of stream, which calls invoke.\n",
      "     |      Subclasses should override this method if they support streaming output.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  raise_deprecation(values: 'Dict') -> 'Dict' from pydantic.main.ModelMetaclass\n",
      "     |      Raise deprecation warning if callback_manager is used.\n",
      "     |  \n",
      "     |  set_verbose(verbose: 'Optional[bool]') -> 'bool' from pydantic.main.ModelMetaclass\n",
      "     |      If verbose is None, set it.\n",
      "     |      \n",
      "     |      This allows users to pass in None as verbose to access the global setting.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  Config = <class 'langchain.llms.base.BaseLLM.Config'>\n",
      "     |      Configuration for this pydantic object.\n",
      "     |  \n",
      "     |  \n",
      "     |  __orig_bases__ = (langchain.schema.language_model.BaseLanguageModel[st...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.schema.language_model.BaseLanguageModel:\n",
      "     |  \n",
      "     |  get_num_tokens(self, text: 'str') -> 'int'\n",
      "     |      Get the number of tokens present in the text.\n",
      "     |      \n",
      "     |      Useful for checking if an input will fit in a model's context window.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: The string input to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The integer number of tokens in the text.\n",
      "     |  \n",
      "     |  get_num_tokens_from_messages(self, messages: 'List[BaseMessage]') -> 'int'\n",
      "     |      Get the number of tokens in the messages.\n",
      "     |      \n",
      "     |      Useful for checking if an input will fit in a model's context window.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: The message inputs to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The sum of the number of tokens across the messages.\n",
      "     |  \n",
      "     |  get_token_ids(self, text: 'str') -> 'List[int]'\n",
      "     |      Return the ordered ids of the tokens in a text.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: The string input to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          A list of ids corresponding to the tokens in the text, in order they occur\n",
      "     |              in the text.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from langchain.schema.language_model.BaseLanguageModel:\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.load.serializable.Serializable:\n",
      "     |  \n",
      "     |  __init__(self, **kwargs: Any) -> None\n",
      "     |      Create a new model by parsing and validating input data from keyword arguments.\n",
      "     |      \n",
      "     |      Raises ValidationError if the input data cannot be parsed to form a valid model.\n",
      "     |  \n",
      "     |  to_json(self) -> Union[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\n",
      "     |  \n",
      "     |  to_json_not_implemented(self) -> langchain.load.serializable.SerializedNotImplemented\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from langchain.load.serializable.Serializable:\n",
      "     |  \n",
      "     |  lc_attributes\n",
      "     |      Return a list of attribute names that should be included in the\n",
      "     |      serialized kwargs. These attributes must be accepted by the\n",
      "     |      constructor.\n",
      "     |  \n",
      "     |  lc_namespace\n",
      "     |      Return the namespace of the langchain object.\n",
      "     |      eg. [\"langchain\", \"llms\", \"openai\"]\n",
      "     |  \n",
      "     |  lc_secrets\n",
      "     |      Return a map of constructor argument names to secret ids.\n",
      "     |      eg. {\"openai_api_key\": \"OPENAI_API_KEY\"}\n",
      "     |  \n",
      "     |  lc_serializable\n",
      "     |      Return whether or not the class is serializable.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __eq__(self, other: Any) -> bool\n",
      "     |  \n",
      "     |  __getstate__(self) -> 'DictAny'\n",
      "     |  \n",
      "     |  __iter__(self) -> 'TupleGenerator'\n",
      "     |      so `dict(model)` works\n",
      "     |  \n",
      "     |  __repr_args__(self) -> 'ReprArgs'\n",
      "     |  \n",
      "     |  __setattr__(self, name, value)\n",
      "     |  \n",
      "     |  __setstate__(self, state: 'DictAny') -> None\n",
      "     |  \n",
      "     |  copy(self: 'Model', *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, update: Optional[ForwardRef('DictStrAny')] = None, deep: bool = False) -> 'Model'\n",
      "     |      Duplicate a model, optionally choose which fields to include, exclude and change.\n",
      "     |      \n",
      "     |      :param include: fields to include in new model\n",
      "     |      :param exclude: fields to exclude from new model, as with values this takes precedence over include\n",
      "     |      :param update: values to change/add in the new model. Note: the data is not validated before creating\n",
      "     |          the new model: you should trust this data\n",
      "     |      :param deep: set to `True` to make a deep copy of the model\n",
      "     |      :return: new model instance\n",
      "     |  \n",
      "     |  json(self, *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) -> 'unicode'\n",
      "     |      Generate a JSON representation of the model, `include` and `exclude` arguments as per `dict()`.\n",
      "     |      \n",
      "     |      `encoder` is an optional function to supply as `default` to json.dumps(), other arguments as per `json.dumps()`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __get_validators__() -> 'CallableGenerator' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  __try_update_forward_refs__(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      "     |      Same as update_forward_refs but will not raise exception\n",
      "     |      when forward references are not defined.\n",
      "     |  \n",
      "     |  construct(_fields_set: Optional[ForwardRef('SetStr')] = None, **values: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |      Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data.\n",
      "     |      Default values are respected, but no other validation is performed.\n",
      "     |      Behaves as if `Config.extra = 'allow'` was set since it adds all passed values\n",
      "     |  \n",
      "     |  from_orm(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_file(path: Union[str, pathlib.Path], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_obj(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_raw(b: Union[str, bytes], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema(by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}') -> 'DictStrAny' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema_json(*, by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}', **dumps_kwargs: Any) -> 'unicode' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  update_forward_refs(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      "     |      Try to update ForwardRefs on fields based on this Model, globalns and localns.\n",
      "     |  \n",
      "     |  validate(value: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __fields_set__\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.utils.Representation:\n",
      "     |  \n",
      "     |  __pretty__(self, fmt: Callable[[Any], Any], **kwargs: Any) -> Generator[Any, NoneType, NoneType]\n",
      "     |      Used by devtools (https://python-devtools.helpmanual.io/) to provide a human readable representations of objects\n",
      "     |  \n",
      "     |  __repr__(self) -> 'unicode'\n",
      "     |  \n",
      "     |  __repr_name__(self) -> 'unicode'\n",
      "     |      Name of the instance's class, used in __repr__.\n",
      "     |  \n",
      "     |  __repr_str__(self, join_str: 'unicode') -> 'unicode'\n",
      "     |  \n",
      "     |  __rich_repr__(self) -> 'RichReprResult'\n",
      "     |      Get fields for Rich library\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.schema.runnable.base.Runnable:\n",
      "     |  \n",
      "     |  __or__(self, other: 'Union[Runnable[Any, Other], Callable[[Any], Other], Mapping[str, Union[Runnable[Any, Other], Callable[[Any], Other], Any]]]') -> 'RunnableSequence[Input, Other]'\n",
      "     |      Return self|value.\n",
      "     |  \n",
      "     |  __ror__(self, other: 'Union[Runnable[Other, Any], Callable[[Any], Other], Mapping[str, Union[Runnable[Other, Any], Callable[[Other], Any], Any]]]') -> 'RunnableSequence[Other, Output]'\n",
      "     |      Return value|self.\n",
      "     |  \n",
      "     |  async atransform(self, input: 'AsyncIterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'AsyncIterator[Output]'\n",
      "     |      Default implementation of atransform, which buffers input and calls astream.\n",
      "     |      Subclasses should override this method if they can start producing output while\n",
      "     |      input is still being generated.\n",
      "     |  \n",
      "     |  bind(self, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      "     |      Bind arguments to a Runnable, returning a new Runnable.\n",
      "     |  \n",
      "     |  map(self) -> 'Runnable[List[Input], List[Output]]'\n",
      "     |      Return a new Runnable that maps a list of inputs to a list of outputs,\n",
      "     |      by calling invoke() with each input.\n",
      "     |  \n",
      "     |  transform(self, input: 'Iterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'Iterator[Output]'\n",
      "     |      Default implementation of transform, which buffers input and then calls stream.\n",
      "     |      Subclasses should override this method if they can start producing output while\n",
      "     |      input is still being generated.\n",
      "     |  \n",
      "     |  with_config(self, config: 'Optional[RunnableConfig]' = None, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      "     |      Bind config to a Runnable, returning a new Runnable.\n",
      "     |  \n",
      "     |  with_fallbacks(self, fallbacks: 'Sequence[Runnable[Input, Output]]', *, exceptions_to_handle: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,)) -> 'RunnableWithFallbacks[Input, Output]'\n",
      "     |  \n",
      "     |  with_retry(self, *, retry_if_exception_type: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,), wait_exponential_jitter: 'bool' = True, stop_after_attempt: 'int' = 3) -> 'Runnable[Input, Output]'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __class_getitem__(params) from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  __init_subclass__(*args, **kwargs) from pydantic.main.ModelMetaclass\n",
      "     |      This method is called when a class is subclassed.\n",
      "     |      \n",
      "     |      The default implementation does nothing. It may be\n",
      "     |      overridden to extend subclasses.\n",
      "    \n",
      "    class CerebriumAI(langchain.llms.base.LLM)\n",
      "     |  CerebriumAI(*, cache: Optional[bool] = None, verbose: bool = None, callbacks: Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager, NoneType] = None, callback_manager: Optional[langchain.callbacks.base.BaseCallbackManager] = None, tags: Optional[List[str]] = None, metadata: Optional[Dict[str, Any]] = None, endpoint_url: str = '', model_kwargs: Dict[str, Any] = None, cerebriumai_api_key: Optional[str] = None) -> None\n",
      "     |  \n",
      "     |  CerebriumAI large language models.\n",
      "     |  \n",
      "     |  To use, you should have the ``cerebrium`` python package installed, and the\n",
      "     |  environment variable ``CEREBRIUMAI_API_KEY`` set with your API key.\n",
      "     |  \n",
      "     |  Any parameters that are valid to be passed to the call can be passed\n",
      "     |  in, even if not explicitly saved on this class.\n",
      "     |  \n",
      "     |  Example:\n",
      "     |      .. code-block:: python\n",
      "     |  \n",
      "     |          from langchain.llms import CerebriumAI\n",
      "     |          cerebrium = CerebriumAI(endpoint_url=\"\")\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      CerebriumAI\n",
      "     |      langchain.llms.base.LLM\n",
      "     |      langchain.llms.base.BaseLLM\n",
      "     |      langchain.schema.language_model.BaseLanguageModel\n",
      "     |      langchain.load.serializable.Serializable\n",
      "     |      pydantic.main.BaseModel\n",
      "     |      pydantic.utils.Representation\n",
      "     |      langchain.schema.runnable.base.Runnable\n",
      "     |      typing.Generic\n",
      "     |      abc.ABC\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  build_extra(values: Dict[str, Any]) -> Dict[str, Any] from pydantic.main.ModelMetaclass\n",
      "     |      Build extra kwargs from additional params that were passed in.\n",
      "     |  \n",
      "     |  validate_environment(values: Dict) -> Dict from pydantic.main.ModelMetaclass\n",
      "     |      Validate that api key and python package exists in environment.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __json_encoder__ = pydantic_encoder(obj: Any) -> Any\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  Config = <class 'langchain.llms.cerebriumai.CerebriumAI.Config'>\n",
      "     |      Configuration for this pydantic config.\n",
      "     |  \n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {'cerebriumai_api_key': typing.Optional[str], 'endpo...\n",
      "     |  \n",
      "     |  __class_vars__ = set()\n",
      "     |  \n",
      "     |  __config__ = <class 'langchain.llms.cerebriumai.Config'>\n",
      "     |  \n",
      "     |  __custom_root_type__ = False\n",
      "     |  \n",
      "     |  __exclude_fields__ = {'callback_manager': True, 'callbacks': True, 'me...\n",
      "     |  \n",
      "     |  __fields__ = {'cache': ModelField(name='cache', type=Optional[bool], r...\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "     |  \n",
      "     |  __include_fields__ = None\n",
      "     |  \n",
      "     |  __parameters__ = ()\n",
      "     |  \n",
      "     |  __post_root_validators__ = [(False, <function BaseLLM.raise_deprecatio...\n",
      "     |  \n",
      "     |  __pre_root_validators__ = [<function CerebriumAI.build_extra>]\n",
      "     |  \n",
      "     |  __private_attributes__ = {'_lc_kwargs': ModelPrivateAttr(default=Pydan...\n",
      "     |  \n",
      "     |  __schema_cache__ = {}\n",
      "     |  \n",
      "     |  __signature__ = <Signature (*, cache: Optional[bool] = None, ver...reb...\n",
      "     |  \n",
      "     |  __validators__ = {'verbose': [<pydantic.class_validators.Validator obj...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  __call__(self, prompt: 'str', stop: 'Optional[List[str]]' = None, callbacks: 'Callbacks' = None, *, tags: 'Optional[List[str]]' = None, metadata: 'Optional[Dict[str, Any]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Check Cache and run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  __str__(self) -> 'str'\n",
      "     |      Get a string representation of the object for printing.\n",
      "     |  \n",
      "     |  async abatch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      "     |      Default implementation of abatch, which calls ainvoke N times.\n",
      "     |      Subclasses should override this method if they can batch more efficiently.\n",
      "     |  \n",
      "     |  async agenerate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  async agenerate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Asynchronously pass a sequence of prompts and return model generations.\n",
      "     |      \n",
      "     |      This method should make use of batched calls for models that expose a batched\n",
      "     |      API.\n",
      "     |      \n",
      "     |      Use this method when you want to:\n",
      "     |          1. take advantage of batched calls,\n",
      "     |          2. need more output from the model than just the top generated value,\n",
      "     |          3. are building chains that are agnostic to the underlying language model\n",
      "     |              type (e.g., pure text completion models vs chat models).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      "     |              converted to match the format of any language model (string for pure\n",
      "     |              text generation models and BaseMessages for chat models).\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          callbacks: Callbacks to pass through. Used for executing additional\n",
      "     |              functionality, such as logging or streaming, throughout generation.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          An LLMResult, which contains a list of candidate Generations for each input\n",
      "     |              prompt and additional model provider-specific output.\n",
      "     |  \n",
      "     |  async ainvoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Default implementation of ainvoke, which calls invoke in a thread pool.\n",
      "     |      Subclasses should override this method if they can run asynchronously.\n",
      "     |  \n",
      "     |  async apredict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Asynchronously pass a string to the model and return a string prediction.\n",
      "     |      \n",
      "     |      Use this method when calling pure text generation models and only the top\n",
      "     |          candidate generation is needed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: String input to pass to the model.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a string.\n",
      "     |  \n",
      "     |  async apredict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      "     |      Asynchronously pass messages to the model and return a message prediction.\n",
      "     |      \n",
      "     |      Use this method when calling chat models and only the top\n",
      "     |          candidate generation is needed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: A sequence of chat messages corresponding to a single model input.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a message.\n",
      "     |  \n",
      "     |  async astream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'AsyncIterator[str]'\n",
      "     |      Default implementation of astream, which calls ainvoke.\n",
      "     |      Subclasses should override this method if they support streaming output.\n",
      "     |  \n",
      "     |  batch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      "     |      Default implementation of batch, which calls invoke N times.\n",
      "     |      Subclasses should override this method if they can batch more efficiently.\n",
      "     |  \n",
      "     |  dict(self, **kwargs: 'Any') -> 'Dict'\n",
      "     |      Return a dictionary of the LLM.\n",
      "     |  \n",
      "     |  generate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  generate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Pass a sequence of prompts to the model and return model generations.\n",
      "     |      \n",
      "     |      This method should make use of batched calls for models that expose a batched\n",
      "     |      API.\n",
      "     |      \n",
      "     |      Use this method when you want to:\n",
      "     |          1. take advantage of batched calls,\n",
      "     |          2. need more output from the model than just the top generated value,\n",
      "     |          3. are building chains that are agnostic to the underlying language model\n",
      "     |              type (e.g., pure text completion models vs chat models).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      "     |              converted to match the format of any language model (string for pure\n",
      "     |              text generation models and BaseMessages for chat models).\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          callbacks: Callbacks to pass through. Used for executing additional\n",
      "     |              functionality, such as logging or streaming, throughout generation.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          An LLMResult, which contains a list of candidate Generations for each input\n",
      "     |              prompt and additional model provider-specific output.\n",
      "     |  \n",
      "     |  invoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |  \n",
      "     |  predict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Pass a single string input to the model and return a string prediction.\n",
      "     |      \n",
      "     |       Use this method when passing in raw text. If you want to pass in specific\n",
      "     |          types of chat messages, use predict_messages.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: String input to pass to the model.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a string.\n",
      "     |  \n",
      "     |  predict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      "     |      Pass a message sequence to the model and return a message prediction.\n",
      "     |      \n",
      "     |      Use this method when passing in chat messages. If you want to pass in raw text,\n",
      "     |          use predict.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: A sequence of chat messages corresponding to a single model input.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a message.\n",
      "     |  \n",
      "     |  save(self, file_path: 'Union[Path, str]') -> 'None'\n",
      "     |      Save the LLM.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          file_path: Path to file to save the LLM to.\n",
      "     |      \n",
      "     |      Example:\n",
      "     |      .. code-block:: python\n",
      "     |      \n",
      "     |          llm.save(file_path=\"path/llm.yaml\")\n",
      "     |  \n",
      "     |  stream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'Iterator[str]'\n",
      "     |      Default implementation of stream, which calls invoke.\n",
      "     |      Subclasses should override this method if they support streaming output.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  raise_deprecation(values: 'Dict') -> 'Dict' from pydantic.main.ModelMetaclass\n",
      "     |      Raise deprecation warning if callback_manager is used.\n",
      "     |  \n",
      "     |  set_verbose(verbose: 'Optional[bool]') -> 'bool' from pydantic.main.ModelMetaclass\n",
      "     |      If verbose is None, set it.\n",
      "     |      \n",
      "     |      This allows users to pass in None as verbose to access the global setting.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  __orig_bases__ = (langchain.schema.language_model.BaseLanguageModel[st...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.schema.language_model.BaseLanguageModel:\n",
      "     |  \n",
      "     |  get_num_tokens(self, text: 'str') -> 'int'\n",
      "     |      Get the number of tokens present in the text.\n",
      "     |      \n",
      "     |      Useful for checking if an input will fit in a model's context window.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: The string input to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The integer number of tokens in the text.\n",
      "     |  \n",
      "     |  get_num_tokens_from_messages(self, messages: 'List[BaseMessage]') -> 'int'\n",
      "     |      Get the number of tokens in the messages.\n",
      "     |      \n",
      "     |      Useful for checking if an input will fit in a model's context window.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: The message inputs to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The sum of the number of tokens across the messages.\n",
      "     |  \n",
      "     |  get_token_ids(self, text: 'str') -> 'List[int]'\n",
      "     |      Return the ordered ids of the tokens in a text.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: The string input to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          A list of ids corresponding to the tokens in the text, in order they occur\n",
      "     |              in the text.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from langchain.schema.language_model.BaseLanguageModel:\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.load.serializable.Serializable:\n",
      "     |  \n",
      "     |  __init__(self, **kwargs: Any) -> None\n",
      "     |      Create a new model by parsing and validating input data from keyword arguments.\n",
      "     |      \n",
      "     |      Raises ValidationError if the input data cannot be parsed to form a valid model.\n",
      "     |  \n",
      "     |  to_json(self) -> Union[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\n",
      "     |  \n",
      "     |  to_json_not_implemented(self) -> langchain.load.serializable.SerializedNotImplemented\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from langchain.load.serializable.Serializable:\n",
      "     |  \n",
      "     |  lc_attributes\n",
      "     |      Return a list of attribute names that should be included in the\n",
      "     |      serialized kwargs. These attributes must be accepted by the\n",
      "     |      constructor.\n",
      "     |  \n",
      "     |  lc_namespace\n",
      "     |      Return the namespace of the langchain object.\n",
      "     |      eg. [\"langchain\", \"llms\", \"openai\"]\n",
      "     |  \n",
      "     |  lc_secrets\n",
      "     |      Return a map of constructor argument names to secret ids.\n",
      "     |      eg. {\"openai_api_key\": \"OPENAI_API_KEY\"}\n",
      "     |  \n",
      "     |  lc_serializable\n",
      "     |      Return whether or not the class is serializable.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __eq__(self, other: Any) -> bool\n",
      "     |  \n",
      "     |  __getstate__(self) -> 'DictAny'\n",
      "     |  \n",
      "     |  __iter__(self) -> 'TupleGenerator'\n",
      "     |      so `dict(model)` works\n",
      "     |  \n",
      "     |  __repr_args__(self) -> 'ReprArgs'\n",
      "     |  \n",
      "     |  __setattr__(self, name, value)\n",
      "     |  \n",
      "     |  __setstate__(self, state: 'DictAny') -> None\n",
      "     |  \n",
      "     |  copy(self: 'Model', *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, update: Optional[ForwardRef('DictStrAny')] = None, deep: bool = False) -> 'Model'\n",
      "     |      Duplicate a model, optionally choose which fields to include, exclude and change.\n",
      "     |      \n",
      "     |      :param include: fields to include in new model\n",
      "     |      :param exclude: fields to exclude from new model, as with values this takes precedence over include\n",
      "     |      :param update: values to change/add in the new model. Note: the data is not validated before creating\n",
      "     |          the new model: you should trust this data\n",
      "     |      :param deep: set to `True` to make a deep copy of the model\n",
      "     |      :return: new model instance\n",
      "     |  \n",
      "     |  json(self, *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) -> 'unicode'\n",
      "     |      Generate a JSON representation of the model, `include` and `exclude` arguments as per `dict()`.\n",
      "     |      \n",
      "     |      `encoder` is an optional function to supply as `default` to json.dumps(), other arguments as per `json.dumps()`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __get_validators__() -> 'CallableGenerator' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  __try_update_forward_refs__(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      "     |      Same as update_forward_refs but will not raise exception\n",
      "     |      when forward references are not defined.\n",
      "     |  \n",
      "     |  construct(_fields_set: Optional[ForwardRef('SetStr')] = None, **values: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |      Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data.\n",
      "     |      Default values are respected, but no other validation is performed.\n",
      "     |      Behaves as if `Config.extra = 'allow'` was set since it adds all passed values\n",
      "     |  \n",
      "     |  from_orm(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_file(path: Union[str, pathlib.Path], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_obj(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_raw(b: Union[str, bytes], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema(by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}') -> 'DictStrAny' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema_json(*, by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}', **dumps_kwargs: Any) -> 'unicode' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  update_forward_refs(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      "     |      Try to update ForwardRefs on fields based on this Model, globalns and localns.\n",
      "     |  \n",
      "     |  validate(value: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __fields_set__\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.utils.Representation:\n",
      "     |  \n",
      "     |  __pretty__(self, fmt: Callable[[Any], Any], **kwargs: Any) -> Generator[Any, NoneType, NoneType]\n",
      "     |      Used by devtools (https://python-devtools.helpmanual.io/) to provide a human readable representations of objects\n",
      "     |  \n",
      "     |  __repr__(self) -> 'unicode'\n",
      "     |  \n",
      "     |  __repr_name__(self) -> 'unicode'\n",
      "     |      Name of the instance's class, used in __repr__.\n",
      "     |  \n",
      "     |  __repr_str__(self, join_str: 'unicode') -> 'unicode'\n",
      "     |  \n",
      "     |  __rich_repr__(self) -> 'RichReprResult'\n",
      "     |      Get fields for Rich library\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.schema.runnable.base.Runnable:\n",
      "     |  \n",
      "     |  __or__(self, other: 'Union[Runnable[Any, Other], Callable[[Any], Other], Mapping[str, Union[Runnable[Any, Other], Callable[[Any], Other], Any]]]') -> 'RunnableSequence[Input, Other]'\n",
      "     |      Return self|value.\n",
      "     |  \n",
      "     |  __ror__(self, other: 'Union[Runnable[Other, Any], Callable[[Any], Other], Mapping[str, Union[Runnable[Other, Any], Callable[[Other], Any], Any]]]') -> 'RunnableSequence[Other, Output]'\n",
      "     |      Return value|self.\n",
      "     |  \n",
      "     |  async atransform(self, input: 'AsyncIterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'AsyncIterator[Output]'\n",
      "     |      Default implementation of atransform, which buffers input and calls astream.\n",
      "     |      Subclasses should override this method if they can start producing output while\n",
      "     |      input is still being generated.\n",
      "     |  \n",
      "     |  bind(self, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      "     |      Bind arguments to a Runnable, returning a new Runnable.\n",
      "     |  \n",
      "     |  map(self) -> 'Runnable[List[Input], List[Output]]'\n",
      "     |      Return a new Runnable that maps a list of inputs to a list of outputs,\n",
      "     |      by calling invoke() with each input.\n",
      "     |  \n",
      "     |  transform(self, input: 'Iterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'Iterator[Output]'\n",
      "     |      Default implementation of transform, which buffers input and then calls stream.\n",
      "     |      Subclasses should override this method if they can start producing output while\n",
      "     |      input is still being generated.\n",
      "     |  \n",
      "     |  with_config(self, config: 'Optional[RunnableConfig]' = None, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      "     |      Bind config to a Runnable, returning a new Runnable.\n",
      "     |  \n",
      "     |  with_fallbacks(self, fallbacks: 'Sequence[Runnable[Input, Output]]', *, exceptions_to_handle: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,)) -> 'RunnableWithFallbacks[Input, Output]'\n",
      "     |  \n",
      "     |  with_retry(self, *, retry_if_exception_type: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,), wait_exponential_jitter: 'bool' = True, stop_after_attempt: 'int' = 3) -> 'Runnable[Input, Output]'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __class_getitem__(params) from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  __init_subclass__(*args, **kwargs) from pydantic.main.ModelMetaclass\n",
      "     |      This method is called when a class is subclassed.\n",
      "     |      \n",
      "     |      The default implementation does nothing. It may be\n",
      "     |      overridden to extend subclasses.\n",
      "    \n",
      "    class ChatGLM(langchain.llms.base.LLM)\n",
      "     |  ChatGLM(*, cache: Optional[bool] = None, verbose: bool = None, callbacks: Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager, NoneType] = None, callback_manager: Optional[langchain.callbacks.base.BaseCallbackManager] = None, tags: Optional[List[str]] = None, metadata: Optional[Dict[str, Any]] = None, endpoint_url: str = 'http://127.0.0.1:8000/', model_kwargs: Optional[dict] = None, max_token: int = 20000, temperature: float = 0.1, history: List[List] = [], top_p: float = 0.7, with_history: bool = False) -> None\n",
      "     |  \n",
      "     |  ChatGLM LLM service.\n",
      "     |  \n",
      "     |  Example:\n",
      "     |      .. code-block:: python\n",
      "     |  \n",
      "     |          from langchain.llms import ChatGLM\n",
      "     |          endpoint_url = (\n",
      "     |              \"http://127.0.0.1:8000\"\n",
      "     |          )\n",
      "     |          ChatGLM_llm = ChatGLM(\n",
      "     |              endpoint_url=endpoint_url\n",
      "     |          )\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      ChatGLM\n",
      "     |      langchain.llms.base.LLM\n",
      "     |      langchain.llms.base.BaseLLM\n",
      "     |      langchain.schema.language_model.BaseLanguageModel\n",
      "     |      langchain.load.serializable.Serializable\n",
      "     |      pydantic.main.BaseModel\n",
      "     |      pydantic.utils.Representation\n",
      "     |      langchain.schema.runnable.base.Runnable\n",
      "     |      typing.Generic\n",
      "     |      abc.ABC\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __json_encoder__ = pydantic_encoder(obj: Any) -> Any\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {'endpoint_url': <class 'str'>, 'history': typing.Li...\n",
      "     |  \n",
      "     |  __class_vars__ = set()\n",
      "     |  \n",
      "     |  __config__ = <class 'langchain.llms.chatglm.Config'>\n",
      "     |  \n",
      "     |  __custom_root_type__ = False\n",
      "     |  \n",
      "     |  __exclude_fields__ = {'callback_manager': True, 'callbacks': True, 'me...\n",
      "     |  \n",
      "     |  __fields__ = {'cache': ModelField(name='cache', type=Optional[bool], r...\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "     |  \n",
      "     |  __include_fields__ = None\n",
      "     |  \n",
      "     |  __parameters__ = ()\n",
      "     |  \n",
      "     |  __post_root_validators__ = [(False, <function BaseLLM.raise_deprecatio...\n",
      "     |  \n",
      "     |  __pre_root_validators__ = []\n",
      "     |  \n",
      "     |  __private_attributes__ = {'_lc_kwargs': ModelPrivateAttr(default=Pydan...\n",
      "     |  \n",
      "     |  __schema_cache__ = {}\n",
      "     |  \n",
      "     |  __signature__ = <Signature (*, cache: Optional[bool] = None, ver...flo...\n",
      "     |  \n",
      "     |  __validators__ = {'verbose': [<pydantic.class_validators.Validator obj...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  __call__(self, prompt: 'str', stop: 'Optional[List[str]]' = None, callbacks: 'Callbacks' = None, *, tags: 'Optional[List[str]]' = None, metadata: 'Optional[Dict[str, Any]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Check Cache and run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  __str__(self) -> 'str'\n",
      "     |      Get a string representation of the object for printing.\n",
      "     |  \n",
      "     |  async abatch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      "     |      Default implementation of abatch, which calls ainvoke N times.\n",
      "     |      Subclasses should override this method if they can batch more efficiently.\n",
      "     |  \n",
      "     |  async agenerate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  async agenerate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Asynchronously pass a sequence of prompts and return model generations.\n",
      "     |      \n",
      "     |      This method should make use of batched calls for models that expose a batched\n",
      "     |      API.\n",
      "     |      \n",
      "     |      Use this method when you want to:\n",
      "     |          1. take advantage of batched calls,\n",
      "     |          2. need more output from the model than just the top generated value,\n",
      "     |          3. are building chains that are agnostic to the underlying language model\n",
      "     |              type (e.g., pure text completion models vs chat models).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      "     |              converted to match the format of any language model (string for pure\n",
      "     |              text generation models and BaseMessages for chat models).\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          callbacks: Callbacks to pass through. Used for executing additional\n",
      "     |              functionality, such as logging or streaming, throughout generation.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          An LLMResult, which contains a list of candidate Generations for each input\n",
      "     |              prompt and additional model provider-specific output.\n",
      "     |  \n",
      "     |  async ainvoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Default implementation of ainvoke, which calls invoke in a thread pool.\n",
      "     |      Subclasses should override this method if they can run asynchronously.\n",
      "     |  \n",
      "     |  async apredict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Asynchronously pass a string to the model and return a string prediction.\n",
      "     |      \n",
      "     |      Use this method when calling pure text generation models and only the top\n",
      "     |          candidate generation is needed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: String input to pass to the model.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a string.\n",
      "     |  \n",
      "     |  async apredict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      "     |      Asynchronously pass messages to the model and return a message prediction.\n",
      "     |      \n",
      "     |      Use this method when calling chat models and only the top\n",
      "     |          candidate generation is needed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: A sequence of chat messages corresponding to a single model input.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a message.\n",
      "     |  \n",
      "     |  async astream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'AsyncIterator[str]'\n",
      "     |      Default implementation of astream, which calls ainvoke.\n",
      "     |      Subclasses should override this method if they support streaming output.\n",
      "     |  \n",
      "     |  batch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      "     |      Default implementation of batch, which calls invoke N times.\n",
      "     |      Subclasses should override this method if they can batch more efficiently.\n",
      "     |  \n",
      "     |  dict(self, **kwargs: 'Any') -> 'Dict'\n",
      "     |      Return a dictionary of the LLM.\n",
      "     |  \n",
      "     |  generate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  generate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Pass a sequence of prompts to the model and return model generations.\n",
      "     |      \n",
      "     |      This method should make use of batched calls for models that expose a batched\n",
      "     |      API.\n",
      "     |      \n",
      "     |      Use this method when you want to:\n",
      "     |          1. take advantage of batched calls,\n",
      "     |          2. need more output from the model than just the top generated value,\n",
      "     |          3. are building chains that are agnostic to the underlying language model\n",
      "     |              type (e.g., pure text completion models vs chat models).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      "     |              converted to match the format of any language model (string for pure\n",
      "     |              text generation models and BaseMessages for chat models).\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          callbacks: Callbacks to pass through. Used for executing additional\n",
      "     |              functionality, such as logging or streaming, throughout generation.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          An LLMResult, which contains a list of candidate Generations for each input\n",
      "     |              prompt and additional model provider-specific output.\n",
      "     |  \n",
      "     |  invoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |  \n",
      "     |  predict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Pass a single string input to the model and return a string prediction.\n",
      "     |      \n",
      "     |       Use this method when passing in raw text. If you want to pass in specific\n",
      "     |          types of chat messages, use predict_messages.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: String input to pass to the model.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a string.\n",
      "     |  \n",
      "     |  predict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      "     |      Pass a message sequence to the model and return a message prediction.\n",
      "     |      \n",
      "     |      Use this method when passing in chat messages. If you want to pass in raw text,\n",
      "     |          use predict.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: A sequence of chat messages corresponding to a single model input.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a message.\n",
      "     |  \n",
      "     |  save(self, file_path: 'Union[Path, str]') -> 'None'\n",
      "     |      Save the LLM.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          file_path: Path to file to save the LLM to.\n",
      "     |      \n",
      "     |      Example:\n",
      "     |      .. code-block:: python\n",
      "     |      \n",
      "     |          llm.save(file_path=\"path/llm.yaml\")\n",
      "     |  \n",
      "     |  stream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'Iterator[str]'\n",
      "     |      Default implementation of stream, which calls invoke.\n",
      "     |      Subclasses should override this method if they support streaming output.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  raise_deprecation(values: 'Dict') -> 'Dict' from pydantic.main.ModelMetaclass\n",
      "     |      Raise deprecation warning if callback_manager is used.\n",
      "     |  \n",
      "     |  set_verbose(verbose: 'Optional[bool]') -> 'bool' from pydantic.main.ModelMetaclass\n",
      "     |      If verbose is None, set it.\n",
      "     |      \n",
      "     |      This allows users to pass in None as verbose to access the global setting.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  Config = <class 'langchain.llms.base.BaseLLM.Config'>\n",
      "     |      Configuration for this pydantic object.\n",
      "     |  \n",
      "     |  \n",
      "     |  __orig_bases__ = (langchain.schema.language_model.BaseLanguageModel[st...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.schema.language_model.BaseLanguageModel:\n",
      "     |  \n",
      "     |  get_num_tokens(self, text: 'str') -> 'int'\n",
      "     |      Get the number of tokens present in the text.\n",
      "     |      \n",
      "     |      Useful for checking if an input will fit in a model's context window.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: The string input to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The integer number of tokens in the text.\n",
      "     |  \n",
      "     |  get_num_tokens_from_messages(self, messages: 'List[BaseMessage]') -> 'int'\n",
      "     |      Get the number of tokens in the messages.\n",
      "     |      \n",
      "     |      Useful for checking if an input will fit in a model's context window.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: The message inputs to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The sum of the number of tokens across the messages.\n",
      "     |  \n",
      "     |  get_token_ids(self, text: 'str') -> 'List[int]'\n",
      "     |      Return the ordered ids of the tokens in a text.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: The string input to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          A list of ids corresponding to the tokens in the text, in order they occur\n",
      "     |              in the text.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from langchain.schema.language_model.BaseLanguageModel:\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.load.serializable.Serializable:\n",
      "     |  \n",
      "     |  __init__(self, **kwargs: Any) -> None\n",
      "     |      Create a new model by parsing and validating input data from keyword arguments.\n",
      "     |      \n",
      "     |      Raises ValidationError if the input data cannot be parsed to form a valid model.\n",
      "     |  \n",
      "     |  to_json(self) -> Union[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\n",
      "     |  \n",
      "     |  to_json_not_implemented(self) -> langchain.load.serializable.SerializedNotImplemented\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from langchain.load.serializable.Serializable:\n",
      "     |  \n",
      "     |  lc_attributes\n",
      "     |      Return a list of attribute names that should be included in the\n",
      "     |      serialized kwargs. These attributes must be accepted by the\n",
      "     |      constructor.\n",
      "     |  \n",
      "     |  lc_namespace\n",
      "     |      Return the namespace of the langchain object.\n",
      "     |      eg. [\"langchain\", \"llms\", \"openai\"]\n",
      "     |  \n",
      "     |  lc_secrets\n",
      "     |      Return a map of constructor argument names to secret ids.\n",
      "     |      eg. {\"openai_api_key\": \"OPENAI_API_KEY\"}\n",
      "     |  \n",
      "     |  lc_serializable\n",
      "     |      Return whether or not the class is serializable.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __eq__(self, other: Any) -> bool\n",
      "     |  \n",
      "     |  __getstate__(self) -> 'DictAny'\n",
      "     |  \n",
      "     |  __iter__(self) -> 'TupleGenerator'\n",
      "     |      so `dict(model)` works\n",
      "     |  \n",
      "     |  __repr_args__(self) -> 'ReprArgs'\n",
      "     |  \n",
      "     |  __setattr__(self, name, value)\n",
      "     |  \n",
      "     |  __setstate__(self, state: 'DictAny') -> None\n",
      "     |  \n",
      "     |  copy(self: 'Model', *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, update: Optional[ForwardRef('DictStrAny')] = None, deep: bool = False) -> 'Model'\n",
      "     |      Duplicate a model, optionally choose which fields to include, exclude and change.\n",
      "     |      \n",
      "     |      :param include: fields to include in new model\n",
      "     |      :param exclude: fields to exclude from new model, as with values this takes precedence over include\n",
      "     |      :param update: values to change/add in the new model. Note: the data is not validated before creating\n",
      "     |          the new model: you should trust this data\n",
      "     |      :param deep: set to `True` to make a deep copy of the model\n",
      "     |      :return: new model instance\n",
      "     |  \n",
      "     |  json(self, *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) -> 'unicode'\n",
      "     |      Generate a JSON representation of the model, `include` and `exclude` arguments as per `dict()`.\n",
      "     |      \n",
      "     |      `encoder` is an optional function to supply as `default` to json.dumps(), other arguments as per `json.dumps()`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __get_validators__() -> 'CallableGenerator' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  __try_update_forward_refs__(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      "     |      Same as update_forward_refs but will not raise exception\n",
      "     |      when forward references are not defined.\n",
      "     |  \n",
      "     |  construct(_fields_set: Optional[ForwardRef('SetStr')] = None, **values: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |      Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data.\n",
      "     |      Default values are respected, but no other validation is performed.\n",
      "     |      Behaves as if `Config.extra = 'allow'` was set since it adds all passed values\n",
      "     |  \n",
      "     |  from_orm(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_file(path: Union[str, pathlib.Path], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_obj(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_raw(b: Union[str, bytes], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema(by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}') -> 'DictStrAny' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema_json(*, by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}', **dumps_kwargs: Any) -> 'unicode' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  update_forward_refs(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      "     |      Try to update ForwardRefs on fields based on this Model, globalns and localns.\n",
      "     |  \n",
      "     |  validate(value: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __fields_set__\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.utils.Representation:\n",
      "     |  \n",
      "     |  __pretty__(self, fmt: Callable[[Any], Any], **kwargs: Any) -> Generator[Any, NoneType, NoneType]\n",
      "     |      Used by devtools (https://python-devtools.helpmanual.io/) to provide a human readable representations of objects\n",
      "     |  \n",
      "     |  __repr__(self) -> 'unicode'\n",
      "     |  \n",
      "     |  __repr_name__(self) -> 'unicode'\n",
      "     |      Name of the instance's class, used in __repr__.\n",
      "     |  \n",
      "     |  __repr_str__(self, join_str: 'unicode') -> 'unicode'\n",
      "     |  \n",
      "     |  __rich_repr__(self) -> 'RichReprResult'\n",
      "     |      Get fields for Rich library\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.schema.runnable.base.Runnable:\n",
      "     |  \n",
      "     |  __or__(self, other: 'Union[Runnable[Any, Other], Callable[[Any], Other], Mapping[str, Union[Runnable[Any, Other], Callable[[Any], Other], Any]]]') -> 'RunnableSequence[Input, Other]'\n",
      "     |      Return self|value.\n",
      "     |  \n",
      "     |  __ror__(self, other: 'Union[Runnable[Other, Any], Callable[[Any], Other], Mapping[str, Union[Runnable[Other, Any], Callable[[Other], Any], Any]]]') -> 'RunnableSequence[Other, Output]'\n",
      "     |      Return value|self.\n",
      "     |  \n",
      "     |  async atransform(self, input: 'AsyncIterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'AsyncIterator[Output]'\n",
      "     |      Default implementation of atransform, which buffers input and calls astream.\n",
      "     |      Subclasses should override this method if they can start producing output while\n",
      "     |      input is still being generated.\n",
      "     |  \n",
      "     |  bind(self, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      "     |      Bind arguments to a Runnable, returning a new Runnable.\n",
      "     |  \n",
      "     |  map(self) -> 'Runnable[List[Input], List[Output]]'\n",
      "     |      Return a new Runnable that maps a list of inputs to a list of outputs,\n",
      "     |      by calling invoke() with each input.\n",
      "     |  \n",
      "     |  transform(self, input: 'Iterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'Iterator[Output]'\n",
      "     |      Default implementation of transform, which buffers input and then calls stream.\n",
      "     |      Subclasses should override this method if they can start producing output while\n",
      "     |      input is still being generated.\n",
      "     |  \n",
      "     |  with_config(self, config: 'Optional[RunnableConfig]' = None, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      "     |      Bind config to a Runnable, returning a new Runnable.\n",
      "     |  \n",
      "     |  with_fallbacks(self, fallbacks: 'Sequence[Runnable[Input, Output]]', *, exceptions_to_handle: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,)) -> 'RunnableWithFallbacks[Input, Output]'\n",
      "     |  \n",
      "     |  with_retry(self, *, retry_if_exception_type: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,), wait_exponential_jitter: 'bool' = True, stop_after_attempt: 'int' = 3) -> 'Runnable[Input, Output]'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __class_getitem__(params) from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  __init_subclass__(*args, **kwargs) from pydantic.main.ModelMetaclass\n",
      "     |      This method is called when a class is subclassed.\n",
      "     |      \n",
      "     |      The default implementation does nothing. It may be\n",
      "     |      overridden to extend subclasses.\n",
      "    \n",
      "    class Clarifai(langchain.llms.base.LLM)\n",
      "     |  Clarifai(*, cache: Optional[bool] = None, verbose: bool = None, callbacks: Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager, NoneType] = None, callback_manager: Optional[langchain.callbacks.base.BaseCallbackManager] = None, tags: Optional[List[str]] = None, metadata: Optional[Dict[str, Any]] = None, stub: Any = None, userDataObject: Any = None, model_id: Optional[str] = None, model_version_id: Optional[str] = None, app_id: Optional[str] = None, user_id: Optional[str] = None, pat: Optional[str] = None, api_base: str = 'https://api.clarifai.com') -> None\n",
      "     |  \n",
      "     |  Clarifai large language models.\n",
      "     |  \n",
      "     |  To use, you should have an account on the Clarifai platform,\n",
      "     |  the ``clarifai`` python package installed, and the\n",
      "     |  environment variable ``CLARIFAI_PAT`` set with your PAT key,\n",
      "     |  or pass it as a named parameter to the constructor.\n",
      "     |  \n",
      "     |  Example:\n",
      "     |      .. code-block:: python\n",
      "     |  \n",
      "     |          from langchain.llms import Clarifai\n",
      "     |          clarifai_llm = Clarifai(pat=CLARIFAI_PAT,                 user_id=USER_ID, app_id=APP_ID, model_id=MODEL_ID)\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Clarifai\n",
      "     |      langchain.llms.base.LLM\n",
      "     |      langchain.llms.base.BaseLLM\n",
      "     |      langchain.schema.language_model.BaseLanguageModel\n",
      "     |      langchain.load.serializable.Serializable\n",
      "     |      pydantic.main.BaseModel\n",
      "     |      pydantic.utils.Representation\n",
      "     |      langchain.schema.runnable.base.Runnable\n",
      "     |      typing.Generic\n",
      "     |      abc.ABC\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  validate_environment(values: Dict) -> Dict from pydantic.main.ModelMetaclass\n",
      "     |      Validate that we have all required info to access Clarifai\n",
      "     |      platform and python package exists in environment.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __json_encoder__ = pydantic_encoder(obj: Any) -> Any\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  Config = <class 'langchain.llms.clarifai.Clarifai.Config'>\n",
      "     |      Configuration for this pydantic object.\n",
      "     |  \n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {'api_base': <class 'str'>, 'app_id': typing.Optiona...\n",
      "     |  \n",
      "     |  __class_vars__ = set()\n",
      "     |  \n",
      "     |  __config__ = <class 'langchain.llms.clarifai.Config'>\n",
      "     |  \n",
      "     |  __custom_root_type__ = False\n",
      "     |  \n",
      "     |  __exclude_fields__ = {'callback_manager': True, 'callbacks': True, 'me...\n",
      "     |  \n",
      "     |  __fields__ = {'api_base': ModelField(name='api_base', type=str, requir...\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "     |  \n",
      "     |  __include_fields__ = None\n",
      "     |  \n",
      "     |  __parameters__ = ()\n",
      "     |  \n",
      "     |  __post_root_validators__ = [(False, <function BaseLLM.raise_deprecatio...\n",
      "     |  \n",
      "     |  __pre_root_validators__ = []\n",
      "     |  \n",
      "     |  __private_attributes__ = {'_lc_kwargs': ModelPrivateAttr(default=Pydan...\n",
      "     |  \n",
      "     |  __schema_cache__ = {}\n",
      "     |  \n",
      "     |  __signature__ = <Signature (*, cache: Optional[bool] = None, ver..._ba...\n",
      "     |  \n",
      "     |  __validators__ = {'verbose': [<pydantic.class_validators.Validator obj...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  __call__(self, prompt: 'str', stop: 'Optional[List[str]]' = None, callbacks: 'Callbacks' = None, *, tags: 'Optional[List[str]]' = None, metadata: 'Optional[Dict[str, Any]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Check Cache and run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  __str__(self) -> 'str'\n",
      "     |      Get a string representation of the object for printing.\n",
      "     |  \n",
      "     |  async abatch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      "     |      Default implementation of abatch, which calls ainvoke N times.\n",
      "     |      Subclasses should override this method if they can batch more efficiently.\n",
      "     |  \n",
      "     |  async agenerate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  async agenerate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Asynchronously pass a sequence of prompts and return model generations.\n",
      "     |      \n",
      "     |      This method should make use of batched calls for models that expose a batched\n",
      "     |      API.\n",
      "     |      \n",
      "     |      Use this method when you want to:\n",
      "     |          1. take advantage of batched calls,\n",
      "     |          2. need more output from the model than just the top generated value,\n",
      "     |          3. are building chains that are agnostic to the underlying language model\n",
      "     |              type (e.g., pure text completion models vs chat models).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      "     |              converted to match the format of any language model (string for pure\n",
      "     |              text generation models and BaseMessages for chat models).\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          callbacks: Callbacks to pass through. Used for executing additional\n",
      "     |              functionality, such as logging or streaming, throughout generation.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          An LLMResult, which contains a list of candidate Generations for each input\n",
      "     |              prompt and additional model provider-specific output.\n",
      "     |  \n",
      "     |  async ainvoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Default implementation of ainvoke, which calls invoke in a thread pool.\n",
      "     |      Subclasses should override this method if they can run asynchronously.\n",
      "     |  \n",
      "     |  async apredict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Asynchronously pass a string to the model and return a string prediction.\n",
      "     |      \n",
      "     |      Use this method when calling pure text generation models and only the top\n",
      "     |          candidate generation is needed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: String input to pass to the model.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a string.\n",
      "     |  \n",
      "     |  async apredict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      "     |      Asynchronously pass messages to the model and return a message prediction.\n",
      "     |      \n",
      "     |      Use this method when calling chat models and only the top\n",
      "     |          candidate generation is needed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: A sequence of chat messages corresponding to a single model input.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a message.\n",
      "     |  \n",
      "     |  async astream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'AsyncIterator[str]'\n",
      "     |      Default implementation of astream, which calls ainvoke.\n",
      "     |      Subclasses should override this method if they support streaming output.\n",
      "     |  \n",
      "     |  batch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      "     |      Default implementation of batch, which calls invoke N times.\n",
      "     |      Subclasses should override this method if they can batch more efficiently.\n",
      "     |  \n",
      "     |  dict(self, **kwargs: 'Any') -> 'Dict'\n",
      "     |      Return a dictionary of the LLM.\n",
      "     |  \n",
      "     |  generate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  generate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Pass a sequence of prompts to the model and return model generations.\n",
      "     |      \n",
      "     |      This method should make use of batched calls for models that expose a batched\n",
      "     |      API.\n",
      "     |      \n",
      "     |      Use this method when you want to:\n",
      "     |          1. take advantage of batched calls,\n",
      "     |          2. need more output from the model than just the top generated value,\n",
      "     |          3. are building chains that are agnostic to the underlying language model\n",
      "     |              type (e.g., pure text completion models vs chat models).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      "     |              converted to match the format of any language model (string for pure\n",
      "     |              text generation models and BaseMessages for chat models).\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          callbacks: Callbacks to pass through. Used for executing additional\n",
      "     |              functionality, such as logging or streaming, throughout generation.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          An LLMResult, which contains a list of candidate Generations for each input\n",
      "     |              prompt and additional model provider-specific output.\n",
      "     |  \n",
      "     |  invoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |  \n",
      "     |  predict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Pass a single string input to the model and return a string prediction.\n",
      "     |      \n",
      "     |       Use this method when passing in raw text. If you want to pass in specific\n",
      "     |          types of chat messages, use predict_messages.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: String input to pass to the model.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a string.\n",
      "     |  \n",
      "     |  predict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      "     |      Pass a message sequence to the model and return a message prediction.\n",
      "     |      \n",
      "     |      Use this method when passing in chat messages. If you want to pass in raw text,\n",
      "     |          use predict.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: A sequence of chat messages corresponding to a single model input.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a message.\n",
      "     |  \n",
      "     |  save(self, file_path: 'Union[Path, str]') -> 'None'\n",
      "     |      Save the LLM.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          file_path: Path to file to save the LLM to.\n",
      "     |      \n",
      "     |      Example:\n",
      "     |      .. code-block:: python\n",
      "     |      \n",
      "     |          llm.save(file_path=\"path/llm.yaml\")\n",
      "     |  \n",
      "     |  stream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'Iterator[str]'\n",
      "     |      Default implementation of stream, which calls invoke.\n",
      "     |      Subclasses should override this method if they support streaming output.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  raise_deprecation(values: 'Dict') -> 'Dict' from pydantic.main.ModelMetaclass\n",
      "     |      Raise deprecation warning if callback_manager is used.\n",
      "     |  \n",
      "     |  set_verbose(verbose: 'Optional[bool]') -> 'bool' from pydantic.main.ModelMetaclass\n",
      "     |      If verbose is None, set it.\n",
      "     |      \n",
      "     |      This allows users to pass in None as verbose to access the global setting.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  __orig_bases__ = (langchain.schema.language_model.BaseLanguageModel[st...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.schema.language_model.BaseLanguageModel:\n",
      "     |  \n",
      "     |  get_num_tokens(self, text: 'str') -> 'int'\n",
      "     |      Get the number of tokens present in the text.\n",
      "     |      \n",
      "     |      Useful for checking if an input will fit in a model's context window.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: The string input to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The integer number of tokens in the text.\n",
      "     |  \n",
      "     |  get_num_tokens_from_messages(self, messages: 'List[BaseMessage]') -> 'int'\n",
      "     |      Get the number of tokens in the messages.\n",
      "     |      \n",
      "     |      Useful for checking if an input will fit in a model's context window.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: The message inputs to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The sum of the number of tokens across the messages.\n",
      "     |  \n",
      "     |  get_token_ids(self, text: 'str') -> 'List[int]'\n",
      "     |      Return the ordered ids of the tokens in a text.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: The string input to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          A list of ids corresponding to the tokens in the text, in order they occur\n",
      "     |              in the text.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from langchain.schema.language_model.BaseLanguageModel:\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.load.serializable.Serializable:\n",
      "     |  \n",
      "     |  __init__(self, **kwargs: Any) -> None\n",
      "     |      Create a new model by parsing and validating input data from keyword arguments.\n",
      "     |      \n",
      "     |      Raises ValidationError if the input data cannot be parsed to form a valid model.\n",
      "     |  \n",
      "     |  to_json(self) -> Union[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\n",
      "     |  \n",
      "     |  to_json_not_implemented(self) -> langchain.load.serializable.SerializedNotImplemented\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from langchain.load.serializable.Serializable:\n",
      "     |  \n",
      "     |  lc_attributes\n",
      "     |      Return a list of attribute names that should be included in the\n",
      "     |      serialized kwargs. These attributes must be accepted by the\n",
      "     |      constructor.\n",
      "     |  \n",
      "     |  lc_namespace\n",
      "     |      Return the namespace of the langchain object.\n",
      "     |      eg. [\"langchain\", \"llms\", \"openai\"]\n",
      "     |  \n",
      "     |  lc_secrets\n",
      "     |      Return a map of constructor argument names to secret ids.\n",
      "     |      eg. {\"openai_api_key\": \"OPENAI_API_KEY\"}\n",
      "     |  \n",
      "     |  lc_serializable\n",
      "     |      Return whether or not the class is serializable.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __eq__(self, other: Any) -> bool\n",
      "     |  \n",
      "     |  __getstate__(self) -> 'DictAny'\n",
      "     |  \n",
      "     |  __iter__(self) -> 'TupleGenerator'\n",
      "     |      so `dict(model)` works\n",
      "     |  \n",
      "     |  __repr_args__(self) -> 'ReprArgs'\n",
      "     |  \n",
      "     |  __setattr__(self, name, value)\n",
      "     |  \n",
      "     |  __setstate__(self, state: 'DictAny') -> None\n",
      "     |  \n",
      "     |  copy(self: 'Model', *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, update: Optional[ForwardRef('DictStrAny')] = None, deep: bool = False) -> 'Model'\n",
      "     |      Duplicate a model, optionally choose which fields to include, exclude and change.\n",
      "     |      \n",
      "     |      :param include: fields to include in new model\n",
      "     |      :param exclude: fields to exclude from new model, as with values this takes precedence over include\n",
      "     |      :param update: values to change/add in the new model. Note: the data is not validated before creating\n",
      "     |          the new model: you should trust this data\n",
      "     |      :param deep: set to `True` to make a deep copy of the model\n",
      "     |      :return: new model instance\n",
      "     |  \n",
      "     |  json(self, *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) -> 'unicode'\n",
      "     |      Generate a JSON representation of the model, `include` and `exclude` arguments as per `dict()`.\n",
      "     |      \n",
      "     |      `encoder` is an optional function to supply as `default` to json.dumps(), other arguments as per `json.dumps()`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __get_validators__() -> 'CallableGenerator' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  __try_update_forward_refs__(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      "     |      Same as update_forward_refs but will not raise exception\n",
      "     |      when forward references are not defined.\n",
      "     |  \n",
      "     |  construct(_fields_set: Optional[ForwardRef('SetStr')] = None, **values: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |      Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data.\n",
      "     |      Default values are respected, but no other validation is performed.\n",
      "     |      Behaves as if `Config.extra = 'allow'` was set since it adds all passed values\n",
      "     |  \n",
      "     |  from_orm(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_file(path: Union[str, pathlib.Path], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_obj(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_raw(b: Union[str, bytes], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema(by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}') -> 'DictStrAny' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema_json(*, by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}', **dumps_kwargs: Any) -> 'unicode' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  update_forward_refs(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      "     |      Try to update ForwardRefs on fields based on this Model, globalns and localns.\n",
      "     |  \n",
      "     |  validate(value: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __fields_set__\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.utils.Representation:\n",
      "     |  \n",
      "     |  __pretty__(self, fmt: Callable[[Any], Any], **kwargs: Any) -> Generator[Any, NoneType, NoneType]\n",
      "     |      Used by devtools (https://python-devtools.helpmanual.io/) to provide a human readable representations of objects\n",
      "     |  \n",
      "     |  __repr__(self) -> 'unicode'\n",
      "     |  \n",
      "     |  __repr_name__(self) -> 'unicode'\n",
      "     |      Name of the instance's class, used in __repr__.\n",
      "     |  \n",
      "     |  __repr_str__(self, join_str: 'unicode') -> 'unicode'\n",
      "     |  \n",
      "     |  __rich_repr__(self) -> 'RichReprResult'\n",
      "     |      Get fields for Rich library\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.schema.runnable.base.Runnable:\n",
      "     |  \n",
      "     |  __or__(self, other: 'Union[Runnable[Any, Other], Callable[[Any], Other], Mapping[str, Union[Runnable[Any, Other], Callable[[Any], Other], Any]]]') -> 'RunnableSequence[Input, Other]'\n",
      "     |      Return self|value.\n",
      "     |  \n",
      "     |  __ror__(self, other: 'Union[Runnable[Other, Any], Callable[[Any], Other], Mapping[str, Union[Runnable[Other, Any], Callable[[Other], Any], Any]]]') -> 'RunnableSequence[Other, Output]'\n",
      "     |      Return value|self.\n",
      "     |  \n",
      "     |  async atransform(self, input: 'AsyncIterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'AsyncIterator[Output]'\n",
      "     |      Default implementation of atransform, which buffers input and calls astream.\n",
      "     |      Subclasses should override this method if they can start producing output while\n",
      "     |      input is still being generated.\n",
      "     |  \n",
      "     |  bind(self, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      "     |      Bind arguments to a Runnable, returning a new Runnable.\n",
      "     |  \n",
      "     |  map(self) -> 'Runnable[List[Input], List[Output]]'\n",
      "     |      Return a new Runnable that maps a list of inputs to a list of outputs,\n",
      "     |      by calling invoke() with each input.\n",
      "     |  \n",
      "     |  transform(self, input: 'Iterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'Iterator[Output]'\n",
      "     |      Default implementation of transform, which buffers input and then calls stream.\n",
      "     |      Subclasses should override this method if they can start producing output while\n",
      "     |      input is still being generated.\n",
      "     |  \n",
      "     |  with_config(self, config: 'Optional[RunnableConfig]' = None, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      "     |      Bind config to a Runnable, returning a new Runnable.\n",
      "     |  \n",
      "     |  with_fallbacks(self, fallbacks: 'Sequence[Runnable[Input, Output]]', *, exceptions_to_handle: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,)) -> 'RunnableWithFallbacks[Input, Output]'\n",
      "     |  \n",
      "     |  with_retry(self, *, retry_if_exception_type: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,), wait_exponential_jitter: 'bool' = True, stop_after_attempt: 'int' = 3) -> 'Runnable[Input, Output]'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __class_getitem__(params) from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  __init_subclass__(*args, **kwargs) from pydantic.main.ModelMetaclass\n",
      "     |      This method is called when a class is subclassed.\n",
      "     |      \n",
      "     |      The default implementation does nothing. It may be\n",
      "     |      overridden to extend subclasses.\n",
      "    \n",
      "    class Cohere(langchain.llms.base.LLM)\n",
      "     |  Cohere(*, cache: Optional[bool] = None, verbose: bool = None, callbacks: Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager, NoneType] = None, callback_manager: Optional[langchain.callbacks.base.BaseCallbackManager] = None, tags: Optional[List[str]] = None, metadata: Optional[Dict[str, Any]] = None, client: Any = None, async_client: Any = None, model: Optional[str] = None, max_tokens: int = 256, temperature: float = 0.75, k: int = 0, p: int = 1, frequency_penalty: float = 0.0, presence_penalty: float = 0.0, truncate: Optional[str] = None, max_retries: int = 10, cohere_api_key: Optional[str] = None, stop: Optional[List[str]] = None) -> None\n",
      "     |  \n",
      "     |  Cohere large language models.\n",
      "     |  \n",
      "     |  To use, you should have the ``cohere`` python package installed, and the\n",
      "     |  environment variable ``COHERE_API_KEY`` set with your API key, or pass\n",
      "     |  it as a named parameter to the constructor.\n",
      "     |  \n",
      "     |  Example:\n",
      "     |      .. code-block:: python\n",
      "     |  \n",
      "     |          from langchain.llms import Cohere\n",
      "     |          cohere = Cohere(model=\"gptd-instruct-tft\", cohere_api_key=\"my-api-key\")\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Cohere\n",
      "     |      langchain.llms.base.LLM\n",
      "     |      langchain.llms.base.BaseLLM\n",
      "     |      langchain.schema.language_model.BaseLanguageModel\n",
      "     |      langchain.load.serializable.Serializable\n",
      "     |      pydantic.main.BaseModel\n",
      "     |      pydantic.utils.Representation\n",
      "     |      langchain.schema.runnable.base.Runnable\n",
      "     |      typing.Generic\n",
      "     |      abc.ABC\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  validate_environment(values: 'Dict') -> 'Dict' from pydantic.main.ModelMetaclass\n",
      "     |      Validate that api key and python package exists in environment.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __json_encoder__ = pydantic_encoder(obj: Any) -> Any\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  Config = <class 'langchain.llms.cohere.Cohere.Config'>\n",
      "     |      Configuration for this pydantic object.\n",
      "     |  \n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {'async_client': 'Any', 'client': 'Any', 'cohere_api...\n",
      "     |  \n",
      "     |  __class_vars__ = set()\n",
      "     |  \n",
      "     |  __config__ = <class 'langchain.llms.cohere.Config'>\n",
      "     |  \n",
      "     |  __custom_root_type__ = False\n",
      "     |  \n",
      "     |  __exclude_fields__ = {'callback_manager': True, 'callbacks': True, 'me...\n",
      "     |  \n",
      "     |  __fields__ = {'async_client': ModelField(name='async_client', type=Opt...\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "     |  \n",
      "     |  __include_fields__ = None\n",
      "     |  \n",
      "     |  __parameters__ = ()\n",
      "     |  \n",
      "     |  __post_root_validators__ = [(False, <function BaseLLM.raise_deprecatio...\n",
      "     |  \n",
      "     |  __pre_root_validators__ = []\n",
      "     |  \n",
      "     |  __private_attributes__ = {'_lc_kwargs': ModelPrivateAttr(default=Pydan...\n",
      "     |  \n",
      "     |  __schema_cache__ = {}\n",
      "     |  \n",
      "     |  __signature__ = <Signature (*, cache: Optional[bool] = None, ver... No...\n",
      "     |  \n",
      "     |  __validators__ = {'verbose': [<pydantic.class_validators.Validator obj...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  __call__(self, prompt: 'str', stop: 'Optional[List[str]]' = None, callbacks: 'Callbacks' = None, *, tags: 'Optional[List[str]]' = None, metadata: 'Optional[Dict[str, Any]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Check Cache and run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  __str__(self) -> 'str'\n",
      "     |      Get a string representation of the object for printing.\n",
      "     |  \n",
      "     |  async abatch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      "     |      Default implementation of abatch, which calls ainvoke N times.\n",
      "     |      Subclasses should override this method if they can batch more efficiently.\n",
      "     |  \n",
      "     |  async agenerate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  async agenerate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Asynchronously pass a sequence of prompts and return model generations.\n",
      "     |      \n",
      "     |      This method should make use of batched calls for models that expose a batched\n",
      "     |      API.\n",
      "     |      \n",
      "     |      Use this method when you want to:\n",
      "     |          1. take advantage of batched calls,\n",
      "     |          2. need more output from the model than just the top generated value,\n",
      "     |          3. are building chains that are agnostic to the underlying language model\n",
      "     |              type (e.g., pure text completion models vs chat models).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      "     |              converted to match the format of any language model (string for pure\n",
      "     |              text generation models and BaseMessages for chat models).\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          callbacks: Callbacks to pass through. Used for executing additional\n",
      "     |              functionality, such as logging or streaming, throughout generation.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          An LLMResult, which contains a list of candidate Generations for each input\n",
      "     |              prompt and additional model provider-specific output.\n",
      "     |  \n",
      "     |  async ainvoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Default implementation of ainvoke, which calls invoke in a thread pool.\n",
      "     |      Subclasses should override this method if they can run asynchronously.\n",
      "     |  \n",
      "     |  async apredict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Asynchronously pass a string to the model and return a string prediction.\n",
      "     |      \n",
      "     |      Use this method when calling pure text generation models and only the top\n",
      "     |          candidate generation is needed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: String input to pass to the model.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a string.\n",
      "     |  \n",
      "     |  async apredict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      "     |      Asynchronously pass messages to the model and return a message prediction.\n",
      "     |      \n",
      "     |      Use this method when calling chat models and only the top\n",
      "     |          candidate generation is needed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: A sequence of chat messages corresponding to a single model input.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a message.\n",
      "     |  \n",
      "     |  async astream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'AsyncIterator[str]'\n",
      "     |      Default implementation of astream, which calls ainvoke.\n",
      "     |      Subclasses should override this method if they support streaming output.\n",
      "     |  \n",
      "     |  batch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      "     |      Default implementation of batch, which calls invoke N times.\n",
      "     |      Subclasses should override this method if they can batch more efficiently.\n",
      "     |  \n",
      "     |  dict(self, **kwargs: 'Any') -> 'Dict'\n",
      "     |      Return a dictionary of the LLM.\n",
      "     |  \n",
      "     |  generate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  generate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Pass a sequence of prompts to the model and return model generations.\n",
      "     |      \n",
      "     |      This method should make use of batched calls for models that expose a batched\n",
      "     |      API.\n",
      "     |      \n",
      "     |      Use this method when you want to:\n",
      "     |          1. take advantage of batched calls,\n",
      "     |          2. need more output from the model than just the top generated value,\n",
      "     |          3. are building chains that are agnostic to the underlying language model\n",
      "     |              type (e.g., pure text completion models vs chat models).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      "     |              converted to match the format of any language model (string for pure\n",
      "     |              text generation models and BaseMessages for chat models).\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          callbacks: Callbacks to pass through. Used for executing additional\n",
      "     |              functionality, such as logging or streaming, throughout generation.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          An LLMResult, which contains a list of candidate Generations for each input\n",
      "     |              prompt and additional model provider-specific output.\n",
      "     |  \n",
      "     |  invoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |  \n",
      "     |  predict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Pass a single string input to the model and return a string prediction.\n",
      "     |      \n",
      "     |       Use this method when passing in raw text. If you want to pass in specific\n",
      "     |          types of chat messages, use predict_messages.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: String input to pass to the model.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a string.\n",
      "     |  \n",
      "     |  predict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      "     |      Pass a message sequence to the model and return a message prediction.\n",
      "     |      \n",
      "     |      Use this method when passing in chat messages. If you want to pass in raw text,\n",
      "     |          use predict.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: A sequence of chat messages corresponding to a single model input.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a message.\n",
      "     |  \n",
      "     |  save(self, file_path: 'Union[Path, str]') -> 'None'\n",
      "     |      Save the LLM.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          file_path: Path to file to save the LLM to.\n",
      "     |      \n",
      "     |      Example:\n",
      "     |      .. code-block:: python\n",
      "     |      \n",
      "     |          llm.save(file_path=\"path/llm.yaml\")\n",
      "     |  \n",
      "     |  stream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'Iterator[str]'\n",
      "     |      Default implementation of stream, which calls invoke.\n",
      "     |      Subclasses should override this method if they support streaming output.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  raise_deprecation(values: 'Dict') -> 'Dict' from pydantic.main.ModelMetaclass\n",
      "     |      Raise deprecation warning if callback_manager is used.\n",
      "     |  \n",
      "     |  set_verbose(verbose: 'Optional[bool]') -> 'bool' from pydantic.main.ModelMetaclass\n",
      "     |      If verbose is None, set it.\n",
      "     |      \n",
      "     |      This allows users to pass in None as verbose to access the global setting.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  __orig_bases__ = (langchain.schema.language_model.BaseLanguageModel[st...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.schema.language_model.BaseLanguageModel:\n",
      "     |  \n",
      "     |  get_num_tokens(self, text: 'str') -> 'int'\n",
      "     |      Get the number of tokens present in the text.\n",
      "     |      \n",
      "     |      Useful for checking if an input will fit in a model's context window.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: The string input to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The integer number of tokens in the text.\n",
      "     |  \n",
      "     |  get_num_tokens_from_messages(self, messages: 'List[BaseMessage]') -> 'int'\n",
      "     |      Get the number of tokens in the messages.\n",
      "     |      \n",
      "     |      Useful for checking if an input will fit in a model's context window.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: The message inputs to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The sum of the number of tokens across the messages.\n",
      "     |  \n",
      "     |  get_token_ids(self, text: 'str') -> 'List[int]'\n",
      "     |      Return the ordered ids of the tokens in a text.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: The string input to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          A list of ids corresponding to the tokens in the text, in order they occur\n",
      "     |              in the text.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from langchain.schema.language_model.BaseLanguageModel:\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.load.serializable.Serializable:\n",
      "     |  \n",
      "     |  __init__(self, **kwargs: Any) -> None\n",
      "     |      Create a new model by parsing and validating input data from keyword arguments.\n",
      "     |      \n",
      "     |      Raises ValidationError if the input data cannot be parsed to form a valid model.\n",
      "     |  \n",
      "     |  to_json(self) -> Union[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\n",
      "     |  \n",
      "     |  to_json_not_implemented(self) -> langchain.load.serializable.SerializedNotImplemented\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from langchain.load.serializable.Serializable:\n",
      "     |  \n",
      "     |  lc_attributes\n",
      "     |      Return a list of attribute names that should be included in the\n",
      "     |      serialized kwargs. These attributes must be accepted by the\n",
      "     |      constructor.\n",
      "     |  \n",
      "     |  lc_namespace\n",
      "     |      Return the namespace of the langchain object.\n",
      "     |      eg. [\"langchain\", \"llms\", \"openai\"]\n",
      "     |  \n",
      "     |  lc_secrets\n",
      "     |      Return a map of constructor argument names to secret ids.\n",
      "     |      eg. {\"openai_api_key\": \"OPENAI_API_KEY\"}\n",
      "     |  \n",
      "     |  lc_serializable\n",
      "     |      Return whether or not the class is serializable.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __eq__(self, other: Any) -> bool\n",
      "     |  \n",
      "     |  __getstate__(self) -> 'DictAny'\n",
      "     |  \n",
      "     |  __iter__(self) -> 'TupleGenerator'\n",
      "     |      so `dict(model)` works\n",
      "     |  \n",
      "     |  __repr_args__(self) -> 'ReprArgs'\n",
      "     |  \n",
      "     |  __setattr__(self, name, value)\n",
      "     |  \n",
      "     |  __setstate__(self, state: 'DictAny') -> None\n",
      "     |  \n",
      "     |  copy(self: 'Model', *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, update: Optional[ForwardRef('DictStrAny')] = None, deep: bool = False) -> 'Model'\n",
      "     |      Duplicate a model, optionally choose which fields to include, exclude and change.\n",
      "     |      \n",
      "     |      :param include: fields to include in new model\n",
      "     |      :param exclude: fields to exclude from new model, as with values this takes precedence over include\n",
      "     |      :param update: values to change/add in the new model. Note: the data is not validated before creating\n",
      "     |          the new model: you should trust this data\n",
      "     |      :param deep: set to `True` to make a deep copy of the model\n",
      "     |      :return: new model instance\n",
      "     |  \n",
      "     |  json(self, *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) -> 'unicode'\n",
      "     |      Generate a JSON representation of the model, `include` and `exclude` arguments as per `dict()`.\n",
      "     |      \n",
      "     |      `encoder` is an optional function to supply as `default` to json.dumps(), other arguments as per `json.dumps()`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __get_validators__() -> 'CallableGenerator' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  __try_update_forward_refs__(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      "     |      Same as update_forward_refs but will not raise exception\n",
      "     |      when forward references are not defined.\n",
      "     |  \n",
      "     |  construct(_fields_set: Optional[ForwardRef('SetStr')] = None, **values: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |      Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data.\n",
      "     |      Default values are respected, but no other validation is performed.\n",
      "     |      Behaves as if `Config.extra = 'allow'` was set since it adds all passed values\n",
      "     |  \n",
      "     |  from_orm(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_file(path: Union[str, pathlib.Path], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_obj(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_raw(b: Union[str, bytes], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema(by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}') -> 'DictStrAny' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema_json(*, by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}', **dumps_kwargs: Any) -> 'unicode' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  update_forward_refs(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      "     |      Try to update ForwardRefs on fields based on this Model, globalns and localns.\n",
      "     |  \n",
      "     |  validate(value: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __fields_set__\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.utils.Representation:\n",
      "     |  \n",
      "     |  __pretty__(self, fmt: Callable[[Any], Any], **kwargs: Any) -> Generator[Any, NoneType, NoneType]\n",
      "     |      Used by devtools (https://python-devtools.helpmanual.io/) to provide a human readable representations of objects\n",
      "     |  \n",
      "     |  __repr__(self) -> 'unicode'\n",
      "     |  \n",
      "     |  __repr_name__(self) -> 'unicode'\n",
      "     |      Name of the instance's class, used in __repr__.\n",
      "     |  \n",
      "     |  __repr_str__(self, join_str: 'unicode') -> 'unicode'\n",
      "     |  \n",
      "     |  __rich_repr__(self) -> 'RichReprResult'\n",
      "     |      Get fields for Rich library\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.schema.runnable.base.Runnable:\n",
      "     |  \n",
      "     |  __or__(self, other: 'Union[Runnable[Any, Other], Callable[[Any], Other], Mapping[str, Union[Runnable[Any, Other], Callable[[Any], Other], Any]]]') -> 'RunnableSequence[Input, Other]'\n",
      "     |      Return self|value.\n",
      "     |  \n",
      "     |  __ror__(self, other: 'Union[Runnable[Other, Any], Callable[[Any], Other], Mapping[str, Union[Runnable[Other, Any], Callable[[Other], Any], Any]]]') -> 'RunnableSequence[Other, Output]'\n",
      "     |      Return value|self.\n",
      "     |  \n",
      "     |  async atransform(self, input: 'AsyncIterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'AsyncIterator[Output]'\n",
      "     |      Default implementation of atransform, which buffers input and calls astream.\n",
      "     |      Subclasses should override this method if they can start producing output while\n",
      "     |      input is still being generated.\n",
      "     |  \n",
      "     |  bind(self, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      "     |      Bind arguments to a Runnable, returning a new Runnable.\n",
      "     |  \n",
      "     |  map(self) -> 'Runnable[List[Input], List[Output]]'\n",
      "     |      Return a new Runnable that maps a list of inputs to a list of outputs,\n",
      "     |      by calling invoke() with each input.\n",
      "     |  \n",
      "     |  transform(self, input: 'Iterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'Iterator[Output]'\n",
      "     |      Default implementation of transform, which buffers input and then calls stream.\n",
      "     |      Subclasses should override this method if they can start producing output while\n",
      "     |      input is still being generated.\n",
      "     |  \n",
      "     |  with_config(self, config: 'Optional[RunnableConfig]' = None, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      "     |      Bind config to a Runnable, returning a new Runnable.\n",
      "     |  \n",
      "     |  with_fallbacks(self, fallbacks: 'Sequence[Runnable[Input, Output]]', *, exceptions_to_handle: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,)) -> 'RunnableWithFallbacks[Input, Output]'\n",
      "     |  \n",
      "     |  with_retry(self, *, retry_if_exception_type: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,), wait_exponential_jitter: 'bool' = True, stop_after_attempt: 'int' = 3) -> 'Runnable[Input, Output]'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __class_getitem__(params) from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  __init_subclass__(*args, **kwargs) from pydantic.main.ModelMetaclass\n",
      "     |      This method is called when a class is subclassed.\n",
      "     |      \n",
      "     |      The default implementation does nothing. It may be\n",
      "     |      overridden to extend subclasses.\n",
      "    \n",
      "    class Databricks(langchain.llms.base.LLM)\n",
      "     |  Databricks(*, cache: Optional[bool] = None, verbose: bool = None, callbacks: Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager, NoneType] = None, callback_manager: Optional[langchain.callbacks.base.BaseCallbackManager] = None, tags: Optional[List[str]] = None, metadata: Optional[Dict[str, Any]] = None, host: str = None, api_token: str = None, endpoint_name: Optional[str] = None, cluster_id: Optional[str] = None, cluster_driver_port: Optional[str] = None, model_kwargs: Optional[Dict[str, Any]] = None, transform_input_fn: Optional[Callable] = None, transform_output_fn: Optional[Callable[..., str]] = None) -> None\n",
      "     |  \n",
      "     |  Databricks serving endpoint or a cluster driver proxy app for LLM.\n",
      "     |  \n",
      "     |  It supports two endpoint types:\n",
      "     |  \n",
      "     |  * **Serving endpoint** (recommended for both production and development).\n",
      "     |    We assume that an LLM was registered and deployed to a serving endpoint.\n",
      "     |    To wrap it as an LLM you must have \"Can Query\" permission to the endpoint.\n",
      "     |    Set ``endpoint_name`` accordingly and do not set ``cluster_id`` and\n",
      "     |    ``cluster_driver_port``.\n",
      "     |    The expected model signature is:\n",
      "     |  \n",
      "     |    * inputs::\n",
      "     |  \n",
      "     |        [{\"name\": \"prompt\", \"type\": \"string\"},\n",
      "     |         {\"name\": \"stop\", \"type\": \"list[string]\"}]\n",
      "     |  \n",
      "     |    * outputs: ``[{\"type\": \"string\"}]``\n",
      "     |  \n",
      "     |  * **Cluster driver proxy app** (recommended for interactive development).\n",
      "     |    One can load an LLM on a Databricks interactive cluster and start a local HTTP\n",
      "     |    server on the driver node to serve the model at ``/`` using HTTP POST method\n",
      "     |    with JSON input/output.\n",
      "     |    Please use a port number between ``[3000, 8000]`` and let the server listen to\n",
      "     |    the driver IP address or simply ``0.0.0.0`` instead of localhost only.\n",
      "     |    To wrap it as an LLM you must have \"Can Attach To\" permission to the cluster.\n",
      "     |    Set ``cluster_id`` and ``cluster_driver_port`` and do not set ``endpoint_name``.\n",
      "     |    The expected server schema (using JSON schema) is:\n",
      "     |  \n",
      "     |    * inputs::\n",
      "     |  \n",
      "     |        {\"type\": \"object\",\n",
      "     |         \"properties\": {\n",
      "     |            \"prompt\": {\"type\": \"string\"},\n",
      "     |            \"stop\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}}},\n",
      "     |         \"required\": [\"prompt\"]}`\n",
      "     |  \n",
      "     |    * outputs: ``{\"type\": \"string\"}``\n",
      "     |  \n",
      "     |  If the endpoint model signature is different or you want to set extra params,\n",
      "     |  you can use `transform_input_fn` and `transform_output_fn` to apply necessary\n",
      "     |  transformations before and after the query.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Databricks\n",
      "     |      langchain.llms.base.LLM\n",
      "     |      langchain.llms.base.BaseLLM\n",
      "     |      langchain.schema.language_model.BaseLanguageModel\n",
      "     |      langchain.load.serializable.Serializable\n",
      "     |      pydantic.main.BaseModel\n",
      "     |      pydantic.utils.Representation\n",
      "     |      langchain.schema.runnable.base.Runnable\n",
      "     |      typing.Generic\n",
      "     |      abc.ABC\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, **data: Any)\n",
      "     |      Create a new model by parsing and validating input data from keyword arguments.\n",
      "     |      \n",
      "     |      Raises ValidationError if the input data cannot be parsed to form a valid model.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  set_cluster_driver_port(v: Any, values: Dict[str, Any]) -> Optional[str] from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  set_cluster_id(v: Any, values: Dict[str, Any]) -> Optional[str] from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  set_model_kwargs(v: Optional[Dict[str, Any]]) -> Optional[Dict[str, Any]] from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __json_encoder__ = pydantic_encoder(obj: Any) -> Any\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  Config = <class 'langchain.llms.databricks.Databricks.Config'>\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {'_client': <class 'langchain.llms.databricks._Datab...\n",
      "     |  \n",
      "     |  __class_vars__ = set()\n",
      "     |  \n",
      "     |  __config__ = <class 'langchain.llms.databricks.Config'>\n",
      "     |  \n",
      "     |  __custom_root_type__ = False\n",
      "     |  \n",
      "     |  __exclude_fields__ = {'callback_manager': True, 'callbacks': True, 'me...\n",
      "     |  \n",
      "     |  __fields__ = {'api_token': ModelField(name='api_token', type=str, requ...\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "     |  \n",
      "     |  __include_fields__ = None\n",
      "     |  \n",
      "     |  __parameters__ = ()\n",
      "     |  \n",
      "     |  __post_root_validators__ = [(False, <function BaseLLM.raise_deprecatio...\n",
      "     |  \n",
      "     |  __pre_root_validators__ = []\n",
      "     |  \n",
      "     |  __private_attributes__ = {'_client': ModelPrivateAttr(default=Pydantic...\n",
      "     |  \n",
      "     |  __schema_cache__ = {}\n",
      "     |  \n",
      "     |  __signature__ = <Signature (*, cache: Optional[bool] = None, ver...fn:...\n",
      "     |  \n",
      "     |  __validators__ = {'cluster_driver_port': [<pydantic.class_validators.V...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  __call__(self, prompt: 'str', stop: 'Optional[List[str]]' = None, callbacks: 'Callbacks' = None, *, tags: 'Optional[List[str]]' = None, metadata: 'Optional[Dict[str, Any]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Check Cache and run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  __str__(self) -> 'str'\n",
      "     |      Get a string representation of the object for printing.\n",
      "     |  \n",
      "     |  async abatch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      "     |      Default implementation of abatch, which calls ainvoke N times.\n",
      "     |      Subclasses should override this method if they can batch more efficiently.\n",
      "     |  \n",
      "     |  async agenerate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  async agenerate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Asynchronously pass a sequence of prompts and return model generations.\n",
      "     |      \n",
      "     |      This method should make use of batched calls for models that expose a batched\n",
      "     |      API.\n",
      "     |      \n",
      "     |      Use this method when you want to:\n",
      "     |          1. take advantage of batched calls,\n",
      "     |          2. need more output from the model than just the top generated value,\n",
      "     |          3. are building chains that are agnostic to the underlying language model\n",
      "     |              type (e.g., pure text completion models vs chat models).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      "     |              converted to match the format of any language model (string for pure\n",
      "     |              text generation models and BaseMessages for chat models).\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          callbacks: Callbacks to pass through. Used for executing additional\n",
      "     |              functionality, such as logging or streaming, throughout generation.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          An LLMResult, which contains a list of candidate Generations for each input\n",
      "     |              prompt and additional model provider-specific output.\n",
      "     |  \n",
      "     |  async ainvoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Default implementation of ainvoke, which calls invoke in a thread pool.\n",
      "     |      Subclasses should override this method if they can run asynchronously.\n",
      "     |  \n",
      "     |  async apredict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Asynchronously pass a string to the model and return a string prediction.\n",
      "     |      \n",
      "     |      Use this method when calling pure text generation models and only the top\n",
      "     |          candidate generation is needed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: String input to pass to the model.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a string.\n",
      "     |  \n",
      "     |  async apredict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      "     |      Asynchronously pass messages to the model and return a message prediction.\n",
      "     |      \n",
      "     |      Use this method when calling chat models and only the top\n",
      "     |          candidate generation is needed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: A sequence of chat messages corresponding to a single model input.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a message.\n",
      "     |  \n",
      "     |  async astream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'AsyncIterator[str]'\n",
      "     |      Default implementation of astream, which calls ainvoke.\n",
      "     |      Subclasses should override this method if they support streaming output.\n",
      "     |  \n",
      "     |  batch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      "     |      Default implementation of batch, which calls invoke N times.\n",
      "     |      Subclasses should override this method if they can batch more efficiently.\n",
      "     |  \n",
      "     |  dict(self, **kwargs: 'Any') -> 'Dict'\n",
      "     |      Return a dictionary of the LLM.\n",
      "     |  \n",
      "     |  generate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  generate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Pass a sequence of prompts to the model and return model generations.\n",
      "     |      \n",
      "     |      This method should make use of batched calls for models that expose a batched\n",
      "     |      API.\n",
      "     |      \n",
      "     |      Use this method when you want to:\n",
      "     |          1. take advantage of batched calls,\n",
      "     |          2. need more output from the model than just the top generated value,\n",
      "     |          3. are building chains that are agnostic to the underlying language model\n",
      "     |              type (e.g., pure text completion models vs chat models).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      "     |              converted to match the format of any language model (string for pure\n",
      "     |              text generation models and BaseMessages for chat models).\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          callbacks: Callbacks to pass through. Used for executing additional\n",
      "     |              functionality, such as logging or streaming, throughout generation.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          An LLMResult, which contains a list of candidate Generations for each input\n",
      "     |              prompt and additional model provider-specific output.\n",
      "     |  \n",
      "     |  invoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |  \n",
      "     |  predict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Pass a single string input to the model and return a string prediction.\n",
      "     |      \n",
      "     |       Use this method when passing in raw text. If you want to pass in specific\n",
      "     |          types of chat messages, use predict_messages.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: String input to pass to the model.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a string.\n",
      "     |  \n",
      "     |  predict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      "     |      Pass a message sequence to the model and return a message prediction.\n",
      "     |      \n",
      "     |      Use this method when passing in chat messages. If you want to pass in raw text,\n",
      "     |          use predict.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: A sequence of chat messages corresponding to a single model input.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a message.\n",
      "     |  \n",
      "     |  save(self, file_path: 'Union[Path, str]') -> 'None'\n",
      "     |      Save the LLM.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          file_path: Path to file to save the LLM to.\n",
      "     |      \n",
      "     |      Example:\n",
      "     |      .. code-block:: python\n",
      "     |      \n",
      "     |          llm.save(file_path=\"path/llm.yaml\")\n",
      "     |  \n",
      "     |  stream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'Iterator[str]'\n",
      "     |      Default implementation of stream, which calls invoke.\n",
      "     |      Subclasses should override this method if they support streaming output.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  raise_deprecation(values: 'Dict') -> 'Dict' from pydantic.main.ModelMetaclass\n",
      "     |      Raise deprecation warning if callback_manager is used.\n",
      "     |  \n",
      "     |  set_verbose(verbose: 'Optional[bool]') -> 'bool' from pydantic.main.ModelMetaclass\n",
      "     |      If verbose is None, set it.\n",
      "     |      \n",
      "     |      This allows users to pass in None as verbose to access the global setting.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  __orig_bases__ = (langchain.schema.language_model.BaseLanguageModel[st...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.schema.language_model.BaseLanguageModel:\n",
      "     |  \n",
      "     |  get_num_tokens(self, text: 'str') -> 'int'\n",
      "     |      Get the number of tokens present in the text.\n",
      "     |      \n",
      "     |      Useful for checking if an input will fit in a model's context window.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: The string input to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The integer number of tokens in the text.\n",
      "     |  \n",
      "     |  get_num_tokens_from_messages(self, messages: 'List[BaseMessage]') -> 'int'\n",
      "     |      Get the number of tokens in the messages.\n",
      "     |      \n",
      "     |      Useful for checking if an input will fit in a model's context window.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: The message inputs to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The sum of the number of tokens across the messages.\n",
      "     |  \n",
      "     |  get_token_ids(self, text: 'str') -> 'List[int]'\n",
      "     |      Return the ordered ids of the tokens in a text.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: The string input to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          A list of ids corresponding to the tokens in the text, in order they occur\n",
      "     |              in the text.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from langchain.schema.language_model.BaseLanguageModel:\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.load.serializable.Serializable:\n",
      "     |  \n",
      "     |  to_json(self) -> Union[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\n",
      "     |  \n",
      "     |  to_json_not_implemented(self) -> langchain.load.serializable.SerializedNotImplemented\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from langchain.load.serializable.Serializable:\n",
      "     |  \n",
      "     |  lc_attributes\n",
      "     |      Return a list of attribute names that should be included in the\n",
      "     |      serialized kwargs. These attributes must be accepted by the\n",
      "     |      constructor.\n",
      "     |  \n",
      "     |  lc_namespace\n",
      "     |      Return the namespace of the langchain object.\n",
      "     |      eg. [\"langchain\", \"llms\", \"openai\"]\n",
      "     |  \n",
      "     |  lc_secrets\n",
      "     |      Return a map of constructor argument names to secret ids.\n",
      "     |      eg. {\"openai_api_key\": \"OPENAI_API_KEY\"}\n",
      "     |  \n",
      "     |  lc_serializable\n",
      "     |      Return whether or not the class is serializable.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __eq__(self, other: Any) -> bool\n",
      "     |  \n",
      "     |  __getstate__(self) -> 'DictAny'\n",
      "     |  \n",
      "     |  __iter__(self) -> 'TupleGenerator'\n",
      "     |      so `dict(model)` works\n",
      "     |  \n",
      "     |  __repr_args__(self) -> 'ReprArgs'\n",
      "     |  \n",
      "     |  __setattr__(self, name, value)\n",
      "     |  \n",
      "     |  __setstate__(self, state: 'DictAny') -> None\n",
      "     |  \n",
      "     |  copy(self: 'Model', *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, update: Optional[ForwardRef('DictStrAny')] = None, deep: bool = False) -> 'Model'\n",
      "     |      Duplicate a model, optionally choose which fields to include, exclude and change.\n",
      "     |      \n",
      "     |      :param include: fields to include in new model\n",
      "     |      :param exclude: fields to exclude from new model, as with values this takes precedence over include\n",
      "     |      :param update: values to change/add in the new model. Note: the data is not validated before creating\n",
      "     |          the new model: you should trust this data\n",
      "     |      :param deep: set to `True` to make a deep copy of the model\n",
      "     |      :return: new model instance\n",
      "     |  \n",
      "     |  json(self, *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) -> 'unicode'\n",
      "     |      Generate a JSON representation of the model, `include` and `exclude` arguments as per `dict()`.\n",
      "     |      \n",
      "     |      `encoder` is an optional function to supply as `default` to json.dumps(), other arguments as per `json.dumps()`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __get_validators__() -> 'CallableGenerator' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  __try_update_forward_refs__(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      "     |      Same as update_forward_refs but will not raise exception\n",
      "     |      when forward references are not defined.\n",
      "     |  \n",
      "     |  construct(_fields_set: Optional[ForwardRef('SetStr')] = None, **values: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |      Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data.\n",
      "     |      Default values are respected, but no other validation is performed.\n",
      "     |      Behaves as if `Config.extra = 'allow'` was set since it adds all passed values\n",
      "     |  \n",
      "     |  from_orm(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_file(path: Union[str, pathlib.Path], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_obj(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_raw(b: Union[str, bytes], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema(by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}') -> 'DictStrAny' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema_json(*, by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}', **dumps_kwargs: Any) -> 'unicode' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  update_forward_refs(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      "     |      Try to update ForwardRefs on fields based on this Model, globalns and localns.\n",
      "     |  \n",
      "     |  validate(value: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __fields_set__\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.utils.Representation:\n",
      "     |  \n",
      "     |  __pretty__(self, fmt: Callable[[Any], Any], **kwargs: Any) -> Generator[Any, NoneType, NoneType]\n",
      "     |      Used by devtools (https://python-devtools.helpmanual.io/) to provide a human readable representations of objects\n",
      "     |  \n",
      "     |  __repr__(self) -> 'unicode'\n",
      "     |  \n",
      "     |  __repr_name__(self) -> 'unicode'\n",
      "     |      Name of the instance's class, used in __repr__.\n",
      "     |  \n",
      "     |  __repr_str__(self, join_str: 'unicode') -> 'unicode'\n",
      "     |  \n",
      "     |  __rich_repr__(self) -> 'RichReprResult'\n",
      "     |      Get fields for Rich library\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.schema.runnable.base.Runnable:\n",
      "     |  \n",
      "     |  __or__(self, other: 'Union[Runnable[Any, Other], Callable[[Any], Other], Mapping[str, Union[Runnable[Any, Other], Callable[[Any], Other], Any]]]') -> 'RunnableSequence[Input, Other]'\n",
      "     |      Return self|value.\n",
      "     |  \n",
      "     |  __ror__(self, other: 'Union[Runnable[Other, Any], Callable[[Any], Other], Mapping[str, Union[Runnable[Other, Any], Callable[[Other], Any], Any]]]') -> 'RunnableSequence[Other, Output]'\n",
      "     |      Return value|self.\n",
      "     |  \n",
      "     |  async atransform(self, input: 'AsyncIterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'AsyncIterator[Output]'\n",
      "     |      Default implementation of atransform, which buffers input and calls astream.\n",
      "     |      Subclasses should override this method if they can start producing output while\n",
      "     |      input is still being generated.\n",
      "     |  \n",
      "     |  bind(self, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      "     |      Bind arguments to a Runnable, returning a new Runnable.\n",
      "     |  \n",
      "     |  map(self) -> 'Runnable[List[Input], List[Output]]'\n",
      "     |      Return a new Runnable that maps a list of inputs to a list of outputs,\n",
      "     |      by calling invoke() with each input.\n",
      "     |  \n",
      "     |  transform(self, input: 'Iterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'Iterator[Output]'\n",
      "     |      Default implementation of transform, which buffers input and then calls stream.\n",
      "     |      Subclasses should override this method if they can start producing output while\n",
      "     |      input is still being generated.\n",
      "     |  \n",
      "     |  with_config(self, config: 'Optional[RunnableConfig]' = None, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      "     |      Bind config to a Runnable, returning a new Runnable.\n",
      "     |  \n",
      "     |  with_fallbacks(self, fallbacks: 'Sequence[Runnable[Input, Output]]', *, exceptions_to_handle: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,)) -> 'RunnableWithFallbacks[Input, Output]'\n",
      "     |  \n",
      "     |  with_retry(self, *, retry_if_exception_type: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,), wait_exponential_jitter: 'bool' = True, stop_after_attempt: 'int' = 3) -> 'Runnable[Input, Output]'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __class_getitem__(params) from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  __init_subclass__(*args, **kwargs) from pydantic.main.ModelMetaclass\n",
      "     |      This method is called when a class is subclassed.\n",
      "     |      \n",
      "     |      The default implementation does nothing. It may be\n",
      "     |      overridden to extend subclasses.\n",
      "    \n",
      "    class DeepInfra(langchain.llms.base.LLM)\n",
      "     |  DeepInfra(*, cache: Optional[bool] = None, verbose: bool = None, callbacks: Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager, NoneType] = None, callback_manager: Optional[langchain.callbacks.base.BaseCallbackManager] = None, tags: Optional[List[str]] = None, metadata: Optional[Dict[str, Any]] = None, model_id: str = 'google/flan-t5-xl', model_kwargs: Optional[dict] = None, deepinfra_api_token: Optional[str] = None) -> None\n",
      "     |  \n",
      "     |  DeepInfra models.\n",
      "     |  \n",
      "     |  To use, you should have the ``requests`` python package installed, and the\n",
      "     |  environment variable ``DEEPINFRA_API_TOKEN`` set with your API token, or pass\n",
      "     |  it as a named parameter to the constructor.\n",
      "     |  \n",
      "     |  Only supports `text-generation` and `text2text-generation` for now.\n",
      "     |  \n",
      "     |  Example:\n",
      "     |      .. code-block:: python\n",
      "     |  \n",
      "     |          from langchain.llms import DeepInfra\n",
      "     |          di = DeepInfra(model_id=\"google/flan-t5-xl\",\n",
      "     |                              deepinfra_api_token=\"my-api-key\")\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      DeepInfra\n",
      "     |      langchain.llms.base.LLM\n",
      "     |      langchain.llms.base.BaseLLM\n",
      "     |      langchain.schema.language_model.BaseLanguageModel\n",
      "     |      langchain.load.serializable.Serializable\n",
      "     |      pydantic.main.BaseModel\n",
      "     |      pydantic.utils.Representation\n",
      "     |      langchain.schema.runnable.base.Runnable\n",
      "     |      typing.Generic\n",
      "     |      abc.ABC\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  validate_environment(values: Dict) -> Dict from pydantic.main.ModelMetaclass\n",
      "     |      Validate that api key and python package exists in environment.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __json_encoder__ = pydantic_encoder(obj: Any) -> Any\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  Config = <class 'langchain.llms.deepinfra.DeepInfra.Config'>\n",
      "     |      Configuration for this pydantic object.\n",
      "     |  \n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {'deepinfra_api_token': typing.Optional[str], 'model...\n",
      "     |  \n",
      "     |  __class_vars__ = set()\n",
      "     |  \n",
      "     |  __config__ = <class 'langchain.llms.deepinfra.Config'>\n",
      "     |  \n",
      "     |  __custom_root_type__ = False\n",
      "     |  \n",
      "     |  __exclude_fields__ = {'callback_manager': True, 'callbacks': True, 'me...\n",
      "     |  \n",
      "     |  __fields__ = {'cache': ModelField(name='cache', type=Optional[bool], r...\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "     |  \n",
      "     |  __include_fields__ = None\n",
      "     |  \n",
      "     |  __parameters__ = ()\n",
      "     |  \n",
      "     |  __post_root_validators__ = [(False, <function BaseLLM.raise_deprecatio...\n",
      "     |  \n",
      "     |  __pre_root_validators__ = []\n",
      "     |  \n",
      "     |  __private_attributes__ = {'_lc_kwargs': ModelPrivateAttr(default=Pydan...\n",
      "     |  \n",
      "     |  __schema_cache__ = {}\n",
      "     |  \n",
      "     |  __signature__ = <Signature (*, cache: Optional[bool] = None, ver...epi...\n",
      "     |  \n",
      "     |  __validators__ = {'verbose': [<pydantic.class_validators.Validator obj...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  __call__(self, prompt: 'str', stop: 'Optional[List[str]]' = None, callbacks: 'Callbacks' = None, *, tags: 'Optional[List[str]]' = None, metadata: 'Optional[Dict[str, Any]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Check Cache and run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  __str__(self) -> 'str'\n",
      "     |      Get a string representation of the object for printing.\n",
      "     |  \n",
      "     |  async abatch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      "     |      Default implementation of abatch, which calls ainvoke N times.\n",
      "     |      Subclasses should override this method if they can batch more efficiently.\n",
      "     |  \n",
      "     |  async agenerate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  async agenerate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Asynchronously pass a sequence of prompts and return model generations.\n",
      "     |      \n",
      "     |      This method should make use of batched calls for models that expose a batched\n",
      "     |      API.\n",
      "     |      \n",
      "     |      Use this method when you want to:\n",
      "     |          1. take advantage of batched calls,\n",
      "     |          2. need more output from the model than just the top generated value,\n",
      "     |          3. are building chains that are agnostic to the underlying language model\n",
      "     |              type (e.g., pure text completion models vs chat models).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      "     |              converted to match the format of any language model (string for pure\n",
      "     |              text generation models and BaseMessages for chat models).\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          callbacks: Callbacks to pass through. Used for executing additional\n",
      "     |              functionality, such as logging or streaming, throughout generation.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          An LLMResult, which contains a list of candidate Generations for each input\n",
      "     |              prompt and additional model provider-specific output.\n",
      "     |  \n",
      "     |  async ainvoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Default implementation of ainvoke, which calls invoke in a thread pool.\n",
      "     |      Subclasses should override this method if they can run asynchronously.\n",
      "     |  \n",
      "     |  async apredict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Asynchronously pass a string to the model and return a string prediction.\n",
      "     |      \n",
      "     |      Use this method when calling pure text generation models and only the top\n",
      "     |          candidate generation is needed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: String input to pass to the model.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a string.\n",
      "     |  \n",
      "     |  async apredict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      "     |      Asynchronously pass messages to the model and return a message prediction.\n",
      "     |      \n",
      "     |      Use this method when calling chat models and only the top\n",
      "     |          candidate generation is needed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: A sequence of chat messages corresponding to a single model input.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a message.\n",
      "     |  \n",
      "     |  async astream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'AsyncIterator[str]'\n",
      "     |      Default implementation of astream, which calls ainvoke.\n",
      "     |      Subclasses should override this method if they support streaming output.\n",
      "     |  \n",
      "     |  batch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      "     |      Default implementation of batch, which calls invoke N times.\n",
      "     |      Subclasses should override this method if they can batch more efficiently.\n",
      "     |  \n",
      "     |  dict(self, **kwargs: 'Any') -> 'Dict'\n",
      "     |      Return a dictionary of the LLM.\n",
      "     |  \n",
      "     |  generate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  generate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Pass a sequence of prompts to the model and return model generations.\n",
      "     |      \n",
      "     |      This method should make use of batched calls for models that expose a batched\n",
      "     |      API.\n",
      "     |      \n",
      "     |      Use this method when you want to:\n",
      "     |          1. take advantage of batched calls,\n",
      "     |          2. need more output from the model than just the top generated value,\n",
      "     |          3. are building chains that are agnostic to the underlying language model\n",
      "     |              type (e.g., pure text completion models vs chat models).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      "     |              converted to match the format of any language model (string for pure\n",
      "     |              text generation models and BaseMessages for chat models).\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          callbacks: Callbacks to pass through. Used for executing additional\n",
      "     |              functionality, such as logging or streaming, throughout generation.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          An LLMResult, which contains a list of candidate Generations for each input\n",
      "     |              prompt and additional model provider-specific output.\n",
      "     |  \n",
      "     |  invoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |  \n",
      "     |  predict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Pass a single string input to the model and return a string prediction.\n",
      "     |      \n",
      "     |       Use this method when passing in raw text. If you want to pass in specific\n",
      "     |          types of chat messages, use predict_messages.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: String input to pass to the model.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a string.\n",
      "     |  \n",
      "     |  predict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      "     |      Pass a message sequence to the model and return a message prediction.\n",
      "     |      \n",
      "     |      Use this method when passing in chat messages. If you want to pass in raw text,\n",
      "     |          use predict.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: A sequence of chat messages corresponding to a single model input.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a message.\n",
      "     |  \n",
      "     |  save(self, file_path: 'Union[Path, str]') -> 'None'\n",
      "     |      Save the LLM.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          file_path: Path to file to save the LLM to.\n",
      "     |      \n",
      "     |      Example:\n",
      "     |      .. code-block:: python\n",
      "     |      \n",
      "     |          llm.save(file_path=\"path/llm.yaml\")\n",
      "     |  \n",
      "     |  stream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'Iterator[str]'\n",
      "     |      Default implementation of stream, which calls invoke.\n",
      "     |      Subclasses should override this method if they support streaming output.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  raise_deprecation(values: 'Dict') -> 'Dict' from pydantic.main.ModelMetaclass\n",
      "     |      Raise deprecation warning if callback_manager is used.\n",
      "     |  \n",
      "     |  set_verbose(verbose: 'Optional[bool]') -> 'bool' from pydantic.main.ModelMetaclass\n",
      "     |      If verbose is None, set it.\n",
      "     |      \n",
      "     |      This allows users to pass in None as verbose to access the global setting.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  __orig_bases__ = (langchain.schema.language_model.BaseLanguageModel[st...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.schema.language_model.BaseLanguageModel:\n",
      "     |  \n",
      "     |  get_num_tokens(self, text: 'str') -> 'int'\n",
      "     |      Get the number of tokens present in the text.\n",
      "     |      \n",
      "     |      Useful for checking if an input will fit in a model's context window.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: The string input to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The integer number of tokens in the text.\n",
      "     |  \n",
      "     |  get_num_tokens_from_messages(self, messages: 'List[BaseMessage]') -> 'int'\n",
      "     |      Get the number of tokens in the messages.\n",
      "     |      \n",
      "     |      Useful for checking if an input will fit in a model's context window.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: The message inputs to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The sum of the number of tokens across the messages.\n",
      "     |  \n",
      "     |  get_token_ids(self, text: 'str') -> 'List[int]'\n",
      "     |      Return the ordered ids of the tokens in a text.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: The string input to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          A list of ids corresponding to the tokens in the text, in order they occur\n",
      "     |              in the text.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from langchain.schema.language_model.BaseLanguageModel:\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.load.serializable.Serializable:\n",
      "     |  \n",
      "     |  __init__(self, **kwargs: Any) -> None\n",
      "     |      Create a new model by parsing and validating input data from keyword arguments.\n",
      "     |      \n",
      "     |      Raises ValidationError if the input data cannot be parsed to form a valid model.\n",
      "     |  \n",
      "     |  to_json(self) -> Union[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\n",
      "     |  \n",
      "     |  to_json_not_implemented(self) -> langchain.load.serializable.SerializedNotImplemented\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from langchain.load.serializable.Serializable:\n",
      "     |  \n",
      "     |  lc_attributes\n",
      "     |      Return a list of attribute names that should be included in the\n",
      "     |      serialized kwargs. These attributes must be accepted by the\n",
      "     |      constructor.\n",
      "     |  \n",
      "     |  lc_namespace\n",
      "     |      Return the namespace of the langchain object.\n",
      "     |      eg. [\"langchain\", \"llms\", \"openai\"]\n",
      "     |  \n",
      "     |  lc_secrets\n",
      "     |      Return a map of constructor argument names to secret ids.\n",
      "     |      eg. {\"openai_api_key\": \"OPENAI_API_KEY\"}\n",
      "     |  \n",
      "     |  lc_serializable\n",
      "     |      Return whether or not the class is serializable.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __eq__(self, other: Any) -> bool\n",
      "     |  \n",
      "     |  __getstate__(self) -> 'DictAny'\n",
      "     |  \n",
      "     |  __iter__(self) -> 'TupleGenerator'\n",
      "     |      so `dict(model)` works\n",
      "     |  \n",
      "     |  __repr_args__(self) -> 'ReprArgs'\n",
      "     |  \n",
      "     |  __setattr__(self, name, value)\n",
      "     |  \n",
      "     |  __setstate__(self, state: 'DictAny') -> None\n",
      "     |  \n",
      "     |  copy(self: 'Model', *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, update: Optional[ForwardRef('DictStrAny')] = None, deep: bool = False) -> 'Model'\n",
      "     |      Duplicate a model, optionally choose which fields to include, exclude and change.\n",
      "     |      \n",
      "     |      :param include: fields to include in new model\n",
      "     |      :param exclude: fields to exclude from new model, as with values this takes precedence over include\n",
      "     |      :param update: values to change/add in the new model. Note: the data is not validated before creating\n",
      "     |          the new model: you should trust this data\n",
      "     |      :param deep: set to `True` to make a deep copy of the model\n",
      "     |      :return: new model instance\n",
      "     |  \n",
      "     |  json(self, *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) -> 'unicode'\n",
      "     |      Generate a JSON representation of the model, `include` and `exclude` arguments as per `dict()`.\n",
      "     |      \n",
      "     |      `encoder` is an optional function to supply as `default` to json.dumps(), other arguments as per `json.dumps()`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __get_validators__() -> 'CallableGenerator' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  __try_update_forward_refs__(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      "     |      Same as update_forward_refs but will not raise exception\n",
      "     |      when forward references are not defined.\n",
      "     |  \n",
      "     |  construct(_fields_set: Optional[ForwardRef('SetStr')] = None, **values: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |      Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data.\n",
      "     |      Default values are respected, but no other validation is performed.\n",
      "     |      Behaves as if `Config.extra = 'allow'` was set since it adds all passed values\n",
      "     |  \n",
      "     |  from_orm(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_file(path: Union[str, pathlib.Path], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_obj(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_raw(b: Union[str, bytes], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema(by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}') -> 'DictStrAny' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema_json(*, by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}', **dumps_kwargs: Any) -> 'unicode' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  update_forward_refs(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      "     |      Try to update ForwardRefs on fields based on this Model, globalns and localns.\n",
      "     |  \n",
      "     |  validate(value: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __fields_set__\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.utils.Representation:\n",
      "     |  \n",
      "     |  __pretty__(self, fmt: Callable[[Any], Any], **kwargs: Any) -> Generator[Any, NoneType, NoneType]\n",
      "     |      Used by devtools (https://python-devtools.helpmanual.io/) to provide a human readable representations of objects\n",
      "     |  \n",
      "     |  __repr__(self) -> 'unicode'\n",
      "     |  \n",
      "     |  __repr_name__(self) -> 'unicode'\n",
      "     |      Name of the instance's class, used in __repr__.\n",
      "     |  \n",
      "     |  __repr_str__(self, join_str: 'unicode') -> 'unicode'\n",
      "     |  \n",
      "     |  __rich_repr__(self) -> 'RichReprResult'\n",
      "     |      Get fields for Rich library\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.schema.runnable.base.Runnable:\n",
      "     |  \n",
      "     |  __or__(self, other: 'Union[Runnable[Any, Other], Callable[[Any], Other], Mapping[str, Union[Runnable[Any, Other], Callable[[Any], Other], Any]]]') -> 'RunnableSequence[Input, Other]'\n",
      "     |      Return self|value.\n",
      "     |  \n",
      "     |  __ror__(self, other: 'Union[Runnable[Other, Any], Callable[[Any], Other], Mapping[str, Union[Runnable[Other, Any], Callable[[Other], Any], Any]]]') -> 'RunnableSequence[Other, Output]'\n",
      "     |      Return value|self.\n",
      "     |  \n",
      "     |  async atransform(self, input: 'AsyncIterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'AsyncIterator[Output]'\n",
      "     |      Default implementation of atransform, which buffers input and calls astream.\n",
      "     |      Subclasses should override this method if they can start producing output while\n",
      "     |      input is still being generated.\n",
      "     |  \n",
      "     |  bind(self, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      "     |      Bind arguments to a Runnable, returning a new Runnable.\n",
      "     |  \n",
      "     |  map(self) -> 'Runnable[List[Input], List[Output]]'\n",
      "     |      Return a new Runnable that maps a list of inputs to a list of outputs,\n",
      "     |      by calling invoke() with each input.\n",
      "     |  \n",
      "     |  transform(self, input: 'Iterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'Iterator[Output]'\n",
      "     |      Default implementation of transform, which buffers input and then calls stream.\n",
      "     |      Subclasses should override this method if they can start producing output while\n",
      "     |      input is still being generated.\n",
      "     |  \n",
      "     |  with_config(self, config: 'Optional[RunnableConfig]' = None, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      "     |      Bind config to a Runnable, returning a new Runnable.\n",
      "     |  \n",
      "     |  with_fallbacks(self, fallbacks: 'Sequence[Runnable[Input, Output]]', *, exceptions_to_handle: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,)) -> 'RunnableWithFallbacks[Input, Output]'\n",
      "     |  \n",
      "     |  with_retry(self, *, retry_if_exception_type: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,), wait_exponential_jitter: 'bool' = True, stop_after_attempt: 'int' = 3) -> 'Runnable[Input, Output]'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __class_getitem__(params) from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  __init_subclass__(*args, **kwargs) from pydantic.main.ModelMetaclass\n",
      "     |      This method is called when a class is subclassed.\n",
      "     |      \n",
      "     |      The default implementation does nothing. It may be\n",
      "     |      overridden to extend subclasses.\n",
      "    \n",
      "    class DeepSparse(langchain.llms.base.LLM)\n",
      "     |  DeepSparse(*, cache: Optional[bool] = None, verbose: bool = None, callbacks: Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager, NoneType] = None, callback_manager: Optional[langchain.callbacks.base.BaseCallbackManager] = None, tags: Optional[List[str]] = None, metadata: Optional[Dict[str, Any]] = None, pipeline: Any = None, model: str, config: Optional[Dict[str, Any]] = None) -> None\n",
      "     |  \n",
      "     |  Neural Magic DeepSparse LLM interface.\n",
      "     |  \n",
      "     |  To use, you should have the ``deepsparse`` or ``deepsparse-nightly``\n",
      "     |  python package installed. See https://github.com/neuralmagic/deepsparse\n",
      "     |  \n",
      "     |  This interface let's you deploy optimized LLMs straight from the\n",
      "     |  [SparseZoo](https://sparsezoo.neuralmagic.com/?useCase=text_generation)\n",
      "     |  Example:\n",
      "     |      .. code-block:: python\n",
      "     |          from langchain.llms import DeepSparse\n",
      "     |          llm = DeepSparse(model=\"zoo:nlg/text_generation/codegen_mono-350m/pytorch/huggingface/bigpython_bigquery_thepile/base-none\")\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      DeepSparse\n",
      "     |      langchain.llms.base.LLM\n",
      "     |      langchain.llms.base.BaseLLM\n",
      "     |      langchain.schema.language_model.BaseLanguageModel\n",
      "     |      langchain.load.serializable.Serializable\n",
      "     |      pydantic.main.BaseModel\n",
      "     |      pydantic.utils.Representation\n",
      "     |      langchain.schema.runnable.base.Runnable\n",
      "     |      typing.Generic\n",
      "     |      abc.ABC\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  validate_environment(values: Dict) -> Dict from pydantic.main.ModelMetaclass\n",
      "     |      Validate that ``deepsparse`` package is installed.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __json_encoder__ = pydantic_encoder(obj: Any) -> Any\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {'config': typing.Optional[typing.Dict[str, typing.A...\n",
      "     |  \n",
      "     |  __class_vars__ = set()\n",
      "     |  \n",
      "     |  __config__ = <class 'langchain.llms.deepsparse.Config'>\n",
      "     |  \n",
      "     |  __custom_root_type__ = False\n",
      "     |  \n",
      "     |  __exclude_fields__ = {'callback_manager': True, 'callbacks': True, 'me...\n",
      "     |  \n",
      "     |  __fields__ = {'cache': ModelField(name='cache', type=Optional[bool], r...\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "     |  \n",
      "     |  __include_fields__ = None\n",
      "     |  \n",
      "     |  __parameters__ = ()\n",
      "     |  \n",
      "     |  __post_root_validators__ = [(False, <function BaseLLM.raise_deprecatio...\n",
      "     |  \n",
      "     |  __pre_root_validators__ = []\n",
      "     |  \n",
      "     |  __private_attributes__ = {'_lc_kwargs': ModelPrivateAttr(default=Pydan...\n",
      "     |  \n",
      "     |  __schema_cache__ = {}\n",
      "     |  \n",
      "     |  __signature__ = <Signature (*, cache: Optional[bool] = None, ver...con...\n",
      "     |  \n",
      "     |  __validators__ = {'verbose': [<pydantic.class_validators.Validator obj...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  __call__(self, prompt: 'str', stop: 'Optional[List[str]]' = None, callbacks: 'Callbacks' = None, *, tags: 'Optional[List[str]]' = None, metadata: 'Optional[Dict[str, Any]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Check Cache and run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  __str__(self) -> 'str'\n",
      "     |      Get a string representation of the object for printing.\n",
      "     |  \n",
      "     |  async abatch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      "     |      Default implementation of abatch, which calls ainvoke N times.\n",
      "     |      Subclasses should override this method if they can batch more efficiently.\n",
      "     |  \n",
      "     |  async agenerate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  async agenerate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Asynchronously pass a sequence of prompts and return model generations.\n",
      "     |      \n",
      "     |      This method should make use of batched calls for models that expose a batched\n",
      "     |      API.\n",
      "     |      \n",
      "     |      Use this method when you want to:\n",
      "     |          1. take advantage of batched calls,\n",
      "     |          2. need more output from the model than just the top generated value,\n",
      "     |          3. are building chains that are agnostic to the underlying language model\n",
      "     |              type (e.g., pure text completion models vs chat models).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      "     |              converted to match the format of any language model (string for pure\n",
      "     |              text generation models and BaseMessages for chat models).\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          callbacks: Callbacks to pass through. Used for executing additional\n",
      "     |              functionality, such as logging or streaming, throughout generation.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          An LLMResult, which contains a list of candidate Generations for each input\n",
      "     |              prompt and additional model provider-specific output.\n",
      "     |  \n",
      "     |  async ainvoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Default implementation of ainvoke, which calls invoke in a thread pool.\n",
      "     |      Subclasses should override this method if they can run asynchronously.\n",
      "     |  \n",
      "     |  async apredict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Asynchronously pass a string to the model and return a string prediction.\n",
      "     |      \n",
      "     |      Use this method when calling pure text generation models and only the top\n",
      "     |          candidate generation is needed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: String input to pass to the model.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a string.\n",
      "     |  \n",
      "     |  async apredict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      "     |      Asynchronously pass messages to the model and return a message prediction.\n",
      "     |      \n",
      "     |      Use this method when calling chat models and only the top\n",
      "     |          candidate generation is needed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: A sequence of chat messages corresponding to a single model input.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a message.\n",
      "     |  \n",
      "     |  async astream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'AsyncIterator[str]'\n",
      "     |      Default implementation of astream, which calls ainvoke.\n",
      "     |      Subclasses should override this method if they support streaming output.\n",
      "     |  \n",
      "     |  batch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      "     |      Default implementation of batch, which calls invoke N times.\n",
      "     |      Subclasses should override this method if they can batch more efficiently.\n",
      "     |  \n",
      "     |  dict(self, **kwargs: 'Any') -> 'Dict'\n",
      "     |      Return a dictionary of the LLM.\n",
      "     |  \n",
      "     |  generate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  generate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Pass a sequence of prompts to the model and return model generations.\n",
      "     |      \n",
      "     |      This method should make use of batched calls for models that expose a batched\n",
      "     |      API.\n",
      "     |      \n",
      "     |      Use this method when you want to:\n",
      "     |          1. take advantage of batched calls,\n",
      "     |          2. need more output from the model than just the top generated value,\n",
      "     |          3. are building chains that are agnostic to the underlying language model\n",
      "     |              type (e.g., pure text completion models vs chat models).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      "     |              converted to match the format of any language model (string for pure\n",
      "     |              text generation models and BaseMessages for chat models).\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          callbacks: Callbacks to pass through. Used for executing additional\n",
      "     |              functionality, such as logging or streaming, throughout generation.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          An LLMResult, which contains a list of candidate Generations for each input\n",
      "     |              prompt and additional model provider-specific output.\n",
      "     |  \n",
      "     |  invoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |  \n",
      "     |  predict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Pass a single string input to the model and return a string prediction.\n",
      "     |      \n",
      "     |       Use this method when passing in raw text. If you want to pass in specific\n",
      "     |          types of chat messages, use predict_messages.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: String input to pass to the model.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a string.\n",
      "     |  \n",
      "     |  predict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      "     |      Pass a message sequence to the model and return a message prediction.\n",
      "     |      \n",
      "     |      Use this method when passing in chat messages. If you want to pass in raw text,\n",
      "     |          use predict.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: A sequence of chat messages corresponding to a single model input.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a message.\n",
      "     |  \n",
      "     |  save(self, file_path: 'Union[Path, str]') -> 'None'\n",
      "     |      Save the LLM.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          file_path: Path to file to save the LLM to.\n",
      "     |      \n",
      "     |      Example:\n",
      "     |      .. code-block:: python\n",
      "     |      \n",
      "     |          llm.save(file_path=\"path/llm.yaml\")\n",
      "     |  \n",
      "     |  stream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'Iterator[str]'\n",
      "     |      Default implementation of stream, which calls invoke.\n",
      "     |      Subclasses should override this method if they support streaming output.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  raise_deprecation(values: 'Dict') -> 'Dict' from pydantic.main.ModelMetaclass\n",
      "     |      Raise deprecation warning if callback_manager is used.\n",
      "     |  \n",
      "     |  set_verbose(verbose: 'Optional[bool]') -> 'bool' from pydantic.main.ModelMetaclass\n",
      "     |      If verbose is None, set it.\n",
      "     |      \n",
      "     |      This allows users to pass in None as verbose to access the global setting.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  Config = <class 'langchain.llms.base.BaseLLM.Config'>\n",
      "     |      Configuration for this pydantic object.\n",
      "     |  \n",
      "     |  \n",
      "     |  __orig_bases__ = (langchain.schema.language_model.BaseLanguageModel[st...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.schema.language_model.BaseLanguageModel:\n",
      "     |  \n",
      "     |  get_num_tokens(self, text: 'str') -> 'int'\n",
      "     |      Get the number of tokens present in the text.\n",
      "     |      \n",
      "     |      Useful for checking if an input will fit in a model's context window.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: The string input to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The integer number of tokens in the text.\n",
      "     |  \n",
      "     |  get_num_tokens_from_messages(self, messages: 'List[BaseMessage]') -> 'int'\n",
      "     |      Get the number of tokens in the messages.\n",
      "     |      \n",
      "     |      Useful for checking if an input will fit in a model's context window.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: The message inputs to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The sum of the number of tokens across the messages.\n",
      "     |  \n",
      "     |  get_token_ids(self, text: 'str') -> 'List[int]'\n",
      "     |      Return the ordered ids of the tokens in a text.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: The string input to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          A list of ids corresponding to the tokens in the text, in order they occur\n",
      "     |              in the text.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from langchain.schema.language_model.BaseLanguageModel:\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.load.serializable.Serializable:\n",
      "     |  \n",
      "     |  __init__(self, **kwargs: Any) -> None\n",
      "     |      Create a new model by parsing and validating input data from keyword arguments.\n",
      "     |      \n",
      "     |      Raises ValidationError if the input data cannot be parsed to form a valid model.\n",
      "     |  \n",
      "     |  to_json(self) -> Union[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\n",
      "     |  \n",
      "     |  to_json_not_implemented(self) -> langchain.load.serializable.SerializedNotImplemented\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from langchain.load.serializable.Serializable:\n",
      "     |  \n",
      "     |  lc_attributes\n",
      "     |      Return a list of attribute names that should be included in the\n",
      "     |      serialized kwargs. These attributes must be accepted by the\n",
      "     |      constructor.\n",
      "     |  \n",
      "     |  lc_namespace\n",
      "     |      Return the namespace of the langchain object.\n",
      "     |      eg. [\"langchain\", \"llms\", \"openai\"]\n",
      "     |  \n",
      "     |  lc_secrets\n",
      "     |      Return a map of constructor argument names to secret ids.\n",
      "     |      eg. {\"openai_api_key\": \"OPENAI_API_KEY\"}\n",
      "     |  \n",
      "     |  lc_serializable\n",
      "     |      Return whether or not the class is serializable.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __eq__(self, other: Any) -> bool\n",
      "     |  \n",
      "     |  __getstate__(self) -> 'DictAny'\n",
      "     |  \n",
      "     |  __iter__(self) -> 'TupleGenerator'\n",
      "     |      so `dict(model)` works\n",
      "     |  \n",
      "     |  __repr_args__(self) -> 'ReprArgs'\n",
      "     |  \n",
      "     |  __setattr__(self, name, value)\n",
      "     |  \n",
      "     |  __setstate__(self, state: 'DictAny') -> None\n",
      "     |  \n",
      "     |  copy(self: 'Model', *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, update: Optional[ForwardRef('DictStrAny')] = None, deep: bool = False) -> 'Model'\n",
      "     |      Duplicate a model, optionally choose which fields to include, exclude and change.\n",
      "     |      \n",
      "     |      :param include: fields to include in new model\n",
      "     |      :param exclude: fields to exclude from new model, as with values this takes precedence over include\n",
      "     |      :param update: values to change/add in the new model. Note: the data is not validated before creating\n",
      "     |          the new model: you should trust this data\n",
      "     |      :param deep: set to `True` to make a deep copy of the model\n",
      "     |      :return: new model instance\n",
      "     |  \n",
      "     |  json(self, *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) -> 'unicode'\n",
      "     |      Generate a JSON representation of the model, `include` and `exclude` arguments as per `dict()`.\n",
      "     |      \n",
      "     |      `encoder` is an optional function to supply as `default` to json.dumps(), other arguments as per `json.dumps()`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __get_validators__() -> 'CallableGenerator' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  __try_update_forward_refs__(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      "     |      Same as update_forward_refs but will not raise exception\n",
      "     |      when forward references are not defined.\n",
      "     |  \n",
      "     |  construct(_fields_set: Optional[ForwardRef('SetStr')] = None, **values: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |      Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data.\n",
      "     |      Default values are respected, but no other validation is performed.\n",
      "     |      Behaves as if `Config.extra = 'allow'` was set since it adds all passed values\n",
      "     |  \n",
      "     |  from_orm(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_file(path: Union[str, pathlib.Path], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_obj(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_raw(b: Union[str, bytes], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema(by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}') -> 'DictStrAny' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema_json(*, by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}', **dumps_kwargs: Any) -> 'unicode' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  update_forward_refs(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      "     |      Try to update ForwardRefs on fields based on this Model, globalns and localns.\n",
      "     |  \n",
      "     |  validate(value: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __fields_set__\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.utils.Representation:\n",
      "     |  \n",
      "     |  __pretty__(self, fmt: Callable[[Any], Any], **kwargs: Any) -> Generator[Any, NoneType, NoneType]\n",
      "     |      Used by devtools (https://python-devtools.helpmanual.io/) to provide a human readable representations of objects\n",
      "     |  \n",
      "     |  __repr__(self) -> 'unicode'\n",
      "     |  \n",
      "     |  __repr_name__(self) -> 'unicode'\n",
      "     |      Name of the instance's class, used in __repr__.\n",
      "     |  \n",
      "     |  __repr_str__(self, join_str: 'unicode') -> 'unicode'\n",
      "     |  \n",
      "     |  __rich_repr__(self) -> 'RichReprResult'\n",
      "     |      Get fields for Rich library\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.schema.runnable.base.Runnable:\n",
      "     |  \n",
      "     |  __or__(self, other: 'Union[Runnable[Any, Other], Callable[[Any], Other], Mapping[str, Union[Runnable[Any, Other], Callable[[Any], Other], Any]]]') -> 'RunnableSequence[Input, Other]'\n",
      "     |      Return self|value.\n",
      "     |  \n",
      "     |  __ror__(self, other: 'Union[Runnable[Other, Any], Callable[[Any], Other], Mapping[str, Union[Runnable[Other, Any], Callable[[Other], Any], Any]]]') -> 'RunnableSequence[Other, Output]'\n",
      "     |      Return value|self.\n",
      "     |  \n",
      "     |  async atransform(self, input: 'AsyncIterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'AsyncIterator[Output]'\n",
      "     |      Default implementation of atransform, which buffers input and calls astream.\n",
      "     |      Subclasses should override this method if they can start producing output while\n",
      "     |      input is still being generated.\n",
      "     |  \n",
      "     |  bind(self, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      "     |      Bind arguments to a Runnable, returning a new Runnable.\n",
      "     |  \n",
      "     |  map(self) -> 'Runnable[List[Input], List[Output]]'\n",
      "     |      Return a new Runnable that maps a list of inputs to a list of outputs,\n",
      "     |      by calling invoke() with each input.\n",
      "     |  \n",
      "     |  transform(self, input: 'Iterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'Iterator[Output]'\n",
      "     |      Default implementation of transform, which buffers input and then calls stream.\n",
      "     |      Subclasses should override this method if they can start producing output while\n",
      "     |      input is still being generated.\n",
      "     |  \n",
      "     |  with_config(self, config: 'Optional[RunnableConfig]' = None, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      "     |      Bind config to a Runnable, returning a new Runnable.\n",
      "     |  \n",
      "     |  with_fallbacks(self, fallbacks: 'Sequence[Runnable[Input, Output]]', *, exceptions_to_handle: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,)) -> 'RunnableWithFallbacks[Input, Output]'\n",
      "     |  \n",
      "     |  with_retry(self, *, retry_if_exception_type: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,), wait_exponential_jitter: 'bool' = True, stop_after_attempt: 'int' = 3) -> 'Runnable[Input, Output]'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __class_getitem__(params) from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  __init_subclass__(*args, **kwargs) from pydantic.main.ModelMetaclass\n",
      "     |      This method is called when a class is subclassed.\n",
      "     |      \n",
      "     |      The default implementation does nothing. It may be\n",
      "     |      overridden to extend subclasses.\n",
      "    \n",
      "    class EdenAI(langchain.llms.base.LLM)\n",
      "     |  EdenAI(*, cache: Optional[bool] = None, verbose: bool = None, callbacks: Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager, NoneType] = None, callback_manager: Optional[langchain.callbacks.base.BaseCallbackManager] = None, tags: Optional[List[str]] = None, metadata: Optional[Dict[str, Any]] = None, base_url: str = 'https://api.edenai.run/v2', edenai_api_key: Optional[str] = None, feature: Literal['text', 'image'] = 'text', subfeature: Literal['generation'] = 'generation', provider: str, model: Optional[str] = None, temperature: Optional[langchain.llms.edenai.ConstrainedFloatValue] = None, max_tokens: Optional[langchain.llms.edenai.ConstrainedIntValue] = None, resolution: Optional[Literal['256x256', '512x512', '1024x1024']] = None, params: Dict[str, Any] = None, model_kwargs: Dict[str, Any] = None, stop_sequences: Optional[List[str]] = None) -> None\n",
      "     |  \n",
      "     |  Wrapper around edenai models.\n",
      "     |  \n",
      "     |  To use, you should have\n",
      "     |  the environment variable ``EDENAI_API_KEY`` set with your API token.\n",
      "     |  You can find your token here: https://app.edenai.run/admin/account/settings\n",
      "     |  \n",
      "     |  `feature` and `subfeature` are required, but any other model parameters can also be\n",
      "     |  passed in with the format params={model_param: value, ...}\n",
      "     |  \n",
      "     |  for api reference check edenai documentation: http://docs.edenai.co.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      EdenAI\n",
      "     |      langchain.llms.base.LLM\n",
      "     |      langchain.llms.base.BaseLLM\n",
      "     |      langchain.schema.language_model.BaseLanguageModel\n",
      "     |      langchain.load.serializable.Serializable\n",
      "     |      pydantic.main.BaseModel\n",
      "     |      pydantic.utils.Representation\n",
      "     |      langchain.schema.runnable.base.Runnable\n",
      "     |      typing.Generic\n",
      "     |      abc.ABC\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  build_extra(values: Dict[str, Any]) -> Dict[str, Any] from pydantic.main.ModelMetaclass\n",
      "     |      Build extra kwargs from additional params that were passed in.\n",
      "     |  \n",
      "     |  validate_environment(values: Dict) -> Dict from pydantic.main.ModelMetaclass\n",
      "     |      Validate that api key exists in environment.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __json_encoder__ = pydantic_encoder(obj: Any) -> Any\n",
      "     |  \n",
      "     |  get_user_agent() -> str\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  Config = <class 'langchain.llms.edenai.EdenAI.Config'>\n",
      "     |      Configuration for this pydantic object.\n",
      "     |  \n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {'base_url': <class 'str'>, 'edenai_api_key': typing...\n",
      "     |  \n",
      "     |  __class_vars__ = set()\n",
      "     |  \n",
      "     |  __config__ = <class 'langchain.llms.edenai.Config'>\n",
      "     |  \n",
      "     |  __custom_root_type__ = False\n",
      "     |  \n",
      "     |  __exclude_fields__ = {'callback_manager': True, 'callbacks': True, 'me...\n",
      "     |  \n",
      "     |  __fields__ = {'base_url': ModelField(name='base_url', type=str, requir...\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "     |  \n",
      "     |  __include_fields__ = None\n",
      "     |  \n",
      "     |  __parameters__ = ()\n",
      "     |  \n",
      "     |  __post_root_validators__ = [(False, <function BaseLLM.raise_deprecatio...\n",
      "     |  \n",
      "     |  __pre_root_validators__ = [<function EdenAI.build_extra>]\n",
      "     |  \n",
      "     |  __private_attributes__ = {'_lc_kwargs': ModelPrivateAttr(default=Pydan...\n",
      "     |  \n",
      "     |  __schema_cache__ = {}\n",
      "     |  \n",
      "     |  __signature__ = <Signature (*, cache: Optional[bool] = None, ver...p_s...\n",
      "     |  \n",
      "     |  __validators__ = {'verbose': [<pydantic.class_validators.Validator obj...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  __call__(self, prompt: 'str', stop: 'Optional[List[str]]' = None, callbacks: 'Callbacks' = None, *, tags: 'Optional[List[str]]' = None, metadata: 'Optional[Dict[str, Any]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Check Cache and run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  __str__(self) -> 'str'\n",
      "     |      Get a string representation of the object for printing.\n",
      "     |  \n",
      "     |  async abatch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      "     |      Default implementation of abatch, which calls ainvoke N times.\n",
      "     |      Subclasses should override this method if they can batch more efficiently.\n",
      "     |  \n",
      "     |  async agenerate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  async agenerate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Asynchronously pass a sequence of prompts and return model generations.\n",
      "     |      \n",
      "     |      This method should make use of batched calls for models that expose a batched\n",
      "     |      API.\n",
      "     |      \n",
      "     |      Use this method when you want to:\n",
      "     |          1. take advantage of batched calls,\n",
      "     |          2. need more output from the model than just the top generated value,\n",
      "     |          3. are building chains that are agnostic to the underlying language model\n",
      "     |              type (e.g., pure text completion models vs chat models).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      "     |              converted to match the format of any language model (string for pure\n",
      "     |              text generation models and BaseMessages for chat models).\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          callbacks: Callbacks to pass through. Used for executing additional\n",
      "     |              functionality, such as logging or streaming, throughout generation.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          An LLMResult, which contains a list of candidate Generations for each input\n",
      "     |              prompt and additional model provider-specific output.\n",
      "     |  \n",
      "     |  async ainvoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Default implementation of ainvoke, which calls invoke in a thread pool.\n",
      "     |      Subclasses should override this method if they can run asynchronously.\n",
      "     |  \n",
      "     |  async apredict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Asynchronously pass a string to the model and return a string prediction.\n",
      "     |      \n",
      "     |      Use this method when calling pure text generation models and only the top\n",
      "     |          candidate generation is needed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: String input to pass to the model.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a string.\n",
      "     |  \n",
      "     |  async apredict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      "     |      Asynchronously pass messages to the model and return a message prediction.\n",
      "     |      \n",
      "     |      Use this method when calling chat models and only the top\n",
      "     |          candidate generation is needed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: A sequence of chat messages corresponding to a single model input.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a message.\n",
      "     |  \n",
      "     |  async astream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'AsyncIterator[str]'\n",
      "     |      Default implementation of astream, which calls ainvoke.\n",
      "     |      Subclasses should override this method if they support streaming output.\n",
      "     |  \n",
      "     |  batch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      "     |      Default implementation of batch, which calls invoke N times.\n",
      "     |      Subclasses should override this method if they can batch more efficiently.\n",
      "     |  \n",
      "     |  dict(self, **kwargs: 'Any') -> 'Dict'\n",
      "     |      Return a dictionary of the LLM.\n",
      "     |  \n",
      "     |  generate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  generate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Pass a sequence of prompts to the model and return model generations.\n",
      "     |      \n",
      "     |      This method should make use of batched calls for models that expose a batched\n",
      "     |      API.\n",
      "     |      \n",
      "     |      Use this method when you want to:\n",
      "     |          1. take advantage of batched calls,\n",
      "     |          2. need more output from the model than just the top generated value,\n",
      "     |          3. are building chains that are agnostic to the underlying language model\n",
      "     |              type (e.g., pure text completion models vs chat models).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      "     |              converted to match the format of any language model (string for pure\n",
      "     |              text generation models and BaseMessages for chat models).\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          callbacks: Callbacks to pass through. Used for executing additional\n",
      "     |              functionality, such as logging or streaming, throughout generation.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          An LLMResult, which contains a list of candidate Generations for each input\n",
      "     |              prompt and additional model provider-specific output.\n",
      "     |  \n",
      "     |  invoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |  \n",
      "     |  predict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Pass a single string input to the model and return a string prediction.\n",
      "     |      \n",
      "     |       Use this method when passing in raw text. If you want to pass in specific\n",
      "     |          types of chat messages, use predict_messages.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: String input to pass to the model.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a string.\n",
      "     |  \n",
      "     |  predict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      "     |      Pass a message sequence to the model and return a message prediction.\n",
      "     |      \n",
      "     |      Use this method when passing in chat messages. If you want to pass in raw text,\n",
      "     |          use predict.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: A sequence of chat messages corresponding to a single model input.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a message.\n",
      "     |  \n",
      "     |  save(self, file_path: 'Union[Path, str]') -> 'None'\n",
      "     |      Save the LLM.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          file_path: Path to file to save the LLM to.\n",
      "     |      \n",
      "     |      Example:\n",
      "     |      .. code-block:: python\n",
      "     |      \n",
      "     |          llm.save(file_path=\"path/llm.yaml\")\n",
      "     |  \n",
      "     |  stream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'Iterator[str]'\n",
      "     |      Default implementation of stream, which calls invoke.\n",
      "     |      Subclasses should override this method if they support streaming output.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  raise_deprecation(values: 'Dict') -> 'Dict' from pydantic.main.ModelMetaclass\n",
      "     |      Raise deprecation warning if callback_manager is used.\n",
      "     |  \n",
      "     |  set_verbose(verbose: 'Optional[bool]') -> 'bool' from pydantic.main.ModelMetaclass\n",
      "     |      If verbose is None, set it.\n",
      "     |      \n",
      "     |      This allows users to pass in None as verbose to access the global setting.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  __orig_bases__ = (langchain.schema.language_model.BaseLanguageModel[st...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.schema.language_model.BaseLanguageModel:\n",
      "     |  \n",
      "     |  get_num_tokens(self, text: 'str') -> 'int'\n",
      "     |      Get the number of tokens present in the text.\n",
      "     |      \n",
      "     |      Useful for checking if an input will fit in a model's context window.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: The string input to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The integer number of tokens in the text.\n",
      "     |  \n",
      "     |  get_num_tokens_from_messages(self, messages: 'List[BaseMessage]') -> 'int'\n",
      "     |      Get the number of tokens in the messages.\n",
      "     |      \n",
      "     |      Useful for checking if an input will fit in a model's context window.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: The message inputs to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The sum of the number of tokens across the messages.\n",
      "     |  \n",
      "     |  get_token_ids(self, text: 'str') -> 'List[int]'\n",
      "     |      Return the ordered ids of the tokens in a text.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: The string input to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          A list of ids corresponding to the tokens in the text, in order they occur\n",
      "     |              in the text.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from langchain.schema.language_model.BaseLanguageModel:\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.load.serializable.Serializable:\n",
      "     |  \n",
      "     |  __init__(self, **kwargs: Any) -> None\n",
      "     |      Create a new model by parsing and validating input data from keyword arguments.\n",
      "     |      \n",
      "     |      Raises ValidationError if the input data cannot be parsed to form a valid model.\n",
      "     |  \n",
      "     |  to_json(self) -> Union[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\n",
      "     |  \n",
      "     |  to_json_not_implemented(self) -> langchain.load.serializable.SerializedNotImplemented\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from langchain.load.serializable.Serializable:\n",
      "     |  \n",
      "     |  lc_attributes\n",
      "     |      Return a list of attribute names that should be included in the\n",
      "     |      serialized kwargs. These attributes must be accepted by the\n",
      "     |      constructor.\n",
      "     |  \n",
      "     |  lc_namespace\n",
      "     |      Return the namespace of the langchain object.\n",
      "     |      eg. [\"langchain\", \"llms\", \"openai\"]\n",
      "     |  \n",
      "     |  lc_secrets\n",
      "     |      Return a map of constructor argument names to secret ids.\n",
      "     |      eg. {\"openai_api_key\": \"OPENAI_API_KEY\"}\n",
      "     |  \n",
      "     |  lc_serializable\n",
      "     |      Return whether or not the class is serializable.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __eq__(self, other: Any) -> bool\n",
      "     |  \n",
      "     |  __getstate__(self) -> 'DictAny'\n",
      "     |  \n",
      "     |  __iter__(self) -> 'TupleGenerator'\n",
      "     |      so `dict(model)` works\n",
      "     |  \n",
      "     |  __repr_args__(self) -> 'ReprArgs'\n",
      "     |  \n",
      "     |  __setattr__(self, name, value)\n",
      "     |  \n",
      "     |  __setstate__(self, state: 'DictAny') -> None\n",
      "     |  \n",
      "     |  copy(self: 'Model', *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, update: Optional[ForwardRef('DictStrAny')] = None, deep: bool = False) -> 'Model'\n",
      "     |      Duplicate a model, optionally choose which fields to include, exclude and change.\n",
      "     |      \n",
      "     |      :param include: fields to include in new model\n",
      "     |      :param exclude: fields to exclude from new model, as with values this takes precedence over include\n",
      "     |      :param update: values to change/add in the new model. Note: the data is not validated before creating\n",
      "     |          the new model: you should trust this data\n",
      "     |      :param deep: set to `True` to make a deep copy of the model\n",
      "     |      :return: new model instance\n",
      "     |  \n",
      "     |  json(self, *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) -> 'unicode'\n",
      "     |      Generate a JSON representation of the model, `include` and `exclude` arguments as per `dict()`.\n",
      "     |      \n",
      "     |      `encoder` is an optional function to supply as `default` to json.dumps(), other arguments as per `json.dumps()`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __get_validators__() -> 'CallableGenerator' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  __try_update_forward_refs__(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      "     |      Same as update_forward_refs but will not raise exception\n",
      "     |      when forward references are not defined.\n",
      "     |  \n",
      "     |  construct(_fields_set: Optional[ForwardRef('SetStr')] = None, **values: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |      Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data.\n",
      "     |      Default values are respected, but no other validation is performed.\n",
      "     |      Behaves as if `Config.extra = 'allow'` was set since it adds all passed values\n",
      "     |  \n",
      "     |  from_orm(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_file(path: Union[str, pathlib.Path], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_obj(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_raw(b: Union[str, bytes], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema(by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}') -> 'DictStrAny' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema_json(*, by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}', **dumps_kwargs: Any) -> 'unicode' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  update_forward_refs(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      "     |      Try to update ForwardRefs on fields based on this Model, globalns and localns.\n",
      "     |  \n",
      "     |  validate(value: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __fields_set__\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.utils.Representation:\n",
      "     |  \n",
      "     |  __pretty__(self, fmt: Callable[[Any], Any], **kwargs: Any) -> Generator[Any, NoneType, NoneType]\n",
      "     |      Used by devtools (https://python-devtools.helpmanual.io/) to provide a human readable representations of objects\n",
      "     |  \n",
      "     |  __repr__(self) -> 'unicode'\n",
      "     |  \n",
      "     |  __repr_name__(self) -> 'unicode'\n",
      "     |      Name of the instance's class, used in __repr__.\n",
      "     |  \n",
      "     |  __repr_str__(self, join_str: 'unicode') -> 'unicode'\n",
      "     |  \n",
      "     |  __rich_repr__(self) -> 'RichReprResult'\n",
      "     |      Get fields for Rich library\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.schema.runnable.base.Runnable:\n",
      "     |  \n",
      "     |  __or__(self, other: 'Union[Runnable[Any, Other], Callable[[Any], Other], Mapping[str, Union[Runnable[Any, Other], Callable[[Any], Other], Any]]]') -> 'RunnableSequence[Input, Other]'\n",
      "     |      Return self|value.\n",
      "     |  \n",
      "     |  __ror__(self, other: 'Union[Runnable[Other, Any], Callable[[Any], Other], Mapping[str, Union[Runnable[Other, Any], Callable[[Other], Any], Any]]]') -> 'RunnableSequence[Other, Output]'\n",
      "     |      Return value|self.\n",
      "     |  \n",
      "     |  async atransform(self, input: 'AsyncIterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'AsyncIterator[Output]'\n",
      "     |      Default implementation of atransform, which buffers input and calls astream.\n",
      "     |      Subclasses should override this method if they can start producing output while\n",
      "     |      input is still being generated.\n",
      "     |  \n",
      "     |  bind(self, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      "     |      Bind arguments to a Runnable, returning a new Runnable.\n",
      "     |  \n",
      "     |  map(self) -> 'Runnable[List[Input], List[Output]]'\n",
      "     |      Return a new Runnable that maps a list of inputs to a list of outputs,\n",
      "     |      by calling invoke() with each input.\n",
      "     |  \n",
      "     |  transform(self, input: 'Iterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'Iterator[Output]'\n",
      "     |      Default implementation of transform, which buffers input and then calls stream.\n",
      "     |      Subclasses should override this method if they can start producing output while\n",
      "     |      input is still being generated.\n",
      "     |  \n",
      "     |  with_config(self, config: 'Optional[RunnableConfig]' = None, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      "     |      Bind config to a Runnable, returning a new Runnable.\n",
      "     |  \n",
      "     |  with_fallbacks(self, fallbacks: 'Sequence[Runnable[Input, Output]]', *, exceptions_to_handle: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,)) -> 'RunnableWithFallbacks[Input, Output]'\n",
      "     |  \n",
      "     |  with_retry(self, *, retry_if_exception_type: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,), wait_exponential_jitter: 'bool' = True, stop_after_attempt: 'int' = 3) -> 'Runnable[Input, Output]'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __class_getitem__(params) from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  __init_subclass__(*args, **kwargs) from pydantic.main.ModelMetaclass\n",
      "     |      This method is called when a class is subclassed.\n",
      "     |      \n",
      "     |      The default implementation does nothing. It may be\n",
      "     |      overridden to extend subclasses.\n",
      "    \n",
      "    class FakeListLLM(langchain.llms.base.LLM)\n",
      "     |  FakeListLLM(*, cache: Optional[bool] = None, verbose: bool = None, callbacks: Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager, NoneType] = None, callback_manager: Optional[langchain.callbacks.base.BaseCallbackManager] = None, tags: Optional[List[str]] = None, metadata: Optional[Dict[str, Any]] = None, responses: List, sleep: Optional[float] = None, i: int = 0) -> None\n",
      "     |  \n",
      "     |  Fake LLM for testing purposes.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      FakeListLLM\n",
      "     |      langchain.llms.base.LLM\n",
      "     |      langchain.llms.base.BaseLLM\n",
      "     |      langchain.schema.language_model.BaseLanguageModel\n",
      "     |      langchain.load.serializable.Serializable\n",
      "     |      pydantic.main.BaseModel\n",
      "     |      pydantic.utils.Representation\n",
      "     |      langchain.schema.runnable.base.Runnable\n",
      "     |      typing.Generic\n",
      "     |      abc.ABC\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __json_encoder__ = pydantic_encoder(obj: Any) -> Any\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {'i': <class 'int'>, 'responses': typing.List, 'slee...\n",
      "     |  \n",
      "     |  __class_vars__ = set()\n",
      "     |  \n",
      "     |  __config__ = <class 'langchain.llms.fake.Config'>\n",
      "     |  \n",
      "     |  __custom_root_type__ = False\n",
      "     |  \n",
      "     |  __exclude_fields__ = {'callback_manager': True, 'callbacks': True, 'me...\n",
      "     |  \n",
      "     |  __fields__ = {'cache': ModelField(name='cache', type=Optional[bool], r...\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "     |  \n",
      "     |  __include_fields__ = None\n",
      "     |  \n",
      "     |  __parameters__ = ()\n",
      "     |  \n",
      "     |  __post_root_validators__ = [(False, <function BaseLLM.raise_deprecatio...\n",
      "     |  \n",
      "     |  __pre_root_validators__ = []\n",
      "     |  \n",
      "     |  __private_attributes__ = {'_lc_kwargs': ModelPrivateAttr(default=Pydan...\n",
      "     |  \n",
      "     |  __schema_cache__ = {}\n",
      "     |  \n",
      "     |  __signature__ = <Signature (*, cache: Optional[bool] = None, ver...eep...\n",
      "     |  \n",
      "     |  __validators__ = {'verbose': [<pydantic.class_validators.Validator obj...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  __call__(self, prompt: 'str', stop: 'Optional[List[str]]' = None, callbacks: 'Callbacks' = None, *, tags: 'Optional[List[str]]' = None, metadata: 'Optional[Dict[str, Any]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Check Cache and run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  __str__(self) -> 'str'\n",
      "     |      Get a string representation of the object for printing.\n",
      "     |  \n",
      "     |  async abatch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      "     |      Default implementation of abatch, which calls ainvoke N times.\n",
      "     |      Subclasses should override this method if they can batch more efficiently.\n",
      "     |  \n",
      "     |  async agenerate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  async agenerate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Asynchronously pass a sequence of prompts and return model generations.\n",
      "     |      \n",
      "     |      This method should make use of batched calls for models that expose a batched\n",
      "     |      API.\n",
      "     |      \n",
      "     |      Use this method when you want to:\n",
      "     |          1. take advantage of batched calls,\n",
      "     |          2. need more output from the model than just the top generated value,\n",
      "     |          3. are building chains that are agnostic to the underlying language model\n",
      "     |              type (e.g., pure text completion models vs chat models).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      "     |              converted to match the format of any language model (string for pure\n",
      "     |              text generation models and BaseMessages for chat models).\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          callbacks: Callbacks to pass through. Used for executing additional\n",
      "     |              functionality, such as logging or streaming, throughout generation.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          An LLMResult, which contains a list of candidate Generations for each input\n",
      "     |              prompt and additional model provider-specific output.\n",
      "     |  \n",
      "     |  async ainvoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Default implementation of ainvoke, which calls invoke in a thread pool.\n",
      "     |      Subclasses should override this method if they can run asynchronously.\n",
      "     |  \n",
      "     |  async apredict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Asynchronously pass a string to the model and return a string prediction.\n",
      "     |      \n",
      "     |      Use this method when calling pure text generation models and only the top\n",
      "     |          candidate generation is needed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: String input to pass to the model.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a string.\n",
      "     |  \n",
      "     |  async apredict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      "     |      Asynchronously pass messages to the model and return a message prediction.\n",
      "     |      \n",
      "     |      Use this method when calling chat models and only the top\n",
      "     |          candidate generation is needed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: A sequence of chat messages corresponding to a single model input.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a message.\n",
      "     |  \n",
      "     |  async astream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'AsyncIterator[str]'\n",
      "     |      Default implementation of astream, which calls ainvoke.\n",
      "     |      Subclasses should override this method if they support streaming output.\n",
      "     |  \n",
      "     |  batch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      "     |      Default implementation of batch, which calls invoke N times.\n",
      "     |      Subclasses should override this method if they can batch more efficiently.\n",
      "     |  \n",
      "     |  dict(self, **kwargs: 'Any') -> 'Dict'\n",
      "     |      Return a dictionary of the LLM.\n",
      "     |  \n",
      "     |  generate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  generate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Pass a sequence of prompts to the model and return model generations.\n",
      "     |      \n",
      "     |      This method should make use of batched calls for models that expose a batched\n",
      "     |      API.\n",
      "     |      \n",
      "     |      Use this method when you want to:\n",
      "     |          1. take advantage of batched calls,\n",
      "     |          2. need more output from the model than just the top generated value,\n",
      "     |          3. are building chains that are agnostic to the underlying language model\n",
      "     |              type (e.g., pure text completion models vs chat models).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      "     |              converted to match the format of any language model (string for pure\n",
      "     |              text generation models and BaseMessages for chat models).\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          callbacks: Callbacks to pass through. Used for executing additional\n",
      "     |              functionality, such as logging or streaming, throughout generation.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          An LLMResult, which contains a list of candidate Generations for each input\n",
      "     |              prompt and additional model provider-specific output.\n",
      "     |  \n",
      "     |  invoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |  \n",
      "     |  predict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Pass a single string input to the model and return a string prediction.\n",
      "     |      \n",
      "     |       Use this method when passing in raw text. If you want to pass in specific\n",
      "     |          types of chat messages, use predict_messages.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: String input to pass to the model.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a string.\n",
      "     |  \n",
      "     |  predict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      "     |      Pass a message sequence to the model and return a message prediction.\n",
      "     |      \n",
      "     |      Use this method when passing in chat messages. If you want to pass in raw text,\n",
      "     |          use predict.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: A sequence of chat messages corresponding to a single model input.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a message.\n",
      "     |  \n",
      "     |  save(self, file_path: 'Union[Path, str]') -> 'None'\n",
      "     |      Save the LLM.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          file_path: Path to file to save the LLM to.\n",
      "     |      \n",
      "     |      Example:\n",
      "     |      .. code-block:: python\n",
      "     |      \n",
      "     |          llm.save(file_path=\"path/llm.yaml\")\n",
      "     |  \n",
      "     |  stream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'Iterator[str]'\n",
      "     |      Default implementation of stream, which calls invoke.\n",
      "     |      Subclasses should override this method if they support streaming output.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  raise_deprecation(values: 'Dict') -> 'Dict' from pydantic.main.ModelMetaclass\n",
      "     |      Raise deprecation warning if callback_manager is used.\n",
      "     |  \n",
      "     |  set_verbose(verbose: 'Optional[bool]') -> 'bool' from pydantic.main.ModelMetaclass\n",
      "     |      If verbose is None, set it.\n",
      "     |      \n",
      "     |      This allows users to pass in None as verbose to access the global setting.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  Config = <class 'langchain.llms.base.BaseLLM.Config'>\n",
      "     |      Configuration for this pydantic object.\n",
      "     |  \n",
      "     |  \n",
      "     |  __orig_bases__ = (langchain.schema.language_model.BaseLanguageModel[st...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.schema.language_model.BaseLanguageModel:\n",
      "     |  \n",
      "     |  get_num_tokens(self, text: 'str') -> 'int'\n",
      "     |      Get the number of tokens present in the text.\n",
      "     |      \n",
      "     |      Useful for checking if an input will fit in a model's context window.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: The string input to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The integer number of tokens in the text.\n",
      "     |  \n",
      "     |  get_num_tokens_from_messages(self, messages: 'List[BaseMessage]') -> 'int'\n",
      "     |      Get the number of tokens in the messages.\n",
      "     |      \n",
      "     |      Useful for checking if an input will fit in a model's context window.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: The message inputs to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The sum of the number of tokens across the messages.\n",
      "     |  \n",
      "     |  get_token_ids(self, text: 'str') -> 'List[int]'\n",
      "     |      Return the ordered ids of the tokens in a text.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: The string input to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          A list of ids corresponding to the tokens in the text, in order they occur\n",
      "     |              in the text.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from langchain.schema.language_model.BaseLanguageModel:\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.load.serializable.Serializable:\n",
      "     |  \n",
      "     |  __init__(self, **kwargs: Any) -> None\n",
      "     |      Create a new model by parsing and validating input data from keyword arguments.\n",
      "     |      \n",
      "     |      Raises ValidationError if the input data cannot be parsed to form a valid model.\n",
      "     |  \n",
      "     |  to_json(self) -> Union[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\n",
      "     |  \n",
      "     |  to_json_not_implemented(self) -> langchain.load.serializable.SerializedNotImplemented\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from langchain.load.serializable.Serializable:\n",
      "     |  \n",
      "     |  lc_attributes\n",
      "     |      Return a list of attribute names that should be included in the\n",
      "     |      serialized kwargs. These attributes must be accepted by the\n",
      "     |      constructor.\n",
      "     |  \n",
      "     |  lc_namespace\n",
      "     |      Return the namespace of the langchain object.\n",
      "     |      eg. [\"langchain\", \"llms\", \"openai\"]\n",
      "     |  \n",
      "     |  lc_secrets\n",
      "     |      Return a map of constructor argument names to secret ids.\n",
      "     |      eg. {\"openai_api_key\": \"OPENAI_API_KEY\"}\n",
      "     |  \n",
      "     |  lc_serializable\n",
      "     |      Return whether or not the class is serializable.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __eq__(self, other: Any) -> bool\n",
      "     |  \n",
      "     |  __getstate__(self) -> 'DictAny'\n",
      "     |  \n",
      "     |  __iter__(self) -> 'TupleGenerator'\n",
      "     |      so `dict(model)` works\n",
      "     |  \n",
      "     |  __repr_args__(self) -> 'ReprArgs'\n",
      "     |  \n",
      "     |  __setattr__(self, name, value)\n",
      "     |  \n",
      "     |  __setstate__(self, state: 'DictAny') -> None\n",
      "     |  \n",
      "     |  copy(self: 'Model', *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, update: Optional[ForwardRef('DictStrAny')] = None, deep: bool = False) -> 'Model'\n",
      "     |      Duplicate a model, optionally choose which fields to include, exclude and change.\n",
      "     |      \n",
      "     |      :param include: fields to include in new model\n",
      "     |      :param exclude: fields to exclude from new model, as with values this takes precedence over include\n",
      "     |      :param update: values to change/add in the new model. Note: the data is not validated before creating\n",
      "     |          the new model: you should trust this data\n",
      "     |      :param deep: set to `True` to make a deep copy of the model\n",
      "     |      :return: new model instance\n",
      "     |  \n",
      "     |  json(self, *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) -> 'unicode'\n",
      "     |      Generate a JSON representation of the model, `include` and `exclude` arguments as per `dict()`.\n",
      "     |      \n",
      "     |      `encoder` is an optional function to supply as `default` to json.dumps(), other arguments as per `json.dumps()`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __get_validators__() -> 'CallableGenerator' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  __try_update_forward_refs__(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      "     |      Same as update_forward_refs but will not raise exception\n",
      "     |      when forward references are not defined.\n",
      "     |  \n",
      "     |  construct(_fields_set: Optional[ForwardRef('SetStr')] = None, **values: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |      Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data.\n",
      "     |      Default values are respected, but no other validation is performed.\n",
      "     |      Behaves as if `Config.extra = 'allow'` was set since it adds all passed values\n",
      "     |  \n",
      "     |  from_orm(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_file(path: Union[str, pathlib.Path], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_obj(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_raw(b: Union[str, bytes], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema(by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}') -> 'DictStrAny' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema_json(*, by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}', **dumps_kwargs: Any) -> 'unicode' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  update_forward_refs(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      "     |      Try to update ForwardRefs on fields based on this Model, globalns and localns.\n",
      "     |  \n",
      "     |  validate(value: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __fields_set__\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.utils.Representation:\n",
      "     |  \n",
      "     |  __pretty__(self, fmt: Callable[[Any], Any], **kwargs: Any) -> Generator[Any, NoneType, NoneType]\n",
      "     |      Used by devtools (https://python-devtools.helpmanual.io/) to provide a human readable representations of objects\n",
      "     |  \n",
      "     |  __repr__(self) -> 'unicode'\n",
      "     |  \n",
      "     |  __repr_name__(self) -> 'unicode'\n",
      "     |      Name of the instance's class, used in __repr__.\n",
      "     |  \n",
      "     |  __repr_str__(self, join_str: 'unicode') -> 'unicode'\n",
      "     |  \n",
      "     |  __rich_repr__(self) -> 'RichReprResult'\n",
      "     |      Get fields for Rich library\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.schema.runnable.base.Runnable:\n",
      "     |  \n",
      "     |  __or__(self, other: 'Union[Runnable[Any, Other], Callable[[Any], Other], Mapping[str, Union[Runnable[Any, Other], Callable[[Any], Other], Any]]]') -> 'RunnableSequence[Input, Other]'\n",
      "     |      Return self|value.\n",
      "     |  \n",
      "     |  __ror__(self, other: 'Union[Runnable[Other, Any], Callable[[Any], Other], Mapping[str, Union[Runnable[Other, Any], Callable[[Other], Any], Any]]]') -> 'RunnableSequence[Other, Output]'\n",
      "     |      Return value|self.\n",
      "     |  \n",
      "     |  async atransform(self, input: 'AsyncIterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'AsyncIterator[Output]'\n",
      "     |      Default implementation of atransform, which buffers input and calls astream.\n",
      "     |      Subclasses should override this method if they can start producing output while\n",
      "     |      input is still being generated.\n",
      "     |  \n",
      "     |  bind(self, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      "     |      Bind arguments to a Runnable, returning a new Runnable.\n",
      "     |  \n",
      "     |  map(self) -> 'Runnable[List[Input], List[Output]]'\n",
      "     |      Return a new Runnable that maps a list of inputs to a list of outputs,\n",
      "     |      by calling invoke() with each input.\n",
      "     |  \n",
      "     |  transform(self, input: 'Iterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'Iterator[Output]'\n",
      "     |      Default implementation of transform, which buffers input and then calls stream.\n",
      "     |      Subclasses should override this method if they can start producing output while\n",
      "     |      input is still being generated.\n",
      "     |  \n",
      "     |  with_config(self, config: 'Optional[RunnableConfig]' = None, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      "     |      Bind config to a Runnable, returning a new Runnable.\n",
      "     |  \n",
      "     |  with_fallbacks(self, fallbacks: 'Sequence[Runnable[Input, Output]]', *, exceptions_to_handle: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,)) -> 'RunnableWithFallbacks[Input, Output]'\n",
      "     |  \n",
      "     |  with_retry(self, *, retry_if_exception_type: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,), wait_exponential_jitter: 'bool' = True, stop_after_attempt: 'int' = 3) -> 'Runnable[Input, Output]'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __class_getitem__(params) from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  __init_subclass__(*args, **kwargs) from pydantic.main.ModelMetaclass\n",
      "     |      This method is called when a class is subclassed.\n",
      "     |      \n",
      "     |      The default implementation does nothing. It may be\n",
      "     |      overridden to extend subclasses.\n",
      "    \n",
      "    class Fireworks(BaseFireworks)\n",
      "     |  Fireworks(*, cache: Optional[bool] = None, verbose: bool = None, callbacks: Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager, NoneType] = None, callback_manager: Optional[langchain.callbacks.base.BaseCallbackManager] = None, tags: Optional[List[str]] = None, metadata: Optional[Dict[str, Any]] = None, model: str = 'accounts/fireworks/models/llama-v2-7b-chat', temperature: float = 0.7, max_tokens: int = 512, top_p: float = 1, fireworks_api_key: Optional[str] = None, batch_size: int = 20, request_timeout: Union[float, Tuple[float, float], NoneType] = None, max_retries: int = 6) -> None\n",
      "     |  \n",
      "     |  Wrapper around Fireworks large language models.\n",
      "     |  To use, you should have the ``fireworks`` python package installed, and the\n",
      "     |  environment variable ``FIREWORKS_API_KEY`` set with your API key.\n",
      "     |  Any parameters that are valid to be passed to the fireworks.create\n",
      "     |  call can be passed in, even if not explicitly saved on this class.\n",
      "     |  Example:\n",
      "     |      .. code-block:: python\n",
      "     |          from langchain.llms import fireworks\n",
      "     |          llm = Fireworks(model_id=\"llama-v2-13b\")\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Fireworks\n",
      "     |      BaseFireworks\n",
      "     |      langchain.llms.base.BaseLLM\n",
      "     |      langchain.schema.language_model.BaseLanguageModel\n",
      "     |      langchain.load.serializable.Serializable\n",
      "     |      pydantic.main.BaseModel\n",
      "     |      pydantic.utils.Representation\n",
      "     |      langchain.schema.runnable.base.Runnable\n",
      "     |      typing.Generic\n",
      "     |      abc.ABC\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __json_encoder__ = pydantic_encoder(obj: Any) -> Any\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {}\n",
      "     |  \n",
      "     |  __class_vars__ = set()\n",
      "     |  \n",
      "     |  __config__ = <class 'langchain.llms.fireworks.Config'>\n",
      "     |  \n",
      "     |  __custom_root_type__ = False\n",
      "     |  \n",
      "     |  __exclude_fields__ = {'callback_manager': True, 'callbacks': True, 'me...\n",
      "     |  \n",
      "     |  __fields__ = {'batch_size': ModelField(name='batch_size', type=int, re...\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "     |  \n",
      "     |  __include_fields__ = None\n",
      "     |  \n",
      "     |  __parameters__ = ()\n",
      "     |  \n",
      "     |  __post_root_validators__ = [(False, <function BaseLLM.raise_deprecatio...\n",
      "     |  \n",
      "     |  __pre_root_validators__ = []\n",
      "     |  \n",
      "     |  __private_attributes__ = {'_lc_kwargs': ModelPrivateAttr(default=Pydan...\n",
      "     |  \n",
      "     |  __schema_cache__ = {}\n",
      "     |  \n",
      "     |  __signature__ = <Signature (*, cache: Optional[bool] = None, ver... No...\n",
      "     |  \n",
      "     |  __validators__ = {'verbose': [<pydantic.class_validators.Validator obj...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BaseFireworks:\n",
      "     |  \n",
      "     |  create_llm_result(self, choices: Any, prompts: List[str], token_usage: Dict[str, int]) -> langchain.schema.output.LLMResult\n",
      "     |      Create the LLMResult from the choices and prompts.\n",
      "     |  \n",
      "     |  get_batch_prompts(self, params: Dict[str, Any], prompts: List[str], stop: Optional[List[str]] = None) -> List[List[str]]\n",
      "     |      Get the sub prompts for llm call.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from BaseFireworks:\n",
      "     |  \n",
      "     |  validate_environment(values: Dict) -> Dict from pydantic.main.ModelMetaclass\n",
      "     |      Validate that api key and python package exists in environment.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from BaseFireworks:\n",
      "     |  \n",
      "     |  __new__(cls, **data: Any) -> Any\n",
      "     |      Initialize the Fireworks object.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from BaseFireworks:\n",
      "     |  \n",
      "     |  lc_secrets\n",
      "     |      Return a map of constructor argument names to secret ids.\n",
      "     |      eg. {\"openai_api_key\": \"OPENAI_API_KEY\"}\n",
      "     |  \n",
      "     |  lc_serializable\n",
      "     |      Return whether or not the class is serializable.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from BaseFireworks:\n",
      "     |  \n",
      "     |  Config = <class 'langchain.llms.fireworks.BaseFireworks.Config'>\n",
      "     |      Configuration for this pydantic object.\n",
      "     |  \n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  __call__(self, prompt: 'str', stop: 'Optional[List[str]]' = None, callbacks: 'Callbacks' = None, *, tags: 'Optional[List[str]]' = None, metadata: 'Optional[Dict[str, Any]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Check Cache and run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  __str__(self) -> 'str'\n",
      "     |      Get a string representation of the object for printing.\n",
      "     |  \n",
      "     |  async abatch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      "     |      Default implementation of abatch, which calls ainvoke N times.\n",
      "     |      Subclasses should override this method if they can batch more efficiently.\n",
      "     |  \n",
      "     |  async agenerate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  async agenerate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Asynchronously pass a sequence of prompts and return model generations.\n",
      "     |      \n",
      "     |      This method should make use of batched calls for models that expose a batched\n",
      "     |      API.\n",
      "     |      \n",
      "     |      Use this method when you want to:\n",
      "     |          1. take advantage of batched calls,\n",
      "     |          2. need more output from the model than just the top generated value,\n",
      "     |          3. are building chains that are agnostic to the underlying language model\n",
      "     |              type (e.g., pure text completion models vs chat models).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      "     |              converted to match the format of any language model (string for pure\n",
      "     |              text generation models and BaseMessages for chat models).\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          callbacks: Callbacks to pass through. Used for executing additional\n",
      "     |              functionality, such as logging or streaming, throughout generation.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          An LLMResult, which contains a list of candidate Generations for each input\n",
      "     |              prompt and additional model provider-specific output.\n",
      "     |  \n",
      "     |  async ainvoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Default implementation of ainvoke, which calls invoke in a thread pool.\n",
      "     |      Subclasses should override this method if they can run asynchronously.\n",
      "     |  \n",
      "     |  async apredict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Asynchronously pass a string to the model and return a string prediction.\n",
      "     |      \n",
      "     |      Use this method when calling pure text generation models and only the top\n",
      "     |          candidate generation is needed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: String input to pass to the model.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a string.\n",
      "     |  \n",
      "     |  async apredict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      "     |      Asynchronously pass messages to the model and return a message prediction.\n",
      "     |      \n",
      "     |      Use this method when calling chat models and only the top\n",
      "     |          candidate generation is needed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: A sequence of chat messages corresponding to a single model input.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a message.\n",
      "     |  \n",
      "     |  async astream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'AsyncIterator[str]'\n",
      "     |      Default implementation of astream, which calls ainvoke.\n",
      "     |      Subclasses should override this method if they support streaming output.\n",
      "     |  \n",
      "     |  batch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      "     |      Default implementation of batch, which calls invoke N times.\n",
      "     |      Subclasses should override this method if they can batch more efficiently.\n",
      "     |  \n",
      "     |  dict(self, **kwargs: 'Any') -> 'Dict'\n",
      "     |      Return a dictionary of the LLM.\n",
      "     |  \n",
      "     |  generate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  generate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Pass a sequence of prompts to the model and return model generations.\n",
      "     |      \n",
      "     |      This method should make use of batched calls for models that expose a batched\n",
      "     |      API.\n",
      "     |      \n",
      "     |      Use this method when you want to:\n",
      "     |          1. take advantage of batched calls,\n",
      "     |          2. need more output from the model than just the top generated value,\n",
      "     |          3. are building chains that are agnostic to the underlying language model\n",
      "     |              type (e.g., pure text completion models vs chat models).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      "     |              converted to match the format of any language model (string for pure\n",
      "     |              text generation models and BaseMessages for chat models).\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          callbacks: Callbacks to pass through. Used for executing additional\n",
      "     |              functionality, such as logging or streaming, throughout generation.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          An LLMResult, which contains a list of candidate Generations for each input\n",
      "     |              prompt and additional model provider-specific output.\n",
      "     |  \n",
      "     |  invoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |  \n",
      "     |  predict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Pass a single string input to the model and return a string prediction.\n",
      "     |      \n",
      "     |       Use this method when passing in raw text. If you want to pass in specific\n",
      "     |          types of chat messages, use predict_messages.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: String input to pass to the model.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a string.\n",
      "     |  \n",
      "     |  predict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      "     |      Pass a message sequence to the model and return a message prediction.\n",
      "     |      \n",
      "     |      Use this method when passing in chat messages. If you want to pass in raw text,\n",
      "     |          use predict.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: A sequence of chat messages corresponding to a single model input.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a message.\n",
      "     |  \n",
      "     |  save(self, file_path: 'Union[Path, str]') -> 'None'\n",
      "     |      Save the LLM.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          file_path: Path to file to save the LLM to.\n",
      "     |      \n",
      "     |      Example:\n",
      "     |      .. code-block:: python\n",
      "     |      \n",
      "     |          llm.save(file_path=\"path/llm.yaml\")\n",
      "     |  \n",
      "     |  stream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'Iterator[str]'\n",
      "     |      Default implementation of stream, which calls invoke.\n",
      "     |      Subclasses should override this method if they support streaming output.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  raise_deprecation(values: 'Dict') -> 'Dict' from pydantic.main.ModelMetaclass\n",
      "     |      Raise deprecation warning if callback_manager is used.\n",
      "     |  \n",
      "     |  set_verbose(verbose: 'Optional[bool]') -> 'bool' from pydantic.main.ModelMetaclass\n",
      "     |      If verbose is None, set it.\n",
      "     |      \n",
      "     |      This allows users to pass in None as verbose to access the global setting.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  __orig_bases__ = (langchain.schema.language_model.BaseLanguageModel[st...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.schema.language_model.BaseLanguageModel:\n",
      "     |  \n",
      "     |  get_num_tokens(self, text: 'str') -> 'int'\n",
      "     |      Get the number of tokens present in the text.\n",
      "     |      \n",
      "     |      Useful for checking if an input will fit in a model's context window.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: The string input to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The integer number of tokens in the text.\n",
      "     |  \n",
      "     |  get_num_tokens_from_messages(self, messages: 'List[BaseMessage]') -> 'int'\n",
      "     |      Get the number of tokens in the messages.\n",
      "     |      \n",
      "     |      Useful for checking if an input will fit in a model's context window.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: The message inputs to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The sum of the number of tokens across the messages.\n",
      "     |  \n",
      "     |  get_token_ids(self, text: 'str') -> 'List[int]'\n",
      "     |      Return the ordered ids of the tokens in a text.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: The string input to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          A list of ids corresponding to the tokens in the text, in order they occur\n",
      "     |              in the text.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from langchain.schema.language_model.BaseLanguageModel:\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.load.serializable.Serializable:\n",
      "     |  \n",
      "     |  __init__(self, **kwargs: Any) -> None\n",
      "     |      Create a new model by parsing and validating input data from keyword arguments.\n",
      "     |      \n",
      "     |      Raises ValidationError if the input data cannot be parsed to form a valid model.\n",
      "     |  \n",
      "     |  to_json(self) -> Union[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\n",
      "     |  \n",
      "     |  to_json_not_implemented(self) -> langchain.load.serializable.SerializedNotImplemented\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from langchain.load.serializable.Serializable:\n",
      "     |  \n",
      "     |  lc_attributes\n",
      "     |      Return a list of attribute names that should be included in the\n",
      "     |      serialized kwargs. These attributes must be accepted by the\n",
      "     |      constructor.\n",
      "     |  \n",
      "     |  lc_namespace\n",
      "     |      Return the namespace of the langchain object.\n",
      "     |      eg. [\"langchain\", \"llms\", \"openai\"]\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __eq__(self, other: Any) -> bool\n",
      "     |  \n",
      "     |  __getstate__(self) -> 'DictAny'\n",
      "     |  \n",
      "     |  __iter__(self) -> 'TupleGenerator'\n",
      "     |      so `dict(model)` works\n",
      "     |  \n",
      "     |  __repr_args__(self) -> 'ReprArgs'\n",
      "     |  \n",
      "     |  __setattr__(self, name, value)\n",
      "     |  \n",
      "     |  __setstate__(self, state: 'DictAny') -> None\n",
      "     |  \n",
      "     |  copy(self: 'Model', *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, update: Optional[ForwardRef('DictStrAny')] = None, deep: bool = False) -> 'Model'\n",
      "     |      Duplicate a model, optionally choose which fields to include, exclude and change.\n",
      "     |      \n",
      "     |      :param include: fields to include in new model\n",
      "     |      :param exclude: fields to exclude from new model, as with values this takes precedence over include\n",
      "     |      :param update: values to change/add in the new model. Note: the data is not validated before creating\n",
      "     |          the new model: you should trust this data\n",
      "     |      :param deep: set to `True` to make a deep copy of the model\n",
      "     |      :return: new model instance\n",
      "     |  \n",
      "     |  json(self, *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) -> 'unicode'\n",
      "     |      Generate a JSON representation of the model, `include` and `exclude` arguments as per `dict()`.\n",
      "     |      \n",
      "     |      `encoder` is an optional function to supply as `default` to json.dumps(), other arguments as per `json.dumps()`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __get_validators__() -> 'CallableGenerator' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  __try_update_forward_refs__(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      "     |      Same as update_forward_refs but will not raise exception\n",
      "     |      when forward references are not defined.\n",
      "     |  \n",
      "     |  construct(_fields_set: Optional[ForwardRef('SetStr')] = None, **values: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |      Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data.\n",
      "     |      Default values are respected, but no other validation is performed.\n",
      "     |      Behaves as if `Config.extra = 'allow'` was set since it adds all passed values\n",
      "     |  \n",
      "     |  from_orm(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_file(path: Union[str, pathlib.Path], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_obj(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_raw(b: Union[str, bytes], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema(by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}') -> 'DictStrAny' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema_json(*, by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}', **dumps_kwargs: Any) -> 'unicode' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  update_forward_refs(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      "     |      Try to update ForwardRefs on fields based on this Model, globalns and localns.\n",
      "     |  \n",
      "     |  validate(value: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __fields_set__\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.utils.Representation:\n",
      "     |  \n",
      "     |  __pretty__(self, fmt: Callable[[Any], Any], **kwargs: Any) -> Generator[Any, NoneType, NoneType]\n",
      "     |      Used by devtools (https://python-devtools.helpmanual.io/) to provide a human readable representations of objects\n",
      "     |  \n",
      "     |  __repr__(self) -> 'unicode'\n",
      "     |  \n",
      "     |  __repr_name__(self) -> 'unicode'\n",
      "     |      Name of the instance's class, used in __repr__.\n",
      "     |  \n",
      "     |  __repr_str__(self, join_str: 'unicode') -> 'unicode'\n",
      "     |  \n",
      "     |  __rich_repr__(self) -> 'RichReprResult'\n",
      "     |      Get fields for Rich library\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.schema.runnable.base.Runnable:\n",
      "     |  \n",
      "     |  __or__(self, other: 'Union[Runnable[Any, Other], Callable[[Any], Other], Mapping[str, Union[Runnable[Any, Other], Callable[[Any], Other], Any]]]') -> 'RunnableSequence[Input, Other]'\n",
      "     |      Return self|value.\n",
      "     |  \n",
      "     |  __ror__(self, other: 'Union[Runnable[Other, Any], Callable[[Any], Other], Mapping[str, Union[Runnable[Other, Any], Callable[[Other], Any], Any]]]') -> 'RunnableSequence[Other, Output]'\n",
      "     |      Return value|self.\n",
      "     |  \n",
      "     |  async atransform(self, input: 'AsyncIterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'AsyncIterator[Output]'\n",
      "     |      Default implementation of atransform, which buffers input and calls astream.\n",
      "     |      Subclasses should override this method if they can start producing output while\n",
      "     |      input is still being generated.\n",
      "     |  \n",
      "     |  bind(self, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      "     |      Bind arguments to a Runnable, returning a new Runnable.\n",
      "     |  \n",
      "     |  map(self) -> 'Runnable[List[Input], List[Output]]'\n",
      "     |      Return a new Runnable that maps a list of inputs to a list of outputs,\n",
      "     |      by calling invoke() with each input.\n",
      "     |  \n",
      "     |  transform(self, input: 'Iterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'Iterator[Output]'\n",
      "     |      Default implementation of transform, which buffers input and then calls stream.\n",
      "     |      Subclasses should override this method if they can start producing output while\n",
      "     |      input is still being generated.\n",
      "     |  \n",
      "     |  with_config(self, config: 'Optional[RunnableConfig]' = None, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      "     |      Bind config to a Runnable, returning a new Runnable.\n",
      "     |  \n",
      "     |  with_fallbacks(self, fallbacks: 'Sequence[Runnable[Input, Output]]', *, exceptions_to_handle: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,)) -> 'RunnableWithFallbacks[Input, Output]'\n",
      "     |  \n",
      "     |  with_retry(self, *, retry_if_exception_type: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,), wait_exponential_jitter: 'bool' = True, stop_after_attempt: 'int' = 3) -> 'Runnable[Input, Output]'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __class_getitem__(params) from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  __init_subclass__(*args, **kwargs) from pydantic.main.ModelMetaclass\n",
      "     |      This method is called when a class is subclassed.\n",
      "     |      \n",
      "     |      The default implementation does nothing. It may be\n",
      "     |      overridden to extend subclasses.\n",
      "    \n",
      "    class FireworksChat(langchain.llms.base.BaseLLM)\n",
      "     |  FireworksChat(*, cache: Optional[bool] = None, verbose: bool = None, callbacks: Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager, NoneType] = None, callback_manager: Optional[langchain.callbacks.base.BaseCallbackManager] = None, tags: Optional[List[str]] = None, metadata: Optional[Dict[str, Any]] = None, model_id: str = 'accounts/fireworks/models/llama-v2-7b-chat', temperature: float = 0.7, max_tokens: int = 512, top_p: float = 1, fireworks_api_key: Optional[str] = None, max_retries: int = 6, request_timeout: Union[float, Tuple[float, float], NoneType] = None, prefix_messages: List = None) -> None\n",
      "     |  \n",
      "     |  Wrapper around Fireworks Chat large language models.\n",
      "     |  To use, you should have the ``fireworksai`` python package installed, and the\n",
      "     |  environment variable ``FIREWORKS_API_KEY`` set with your API key.\n",
      "     |  Any parameters that are valid to be passed to the fireworks.create\n",
      "     |  call can be passed in, even if not explicitly saved on this class.\n",
      "     |  Example:\n",
      "     |      .. code-block:: python\n",
      "     |          from langchain.llms import FireworksChat\n",
      "     |          fireworkschat = FireworksChat(model_id=\"\"llama-v2-13b-chat\"\")\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      FireworksChat\n",
      "     |      langchain.llms.base.BaseLLM\n",
      "     |      langchain.schema.language_model.BaseLanguageModel\n",
      "     |      langchain.load.serializable.Serializable\n",
      "     |      pydantic.main.BaseModel\n",
      "     |      pydantic.utils.Representation\n",
      "     |      langchain.schema.runnable.base.Runnable\n",
      "     |      typing.Generic\n",
      "     |      abc.ABC\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  validate_environment(values: Dict) -> Dict from pydantic.main.ModelMetaclass\n",
      "     |      Validate that api key and python package exists in environment\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __json_encoder__ = pydantic_encoder(obj: Any) -> Any\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {'fireworks_api_key': typing.Optional[str], 'max_ret...\n",
      "     |  \n",
      "     |  __class_vars__ = set()\n",
      "     |  \n",
      "     |  __config__ = <class 'langchain.llms.fireworks.Config'>\n",
      "     |  \n",
      "     |  __custom_root_type__ = False\n",
      "     |  \n",
      "     |  __exclude_fields__ = {'callback_manager': True, 'callbacks': True, 'me...\n",
      "     |  \n",
      "     |  __fields__ = {'cache': ModelField(name='cache', type=Optional[bool], r...\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "     |  \n",
      "     |  __include_fields__ = None\n",
      "     |  \n",
      "     |  __parameters__ = ()\n",
      "     |  \n",
      "     |  __post_root_validators__ = [(False, <function BaseLLM.raise_deprecatio...\n",
      "     |  \n",
      "     |  __pre_root_validators__ = []\n",
      "     |  \n",
      "     |  __private_attributes__ = {'_lc_kwargs': ModelPrivateAttr(default=Pydan...\n",
      "     |  \n",
      "     |  __schema_cache__ = {}\n",
      "     |  \n",
      "     |  __signature__ = <Signature (*, cache: Optional[bool] = None, ver...e] ...\n",
      "     |  \n",
      "     |  __validators__ = {'verbose': [<pydantic.class_validators.Validator obj...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  __call__(self, prompt: 'str', stop: 'Optional[List[str]]' = None, callbacks: 'Callbacks' = None, *, tags: 'Optional[List[str]]' = None, metadata: 'Optional[Dict[str, Any]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Check Cache and run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  __str__(self) -> 'str'\n",
      "     |      Get a string representation of the object for printing.\n",
      "     |  \n",
      "     |  async abatch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      "     |      Default implementation of abatch, which calls ainvoke N times.\n",
      "     |      Subclasses should override this method if they can batch more efficiently.\n",
      "     |  \n",
      "     |  async agenerate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  async agenerate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Asynchronously pass a sequence of prompts and return model generations.\n",
      "     |      \n",
      "     |      This method should make use of batched calls for models that expose a batched\n",
      "     |      API.\n",
      "     |      \n",
      "     |      Use this method when you want to:\n",
      "     |          1. take advantage of batched calls,\n",
      "     |          2. need more output from the model than just the top generated value,\n",
      "     |          3. are building chains that are agnostic to the underlying language model\n",
      "     |              type (e.g., pure text completion models vs chat models).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      "     |              converted to match the format of any language model (string for pure\n",
      "     |              text generation models and BaseMessages for chat models).\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          callbacks: Callbacks to pass through. Used for executing additional\n",
      "     |              functionality, such as logging or streaming, throughout generation.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          An LLMResult, which contains a list of candidate Generations for each input\n",
      "     |              prompt and additional model provider-specific output.\n",
      "     |  \n",
      "     |  async ainvoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Default implementation of ainvoke, which calls invoke in a thread pool.\n",
      "     |      Subclasses should override this method if they can run asynchronously.\n",
      "     |  \n",
      "     |  async apredict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Asynchronously pass a string to the model and return a string prediction.\n",
      "     |      \n",
      "     |      Use this method when calling pure text generation models and only the top\n",
      "     |          candidate generation is needed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: String input to pass to the model.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a string.\n",
      "     |  \n",
      "     |  async apredict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      "     |      Asynchronously pass messages to the model and return a message prediction.\n",
      "     |      \n",
      "     |      Use this method when calling chat models and only the top\n",
      "     |          candidate generation is needed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: A sequence of chat messages corresponding to a single model input.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a message.\n",
      "     |  \n",
      "     |  async astream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'AsyncIterator[str]'\n",
      "     |      Default implementation of astream, which calls ainvoke.\n",
      "     |      Subclasses should override this method if they support streaming output.\n",
      "     |  \n",
      "     |  batch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      "     |      Default implementation of batch, which calls invoke N times.\n",
      "     |      Subclasses should override this method if they can batch more efficiently.\n",
      "     |  \n",
      "     |  dict(self, **kwargs: 'Any') -> 'Dict'\n",
      "     |      Return a dictionary of the LLM.\n",
      "     |  \n",
      "     |  generate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  generate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Pass a sequence of prompts to the model and return model generations.\n",
      "     |      \n",
      "     |      This method should make use of batched calls for models that expose a batched\n",
      "     |      API.\n",
      "     |      \n",
      "     |      Use this method when you want to:\n",
      "     |          1. take advantage of batched calls,\n",
      "     |          2. need more output from the model than just the top generated value,\n",
      "     |          3. are building chains that are agnostic to the underlying language model\n",
      "     |              type (e.g., pure text completion models vs chat models).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      "     |              converted to match the format of any language model (string for pure\n",
      "     |              text generation models and BaseMessages for chat models).\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          callbacks: Callbacks to pass through. Used for executing additional\n",
      "     |              functionality, such as logging or streaming, throughout generation.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          An LLMResult, which contains a list of candidate Generations for each input\n",
      "     |              prompt and additional model provider-specific output.\n",
      "     |  \n",
      "     |  invoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |  \n",
      "     |  predict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Pass a single string input to the model and return a string prediction.\n",
      "     |      \n",
      "     |       Use this method when passing in raw text. If you want to pass in specific\n",
      "     |          types of chat messages, use predict_messages.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: String input to pass to the model.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a string.\n",
      "     |  \n",
      "     |  predict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      "     |      Pass a message sequence to the model and return a message prediction.\n",
      "     |      \n",
      "     |      Use this method when passing in chat messages. If you want to pass in raw text,\n",
      "     |          use predict.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: A sequence of chat messages corresponding to a single model input.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a message.\n",
      "     |  \n",
      "     |  save(self, file_path: 'Union[Path, str]') -> 'None'\n",
      "     |      Save the LLM.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          file_path: Path to file to save the LLM to.\n",
      "     |      \n",
      "     |      Example:\n",
      "     |      .. code-block:: python\n",
      "     |      \n",
      "     |          llm.save(file_path=\"path/llm.yaml\")\n",
      "     |  \n",
      "     |  stream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'Iterator[str]'\n",
      "     |      Default implementation of stream, which calls invoke.\n",
      "     |      Subclasses should override this method if they support streaming output.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  raise_deprecation(values: 'Dict') -> 'Dict' from pydantic.main.ModelMetaclass\n",
      "     |      Raise deprecation warning if callback_manager is used.\n",
      "     |  \n",
      "     |  set_verbose(verbose: 'Optional[bool]') -> 'bool' from pydantic.main.ModelMetaclass\n",
      "     |      If verbose is None, set it.\n",
      "     |      \n",
      "     |      This allows users to pass in None as verbose to access the global setting.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  Config = <class 'langchain.llms.base.BaseLLM.Config'>\n",
      "     |      Configuration for this pydantic object.\n",
      "     |  \n",
      "     |  \n",
      "     |  __orig_bases__ = (langchain.schema.language_model.BaseLanguageModel[st...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.schema.language_model.BaseLanguageModel:\n",
      "     |  \n",
      "     |  get_num_tokens(self, text: 'str') -> 'int'\n",
      "     |      Get the number of tokens present in the text.\n",
      "     |      \n",
      "     |      Useful for checking if an input will fit in a model's context window.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: The string input to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The integer number of tokens in the text.\n",
      "     |  \n",
      "     |  get_num_tokens_from_messages(self, messages: 'List[BaseMessage]') -> 'int'\n",
      "     |      Get the number of tokens in the messages.\n",
      "     |      \n",
      "     |      Useful for checking if an input will fit in a model's context window.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: The message inputs to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The sum of the number of tokens across the messages.\n",
      "     |  \n",
      "     |  get_token_ids(self, text: 'str') -> 'List[int]'\n",
      "     |      Return the ordered ids of the tokens in a text.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: The string input to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          A list of ids corresponding to the tokens in the text, in order they occur\n",
      "     |              in the text.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from langchain.schema.language_model.BaseLanguageModel:\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.load.serializable.Serializable:\n",
      "     |  \n",
      "     |  __init__(self, **kwargs: Any) -> None\n",
      "     |      Create a new model by parsing and validating input data from keyword arguments.\n",
      "     |      \n",
      "     |      Raises ValidationError if the input data cannot be parsed to form a valid model.\n",
      "     |  \n",
      "     |  to_json(self) -> Union[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\n",
      "     |  \n",
      "     |  to_json_not_implemented(self) -> langchain.load.serializable.SerializedNotImplemented\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from langchain.load.serializable.Serializable:\n",
      "     |  \n",
      "     |  lc_attributes\n",
      "     |      Return a list of attribute names that should be included in the\n",
      "     |      serialized kwargs. These attributes must be accepted by the\n",
      "     |      constructor.\n",
      "     |  \n",
      "     |  lc_namespace\n",
      "     |      Return the namespace of the langchain object.\n",
      "     |      eg. [\"langchain\", \"llms\", \"openai\"]\n",
      "     |  \n",
      "     |  lc_secrets\n",
      "     |      Return a map of constructor argument names to secret ids.\n",
      "     |      eg. {\"openai_api_key\": \"OPENAI_API_KEY\"}\n",
      "     |  \n",
      "     |  lc_serializable\n",
      "     |      Return whether or not the class is serializable.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __eq__(self, other: Any) -> bool\n",
      "     |  \n",
      "     |  __getstate__(self) -> 'DictAny'\n",
      "     |  \n",
      "     |  __iter__(self) -> 'TupleGenerator'\n",
      "     |      so `dict(model)` works\n",
      "     |  \n",
      "     |  __repr_args__(self) -> 'ReprArgs'\n",
      "     |  \n",
      "     |  __setattr__(self, name, value)\n",
      "     |  \n",
      "     |  __setstate__(self, state: 'DictAny') -> None\n",
      "     |  \n",
      "     |  copy(self: 'Model', *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, update: Optional[ForwardRef('DictStrAny')] = None, deep: bool = False) -> 'Model'\n",
      "     |      Duplicate a model, optionally choose which fields to include, exclude and change.\n",
      "     |      \n",
      "     |      :param include: fields to include in new model\n",
      "     |      :param exclude: fields to exclude from new model, as with values this takes precedence over include\n",
      "     |      :param update: values to change/add in the new model. Note: the data is not validated before creating\n",
      "     |          the new model: you should trust this data\n",
      "     |      :param deep: set to `True` to make a deep copy of the model\n",
      "     |      :return: new model instance\n",
      "     |  \n",
      "     |  json(self, *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) -> 'unicode'\n",
      "     |      Generate a JSON representation of the model, `include` and `exclude` arguments as per `dict()`.\n",
      "     |      \n",
      "     |      `encoder` is an optional function to supply as `default` to json.dumps(), other arguments as per `json.dumps()`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __get_validators__() -> 'CallableGenerator' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  __try_update_forward_refs__(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      "     |      Same as update_forward_refs but will not raise exception\n",
      "     |      when forward references are not defined.\n",
      "     |  \n",
      "     |  construct(_fields_set: Optional[ForwardRef('SetStr')] = None, **values: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |      Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data.\n",
      "     |      Default values are respected, but no other validation is performed.\n",
      "     |      Behaves as if `Config.extra = 'allow'` was set since it adds all passed values\n",
      "     |  \n",
      "     |  from_orm(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_file(path: Union[str, pathlib.Path], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_obj(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_raw(b: Union[str, bytes], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema(by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}') -> 'DictStrAny' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema_json(*, by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}', **dumps_kwargs: Any) -> 'unicode' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  update_forward_refs(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      "     |      Try to update ForwardRefs on fields based on this Model, globalns and localns.\n",
      "     |  \n",
      "     |  validate(value: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __fields_set__\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.utils.Representation:\n",
      "     |  \n",
      "     |  __pretty__(self, fmt: Callable[[Any], Any], **kwargs: Any) -> Generator[Any, NoneType, NoneType]\n",
      "     |      Used by devtools (https://python-devtools.helpmanual.io/) to provide a human readable representations of objects\n",
      "     |  \n",
      "     |  __repr__(self) -> 'unicode'\n",
      "     |  \n",
      "     |  __repr_name__(self) -> 'unicode'\n",
      "     |      Name of the instance's class, used in __repr__.\n",
      "     |  \n",
      "     |  __repr_str__(self, join_str: 'unicode') -> 'unicode'\n",
      "     |  \n",
      "     |  __rich_repr__(self) -> 'RichReprResult'\n",
      "     |      Get fields for Rich library\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.schema.runnable.base.Runnable:\n",
      "     |  \n",
      "     |  __or__(self, other: 'Union[Runnable[Any, Other], Callable[[Any], Other], Mapping[str, Union[Runnable[Any, Other], Callable[[Any], Other], Any]]]') -> 'RunnableSequence[Input, Other]'\n",
      "     |      Return self|value.\n",
      "     |  \n",
      "     |  __ror__(self, other: 'Union[Runnable[Other, Any], Callable[[Any], Other], Mapping[str, Union[Runnable[Other, Any], Callable[[Other], Any], Any]]]') -> 'RunnableSequence[Other, Output]'\n",
      "     |      Return value|self.\n",
      "     |  \n",
      "     |  async atransform(self, input: 'AsyncIterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'AsyncIterator[Output]'\n",
      "     |      Default implementation of atransform, which buffers input and calls astream.\n",
      "     |      Subclasses should override this method if they can start producing output while\n",
      "     |      input is still being generated.\n",
      "     |  \n",
      "     |  bind(self, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      "     |      Bind arguments to a Runnable, returning a new Runnable.\n",
      "     |  \n",
      "     |  map(self) -> 'Runnable[List[Input], List[Output]]'\n",
      "     |      Return a new Runnable that maps a list of inputs to a list of outputs,\n",
      "     |      by calling invoke() with each input.\n",
      "     |  \n",
      "     |  transform(self, input: 'Iterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'Iterator[Output]'\n",
      "     |      Default implementation of transform, which buffers input and then calls stream.\n",
      "     |      Subclasses should override this method if they can start producing output while\n",
      "     |      input is still being generated.\n",
      "     |  \n",
      "     |  with_config(self, config: 'Optional[RunnableConfig]' = None, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      "     |      Bind config to a Runnable, returning a new Runnable.\n",
      "     |  \n",
      "     |  with_fallbacks(self, fallbacks: 'Sequence[Runnable[Input, Output]]', *, exceptions_to_handle: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,)) -> 'RunnableWithFallbacks[Input, Output]'\n",
      "     |  \n",
      "     |  with_retry(self, *, retry_if_exception_type: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,), wait_exponential_jitter: 'bool' = True, stop_after_attempt: 'int' = 3) -> 'Runnable[Input, Output]'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __class_getitem__(params) from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  __init_subclass__(*args, **kwargs) from pydantic.main.ModelMetaclass\n",
      "     |      This method is called when a class is subclassed.\n",
      "     |      \n",
      "     |      The default implementation does nothing. It may be\n",
      "     |      overridden to extend subclasses.\n",
      "    \n",
      "    class ForefrontAI(langchain.llms.base.LLM)\n",
      "     |  ForefrontAI(*, cache: Optional[bool] = None, verbose: bool = None, callbacks: Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager, NoneType] = None, callback_manager: Optional[langchain.callbacks.base.BaseCallbackManager] = None, tags: Optional[List[str]] = None, metadata: Optional[Dict[str, Any]] = None, endpoint_url: str = '', temperature: float = 0.7, length: int = 256, top_p: float = 1.0, top_k: int = 40, repetition_penalty: int = 1, forefrontai_api_key: Optional[str] = None, base_url: Optional[str] = None) -> None\n",
      "     |  \n",
      "     |  ForefrontAI large language models.\n",
      "     |  \n",
      "     |  To use, you should have the environment variable ``FOREFRONTAI_API_KEY``\n",
      "     |  set with your API key.\n",
      "     |  \n",
      "     |  Example:\n",
      "     |      .. code-block:: python\n",
      "     |  \n",
      "     |          from langchain.llms import ForefrontAI\n",
      "     |          forefrontai = ForefrontAI(endpoint_url=\"\")\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      ForefrontAI\n",
      "     |      langchain.llms.base.LLM\n",
      "     |      langchain.llms.base.BaseLLM\n",
      "     |      langchain.schema.language_model.BaseLanguageModel\n",
      "     |      langchain.load.serializable.Serializable\n",
      "     |      pydantic.main.BaseModel\n",
      "     |      pydantic.utils.Representation\n",
      "     |      langchain.schema.runnable.base.Runnable\n",
      "     |      typing.Generic\n",
      "     |      abc.ABC\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  validate_environment(values: Dict) -> Dict from pydantic.main.ModelMetaclass\n",
      "     |      Validate that api key exists in environment.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __json_encoder__ = pydantic_encoder(obj: Any) -> Any\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  Config = <class 'langchain.llms.forefrontai.ForefrontAI.Config'>\n",
      "     |      Configuration for this pydantic object.\n",
      "     |  \n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {'base_url': typing.Optional[str], 'endpoint_url': <...\n",
      "     |  \n",
      "     |  __class_vars__ = set()\n",
      "     |  \n",
      "     |  __config__ = <class 'langchain.llms.forefrontai.Config'>\n",
      "     |  \n",
      "     |  __custom_root_type__ = False\n",
      "     |  \n",
      "     |  __exclude_fields__ = {'callback_manager': True, 'callbacks': True, 'me...\n",
      "     |  \n",
      "     |  __fields__ = {'base_url': ModelField(name='base_url', type=Optional[st...\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "     |  \n",
      "     |  __include_fields__ = None\n",
      "     |  \n",
      "     |  __parameters__ = ()\n",
      "     |  \n",
      "     |  __post_root_validators__ = [(False, <function BaseLLM.raise_deprecatio...\n",
      "     |  \n",
      "     |  __pre_root_validators__ = []\n",
      "     |  \n",
      "     |  __private_attributes__ = {'_lc_kwargs': ModelPrivateAttr(default=Pydan...\n",
      "     |  \n",
      "     |  __schema_cache__ = {}\n",
      "     |  \n",
      "     |  __signature__ = <Signature (*, cache: Optional[bool] = None, ver... = ...\n",
      "     |  \n",
      "     |  __validators__ = {'verbose': [<pydantic.class_validators.Validator obj...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  __call__(self, prompt: 'str', stop: 'Optional[List[str]]' = None, callbacks: 'Callbacks' = None, *, tags: 'Optional[List[str]]' = None, metadata: 'Optional[Dict[str, Any]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Check Cache and run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  __str__(self) -> 'str'\n",
      "     |      Get a string representation of the object for printing.\n",
      "     |  \n",
      "     |  async abatch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      "     |      Default implementation of abatch, which calls ainvoke N times.\n",
      "     |      Subclasses should override this method if they can batch more efficiently.\n",
      "     |  \n",
      "     |  async agenerate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  async agenerate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Asynchronously pass a sequence of prompts and return model generations.\n",
      "     |      \n",
      "     |      This method should make use of batched calls for models that expose a batched\n",
      "     |      API.\n",
      "     |      \n",
      "     |      Use this method when you want to:\n",
      "     |          1. take advantage of batched calls,\n",
      "     |          2. need more output from the model than just the top generated value,\n",
      "     |          3. are building chains that are agnostic to the underlying language model\n",
      "     |              type (e.g., pure text completion models vs chat models).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      "     |              converted to match the format of any language model (string for pure\n",
      "     |              text generation models and BaseMessages for chat models).\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          callbacks: Callbacks to pass through. Used for executing additional\n",
      "     |              functionality, such as logging or streaming, throughout generation.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          An LLMResult, which contains a list of candidate Generations for each input\n",
      "     |              prompt and additional model provider-specific output.\n",
      "     |  \n",
      "     |  async ainvoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Default implementation of ainvoke, which calls invoke in a thread pool.\n",
      "     |      Subclasses should override this method if they can run asynchronously.\n",
      "     |  \n",
      "     |  async apredict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Asynchronously pass a string to the model and return a string prediction.\n",
      "     |      \n",
      "     |      Use this method when calling pure text generation models and only the top\n",
      "     |          candidate generation is needed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: String input to pass to the model.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a string.\n",
      "     |  \n",
      "     |  async apredict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      "     |      Asynchronously pass messages to the model and return a message prediction.\n",
      "     |      \n",
      "     |      Use this method when calling chat models and only the top\n",
      "     |          candidate generation is needed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: A sequence of chat messages corresponding to a single model input.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a message.\n",
      "     |  \n",
      "     |  async astream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'AsyncIterator[str]'\n",
      "     |      Default implementation of astream, which calls ainvoke.\n",
      "     |      Subclasses should override this method if they support streaming output.\n",
      "     |  \n",
      "     |  batch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      "     |      Default implementation of batch, which calls invoke N times.\n",
      "     |      Subclasses should override this method if they can batch more efficiently.\n",
      "     |  \n",
      "     |  dict(self, **kwargs: 'Any') -> 'Dict'\n",
      "     |      Return a dictionary of the LLM.\n",
      "     |  \n",
      "     |  generate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  generate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Pass a sequence of prompts to the model and return model generations.\n",
      "     |      \n",
      "     |      This method should make use of batched calls for models that expose a batched\n",
      "     |      API.\n",
      "     |      \n",
      "     |      Use this method when you want to:\n",
      "     |          1. take advantage of batched calls,\n",
      "     |          2. need more output from the model than just the top generated value,\n",
      "     |          3. are building chains that are agnostic to the underlying language model\n",
      "     |              type (e.g., pure text completion models vs chat models).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      "     |              converted to match the format of any language model (string for pure\n",
      "     |              text generation models and BaseMessages for chat models).\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          callbacks: Callbacks to pass through. Used for executing additional\n",
      "     |              functionality, such as logging or streaming, throughout generation.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          An LLMResult, which contains a list of candidate Generations for each input\n",
      "     |              prompt and additional model provider-specific output.\n",
      "     |  \n",
      "     |  invoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |  \n",
      "     |  predict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Pass a single string input to the model and return a string prediction.\n",
      "     |      \n",
      "     |       Use this method when passing in raw text. If you want to pass in specific\n",
      "     |          types of chat messages, use predict_messages.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: String input to pass to the model.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a string.\n",
      "     |  \n",
      "     |  predict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      "     |      Pass a message sequence to the model and return a message prediction.\n",
      "     |      \n",
      "     |      Use this method when passing in chat messages. If you want to pass in raw text,\n",
      "     |          use predict.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: A sequence of chat messages corresponding to a single model input.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a message.\n",
      "     |  \n",
      "     |  save(self, file_path: 'Union[Path, str]') -> 'None'\n",
      "     |      Save the LLM.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          file_path: Path to file to save the LLM to.\n",
      "     |      \n",
      "     |      Example:\n",
      "     |      .. code-block:: python\n",
      "     |      \n",
      "     |          llm.save(file_path=\"path/llm.yaml\")\n",
      "     |  \n",
      "     |  stream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'Iterator[str]'\n",
      "     |      Default implementation of stream, which calls invoke.\n",
      "     |      Subclasses should override this method if they support streaming output.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  raise_deprecation(values: 'Dict') -> 'Dict' from pydantic.main.ModelMetaclass\n",
      "     |      Raise deprecation warning if callback_manager is used.\n",
      "     |  \n",
      "     |  set_verbose(verbose: 'Optional[bool]') -> 'bool' from pydantic.main.ModelMetaclass\n",
      "     |      If verbose is None, set it.\n",
      "     |      \n",
      "     |      This allows users to pass in None as verbose to access the global setting.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  __orig_bases__ = (langchain.schema.language_model.BaseLanguageModel[st...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.schema.language_model.BaseLanguageModel:\n",
      "     |  \n",
      "     |  get_num_tokens(self, text: 'str') -> 'int'\n",
      "     |      Get the number of tokens present in the text.\n",
      "     |      \n",
      "     |      Useful for checking if an input will fit in a model's context window.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: The string input to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The integer number of tokens in the text.\n",
      "     |  \n",
      "     |  get_num_tokens_from_messages(self, messages: 'List[BaseMessage]') -> 'int'\n",
      "     |      Get the number of tokens in the messages.\n",
      "     |      \n",
      "     |      Useful for checking if an input will fit in a model's context window.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: The message inputs to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The sum of the number of tokens across the messages.\n",
      "     |  \n",
      "     |  get_token_ids(self, text: 'str') -> 'List[int]'\n",
      "     |      Return the ordered ids of the tokens in a text.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: The string input to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          A list of ids corresponding to the tokens in the text, in order they occur\n",
      "     |              in the text.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from langchain.schema.language_model.BaseLanguageModel:\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.load.serializable.Serializable:\n",
      "     |  \n",
      "     |  __init__(self, **kwargs: Any) -> None\n",
      "     |      Create a new model by parsing and validating input data from keyword arguments.\n",
      "     |      \n",
      "     |      Raises ValidationError if the input data cannot be parsed to form a valid model.\n",
      "     |  \n",
      "     |  to_json(self) -> Union[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\n",
      "     |  \n",
      "     |  to_json_not_implemented(self) -> langchain.load.serializable.SerializedNotImplemented\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from langchain.load.serializable.Serializable:\n",
      "     |  \n",
      "     |  lc_attributes\n",
      "     |      Return a list of attribute names that should be included in the\n",
      "     |      serialized kwargs. These attributes must be accepted by the\n",
      "     |      constructor.\n",
      "     |  \n",
      "     |  lc_namespace\n",
      "     |      Return the namespace of the langchain object.\n",
      "     |      eg. [\"langchain\", \"llms\", \"openai\"]\n",
      "     |  \n",
      "     |  lc_secrets\n",
      "     |      Return a map of constructor argument names to secret ids.\n",
      "     |      eg. {\"openai_api_key\": \"OPENAI_API_KEY\"}\n",
      "     |  \n",
      "     |  lc_serializable\n",
      "     |      Return whether or not the class is serializable.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __eq__(self, other: Any) -> bool\n",
      "     |  \n",
      "     |  __getstate__(self) -> 'DictAny'\n",
      "     |  \n",
      "     |  __iter__(self) -> 'TupleGenerator'\n",
      "     |      so `dict(model)` works\n",
      "     |  \n",
      "     |  __repr_args__(self) -> 'ReprArgs'\n",
      "     |  \n",
      "     |  __setattr__(self, name, value)\n",
      "     |  \n",
      "     |  __setstate__(self, state: 'DictAny') -> None\n",
      "     |  \n",
      "     |  copy(self: 'Model', *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, update: Optional[ForwardRef('DictStrAny')] = None, deep: bool = False) -> 'Model'\n",
      "     |      Duplicate a model, optionally choose which fields to include, exclude and change.\n",
      "     |      \n",
      "     |      :param include: fields to include in new model\n",
      "     |      :param exclude: fields to exclude from new model, as with values this takes precedence over include\n",
      "     |      :param update: values to change/add in the new model. Note: the data is not validated before creating\n",
      "     |          the new model: you should trust this data\n",
      "     |      :param deep: set to `True` to make a deep copy of the model\n",
      "     |      :return: new model instance\n",
      "     |  \n",
      "     |  json(self, *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) -> 'unicode'\n",
      "     |      Generate a JSON representation of the model, `include` and `exclude` arguments as per `dict()`.\n",
      "     |      \n",
      "     |      `encoder` is an optional function to supply as `default` to json.dumps(), other arguments as per `json.dumps()`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __get_validators__() -> 'CallableGenerator' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  __try_update_forward_refs__(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      "     |      Same as update_forward_refs but will not raise exception\n",
      "     |      when forward references are not defined.\n",
      "     |  \n",
      "     |  construct(_fields_set: Optional[ForwardRef('SetStr')] = None, **values: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |      Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data.\n",
      "     |      Default values are respected, but no other validation is performed.\n",
      "     |      Behaves as if `Config.extra = 'allow'` was set since it adds all passed values\n",
      "     |  \n",
      "     |  from_orm(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_file(path: Union[str, pathlib.Path], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_obj(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_raw(b: Union[str, bytes], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema(by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}') -> 'DictStrAny' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema_json(*, by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}', **dumps_kwargs: Any) -> 'unicode' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  update_forward_refs(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      "     |      Try to update ForwardRefs on fields based on this Model, globalns and localns.\n",
      "     |  \n",
      "     |  validate(value: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __fields_set__\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.utils.Representation:\n",
      "     |  \n",
      "     |  __pretty__(self, fmt: Callable[[Any], Any], **kwargs: Any) -> Generator[Any, NoneType, NoneType]\n",
      "     |      Used by devtools (https://python-devtools.helpmanual.io/) to provide a human readable representations of objects\n",
      "     |  \n",
      "     |  __repr__(self) -> 'unicode'\n",
      "     |  \n",
      "     |  __repr_name__(self) -> 'unicode'\n",
      "     |      Name of the instance's class, used in __repr__.\n",
      "     |  \n",
      "     |  __repr_str__(self, join_str: 'unicode') -> 'unicode'\n",
      "     |  \n",
      "     |  __rich_repr__(self) -> 'RichReprResult'\n",
      "     |      Get fields for Rich library\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.schema.runnable.base.Runnable:\n",
      "     |  \n",
      "     |  __or__(self, other: 'Union[Runnable[Any, Other], Callable[[Any], Other], Mapping[str, Union[Runnable[Any, Other], Callable[[Any], Other], Any]]]') -> 'RunnableSequence[Input, Other]'\n",
      "     |      Return self|value.\n",
      "     |  \n",
      "     |  __ror__(self, other: 'Union[Runnable[Other, Any], Callable[[Any], Other], Mapping[str, Union[Runnable[Other, Any], Callable[[Other], Any], Any]]]') -> 'RunnableSequence[Other, Output]'\n",
      "     |      Return value|self.\n",
      "     |  \n",
      "     |  async atransform(self, input: 'AsyncIterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'AsyncIterator[Output]'\n",
      "     |      Default implementation of atransform, which buffers input and calls astream.\n",
      "     |      Subclasses should override this method if they can start producing output while\n",
      "     |      input is still being generated.\n",
      "     |  \n",
      "     |  bind(self, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      "     |      Bind arguments to a Runnable, returning a new Runnable.\n",
      "     |  \n",
      "     |  map(self) -> 'Runnable[List[Input], List[Output]]'\n",
      "     |      Return a new Runnable that maps a list of inputs to a list of outputs,\n",
      "     |      by calling invoke() with each input.\n",
      "     |  \n",
      "     |  transform(self, input: 'Iterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'Iterator[Output]'\n",
      "     |      Default implementation of transform, which buffers input and then calls stream.\n",
      "     |      Subclasses should override this method if they can start producing output while\n",
      "     |      input is still being generated.\n",
      "     |  \n",
      "     |  with_config(self, config: 'Optional[RunnableConfig]' = None, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      "     |      Bind config to a Runnable, returning a new Runnable.\n",
      "     |  \n",
      "     |  with_fallbacks(self, fallbacks: 'Sequence[Runnable[Input, Output]]', *, exceptions_to_handle: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,)) -> 'RunnableWithFallbacks[Input, Output]'\n",
      "     |  \n",
      "     |  with_retry(self, *, retry_if_exception_type: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,), wait_exponential_jitter: 'bool' = True, stop_after_attempt: 'int' = 3) -> 'Runnable[Input, Output]'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __class_getitem__(params) from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  __init_subclass__(*args, **kwargs) from pydantic.main.ModelMetaclass\n",
      "     |      This method is called when a class is subclassed.\n",
      "     |      \n",
      "     |      The default implementation does nothing. It may be\n",
      "     |      overridden to extend subclasses.\n",
      "    \n",
      "    class GPT4All(langchain.llms.base.LLM)\n",
      "     |  GPT4All(*, cache: Optional[bool] = None, verbose: bool = None, callbacks: Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager, NoneType] = None, callback_manager: Optional[langchain.callbacks.base.BaseCallbackManager] = None, tags: Optional[List[str]] = None, metadata: Optional[Dict[str, Any]] = None, model: str, backend: Optional[str] = None, max_tokens: int = 200, n_parts: int = -1, seed: int = 0, f16_kv: bool = False, logits_all: bool = False, vocab_only: bool = False, use_mlock: bool = False, embedding: bool = False, n_threads: Optional[int] = 4, n_predict: Optional[int] = 256, temp: Optional[float] = 0.7, top_p: Optional[float] = 0.1, top_k: Optional[int] = 40, echo: Optional[bool] = False, stop: Optional[List[str]] = [], repeat_last_n: Optional[int] = 64, repeat_penalty: Optional[float] = 1.18, n_batch: int = 8, streaming: bool = False, allow_download: bool = False, client: Any = None) -> None\n",
      "     |  \n",
      "     |  GPT4All language models.\n",
      "     |  \n",
      "     |  To use, you should have the ``gpt4all`` python package installed, the\n",
      "     |  pre-trained model file, and the model's config information.\n",
      "     |  \n",
      "     |  Example:\n",
      "     |      .. code-block:: python\n",
      "     |  \n",
      "     |          from langchain.llms import GPT4All\n",
      "     |          model = GPT4All(model=\"./models/gpt4all-model.bin\", n_threads=8)\n",
      "     |  \n",
      "     |          # Simplest invocation\n",
      "     |          response = model(\"Once upon a time, \")\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      GPT4All\n",
      "     |      langchain.llms.base.LLM\n",
      "     |      langchain.llms.base.BaseLLM\n",
      "     |      langchain.schema.language_model.BaseLanguageModel\n",
      "     |      langchain.load.serializable.Serializable\n",
      "     |      pydantic.main.BaseModel\n",
      "     |      pydantic.utils.Representation\n",
      "     |      langchain.schema.runnable.base.Runnable\n",
      "     |      typing.Generic\n",
      "     |      abc.ABC\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  validate_environment(values: Dict) -> Dict from pydantic.main.ModelMetaclass\n",
      "     |      Validate that the python package exists in the environment.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __json_encoder__ = pydantic_encoder(obj: Any) -> Any\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  Config = <class 'langchain.llms.gpt4all.GPT4All.Config'>\n",
      "     |      Configuration for this pydantic object.\n",
      "     |  \n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {'allow_download': <class 'bool'>, 'backend': typing...\n",
      "     |  \n",
      "     |  __class_vars__ = set()\n",
      "     |  \n",
      "     |  __config__ = <class 'langchain.llms.gpt4all.Config'>\n",
      "     |  \n",
      "     |  __custom_root_type__ = False\n",
      "     |  \n",
      "     |  __exclude_fields__ = {'callback_manager': True, 'callbacks': True, 'me...\n",
      "     |  \n",
      "     |  __fields__ = {'allow_download': ModelField(name='allow_download', type...\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "     |  \n",
      "     |  __include_fields__ = None\n",
      "     |  \n",
      "     |  __parameters__ = ()\n",
      "     |  \n",
      "     |  __post_root_validators__ = [(False, <function BaseLLM.raise_deprecatio...\n",
      "     |  \n",
      "     |  __pre_root_validators__ = []\n",
      "     |  \n",
      "     |  __private_attributes__ = {'_lc_kwargs': ModelPrivateAttr(default=Pydan...\n",
      "     |  \n",
      "     |  __schema_cache__ = {}\n",
      "     |  \n",
      "     |  __signature__ = <Signature (*, cache: Optional[bool] = None, ver...nlo...\n",
      "     |  \n",
      "     |  __validators__ = {'verbose': [<pydantic.class_validators.Validator obj...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  __call__(self, prompt: 'str', stop: 'Optional[List[str]]' = None, callbacks: 'Callbacks' = None, *, tags: 'Optional[List[str]]' = None, metadata: 'Optional[Dict[str, Any]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Check Cache and run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  __str__(self) -> 'str'\n",
      "     |      Get a string representation of the object for printing.\n",
      "     |  \n",
      "     |  async abatch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      "     |      Default implementation of abatch, which calls ainvoke N times.\n",
      "     |      Subclasses should override this method if they can batch more efficiently.\n",
      "     |  \n",
      "     |  async agenerate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  async agenerate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Asynchronously pass a sequence of prompts and return model generations.\n",
      "     |      \n",
      "     |      This method should make use of batched calls for models that expose a batched\n",
      "     |      API.\n",
      "     |      \n",
      "     |      Use this method when you want to:\n",
      "     |          1. take advantage of batched calls,\n",
      "     |          2. need more output from the model than just the top generated value,\n",
      "     |          3. are building chains that are agnostic to the underlying language model\n",
      "     |              type (e.g., pure text completion models vs chat models).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      "     |              converted to match the format of any language model (string for pure\n",
      "     |              text generation models and BaseMessages for chat models).\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          callbacks: Callbacks to pass through. Used for executing additional\n",
      "     |              functionality, such as logging or streaming, throughout generation.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          An LLMResult, which contains a list of candidate Generations for each input\n",
      "     |              prompt and additional model provider-specific output.\n",
      "     |  \n",
      "     |  async ainvoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Default implementation of ainvoke, which calls invoke in a thread pool.\n",
      "     |      Subclasses should override this method if they can run asynchronously.\n",
      "     |  \n",
      "     |  async apredict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Asynchronously pass a string to the model and return a string prediction.\n",
      "     |      \n",
      "     |      Use this method when calling pure text generation models and only the top\n",
      "     |          candidate generation is needed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: String input to pass to the model.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a string.\n",
      "     |  \n",
      "     |  async apredict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      "     |      Asynchronously pass messages to the model and return a message prediction.\n",
      "     |      \n",
      "     |      Use this method when calling chat models and only the top\n",
      "     |          candidate generation is needed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: A sequence of chat messages corresponding to a single model input.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a message.\n",
      "     |  \n",
      "     |  async astream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'AsyncIterator[str]'\n",
      "     |      Default implementation of astream, which calls ainvoke.\n",
      "     |      Subclasses should override this method if they support streaming output.\n",
      "     |  \n",
      "     |  batch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      "     |      Default implementation of batch, which calls invoke N times.\n",
      "     |      Subclasses should override this method if they can batch more efficiently.\n",
      "     |  \n",
      "     |  dict(self, **kwargs: 'Any') -> 'Dict'\n",
      "     |      Return a dictionary of the LLM.\n",
      "     |  \n",
      "     |  generate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  generate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Pass a sequence of prompts to the model and return model generations.\n",
      "     |      \n",
      "     |      This method should make use of batched calls for models that expose a batched\n",
      "     |      API.\n",
      "     |      \n",
      "     |      Use this method when you want to:\n",
      "     |          1. take advantage of batched calls,\n",
      "     |          2. need more output from the model than just the top generated value,\n",
      "     |          3. are building chains that are agnostic to the underlying language model\n",
      "     |              type (e.g., pure text completion models vs chat models).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      "     |              converted to match the format of any language model (string for pure\n",
      "     |              text generation models and BaseMessages for chat models).\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          callbacks: Callbacks to pass through. Used for executing additional\n",
      "     |              functionality, such as logging or streaming, throughout generation.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          An LLMResult, which contains a list of candidate Generations for each input\n",
      "     |              prompt and additional model provider-specific output.\n",
      "     |  \n",
      "     |  invoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |  \n",
      "     |  predict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Pass a single string input to the model and return a string prediction.\n",
      "     |      \n",
      "     |       Use this method when passing in raw text. If you want to pass in specific\n",
      "     |          types of chat messages, use predict_messages.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: String input to pass to the model.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a string.\n",
      "     |  \n",
      "     |  predict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      "     |      Pass a message sequence to the model and return a message prediction.\n",
      "     |      \n",
      "     |      Use this method when passing in chat messages. If you want to pass in raw text,\n",
      "     |          use predict.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: A sequence of chat messages corresponding to a single model input.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a message.\n",
      "     |  \n",
      "     |  save(self, file_path: 'Union[Path, str]') -> 'None'\n",
      "     |      Save the LLM.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          file_path: Path to file to save the LLM to.\n",
      "     |      \n",
      "     |      Example:\n",
      "     |      .. code-block:: python\n",
      "     |      \n",
      "     |          llm.save(file_path=\"path/llm.yaml\")\n",
      "     |  \n",
      "     |  stream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'Iterator[str]'\n",
      "     |      Default implementation of stream, which calls invoke.\n",
      "     |      Subclasses should override this method if they support streaming output.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  raise_deprecation(values: 'Dict') -> 'Dict' from pydantic.main.ModelMetaclass\n",
      "     |      Raise deprecation warning if callback_manager is used.\n",
      "     |  \n",
      "     |  set_verbose(verbose: 'Optional[bool]') -> 'bool' from pydantic.main.ModelMetaclass\n",
      "     |      If verbose is None, set it.\n",
      "     |      \n",
      "     |      This allows users to pass in None as verbose to access the global setting.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  __orig_bases__ = (langchain.schema.language_model.BaseLanguageModel[st...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.schema.language_model.BaseLanguageModel:\n",
      "     |  \n",
      "     |  get_num_tokens(self, text: 'str') -> 'int'\n",
      "     |      Get the number of tokens present in the text.\n",
      "     |      \n",
      "     |      Useful for checking if an input will fit in a model's context window.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: The string input to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The integer number of tokens in the text.\n",
      "     |  \n",
      "     |  get_num_tokens_from_messages(self, messages: 'List[BaseMessage]') -> 'int'\n",
      "     |      Get the number of tokens in the messages.\n",
      "     |      \n",
      "     |      Useful for checking if an input will fit in a model's context window.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: The message inputs to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The sum of the number of tokens across the messages.\n",
      "     |  \n",
      "     |  get_token_ids(self, text: 'str') -> 'List[int]'\n",
      "     |      Return the ordered ids of the tokens in a text.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: The string input to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          A list of ids corresponding to the tokens in the text, in order they occur\n",
      "     |              in the text.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from langchain.schema.language_model.BaseLanguageModel:\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.load.serializable.Serializable:\n",
      "     |  \n",
      "     |  __init__(self, **kwargs: Any) -> None\n",
      "     |      Create a new model by parsing and validating input data from keyword arguments.\n",
      "     |      \n",
      "     |      Raises ValidationError if the input data cannot be parsed to form a valid model.\n",
      "     |  \n",
      "     |  to_json(self) -> Union[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\n",
      "     |  \n",
      "     |  to_json_not_implemented(self) -> langchain.load.serializable.SerializedNotImplemented\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from langchain.load.serializable.Serializable:\n",
      "     |  \n",
      "     |  lc_attributes\n",
      "     |      Return a list of attribute names that should be included in the\n",
      "     |      serialized kwargs. These attributes must be accepted by the\n",
      "     |      constructor.\n",
      "     |  \n",
      "     |  lc_namespace\n",
      "     |      Return the namespace of the langchain object.\n",
      "     |      eg. [\"langchain\", \"llms\", \"openai\"]\n",
      "     |  \n",
      "     |  lc_secrets\n",
      "     |      Return a map of constructor argument names to secret ids.\n",
      "     |      eg. {\"openai_api_key\": \"OPENAI_API_KEY\"}\n",
      "     |  \n",
      "     |  lc_serializable\n",
      "     |      Return whether or not the class is serializable.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __eq__(self, other: Any) -> bool\n",
      "     |  \n",
      "     |  __getstate__(self) -> 'DictAny'\n",
      "     |  \n",
      "     |  __iter__(self) -> 'TupleGenerator'\n",
      "     |      so `dict(model)` works\n",
      "     |  \n",
      "     |  __repr_args__(self) -> 'ReprArgs'\n",
      "     |  \n",
      "     |  __setattr__(self, name, value)\n",
      "     |  \n",
      "     |  __setstate__(self, state: 'DictAny') -> None\n",
      "     |  \n",
      "     |  copy(self: 'Model', *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, update: Optional[ForwardRef('DictStrAny')] = None, deep: bool = False) -> 'Model'\n",
      "     |      Duplicate a model, optionally choose which fields to include, exclude and change.\n",
      "     |      \n",
      "     |      :param include: fields to include in new model\n",
      "     |      :param exclude: fields to exclude from new model, as with values this takes precedence over include\n",
      "     |      :param update: values to change/add in the new model. Note: the data is not validated before creating\n",
      "     |          the new model: you should trust this data\n",
      "     |      :param deep: set to `True` to make a deep copy of the model\n",
      "     |      :return: new model instance\n",
      "     |  \n",
      "     |  json(self, *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) -> 'unicode'\n",
      "     |      Generate a JSON representation of the model, `include` and `exclude` arguments as per `dict()`.\n",
      "     |      \n",
      "     |      `encoder` is an optional function to supply as `default` to json.dumps(), other arguments as per `json.dumps()`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __get_validators__() -> 'CallableGenerator' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  __try_update_forward_refs__(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      "     |      Same as update_forward_refs but will not raise exception\n",
      "     |      when forward references are not defined.\n",
      "     |  \n",
      "     |  construct(_fields_set: Optional[ForwardRef('SetStr')] = None, **values: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |      Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data.\n",
      "     |      Default values are respected, but no other validation is performed.\n",
      "     |      Behaves as if `Config.extra = 'allow'` was set since it adds all passed values\n",
      "     |  \n",
      "     |  from_orm(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_file(path: Union[str, pathlib.Path], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_obj(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_raw(b: Union[str, bytes], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema(by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}') -> 'DictStrAny' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema_json(*, by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}', **dumps_kwargs: Any) -> 'unicode' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  update_forward_refs(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      "     |      Try to update ForwardRefs on fields based on this Model, globalns and localns.\n",
      "     |  \n",
      "     |  validate(value: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __fields_set__\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.utils.Representation:\n",
      "     |  \n",
      "     |  __pretty__(self, fmt: Callable[[Any], Any], **kwargs: Any) -> Generator[Any, NoneType, NoneType]\n",
      "     |      Used by devtools (https://python-devtools.helpmanual.io/) to provide a human readable representations of objects\n",
      "     |  \n",
      "     |  __repr__(self) -> 'unicode'\n",
      "     |  \n",
      "     |  __repr_name__(self) -> 'unicode'\n",
      "     |      Name of the instance's class, used in __repr__.\n",
      "     |  \n",
      "     |  __repr_str__(self, join_str: 'unicode') -> 'unicode'\n",
      "     |  \n",
      "     |  __rich_repr__(self) -> 'RichReprResult'\n",
      "     |      Get fields for Rich library\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.schema.runnable.base.Runnable:\n",
      "     |  \n",
      "     |  __or__(self, other: 'Union[Runnable[Any, Other], Callable[[Any], Other], Mapping[str, Union[Runnable[Any, Other], Callable[[Any], Other], Any]]]') -> 'RunnableSequence[Input, Other]'\n",
      "     |      Return self|value.\n",
      "     |  \n",
      "     |  __ror__(self, other: 'Union[Runnable[Other, Any], Callable[[Any], Other], Mapping[str, Union[Runnable[Other, Any], Callable[[Other], Any], Any]]]') -> 'RunnableSequence[Other, Output]'\n",
      "     |      Return value|self.\n",
      "     |  \n",
      "     |  async atransform(self, input: 'AsyncIterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'AsyncIterator[Output]'\n",
      "     |      Default implementation of atransform, which buffers input and calls astream.\n",
      "     |      Subclasses should override this method if they can start producing output while\n",
      "     |      input is still being generated.\n",
      "     |  \n",
      "     |  bind(self, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      "     |      Bind arguments to a Runnable, returning a new Runnable.\n",
      "     |  \n",
      "     |  map(self) -> 'Runnable[List[Input], List[Output]]'\n",
      "     |      Return a new Runnable that maps a list of inputs to a list of outputs,\n",
      "     |      by calling invoke() with each input.\n",
      "     |  \n",
      "     |  transform(self, input: 'Iterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'Iterator[Output]'\n",
      "     |      Default implementation of transform, which buffers input and then calls stream.\n",
      "     |      Subclasses should override this method if they can start producing output while\n",
      "     |      input is still being generated.\n",
      "     |  \n",
      "     |  with_config(self, config: 'Optional[RunnableConfig]' = None, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      "     |      Bind config to a Runnable, returning a new Runnable.\n",
      "     |  \n",
      "     |  with_fallbacks(self, fallbacks: 'Sequence[Runnable[Input, Output]]', *, exceptions_to_handle: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,)) -> 'RunnableWithFallbacks[Input, Output]'\n",
      "     |  \n",
      "     |  with_retry(self, *, retry_if_exception_type: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,), wait_exponential_jitter: 'bool' = True, stop_after_attempt: 'int' = 3) -> 'Runnable[Input, Output]'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __class_getitem__(params) from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  __init_subclass__(*args, **kwargs) from pydantic.main.ModelMetaclass\n",
      "     |      This method is called when a class is subclassed.\n",
      "     |      \n",
      "     |      The default implementation does nothing. It may be\n",
      "     |      overridden to extend subclasses.\n",
      "    \n",
      "    class GooglePalm(langchain.llms.base.BaseLLM, pydantic.main.BaseModel)\n",
      "     |  GooglePalm(*, cache: Optional[bool] = None, verbose: bool = None, callbacks: Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager, NoneType] = None, callback_manager: Optional[langchain.callbacks.base.BaseCallbackManager] = None, tags: Optional[List[str]] = None, metadata: Optional[Dict[str, Any]] = None, client: Any = None, google_api_key: Optional[str] = None, model_name: str = 'models/text-bison-001', temperature: float = 0.7, top_p: Optional[float] = None, top_k: Optional[int] = None, max_output_tokens: Optional[int] = None, n: int = 1) -> None\n",
      "     |  \n",
      "     |  Google PaLM models.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      GooglePalm\n",
      "     |      langchain.llms.base.BaseLLM\n",
      "     |      langchain.schema.language_model.BaseLanguageModel\n",
      "     |      langchain.load.serializable.Serializable\n",
      "     |      pydantic.main.BaseModel\n",
      "     |      pydantic.utils.Representation\n",
      "     |      langchain.schema.runnable.base.Runnable\n",
      "     |      typing.Generic\n",
      "     |      abc.ABC\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  validate_environment(values: 'Dict') -> 'Dict' from pydantic.main.ModelMetaclass\n",
      "     |      Validate api key, python package exists.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __json_encoder__ = pydantic_encoder(obj: Any) -> Any\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {'client': 'Any', 'google_api_key': 'Optional[str]',...\n",
      "     |  \n",
      "     |  __class_vars__ = set()\n",
      "     |  \n",
      "     |  __config__ = <class 'langchain.llms.google_palm.Config'>\n",
      "     |  \n",
      "     |  __custom_root_type__ = False\n",
      "     |  \n",
      "     |  __exclude_fields__ = {'callback_manager': True, 'callbacks': True, 'me...\n",
      "     |  \n",
      "     |  __fields__ = {'cache': ModelField(name='cache', type=Optional[bool], r...\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "     |  \n",
      "     |  __include_fields__ = None\n",
      "     |  \n",
      "     |  __parameters__ = ()\n",
      "     |  \n",
      "     |  __post_root_validators__ = [(False, <function BaseLLM.raise_deprecatio...\n",
      "     |  \n",
      "     |  __pre_root_validators__ = []\n",
      "     |  \n",
      "     |  __private_attributes__ = {'_lc_kwargs': ModelPrivateAttr(default=Pydan...\n",
      "     |  \n",
      "     |  __schema_cache__ = {}\n",
      "     |  \n",
      "     |  __signature__ = <Signature (*, cache: Optional[bool] = None, ver...oke...\n",
      "     |  \n",
      "     |  __validators__ = {'verbose': [<pydantic.class_validators.Validator obj...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  __call__(self, prompt: 'str', stop: 'Optional[List[str]]' = None, callbacks: 'Callbacks' = None, *, tags: 'Optional[List[str]]' = None, metadata: 'Optional[Dict[str, Any]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Check Cache and run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  __str__(self) -> 'str'\n",
      "     |      Get a string representation of the object for printing.\n",
      "     |  \n",
      "     |  async abatch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      "     |      Default implementation of abatch, which calls ainvoke N times.\n",
      "     |      Subclasses should override this method if they can batch more efficiently.\n",
      "     |  \n",
      "     |  async agenerate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  async agenerate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Asynchronously pass a sequence of prompts and return model generations.\n",
      "     |      \n",
      "     |      This method should make use of batched calls for models that expose a batched\n",
      "     |      API.\n",
      "     |      \n",
      "     |      Use this method when you want to:\n",
      "     |          1. take advantage of batched calls,\n",
      "     |          2. need more output from the model than just the top generated value,\n",
      "     |          3. are building chains that are agnostic to the underlying language model\n",
      "     |              type (e.g., pure text completion models vs chat models).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      "     |              converted to match the format of any language model (string for pure\n",
      "     |              text generation models and BaseMessages for chat models).\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          callbacks: Callbacks to pass through. Used for executing additional\n",
      "     |              functionality, such as logging or streaming, throughout generation.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          An LLMResult, which contains a list of candidate Generations for each input\n",
      "     |              prompt and additional model provider-specific output.\n",
      "     |  \n",
      "     |  async ainvoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Default implementation of ainvoke, which calls invoke in a thread pool.\n",
      "     |      Subclasses should override this method if they can run asynchronously.\n",
      "     |  \n",
      "     |  async apredict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Asynchronously pass a string to the model and return a string prediction.\n",
      "     |      \n",
      "     |      Use this method when calling pure text generation models and only the top\n",
      "     |          candidate generation is needed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: String input to pass to the model.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a string.\n",
      "     |  \n",
      "     |  async apredict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      "     |      Asynchronously pass messages to the model and return a message prediction.\n",
      "     |      \n",
      "     |      Use this method when calling chat models and only the top\n",
      "     |          candidate generation is needed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: A sequence of chat messages corresponding to a single model input.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a message.\n",
      "     |  \n",
      "     |  async astream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'AsyncIterator[str]'\n",
      "     |      Default implementation of astream, which calls ainvoke.\n",
      "     |      Subclasses should override this method if they support streaming output.\n",
      "     |  \n",
      "     |  batch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      "     |      Default implementation of batch, which calls invoke N times.\n",
      "     |      Subclasses should override this method if they can batch more efficiently.\n",
      "     |  \n",
      "     |  dict(self, **kwargs: 'Any') -> 'Dict'\n",
      "     |      Return a dictionary of the LLM.\n",
      "     |  \n",
      "     |  generate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  generate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Pass a sequence of prompts to the model and return model generations.\n",
      "     |      \n",
      "     |      This method should make use of batched calls for models that expose a batched\n",
      "     |      API.\n",
      "     |      \n",
      "     |      Use this method when you want to:\n",
      "     |          1. take advantage of batched calls,\n",
      "     |          2. need more output from the model than just the top generated value,\n",
      "     |          3. are building chains that are agnostic to the underlying language model\n",
      "     |              type (e.g., pure text completion models vs chat models).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      "     |              converted to match the format of any language model (string for pure\n",
      "     |              text generation models and BaseMessages for chat models).\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          callbacks: Callbacks to pass through. Used for executing additional\n",
      "     |              functionality, such as logging or streaming, throughout generation.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          An LLMResult, which contains a list of candidate Generations for each input\n",
      "     |              prompt and additional model provider-specific output.\n",
      "     |  \n",
      "     |  invoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |  \n",
      "     |  predict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Pass a single string input to the model and return a string prediction.\n",
      "     |      \n",
      "     |       Use this method when passing in raw text. If you want to pass in specific\n",
      "     |          types of chat messages, use predict_messages.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: String input to pass to the model.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a string.\n",
      "     |  \n",
      "     |  predict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      "     |      Pass a message sequence to the model and return a message prediction.\n",
      "     |      \n",
      "     |      Use this method when passing in chat messages. If you want to pass in raw text,\n",
      "     |          use predict.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: A sequence of chat messages corresponding to a single model input.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a message.\n",
      "     |  \n",
      "     |  save(self, file_path: 'Union[Path, str]') -> 'None'\n",
      "     |      Save the LLM.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          file_path: Path to file to save the LLM to.\n",
      "     |      \n",
      "     |      Example:\n",
      "     |      .. code-block:: python\n",
      "     |      \n",
      "     |          llm.save(file_path=\"path/llm.yaml\")\n",
      "     |  \n",
      "     |  stream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'Iterator[str]'\n",
      "     |      Default implementation of stream, which calls invoke.\n",
      "     |      Subclasses should override this method if they support streaming output.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  raise_deprecation(values: 'Dict') -> 'Dict' from pydantic.main.ModelMetaclass\n",
      "     |      Raise deprecation warning if callback_manager is used.\n",
      "     |  \n",
      "     |  set_verbose(verbose: 'Optional[bool]') -> 'bool' from pydantic.main.ModelMetaclass\n",
      "     |      If verbose is None, set it.\n",
      "     |      \n",
      "     |      This allows users to pass in None as verbose to access the global setting.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  Config = <class 'langchain.llms.base.BaseLLM.Config'>\n",
      "     |      Configuration for this pydantic object.\n",
      "     |  \n",
      "     |  \n",
      "     |  __orig_bases__ = (langchain.schema.language_model.BaseLanguageModel[st...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.schema.language_model.BaseLanguageModel:\n",
      "     |  \n",
      "     |  get_num_tokens(self, text: 'str') -> 'int'\n",
      "     |      Get the number of tokens present in the text.\n",
      "     |      \n",
      "     |      Useful for checking if an input will fit in a model's context window.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: The string input to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The integer number of tokens in the text.\n",
      "     |  \n",
      "     |  get_num_tokens_from_messages(self, messages: 'List[BaseMessage]') -> 'int'\n",
      "     |      Get the number of tokens in the messages.\n",
      "     |      \n",
      "     |      Useful for checking if an input will fit in a model's context window.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: The message inputs to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The sum of the number of tokens across the messages.\n",
      "     |  \n",
      "     |  get_token_ids(self, text: 'str') -> 'List[int]'\n",
      "     |      Return the ordered ids of the tokens in a text.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: The string input to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          A list of ids corresponding to the tokens in the text, in order they occur\n",
      "     |              in the text.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from langchain.schema.language_model.BaseLanguageModel:\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.load.serializable.Serializable:\n",
      "     |  \n",
      "     |  __init__(self, **kwargs: Any) -> None\n",
      "     |      Create a new model by parsing and validating input data from keyword arguments.\n",
      "     |      \n",
      "     |      Raises ValidationError if the input data cannot be parsed to form a valid model.\n",
      "     |  \n",
      "     |  to_json(self) -> Union[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\n",
      "     |  \n",
      "     |  to_json_not_implemented(self) -> langchain.load.serializable.SerializedNotImplemented\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from langchain.load.serializable.Serializable:\n",
      "     |  \n",
      "     |  lc_attributes\n",
      "     |      Return a list of attribute names that should be included in the\n",
      "     |      serialized kwargs. These attributes must be accepted by the\n",
      "     |      constructor.\n",
      "     |  \n",
      "     |  lc_namespace\n",
      "     |      Return the namespace of the langchain object.\n",
      "     |      eg. [\"langchain\", \"llms\", \"openai\"]\n",
      "     |  \n",
      "     |  lc_secrets\n",
      "     |      Return a map of constructor argument names to secret ids.\n",
      "     |      eg. {\"openai_api_key\": \"OPENAI_API_KEY\"}\n",
      "     |  \n",
      "     |  lc_serializable\n",
      "     |      Return whether or not the class is serializable.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __eq__(self, other: Any) -> bool\n",
      "     |  \n",
      "     |  __getstate__(self) -> 'DictAny'\n",
      "     |  \n",
      "     |  __iter__(self) -> 'TupleGenerator'\n",
      "     |      so `dict(model)` works\n",
      "     |  \n",
      "     |  __repr_args__(self) -> 'ReprArgs'\n",
      "     |  \n",
      "     |  __setattr__(self, name, value)\n",
      "     |  \n",
      "     |  __setstate__(self, state: 'DictAny') -> None\n",
      "     |  \n",
      "     |  copy(self: 'Model', *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, update: Optional[ForwardRef('DictStrAny')] = None, deep: bool = False) -> 'Model'\n",
      "     |      Duplicate a model, optionally choose which fields to include, exclude and change.\n",
      "     |      \n",
      "     |      :param include: fields to include in new model\n",
      "     |      :param exclude: fields to exclude from new model, as with values this takes precedence over include\n",
      "     |      :param update: values to change/add in the new model. Note: the data is not validated before creating\n",
      "     |          the new model: you should trust this data\n",
      "     |      :param deep: set to `True` to make a deep copy of the model\n",
      "     |      :return: new model instance\n",
      "     |  \n",
      "     |  json(self, *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) -> 'unicode'\n",
      "     |      Generate a JSON representation of the model, `include` and `exclude` arguments as per `dict()`.\n",
      "     |      \n",
      "     |      `encoder` is an optional function to supply as `default` to json.dumps(), other arguments as per `json.dumps()`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __get_validators__() -> 'CallableGenerator' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  __try_update_forward_refs__(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      "     |      Same as update_forward_refs but will not raise exception\n",
      "     |      when forward references are not defined.\n",
      "     |  \n",
      "     |  construct(_fields_set: Optional[ForwardRef('SetStr')] = None, **values: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |      Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data.\n",
      "     |      Default values are respected, but no other validation is performed.\n",
      "     |      Behaves as if `Config.extra = 'allow'` was set since it adds all passed values\n",
      "     |  \n",
      "     |  from_orm(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_file(path: Union[str, pathlib.Path], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_obj(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_raw(b: Union[str, bytes], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema(by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}') -> 'DictStrAny' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema_json(*, by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}', **dumps_kwargs: Any) -> 'unicode' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  update_forward_refs(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      "     |      Try to update ForwardRefs on fields based on this Model, globalns and localns.\n",
      "     |  \n",
      "     |  validate(value: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __fields_set__\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.utils.Representation:\n",
      "     |  \n",
      "     |  __pretty__(self, fmt: Callable[[Any], Any], **kwargs: Any) -> Generator[Any, NoneType, NoneType]\n",
      "     |      Used by devtools (https://python-devtools.helpmanual.io/) to provide a human readable representations of objects\n",
      "     |  \n",
      "     |  __repr__(self) -> 'unicode'\n",
      "     |  \n",
      "     |  __repr_name__(self) -> 'unicode'\n",
      "     |      Name of the instance's class, used in __repr__.\n",
      "     |  \n",
      "     |  __repr_str__(self, join_str: 'unicode') -> 'unicode'\n",
      "     |  \n",
      "     |  __rich_repr__(self) -> 'RichReprResult'\n",
      "     |      Get fields for Rich library\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.schema.runnable.base.Runnable:\n",
      "     |  \n",
      "     |  __or__(self, other: 'Union[Runnable[Any, Other], Callable[[Any], Other], Mapping[str, Union[Runnable[Any, Other], Callable[[Any], Other], Any]]]') -> 'RunnableSequence[Input, Other]'\n",
      "     |      Return self|value.\n",
      "     |  \n",
      "     |  __ror__(self, other: 'Union[Runnable[Other, Any], Callable[[Any], Other], Mapping[str, Union[Runnable[Other, Any], Callable[[Other], Any], Any]]]') -> 'RunnableSequence[Other, Output]'\n",
      "     |      Return value|self.\n",
      "     |  \n",
      "     |  async atransform(self, input: 'AsyncIterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'AsyncIterator[Output]'\n",
      "     |      Default implementation of atransform, which buffers input and calls astream.\n",
      "     |      Subclasses should override this method if they can start producing output while\n",
      "     |      input is still being generated.\n",
      "     |  \n",
      "     |  bind(self, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      "     |      Bind arguments to a Runnable, returning a new Runnable.\n",
      "     |  \n",
      "     |  map(self) -> 'Runnable[List[Input], List[Output]]'\n",
      "     |      Return a new Runnable that maps a list of inputs to a list of outputs,\n",
      "     |      by calling invoke() with each input.\n",
      "     |  \n",
      "     |  transform(self, input: 'Iterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'Iterator[Output]'\n",
      "     |      Default implementation of transform, which buffers input and then calls stream.\n",
      "     |      Subclasses should override this method if they can start producing output while\n",
      "     |      input is still being generated.\n",
      "     |  \n",
      "     |  with_config(self, config: 'Optional[RunnableConfig]' = None, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      "     |      Bind config to a Runnable, returning a new Runnable.\n",
      "     |  \n",
      "     |  with_fallbacks(self, fallbacks: 'Sequence[Runnable[Input, Output]]', *, exceptions_to_handle: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,)) -> 'RunnableWithFallbacks[Input, Output]'\n",
      "     |  \n",
      "     |  with_retry(self, *, retry_if_exception_type: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,), wait_exponential_jitter: 'bool' = True, stop_after_attempt: 'int' = 3) -> 'Runnable[Input, Output]'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __class_getitem__(params) from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  __init_subclass__(*args, **kwargs) from pydantic.main.ModelMetaclass\n",
      "     |      This method is called when a class is subclassed.\n",
      "     |      \n",
      "     |      The default implementation does nothing. It may be\n",
      "     |      overridden to extend subclasses.\n",
      "    \n",
      "    class GooseAI(langchain.llms.base.LLM)\n",
      "     |  GooseAI(*, cache: Optional[bool] = None, verbose: bool = None, callbacks: Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager, NoneType] = None, callback_manager: Optional[langchain.callbacks.base.BaseCallbackManager] = None, tags: Optional[List[str]] = None, metadata: Optional[Dict[str, Any]] = None, client: Any = None, model_name: str = 'gpt-neo-20b', temperature: float = 0.7, max_tokens: int = 256, top_p: float = 1, min_tokens: int = 1, frequency_penalty: float = 0, presence_penalty: float = 0, n: int = 1, model_kwargs: Dict[str, Any] = None, logit_bias: Optional[Dict[str, float]] = None, gooseai_api_key: Optional[str] = None) -> None\n",
      "     |  \n",
      "     |  GooseAI large language models.\n",
      "     |  \n",
      "     |  To use, you should have the ``openai`` python package installed, and the\n",
      "     |  environment variable ``GOOSEAI_API_KEY`` set with your API key.\n",
      "     |  \n",
      "     |  Any parameters that are valid to be passed to the openai.create call can be passed\n",
      "     |  in, even if not explicitly saved on this class.\n",
      "     |  \n",
      "     |  Example:\n",
      "     |      .. code-block:: python\n",
      "     |  \n",
      "     |          from langchain.llms import GooseAI\n",
      "     |          gooseai = GooseAI(model_name=\"gpt-neo-20b\")\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      GooseAI\n",
      "     |      langchain.llms.base.LLM\n",
      "     |      langchain.llms.base.BaseLLM\n",
      "     |      langchain.schema.language_model.BaseLanguageModel\n",
      "     |      langchain.load.serializable.Serializable\n",
      "     |      pydantic.main.BaseModel\n",
      "     |      pydantic.utils.Representation\n",
      "     |      langchain.schema.runnable.base.Runnable\n",
      "     |      typing.Generic\n",
      "     |      abc.ABC\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  build_extra(values: Dict[str, Any]) -> Dict[str, Any] from pydantic.main.ModelMetaclass\n",
      "     |      Build extra kwargs from additional params that were passed in.\n",
      "     |  \n",
      "     |  validate_environment(values: Dict) -> Dict from pydantic.main.ModelMetaclass\n",
      "     |      Validate that api key and python package exists in environment.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __json_encoder__ = pydantic_encoder(obj: Any) -> Any\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  Config = <class 'langchain.llms.gooseai.GooseAI.Config'>\n",
      "     |      Configuration for this pydantic config.\n",
      "     |  \n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {'client': typing.Any, 'frequency_penalty': <class '...\n",
      "     |  \n",
      "     |  __class_vars__ = set()\n",
      "     |  \n",
      "     |  __config__ = <class 'langchain.llms.gooseai.Config'>\n",
      "     |  \n",
      "     |  __custom_root_type__ = False\n",
      "     |  \n",
      "     |  __exclude_fields__ = {'callback_manager': True, 'callbacks': True, 'me...\n",
      "     |  \n",
      "     |  __fields__ = {'cache': ModelField(name='cache', type=Optional[bool], r...\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "     |  \n",
      "     |  __include_fields__ = None\n",
      "     |  \n",
      "     |  __parameters__ = ()\n",
      "     |  \n",
      "     |  __post_root_validators__ = [(False, <function BaseLLM.raise_deprecatio...\n",
      "     |  \n",
      "     |  __pre_root_validators__ = [<function GooseAI.build_extra>]\n",
      "     |  \n",
      "     |  __private_attributes__ = {'_lc_kwargs': ModelPrivateAttr(default=Pydan...\n",
      "     |  \n",
      "     |  __schema_cache__ = {}\n",
      "     |  \n",
      "     |  __signature__ = <Signature (*, cache: Optional[bool] = None, ver..., g...\n",
      "     |  \n",
      "     |  __validators__ = {'verbose': [<pydantic.class_validators.Validator obj...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  __call__(self, prompt: 'str', stop: 'Optional[List[str]]' = None, callbacks: 'Callbacks' = None, *, tags: 'Optional[List[str]]' = None, metadata: 'Optional[Dict[str, Any]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Check Cache and run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  __str__(self) -> 'str'\n",
      "     |      Get a string representation of the object for printing.\n",
      "     |  \n",
      "     |  async abatch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      "     |      Default implementation of abatch, which calls ainvoke N times.\n",
      "     |      Subclasses should override this method if they can batch more efficiently.\n",
      "     |  \n",
      "     |  async agenerate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  async agenerate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Asynchronously pass a sequence of prompts and return model generations.\n",
      "     |      \n",
      "     |      This method should make use of batched calls for models that expose a batched\n",
      "     |      API.\n",
      "     |      \n",
      "     |      Use this method when you want to:\n",
      "     |          1. take advantage of batched calls,\n",
      "     |          2. need more output from the model than just the top generated value,\n",
      "     |          3. are building chains that are agnostic to the underlying language model\n",
      "     |              type (e.g., pure text completion models vs chat models).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      "     |              converted to match the format of any language model (string for pure\n",
      "     |              text generation models and BaseMessages for chat models).\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          callbacks: Callbacks to pass through. Used for executing additional\n",
      "     |              functionality, such as logging or streaming, throughout generation.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          An LLMResult, which contains a list of candidate Generations for each input\n",
      "     |              prompt and additional model provider-specific output.\n",
      "     |  \n",
      "     |  async ainvoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Default implementation of ainvoke, which calls invoke in a thread pool.\n",
      "     |      Subclasses should override this method if they can run asynchronously.\n",
      "     |  \n",
      "     |  async apredict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Asynchronously pass a string to the model and return a string prediction.\n",
      "     |      \n",
      "     |      Use this method when calling pure text generation models and only the top\n",
      "     |          candidate generation is needed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: String input to pass to the model.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a string.\n",
      "     |  \n",
      "     |  async apredict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      "     |      Asynchronously pass messages to the model and return a message prediction.\n",
      "     |      \n",
      "     |      Use this method when calling chat models and only the top\n",
      "     |          candidate generation is needed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: A sequence of chat messages corresponding to a single model input.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a message.\n",
      "     |  \n",
      "     |  async astream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'AsyncIterator[str]'\n",
      "     |      Default implementation of astream, which calls ainvoke.\n",
      "     |      Subclasses should override this method if they support streaming output.\n",
      "     |  \n",
      "     |  batch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      "     |      Default implementation of batch, which calls invoke N times.\n",
      "     |      Subclasses should override this method if they can batch more efficiently.\n",
      "     |  \n",
      "     |  dict(self, **kwargs: 'Any') -> 'Dict'\n",
      "     |      Return a dictionary of the LLM.\n",
      "     |  \n",
      "     |  generate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  generate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Pass a sequence of prompts to the model and return model generations.\n",
      "     |      \n",
      "     |      This method should make use of batched calls for models that expose a batched\n",
      "     |      API.\n",
      "     |      \n",
      "     |      Use this method when you want to:\n",
      "     |          1. take advantage of batched calls,\n",
      "     |          2. need more output from the model than just the top generated value,\n",
      "     |          3. are building chains that are agnostic to the underlying language model\n",
      "     |              type (e.g., pure text completion models vs chat models).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      "     |              converted to match the format of any language model (string for pure\n",
      "     |              text generation models and BaseMessages for chat models).\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          callbacks: Callbacks to pass through. Used for executing additional\n",
      "     |              functionality, such as logging or streaming, throughout generation.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          An LLMResult, which contains a list of candidate Generations for each input\n",
      "     |              prompt and additional model provider-specific output.\n",
      "     |  \n",
      "     |  invoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |  \n",
      "     |  predict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Pass a single string input to the model and return a string prediction.\n",
      "     |      \n",
      "     |       Use this method when passing in raw text. If you want to pass in specific\n",
      "     |          types of chat messages, use predict_messages.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: String input to pass to the model.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a string.\n",
      "     |  \n",
      "     |  predict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      "     |      Pass a message sequence to the model and return a message prediction.\n",
      "     |      \n",
      "     |      Use this method when passing in chat messages. If you want to pass in raw text,\n",
      "     |          use predict.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: A sequence of chat messages corresponding to a single model input.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a message.\n",
      "     |  \n",
      "     |  save(self, file_path: 'Union[Path, str]') -> 'None'\n",
      "     |      Save the LLM.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          file_path: Path to file to save the LLM to.\n",
      "     |      \n",
      "     |      Example:\n",
      "     |      .. code-block:: python\n",
      "     |      \n",
      "     |          llm.save(file_path=\"path/llm.yaml\")\n",
      "     |  \n",
      "     |  stream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'Iterator[str]'\n",
      "     |      Default implementation of stream, which calls invoke.\n",
      "     |      Subclasses should override this method if they support streaming output.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  raise_deprecation(values: 'Dict') -> 'Dict' from pydantic.main.ModelMetaclass\n",
      "     |      Raise deprecation warning if callback_manager is used.\n",
      "     |  \n",
      "     |  set_verbose(verbose: 'Optional[bool]') -> 'bool' from pydantic.main.ModelMetaclass\n",
      "     |      If verbose is None, set it.\n",
      "     |      \n",
      "     |      This allows users to pass in None as verbose to access the global setting.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  __orig_bases__ = (langchain.schema.language_model.BaseLanguageModel[st...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.schema.language_model.BaseLanguageModel:\n",
      "     |  \n",
      "     |  get_num_tokens(self, text: 'str') -> 'int'\n",
      "     |      Get the number of tokens present in the text.\n",
      "     |      \n",
      "     |      Useful for checking if an input will fit in a model's context window.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: The string input to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The integer number of tokens in the text.\n",
      "     |  \n",
      "     |  get_num_tokens_from_messages(self, messages: 'List[BaseMessage]') -> 'int'\n",
      "     |      Get the number of tokens in the messages.\n",
      "     |      \n",
      "     |      Useful for checking if an input will fit in a model's context window.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: The message inputs to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The sum of the number of tokens across the messages.\n",
      "     |  \n",
      "     |  get_token_ids(self, text: 'str') -> 'List[int]'\n",
      "     |      Return the ordered ids of the tokens in a text.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: The string input to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          A list of ids corresponding to the tokens in the text, in order they occur\n",
      "     |              in the text.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from langchain.schema.language_model.BaseLanguageModel:\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.load.serializable.Serializable:\n",
      "     |  \n",
      "     |  __init__(self, **kwargs: Any) -> None\n",
      "     |      Create a new model by parsing and validating input data from keyword arguments.\n",
      "     |      \n",
      "     |      Raises ValidationError if the input data cannot be parsed to form a valid model.\n",
      "     |  \n",
      "     |  to_json(self) -> Union[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\n",
      "     |  \n",
      "     |  to_json_not_implemented(self) -> langchain.load.serializable.SerializedNotImplemented\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from langchain.load.serializable.Serializable:\n",
      "     |  \n",
      "     |  lc_attributes\n",
      "     |      Return a list of attribute names that should be included in the\n",
      "     |      serialized kwargs. These attributes must be accepted by the\n",
      "     |      constructor.\n",
      "     |  \n",
      "     |  lc_namespace\n",
      "     |      Return the namespace of the langchain object.\n",
      "     |      eg. [\"langchain\", \"llms\", \"openai\"]\n",
      "     |  \n",
      "     |  lc_secrets\n",
      "     |      Return a map of constructor argument names to secret ids.\n",
      "     |      eg. {\"openai_api_key\": \"OPENAI_API_KEY\"}\n",
      "     |  \n",
      "     |  lc_serializable\n",
      "     |      Return whether or not the class is serializable.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __eq__(self, other: Any) -> bool\n",
      "     |  \n",
      "     |  __getstate__(self) -> 'DictAny'\n",
      "     |  \n",
      "     |  __iter__(self) -> 'TupleGenerator'\n",
      "     |      so `dict(model)` works\n",
      "     |  \n",
      "     |  __repr_args__(self) -> 'ReprArgs'\n",
      "     |  \n",
      "     |  __setattr__(self, name, value)\n",
      "     |  \n",
      "     |  __setstate__(self, state: 'DictAny') -> None\n",
      "     |  \n",
      "     |  copy(self: 'Model', *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, update: Optional[ForwardRef('DictStrAny')] = None, deep: bool = False) -> 'Model'\n",
      "     |      Duplicate a model, optionally choose which fields to include, exclude and change.\n",
      "     |      \n",
      "     |      :param include: fields to include in new model\n",
      "     |      :param exclude: fields to exclude from new model, as with values this takes precedence over include\n",
      "     |      :param update: values to change/add in the new model. Note: the data is not validated before creating\n",
      "     |          the new model: you should trust this data\n",
      "     |      :param deep: set to `True` to make a deep copy of the model\n",
      "     |      :return: new model instance\n",
      "     |  \n",
      "     |  json(self, *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) -> 'unicode'\n",
      "     |      Generate a JSON representation of the model, `include` and `exclude` arguments as per `dict()`.\n",
      "     |      \n",
      "     |      `encoder` is an optional function to supply as `default` to json.dumps(), other arguments as per `json.dumps()`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __get_validators__() -> 'CallableGenerator' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  __try_update_forward_refs__(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      "     |      Same as update_forward_refs but will not raise exception\n",
      "     |      when forward references are not defined.\n",
      "     |  \n",
      "     |  construct(_fields_set: Optional[ForwardRef('SetStr')] = None, **values: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |      Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data.\n",
      "     |      Default values are respected, but no other validation is performed.\n",
      "     |      Behaves as if `Config.extra = 'allow'` was set since it adds all passed values\n",
      "     |  \n",
      "     |  from_orm(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_file(path: Union[str, pathlib.Path], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_obj(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_raw(b: Union[str, bytes], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema(by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}') -> 'DictStrAny' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema_json(*, by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}', **dumps_kwargs: Any) -> 'unicode' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  update_forward_refs(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      "     |      Try to update ForwardRefs on fields based on this Model, globalns and localns.\n",
      "     |  \n",
      "     |  validate(value: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __fields_set__\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.utils.Representation:\n",
      "     |  \n",
      "     |  __pretty__(self, fmt: Callable[[Any], Any], **kwargs: Any) -> Generator[Any, NoneType, NoneType]\n",
      "     |      Used by devtools (https://python-devtools.helpmanual.io/) to provide a human readable representations of objects\n",
      "     |  \n",
      "     |  __repr__(self) -> 'unicode'\n",
      "     |  \n",
      "     |  __repr_name__(self) -> 'unicode'\n",
      "     |      Name of the instance's class, used in __repr__.\n",
      "     |  \n",
      "     |  __repr_str__(self, join_str: 'unicode') -> 'unicode'\n",
      "     |  \n",
      "     |  __rich_repr__(self) -> 'RichReprResult'\n",
      "     |      Get fields for Rich library\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.schema.runnable.base.Runnable:\n",
      "     |  \n",
      "     |  __or__(self, other: 'Union[Runnable[Any, Other], Callable[[Any], Other], Mapping[str, Union[Runnable[Any, Other], Callable[[Any], Other], Any]]]') -> 'RunnableSequence[Input, Other]'\n",
      "     |      Return self|value.\n",
      "     |  \n",
      "     |  __ror__(self, other: 'Union[Runnable[Other, Any], Callable[[Any], Other], Mapping[str, Union[Runnable[Other, Any], Callable[[Other], Any], Any]]]') -> 'RunnableSequence[Other, Output]'\n",
      "     |      Return value|self.\n",
      "     |  \n",
      "     |  async atransform(self, input: 'AsyncIterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'AsyncIterator[Output]'\n",
      "     |      Default implementation of atransform, which buffers input and calls astream.\n",
      "     |      Subclasses should override this method if they can start producing output while\n",
      "     |      input is still being generated.\n",
      "     |  \n",
      "     |  bind(self, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      "     |      Bind arguments to a Runnable, returning a new Runnable.\n",
      "     |  \n",
      "     |  map(self) -> 'Runnable[List[Input], List[Output]]'\n",
      "     |      Return a new Runnable that maps a list of inputs to a list of outputs,\n",
      "     |      by calling invoke() with each input.\n",
      "     |  \n",
      "     |  transform(self, input: 'Iterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'Iterator[Output]'\n",
      "     |      Default implementation of transform, which buffers input and then calls stream.\n",
      "     |      Subclasses should override this method if they can start producing output while\n",
      "     |      input is still being generated.\n",
      "     |  \n",
      "     |  with_config(self, config: 'Optional[RunnableConfig]' = None, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      "     |      Bind config to a Runnable, returning a new Runnable.\n",
      "     |  \n",
      "     |  with_fallbacks(self, fallbacks: 'Sequence[Runnable[Input, Output]]', *, exceptions_to_handle: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,)) -> 'RunnableWithFallbacks[Input, Output]'\n",
      "     |  \n",
      "     |  with_retry(self, *, retry_if_exception_type: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,), wait_exponential_jitter: 'bool' = True, stop_after_attempt: 'int' = 3) -> 'Runnable[Input, Output]'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __class_getitem__(params) from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  __init_subclass__(*args, **kwargs) from pydantic.main.ModelMetaclass\n",
      "     |      This method is called when a class is subclassed.\n",
      "     |      \n",
      "     |      The default implementation does nothing. It may be\n",
      "     |      overridden to extend subclasses.\n",
      "    \n",
      "    class HuggingFaceEndpoint(langchain.llms.base.LLM)\n",
      "     |  HuggingFaceEndpoint(*, cache: Optional[bool] = None, verbose: bool = None, callbacks: Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager, NoneType] = None, callback_manager: Optional[langchain.callbacks.base.BaseCallbackManager] = None, tags: Optional[List[str]] = None, metadata: Optional[Dict[str, Any]] = None, endpoint_url: str = '', task: Optional[str] = None, model_kwargs: Optional[dict] = None, huggingfacehub_api_token: Optional[str] = None) -> None\n",
      "     |  \n",
      "     |  HuggingFace Endpoint models.\n",
      "     |  \n",
      "     |  To use, you should have the ``huggingface_hub`` python package installed, and the\n",
      "     |  environment variable ``HUGGINGFACEHUB_API_TOKEN`` set with your API token, or pass\n",
      "     |  it as a named parameter to the constructor.\n",
      "     |  \n",
      "     |  Only supports `text-generation` and `text2text-generation` for now.\n",
      "     |  \n",
      "     |  Example:\n",
      "     |      .. code-block:: python\n",
      "     |  \n",
      "     |          from langchain.llms import HuggingFaceEndpoint\n",
      "     |          endpoint_url = (\n",
      "     |              \"https://abcdefghijklmnop.us-east-1.aws.endpoints.huggingface.cloud\"\n",
      "     |          )\n",
      "     |          hf = HuggingFaceEndpoint(\n",
      "     |              endpoint_url=endpoint_url,\n",
      "     |              huggingfacehub_api_token=\"my-api-key\"\n",
      "     |          )\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      HuggingFaceEndpoint\n",
      "     |      langchain.llms.base.LLM\n",
      "     |      langchain.llms.base.BaseLLM\n",
      "     |      langchain.schema.language_model.BaseLanguageModel\n",
      "     |      langchain.load.serializable.Serializable\n",
      "     |      pydantic.main.BaseModel\n",
      "     |      pydantic.utils.Representation\n",
      "     |      langchain.schema.runnable.base.Runnable\n",
      "     |      typing.Generic\n",
      "     |      abc.ABC\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  validate_environment(values: Dict) -> Dict from pydantic.main.ModelMetaclass\n",
      "     |      Validate that api key and python package exists in environment.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __json_encoder__ = pydantic_encoder(obj: Any) -> Any\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  Config = <class 'langchain.llms.huggingface_endpoint.HuggingFaceEndpoi...\n",
      "     |      Configuration for this pydantic object.\n",
      "     |  \n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {'endpoint_url': <class 'str'>, 'huggingfacehub_api_...\n",
      "     |  \n",
      "     |  __class_vars__ = set()\n",
      "     |  \n",
      "     |  __config__ = <class 'langchain.llms.huggingface_endpoint.Config'>\n",
      "     |  \n",
      "     |  __custom_root_type__ = False\n",
      "     |  \n",
      "     |  __exclude_fields__ = {'callback_manager': True, 'callbacks': True, 'me...\n",
      "     |  \n",
      "     |  __fields__ = {'cache': ModelField(name='cache', type=Optional[bool], r...\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "     |  \n",
      "     |  __include_fields__ = None\n",
      "     |  \n",
      "     |  __parameters__ = ()\n",
      "     |  \n",
      "     |  __post_root_validators__ = [(False, <function BaseLLM.raise_deprecatio...\n",
      "     |  \n",
      "     |  __pre_root_validators__ = []\n",
      "     |  \n",
      "     |  __private_attributes__ = {'_lc_kwargs': ModelPrivateAttr(default=Pydan...\n",
      "     |  \n",
      "     |  __schema_cache__ = {}\n",
      "     |  \n",
      "     |  __signature__ = <Signature (*, cache: Optional[bool] = None, ver...fac...\n",
      "     |  \n",
      "     |  __validators__ = {'verbose': [<pydantic.class_validators.Validator obj...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  __call__(self, prompt: 'str', stop: 'Optional[List[str]]' = None, callbacks: 'Callbacks' = None, *, tags: 'Optional[List[str]]' = None, metadata: 'Optional[Dict[str, Any]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Check Cache and run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  __str__(self) -> 'str'\n",
      "     |      Get a string representation of the object for printing.\n",
      "     |  \n",
      "     |  async abatch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      "     |      Default implementation of abatch, which calls ainvoke N times.\n",
      "     |      Subclasses should override this method if they can batch more efficiently.\n",
      "     |  \n",
      "     |  async agenerate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  async agenerate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Asynchronously pass a sequence of prompts and return model generations.\n",
      "     |      \n",
      "     |      This method should make use of batched calls for models that expose a batched\n",
      "     |      API.\n",
      "     |      \n",
      "     |      Use this method when you want to:\n",
      "     |          1. take advantage of batched calls,\n",
      "     |          2. need more output from the model than just the top generated value,\n",
      "     |          3. are building chains that are agnostic to the underlying language model\n",
      "     |              type (e.g., pure text completion models vs chat models).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      "     |              converted to match the format of any language model (string for pure\n",
      "     |              text generation models and BaseMessages for chat models).\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          callbacks: Callbacks to pass through. Used for executing additional\n",
      "     |              functionality, such as logging or streaming, throughout generation.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          An LLMResult, which contains a list of candidate Generations for each input\n",
      "     |              prompt and additional model provider-specific output.\n",
      "     |  \n",
      "     |  async ainvoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Default implementation of ainvoke, which calls invoke in a thread pool.\n",
      "     |      Subclasses should override this method if they can run asynchronously.\n",
      "     |  \n",
      "     |  async apredict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Asynchronously pass a string to the model and return a string prediction.\n",
      "     |      \n",
      "     |      Use this method when calling pure text generation models and only the top\n",
      "     |          candidate generation is needed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: String input to pass to the model.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a string.\n",
      "     |  \n",
      "     |  async apredict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      "     |      Asynchronously pass messages to the model and return a message prediction.\n",
      "     |      \n",
      "     |      Use this method when calling chat models and only the top\n",
      "     |          candidate generation is needed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: A sequence of chat messages corresponding to a single model input.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a message.\n",
      "     |  \n",
      "     |  async astream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'AsyncIterator[str]'\n",
      "     |      Default implementation of astream, which calls ainvoke.\n",
      "     |      Subclasses should override this method if they support streaming output.\n",
      "     |  \n",
      "     |  batch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      "     |      Default implementation of batch, which calls invoke N times.\n",
      "     |      Subclasses should override this method if they can batch more efficiently.\n",
      "     |  \n",
      "     |  dict(self, **kwargs: 'Any') -> 'Dict'\n",
      "     |      Return a dictionary of the LLM.\n",
      "     |  \n",
      "     |  generate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  generate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Pass a sequence of prompts to the model and return model generations.\n",
      "     |      \n",
      "     |      This method should make use of batched calls for models that expose a batched\n",
      "     |      API.\n",
      "     |      \n",
      "     |      Use this method when you want to:\n",
      "     |          1. take advantage of batched calls,\n",
      "     |          2. need more output from the model than just the top generated value,\n",
      "     |          3. are building chains that are agnostic to the underlying language model\n",
      "     |              type (e.g., pure text completion models vs chat models).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      "     |              converted to match the format of any language model (string for pure\n",
      "     |              text generation models and BaseMessages for chat models).\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          callbacks: Callbacks to pass through. Used for executing additional\n",
      "     |              functionality, such as logging or streaming, throughout generation.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          An LLMResult, which contains a list of candidate Generations for each input\n",
      "     |              prompt and additional model provider-specific output.\n",
      "     |  \n",
      "     |  invoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |  \n",
      "     |  predict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Pass a single string input to the model and return a string prediction.\n",
      "     |      \n",
      "     |       Use this method when passing in raw text. If you want to pass in specific\n",
      "     |          types of chat messages, use predict_messages.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: String input to pass to the model.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a string.\n",
      "     |  \n",
      "     |  predict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      "     |      Pass a message sequence to the model and return a message prediction.\n",
      "     |      \n",
      "     |      Use this method when passing in chat messages. If you want to pass in raw text,\n",
      "     |          use predict.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: A sequence of chat messages corresponding to a single model input.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a message.\n",
      "     |  \n",
      "     |  save(self, file_path: 'Union[Path, str]') -> 'None'\n",
      "     |      Save the LLM.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          file_path: Path to file to save the LLM to.\n",
      "     |      \n",
      "     |      Example:\n",
      "     |      .. code-block:: python\n",
      "     |      \n",
      "     |          llm.save(file_path=\"path/llm.yaml\")\n",
      "     |  \n",
      "     |  stream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'Iterator[str]'\n",
      "     |      Default implementation of stream, which calls invoke.\n",
      "     |      Subclasses should override this method if they support streaming output.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  raise_deprecation(values: 'Dict') -> 'Dict' from pydantic.main.ModelMetaclass\n",
      "     |      Raise deprecation warning if callback_manager is used.\n",
      "     |  \n",
      "     |  set_verbose(verbose: 'Optional[bool]') -> 'bool' from pydantic.main.ModelMetaclass\n",
      "     |      If verbose is None, set it.\n",
      "     |      \n",
      "     |      This allows users to pass in None as verbose to access the global setting.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  __orig_bases__ = (langchain.schema.language_model.BaseLanguageModel[st...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.schema.language_model.BaseLanguageModel:\n",
      "     |  \n",
      "     |  get_num_tokens(self, text: 'str') -> 'int'\n",
      "     |      Get the number of tokens present in the text.\n",
      "     |      \n",
      "     |      Useful for checking if an input will fit in a model's context window.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: The string input to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The integer number of tokens in the text.\n",
      "     |  \n",
      "     |  get_num_tokens_from_messages(self, messages: 'List[BaseMessage]') -> 'int'\n",
      "     |      Get the number of tokens in the messages.\n",
      "     |      \n",
      "     |      Useful for checking if an input will fit in a model's context window.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: The message inputs to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The sum of the number of tokens across the messages.\n",
      "     |  \n",
      "     |  get_token_ids(self, text: 'str') -> 'List[int]'\n",
      "     |      Return the ordered ids of the tokens in a text.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: The string input to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          A list of ids corresponding to the tokens in the text, in order they occur\n",
      "     |              in the text.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from langchain.schema.language_model.BaseLanguageModel:\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.load.serializable.Serializable:\n",
      "     |  \n",
      "     |  __init__(self, **kwargs: Any) -> None\n",
      "     |      Create a new model by parsing and validating input data from keyword arguments.\n",
      "     |      \n",
      "     |      Raises ValidationError if the input data cannot be parsed to form a valid model.\n",
      "     |  \n",
      "     |  to_json(self) -> Union[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\n",
      "     |  \n",
      "     |  to_json_not_implemented(self) -> langchain.load.serializable.SerializedNotImplemented\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from langchain.load.serializable.Serializable:\n",
      "     |  \n",
      "     |  lc_attributes\n",
      "     |      Return a list of attribute names that should be included in the\n",
      "     |      serialized kwargs. These attributes must be accepted by the\n",
      "     |      constructor.\n",
      "     |  \n",
      "     |  lc_namespace\n",
      "     |      Return the namespace of the langchain object.\n",
      "     |      eg. [\"langchain\", \"llms\", \"openai\"]\n",
      "     |  \n",
      "     |  lc_secrets\n",
      "     |      Return a map of constructor argument names to secret ids.\n",
      "     |      eg. {\"openai_api_key\": \"OPENAI_API_KEY\"}\n",
      "     |  \n",
      "     |  lc_serializable\n",
      "     |      Return whether or not the class is serializable.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __eq__(self, other: Any) -> bool\n",
      "     |  \n",
      "     |  __getstate__(self) -> 'DictAny'\n",
      "     |  \n",
      "     |  __iter__(self) -> 'TupleGenerator'\n",
      "     |      so `dict(model)` works\n",
      "     |  \n",
      "     |  __repr_args__(self) -> 'ReprArgs'\n",
      "     |  \n",
      "     |  __setattr__(self, name, value)\n",
      "     |  \n",
      "     |  __setstate__(self, state: 'DictAny') -> None\n",
      "     |  \n",
      "     |  copy(self: 'Model', *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, update: Optional[ForwardRef('DictStrAny')] = None, deep: bool = False) -> 'Model'\n",
      "     |      Duplicate a model, optionally choose which fields to include, exclude and change.\n",
      "     |      \n",
      "     |      :param include: fields to include in new model\n",
      "     |      :param exclude: fields to exclude from new model, as with values this takes precedence over include\n",
      "     |      :param update: values to change/add in the new model. Note: the data is not validated before creating\n",
      "     |          the new model: you should trust this data\n",
      "     |      :param deep: set to `True` to make a deep copy of the model\n",
      "     |      :return: new model instance\n",
      "     |  \n",
      "     |  json(self, *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) -> 'unicode'\n",
      "     |      Generate a JSON representation of the model, `include` and `exclude` arguments as per `dict()`.\n",
      "     |      \n",
      "     |      `encoder` is an optional function to supply as `default` to json.dumps(), other arguments as per `json.dumps()`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __get_validators__() -> 'CallableGenerator' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  __try_update_forward_refs__(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      "     |      Same as update_forward_refs but will not raise exception\n",
      "     |      when forward references are not defined.\n",
      "     |  \n",
      "     |  construct(_fields_set: Optional[ForwardRef('SetStr')] = None, **values: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |      Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data.\n",
      "     |      Default values are respected, but no other validation is performed.\n",
      "     |      Behaves as if `Config.extra = 'allow'` was set since it adds all passed values\n",
      "     |  \n",
      "     |  from_orm(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_file(path: Union[str, pathlib.Path], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_obj(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_raw(b: Union[str, bytes], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema(by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}') -> 'DictStrAny' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema_json(*, by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}', **dumps_kwargs: Any) -> 'unicode' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  update_forward_refs(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      "     |      Try to update ForwardRefs on fields based on this Model, globalns and localns.\n",
      "     |  \n",
      "     |  validate(value: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __fields_set__\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.utils.Representation:\n",
      "     |  \n",
      "     |  __pretty__(self, fmt: Callable[[Any], Any], **kwargs: Any) -> Generator[Any, NoneType, NoneType]\n",
      "     |      Used by devtools (https://python-devtools.helpmanual.io/) to provide a human readable representations of objects\n",
      "     |  \n",
      "     |  __repr__(self) -> 'unicode'\n",
      "     |  \n",
      "     |  __repr_name__(self) -> 'unicode'\n",
      "     |      Name of the instance's class, used in __repr__.\n",
      "     |  \n",
      "     |  __repr_str__(self, join_str: 'unicode') -> 'unicode'\n",
      "     |  \n",
      "     |  __rich_repr__(self) -> 'RichReprResult'\n",
      "     |      Get fields for Rich library\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.schema.runnable.base.Runnable:\n",
      "     |  \n",
      "     |  __or__(self, other: 'Union[Runnable[Any, Other], Callable[[Any], Other], Mapping[str, Union[Runnable[Any, Other], Callable[[Any], Other], Any]]]') -> 'RunnableSequence[Input, Other]'\n",
      "     |      Return self|value.\n",
      "     |  \n",
      "     |  __ror__(self, other: 'Union[Runnable[Other, Any], Callable[[Any], Other], Mapping[str, Union[Runnable[Other, Any], Callable[[Other], Any], Any]]]') -> 'RunnableSequence[Other, Output]'\n",
      "     |      Return value|self.\n",
      "     |  \n",
      "     |  async atransform(self, input: 'AsyncIterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'AsyncIterator[Output]'\n",
      "     |      Default implementation of atransform, which buffers input and calls astream.\n",
      "     |      Subclasses should override this method if they can start producing output while\n",
      "     |      input is still being generated.\n",
      "     |  \n",
      "     |  bind(self, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      "     |      Bind arguments to a Runnable, returning a new Runnable.\n",
      "     |  \n",
      "     |  map(self) -> 'Runnable[List[Input], List[Output]]'\n",
      "     |      Return a new Runnable that maps a list of inputs to a list of outputs,\n",
      "     |      by calling invoke() with each input.\n",
      "     |  \n",
      "     |  transform(self, input: 'Iterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'Iterator[Output]'\n",
      "     |      Default implementation of transform, which buffers input and then calls stream.\n",
      "     |      Subclasses should override this method if they can start producing output while\n",
      "     |      input is still being generated.\n",
      "     |  \n",
      "     |  with_config(self, config: 'Optional[RunnableConfig]' = None, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      "     |      Bind config to a Runnable, returning a new Runnable.\n",
      "     |  \n",
      "     |  with_fallbacks(self, fallbacks: 'Sequence[Runnable[Input, Output]]', *, exceptions_to_handle: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,)) -> 'RunnableWithFallbacks[Input, Output]'\n",
      "     |  \n",
      "     |  with_retry(self, *, retry_if_exception_type: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,), wait_exponential_jitter: 'bool' = True, stop_after_attempt: 'int' = 3) -> 'Runnable[Input, Output]'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __class_getitem__(params) from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  __init_subclass__(*args, **kwargs) from pydantic.main.ModelMetaclass\n",
      "     |      This method is called when a class is subclassed.\n",
      "     |      \n",
      "     |      The default implementation does nothing. It may be\n",
      "     |      overridden to extend subclasses.\n",
      "    \n",
      "    class HuggingFaceHub(langchain.llms.base.LLM)\n",
      "     |  HuggingFaceHub(*, cache: Optional[bool] = None, verbose: bool = None, callbacks: Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager, NoneType] = None, callback_manager: Optional[langchain.callbacks.base.BaseCallbackManager] = None, tags: Optional[List[str]] = None, metadata: Optional[Dict[str, Any]] = None, client: Any = None, repo_id: str = 'gpt2', task: Optional[str] = None, model_kwargs: Optional[dict] = None, huggingfacehub_api_token: Optional[str] = None) -> None\n",
      "     |  \n",
      "     |  HuggingFaceHub  models.\n",
      "     |  \n",
      "     |  To use, you should have the ``huggingface_hub`` python package installed, and the\n",
      "     |  environment variable ``HUGGINGFACEHUB_API_TOKEN`` set with your API token, or pass\n",
      "     |  it as a named parameter to the constructor.\n",
      "     |  \n",
      "     |  Only supports `text-generation`, `text2text-generation` and `summarization` for now.\n",
      "     |  \n",
      "     |  Example:\n",
      "     |      .. code-block:: python\n",
      "     |  \n",
      "     |          from langchain.llms import HuggingFaceHub\n",
      "     |          hf = HuggingFaceHub(repo_id=\"gpt2\", huggingfacehub_api_token=\"my-api-key\")\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      HuggingFaceHub\n",
      "     |      langchain.llms.base.LLM\n",
      "     |      langchain.llms.base.BaseLLM\n",
      "     |      langchain.schema.language_model.BaseLanguageModel\n",
      "     |      langchain.load.serializable.Serializable\n",
      "     |      pydantic.main.BaseModel\n",
      "     |      pydantic.utils.Representation\n",
      "     |      langchain.schema.runnable.base.Runnable\n",
      "     |      typing.Generic\n",
      "     |      abc.ABC\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  validate_environment(values: Dict) -> Dict from pydantic.main.ModelMetaclass\n",
      "     |      Validate that api key and python package exists in environment.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __json_encoder__ = pydantic_encoder(obj: Any) -> Any\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  Config = <class 'langchain.llms.huggingface_hub.HuggingFaceHub.Config'...\n",
      "     |      Configuration for this pydantic object.\n",
      "     |  \n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {'client': typing.Any, 'huggingfacehub_api_token': t...\n",
      "     |  \n",
      "     |  __class_vars__ = set()\n",
      "     |  \n",
      "     |  __config__ = <class 'langchain.llms.huggingface_hub.Config'>\n",
      "     |  \n",
      "     |  __custom_root_type__ = False\n",
      "     |  \n",
      "     |  __exclude_fields__ = {'callback_manager': True, 'callbacks': True, 'me...\n",
      "     |  \n",
      "     |  __fields__ = {'cache': ModelField(name='cache', type=Optional[bool], r...\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "     |  \n",
      "     |  __include_fields__ = None\n",
      "     |  \n",
      "     |  __parameters__ = ()\n",
      "     |  \n",
      "     |  __post_root_validators__ = [(False, <function BaseLLM.raise_deprecatio...\n",
      "     |  \n",
      "     |  __pre_root_validators__ = []\n",
      "     |  \n",
      "     |  __private_attributes__ = {'_lc_kwargs': ModelPrivateAttr(default=Pydan...\n",
      "     |  \n",
      "     |  __schema_cache__ = {}\n",
      "     |  \n",
      "     |  __signature__ = <Signature (*, cache: Optional[bool] = None, ver...fac...\n",
      "     |  \n",
      "     |  __validators__ = {'verbose': [<pydantic.class_validators.Validator obj...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  __call__(self, prompt: 'str', stop: 'Optional[List[str]]' = None, callbacks: 'Callbacks' = None, *, tags: 'Optional[List[str]]' = None, metadata: 'Optional[Dict[str, Any]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Check Cache and run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  __str__(self) -> 'str'\n",
      "     |      Get a string representation of the object for printing.\n",
      "     |  \n",
      "     |  async abatch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      "     |      Default implementation of abatch, which calls ainvoke N times.\n",
      "     |      Subclasses should override this method if they can batch more efficiently.\n",
      "     |  \n",
      "     |  async agenerate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  async agenerate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Asynchronously pass a sequence of prompts and return model generations.\n",
      "     |      \n",
      "     |      This method should make use of batched calls for models that expose a batched\n",
      "     |      API.\n",
      "     |      \n",
      "     |      Use this method when you want to:\n",
      "     |          1. take advantage of batched calls,\n",
      "     |          2. need more output from the model than just the top generated value,\n",
      "     |          3. are building chains that are agnostic to the underlying language model\n",
      "     |              type (e.g., pure text completion models vs chat models).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      "     |              converted to match the format of any language model (string for pure\n",
      "     |              text generation models and BaseMessages for chat models).\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          callbacks: Callbacks to pass through. Used for executing additional\n",
      "     |              functionality, such as logging or streaming, throughout generation.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          An LLMResult, which contains a list of candidate Generations for each input\n",
      "     |              prompt and additional model provider-specific output.\n",
      "     |  \n",
      "     |  async ainvoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Default implementation of ainvoke, which calls invoke in a thread pool.\n",
      "     |      Subclasses should override this method if they can run asynchronously.\n",
      "     |  \n",
      "     |  async apredict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Asynchronously pass a string to the model and return a string prediction.\n",
      "     |      \n",
      "     |      Use this method when calling pure text generation models and only the top\n",
      "     |          candidate generation is needed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: String input to pass to the model.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a string.\n",
      "     |  \n",
      "     |  async apredict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      "     |      Asynchronously pass messages to the model and return a message prediction.\n",
      "     |      \n",
      "     |      Use this method when calling chat models and only the top\n",
      "     |          candidate generation is needed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: A sequence of chat messages corresponding to a single model input.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a message.\n",
      "     |  \n",
      "     |  async astream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'AsyncIterator[str]'\n",
      "     |      Default implementation of astream, which calls ainvoke.\n",
      "     |      Subclasses should override this method if they support streaming output.\n",
      "     |  \n",
      "     |  batch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      "     |      Default implementation of batch, which calls invoke N times.\n",
      "     |      Subclasses should override this method if they can batch more efficiently.\n",
      "     |  \n",
      "     |  dict(self, **kwargs: 'Any') -> 'Dict'\n",
      "     |      Return a dictionary of the LLM.\n",
      "     |  \n",
      "     |  generate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  generate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Pass a sequence of prompts to the model and return model generations.\n",
      "     |      \n",
      "     |      This method should make use of batched calls for models that expose a batched\n",
      "     |      API.\n",
      "     |      \n",
      "     |      Use this method when you want to:\n",
      "     |          1. take advantage of batched calls,\n",
      "     |          2. need more output from the model than just the top generated value,\n",
      "     |          3. are building chains that are agnostic to the underlying language model\n",
      "     |              type (e.g., pure text completion models vs chat models).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      "     |              converted to match the format of any language model (string for pure\n",
      "     |              text generation models and BaseMessages for chat models).\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          callbacks: Callbacks to pass through. Used for executing additional\n",
      "     |              functionality, such as logging or streaming, throughout generation.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          An LLMResult, which contains a list of candidate Generations for each input\n",
      "     |              prompt and additional model provider-specific output.\n",
      "     |  \n",
      "     |  invoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |  \n",
      "     |  predict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Pass a single string input to the model and return a string prediction.\n",
      "     |      \n",
      "     |       Use this method when passing in raw text. If you want to pass in specific\n",
      "     |          types of chat messages, use predict_messages.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: String input to pass to the model.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a string.\n",
      "     |  \n",
      "     |  predict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      "     |      Pass a message sequence to the model and return a message prediction.\n",
      "     |      \n",
      "     |      Use this method when passing in chat messages. If you want to pass in raw text,\n",
      "     |          use predict.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: A sequence of chat messages corresponding to a single model input.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a message.\n",
      "     |  \n",
      "     |  save(self, file_path: 'Union[Path, str]') -> 'None'\n",
      "     |      Save the LLM.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          file_path: Path to file to save the LLM to.\n",
      "     |      \n",
      "     |      Example:\n",
      "     |      .. code-block:: python\n",
      "     |      \n",
      "     |          llm.save(file_path=\"path/llm.yaml\")\n",
      "     |  \n",
      "     |  stream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'Iterator[str]'\n",
      "     |      Default implementation of stream, which calls invoke.\n",
      "     |      Subclasses should override this method if they support streaming output.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  raise_deprecation(values: 'Dict') -> 'Dict' from pydantic.main.ModelMetaclass\n",
      "     |      Raise deprecation warning if callback_manager is used.\n",
      "     |  \n",
      "     |  set_verbose(verbose: 'Optional[bool]') -> 'bool' from pydantic.main.ModelMetaclass\n",
      "     |      If verbose is None, set it.\n",
      "     |      \n",
      "     |      This allows users to pass in None as verbose to access the global setting.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  __orig_bases__ = (langchain.schema.language_model.BaseLanguageModel[st...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.schema.language_model.BaseLanguageModel:\n",
      "     |  \n",
      "     |  get_num_tokens(self, text: 'str') -> 'int'\n",
      "     |      Get the number of tokens present in the text.\n",
      "     |      \n",
      "     |      Useful for checking if an input will fit in a model's context window.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: The string input to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The integer number of tokens in the text.\n",
      "     |  \n",
      "     |  get_num_tokens_from_messages(self, messages: 'List[BaseMessage]') -> 'int'\n",
      "     |      Get the number of tokens in the messages.\n",
      "     |      \n",
      "     |      Useful for checking if an input will fit in a model's context window.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: The message inputs to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The sum of the number of tokens across the messages.\n",
      "     |  \n",
      "     |  get_token_ids(self, text: 'str') -> 'List[int]'\n",
      "     |      Return the ordered ids of the tokens in a text.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: The string input to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          A list of ids corresponding to the tokens in the text, in order they occur\n",
      "     |              in the text.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from langchain.schema.language_model.BaseLanguageModel:\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.load.serializable.Serializable:\n",
      "     |  \n",
      "     |  __init__(self, **kwargs: Any) -> None\n",
      "     |      Create a new model by parsing and validating input data from keyword arguments.\n",
      "     |      \n",
      "     |      Raises ValidationError if the input data cannot be parsed to form a valid model.\n",
      "     |  \n",
      "     |  to_json(self) -> Union[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\n",
      "     |  \n",
      "     |  to_json_not_implemented(self) -> langchain.load.serializable.SerializedNotImplemented\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from langchain.load.serializable.Serializable:\n",
      "     |  \n",
      "     |  lc_attributes\n",
      "     |      Return a list of attribute names that should be included in the\n",
      "     |      serialized kwargs. These attributes must be accepted by the\n",
      "     |      constructor.\n",
      "     |  \n",
      "     |  lc_namespace\n",
      "     |      Return the namespace of the langchain object.\n",
      "     |      eg. [\"langchain\", \"llms\", \"openai\"]\n",
      "     |  \n",
      "     |  lc_secrets\n",
      "     |      Return a map of constructor argument names to secret ids.\n",
      "     |      eg. {\"openai_api_key\": \"OPENAI_API_KEY\"}\n",
      "     |  \n",
      "     |  lc_serializable\n",
      "     |      Return whether or not the class is serializable.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __eq__(self, other: Any) -> bool\n",
      "     |  \n",
      "     |  __getstate__(self) -> 'DictAny'\n",
      "     |  \n",
      "     |  __iter__(self) -> 'TupleGenerator'\n",
      "     |      so `dict(model)` works\n",
      "     |  \n",
      "     |  __repr_args__(self) -> 'ReprArgs'\n",
      "     |  \n",
      "     |  __setattr__(self, name, value)\n",
      "     |  \n",
      "     |  __setstate__(self, state: 'DictAny') -> None\n",
      "     |  \n",
      "     |  copy(self: 'Model', *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, update: Optional[ForwardRef('DictStrAny')] = None, deep: bool = False) -> 'Model'\n",
      "     |      Duplicate a model, optionally choose which fields to include, exclude and change.\n",
      "     |      \n",
      "     |      :param include: fields to include in new model\n",
      "     |      :param exclude: fields to exclude from new model, as with values this takes precedence over include\n",
      "     |      :param update: values to change/add in the new model. Note: the data is not validated before creating\n",
      "     |          the new model: you should trust this data\n",
      "     |      :param deep: set to `True` to make a deep copy of the model\n",
      "     |      :return: new model instance\n",
      "     |  \n",
      "     |  json(self, *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) -> 'unicode'\n",
      "     |      Generate a JSON representation of the model, `include` and `exclude` arguments as per `dict()`.\n",
      "     |      \n",
      "     |      `encoder` is an optional function to supply as `default` to json.dumps(), other arguments as per `json.dumps()`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __get_validators__() -> 'CallableGenerator' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  __try_update_forward_refs__(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      "     |      Same as update_forward_refs but will not raise exception\n",
      "     |      when forward references are not defined.\n",
      "     |  \n",
      "     |  construct(_fields_set: Optional[ForwardRef('SetStr')] = None, **values: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |      Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data.\n",
      "     |      Default values are respected, but no other validation is performed.\n",
      "     |      Behaves as if `Config.extra = 'allow'` was set since it adds all passed values\n",
      "     |  \n",
      "     |  from_orm(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_file(path: Union[str, pathlib.Path], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_obj(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_raw(b: Union[str, bytes], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema(by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}') -> 'DictStrAny' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema_json(*, by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}', **dumps_kwargs: Any) -> 'unicode' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  update_forward_refs(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      "     |      Try to update ForwardRefs on fields based on this Model, globalns and localns.\n",
      "     |  \n",
      "     |  validate(value: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __fields_set__\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.utils.Representation:\n",
      "     |  \n",
      "     |  __pretty__(self, fmt: Callable[[Any], Any], **kwargs: Any) -> Generator[Any, NoneType, NoneType]\n",
      "     |      Used by devtools (https://python-devtools.helpmanual.io/) to provide a human readable representations of objects\n",
      "     |  \n",
      "     |  __repr__(self) -> 'unicode'\n",
      "     |  \n",
      "     |  __repr_name__(self) -> 'unicode'\n",
      "     |      Name of the instance's class, used in __repr__.\n",
      "     |  \n",
      "     |  __repr_str__(self, join_str: 'unicode') -> 'unicode'\n",
      "     |  \n",
      "     |  __rich_repr__(self) -> 'RichReprResult'\n",
      "     |      Get fields for Rich library\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.schema.runnable.base.Runnable:\n",
      "     |  \n",
      "     |  __or__(self, other: 'Union[Runnable[Any, Other], Callable[[Any], Other], Mapping[str, Union[Runnable[Any, Other], Callable[[Any], Other], Any]]]') -> 'RunnableSequence[Input, Other]'\n",
      "     |      Return self|value.\n",
      "     |  \n",
      "     |  __ror__(self, other: 'Union[Runnable[Other, Any], Callable[[Any], Other], Mapping[str, Union[Runnable[Other, Any], Callable[[Other], Any], Any]]]') -> 'RunnableSequence[Other, Output]'\n",
      "     |      Return value|self.\n",
      "     |  \n",
      "     |  async atransform(self, input: 'AsyncIterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'AsyncIterator[Output]'\n",
      "     |      Default implementation of atransform, which buffers input and calls astream.\n",
      "     |      Subclasses should override this method if they can start producing output while\n",
      "     |      input is still being generated.\n",
      "     |  \n",
      "     |  bind(self, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      "     |      Bind arguments to a Runnable, returning a new Runnable.\n",
      "     |  \n",
      "     |  map(self) -> 'Runnable[List[Input], List[Output]]'\n",
      "     |      Return a new Runnable that maps a list of inputs to a list of outputs,\n",
      "     |      by calling invoke() with each input.\n",
      "     |  \n",
      "     |  transform(self, input: 'Iterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'Iterator[Output]'\n",
      "     |      Default implementation of transform, which buffers input and then calls stream.\n",
      "     |      Subclasses should override this method if they can start producing output while\n",
      "     |      input is still being generated.\n",
      "     |  \n",
      "     |  with_config(self, config: 'Optional[RunnableConfig]' = None, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      "     |      Bind config to a Runnable, returning a new Runnable.\n",
      "     |  \n",
      "     |  with_fallbacks(self, fallbacks: 'Sequence[Runnable[Input, Output]]', *, exceptions_to_handle: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,)) -> 'RunnableWithFallbacks[Input, Output]'\n",
      "     |  \n",
      "     |  with_retry(self, *, retry_if_exception_type: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,), wait_exponential_jitter: 'bool' = True, stop_after_attempt: 'int' = 3) -> 'Runnable[Input, Output]'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __class_getitem__(params) from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  __init_subclass__(*args, **kwargs) from pydantic.main.ModelMetaclass\n",
      "     |      This method is called when a class is subclassed.\n",
      "     |      \n",
      "     |      The default implementation does nothing. It may be\n",
      "     |      overridden to extend subclasses.\n",
      "    \n",
      "    class HuggingFacePipeline(langchain.llms.base.LLM)\n",
      "     |  HuggingFacePipeline(*, cache: Optional[bool] = None, verbose: bool = None, callbacks: Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager, NoneType] = None, callback_manager: Optional[langchain.callbacks.base.BaseCallbackManager] = None, tags: Optional[List[str]] = None, metadata: Optional[Dict[str, Any]] = None, pipeline: Any = None, model_id: str = 'gpt2', model_kwargs: Optional[dict] = None, pipeline_kwargs: Optional[dict] = None) -> None\n",
      "     |  \n",
      "     |  HuggingFace Pipeline API.\n",
      "     |  \n",
      "     |  To use, you should have the ``transformers`` python package installed.\n",
      "     |  \n",
      "     |  Only supports `text-generation`, `text2text-generation` and `summarization` for now.\n",
      "     |  \n",
      "     |  Example using from_model_id:\n",
      "     |      .. code-block:: python\n",
      "     |  \n",
      "     |          from langchain.llms import HuggingFacePipeline\n",
      "     |          hf = HuggingFacePipeline.from_model_id(\n",
      "     |              model_id=\"gpt2\",\n",
      "     |              task=\"text-generation\",\n",
      "     |              pipeline_kwargs={\"max_new_tokens\": 10},\n",
      "     |          )\n",
      "     |  Example passing pipeline in directly:\n",
      "     |      .. code-block:: python\n",
      "     |  \n",
      "     |          from langchain.llms import HuggingFacePipeline\n",
      "     |          from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
      "     |  \n",
      "     |          model_id = \"gpt2\"\n",
      "     |          tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
      "     |          model = AutoModelForCausalLM.from_pretrained(model_id)\n",
      "     |          pipe = pipeline(\n",
      "     |              \"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=10\n",
      "     |          )\n",
      "     |          hf = HuggingFacePipeline(pipeline=pipe)\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      HuggingFacePipeline\n",
      "     |      langchain.llms.base.LLM\n",
      "     |      langchain.llms.base.BaseLLM\n",
      "     |      langchain.schema.language_model.BaseLanguageModel\n",
      "     |      langchain.load.serializable.Serializable\n",
      "     |      pydantic.main.BaseModel\n",
      "     |      pydantic.utils.Representation\n",
      "     |      langchain.schema.runnable.base.Runnable\n",
      "     |      typing.Generic\n",
      "     |      abc.ABC\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  from_model_id(model_id: str, task: str, device: int = -1, model_kwargs: Optional[dict] = None, pipeline_kwargs: Optional[dict] = None, **kwargs: Any) -> langchain.llms.base.LLM from pydantic.main.ModelMetaclass\n",
      "     |      Construct the pipeline object from model_id and task.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __json_encoder__ = pydantic_encoder(obj: Any) -> Any\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  Config = <class 'langchain.llms.huggingface_pipeline.HuggingFacePipeli...\n",
      "     |      Configuration for this pydantic object.\n",
      "     |  \n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {'model_id': <class 'str'>, 'model_kwargs': typing.O...\n",
      "     |  \n",
      "     |  __class_vars__ = set()\n",
      "     |  \n",
      "     |  __config__ = <class 'langchain.llms.huggingface_pipeline.Config'>\n",
      "     |  \n",
      "     |  __custom_root_type__ = False\n",
      "     |  \n",
      "     |  __exclude_fields__ = {'callback_manager': True, 'callbacks': True, 'me...\n",
      "     |  \n",
      "     |  __fields__ = {'cache': ModelField(name='cache', type=Optional[bool], r...\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "     |  \n",
      "     |  __include_fields__ = None\n",
      "     |  \n",
      "     |  __parameters__ = ()\n",
      "     |  \n",
      "     |  __post_root_validators__ = [(False, <function BaseLLM.raise_deprecatio...\n",
      "     |  \n",
      "     |  __pre_root_validators__ = []\n",
      "     |  \n",
      "     |  __private_attributes__ = {'_lc_kwargs': ModelPrivateAttr(default=Pydan...\n",
      "     |  \n",
      "     |  __schema_cache__ = {}\n",
      "     |  \n",
      "     |  __signature__ = <Signature (*, cache: Optional[bool] = None, ver... pi...\n",
      "     |  \n",
      "     |  __validators__ = {'verbose': [<pydantic.class_validators.Validator obj...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  __call__(self, prompt: 'str', stop: 'Optional[List[str]]' = None, callbacks: 'Callbacks' = None, *, tags: 'Optional[List[str]]' = None, metadata: 'Optional[Dict[str, Any]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Check Cache and run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  __str__(self) -> 'str'\n",
      "     |      Get a string representation of the object for printing.\n",
      "     |  \n",
      "     |  async abatch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      "     |      Default implementation of abatch, which calls ainvoke N times.\n",
      "     |      Subclasses should override this method if they can batch more efficiently.\n",
      "     |  \n",
      "     |  async agenerate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  async agenerate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Asynchronously pass a sequence of prompts and return model generations.\n",
      "     |      \n",
      "     |      This method should make use of batched calls for models that expose a batched\n",
      "     |      API.\n",
      "     |      \n",
      "     |      Use this method when you want to:\n",
      "     |          1. take advantage of batched calls,\n",
      "     |          2. need more output from the model than just the top generated value,\n",
      "     |          3. are building chains that are agnostic to the underlying language model\n",
      "     |              type (e.g., pure text completion models vs chat models).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      "     |              converted to match the format of any language model (string for pure\n",
      "     |              text generation models and BaseMessages for chat models).\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          callbacks: Callbacks to pass through. Used for executing additional\n",
      "     |              functionality, such as logging or streaming, throughout generation.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          An LLMResult, which contains a list of candidate Generations for each input\n",
      "     |              prompt and additional model provider-specific output.\n",
      "     |  \n",
      "     |  async ainvoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Default implementation of ainvoke, which calls invoke in a thread pool.\n",
      "     |      Subclasses should override this method if they can run asynchronously.\n",
      "     |  \n",
      "     |  async apredict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Asynchronously pass a string to the model and return a string prediction.\n",
      "     |      \n",
      "     |      Use this method when calling pure text generation models and only the top\n",
      "     |          candidate generation is needed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: String input to pass to the model.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a string.\n",
      "     |  \n",
      "     |  async apredict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      "     |      Asynchronously pass messages to the model and return a message prediction.\n",
      "     |      \n",
      "     |      Use this method when calling chat models and only the top\n",
      "     |          candidate generation is needed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: A sequence of chat messages corresponding to a single model input.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a message.\n",
      "     |  \n",
      "     |  async astream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'AsyncIterator[str]'\n",
      "     |      Default implementation of astream, which calls ainvoke.\n",
      "     |      Subclasses should override this method if they support streaming output.\n",
      "     |  \n",
      "     |  batch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      "     |      Default implementation of batch, which calls invoke N times.\n",
      "     |      Subclasses should override this method if they can batch more efficiently.\n",
      "     |  \n",
      "     |  dict(self, **kwargs: 'Any') -> 'Dict'\n",
      "     |      Return a dictionary of the LLM.\n",
      "     |  \n",
      "     |  generate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  generate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Pass a sequence of prompts to the model and return model generations.\n",
      "     |      \n",
      "     |      This method should make use of batched calls for models that expose a batched\n",
      "     |      API.\n",
      "     |      \n",
      "     |      Use this method when you want to:\n",
      "     |          1. take advantage of batched calls,\n",
      "     |          2. need more output from the model than just the top generated value,\n",
      "     |          3. are building chains that are agnostic to the underlying language model\n",
      "     |              type (e.g., pure text completion models vs chat models).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      "     |              converted to match the format of any language model (string for pure\n",
      "     |              text generation models and BaseMessages for chat models).\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          callbacks: Callbacks to pass through. Used for executing additional\n",
      "     |              functionality, such as logging or streaming, throughout generation.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          An LLMResult, which contains a list of candidate Generations for each input\n",
      "     |              prompt and additional model provider-specific output.\n",
      "     |  \n",
      "     |  invoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |  \n",
      "     |  predict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Pass a single string input to the model and return a string prediction.\n",
      "     |      \n",
      "     |       Use this method when passing in raw text. If you want to pass in specific\n",
      "     |          types of chat messages, use predict_messages.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: String input to pass to the model.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a string.\n",
      "     |  \n",
      "     |  predict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      "     |      Pass a message sequence to the model and return a message prediction.\n",
      "     |      \n",
      "     |      Use this method when passing in chat messages. If you want to pass in raw text,\n",
      "     |          use predict.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: A sequence of chat messages corresponding to a single model input.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a message.\n",
      "     |  \n",
      "     |  save(self, file_path: 'Union[Path, str]') -> 'None'\n",
      "     |      Save the LLM.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          file_path: Path to file to save the LLM to.\n",
      "     |      \n",
      "     |      Example:\n",
      "     |      .. code-block:: python\n",
      "     |      \n",
      "     |          llm.save(file_path=\"path/llm.yaml\")\n",
      "     |  \n",
      "     |  stream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'Iterator[str]'\n",
      "     |      Default implementation of stream, which calls invoke.\n",
      "     |      Subclasses should override this method if they support streaming output.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  raise_deprecation(values: 'Dict') -> 'Dict' from pydantic.main.ModelMetaclass\n",
      "     |      Raise deprecation warning if callback_manager is used.\n",
      "     |  \n",
      "     |  set_verbose(verbose: 'Optional[bool]') -> 'bool' from pydantic.main.ModelMetaclass\n",
      "     |      If verbose is None, set it.\n",
      "     |      \n",
      "     |      This allows users to pass in None as verbose to access the global setting.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  __orig_bases__ = (langchain.schema.language_model.BaseLanguageModel[st...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.schema.language_model.BaseLanguageModel:\n",
      "     |  \n",
      "     |  get_num_tokens(self, text: 'str') -> 'int'\n",
      "     |      Get the number of tokens present in the text.\n",
      "     |      \n",
      "     |      Useful for checking if an input will fit in a model's context window.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: The string input to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The integer number of tokens in the text.\n",
      "     |  \n",
      "     |  get_num_tokens_from_messages(self, messages: 'List[BaseMessage]') -> 'int'\n",
      "     |      Get the number of tokens in the messages.\n",
      "     |      \n",
      "     |      Useful for checking if an input will fit in a model's context window.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: The message inputs to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The sum of the number of tokens across the messages.\n",
      "     |  \n",
      "     |  get_token_ids(self, text: 'str') -> 'List[int]'\n",
      "     |      Return the ordered ids of the tokens in a text.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: The string input to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          A list of ids corresponding to the tokens in the text, in order they occur\n",
      "     |              in the text.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from langchain.schema.language_model.BaseLanguageModel:\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.load.serializable.Serializable:\n",
      "     |  \n",
      "     |  __init__(self, **kwargs: Any) -> None\n",
      "     |      Create a new model by parsing and validating input data from keyword arguments.\n",
      "     |      \n",
      "     |      Raises ValidationError if the input data cannot be parsed to form a valid model.\n",
      "     |  \n",
      "     |  to_json(self) -> Union[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\n",
      "     |  \n",
      "     |  to_json_not_implemented(self) -> langchain.load.serializable.SerializedNotImplemented\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from langchain.load.serializable.Serializable:\n",
      "     |  \n",
      "     |  lc_attributes\n",
      "     |      Return a list of attribute names that should be included in the\n",
      "     |      serialized kwargs. These attributes must be accepted by the\n",
      "     |      constructor.\n",
      "     |  \n",
      "     |  lc_namespace\n",
      "     |      Return the namespace of the langchain object.\n",
      "     |      eg. [\"langchain\", \"llms\", \"openai\"]\n",
      "     |  \n",
      "     |  lc_secrets\n",
      "     |      Return a map of constructor argument names to secret ids.\n",
      "     |      eg. {\"openai_api_key\": \"OPENAI_API_KEY\"}\n",
      "     |  \n",
      "     |  lc_serializable\n",
      "     |      Return whether or not the class is serializable.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __eq__(self, other: Any) -> bool\n",
      "     |  \n",
      "     |  __getstate__(self) -> 'DictAny'\n",
      "     |  \n",
      "     |  __iter__(self) -> 'TupleGenerator'\n",
      "     |      so `dict(model)` works\n",
      "     |  \n",
      "     |  __repr_args__(self) -> 'ReprArgs'\n",
      "     |  \n",
      "     |  __setattr__(self, name, value)\n",
      "     |  \n",
      "     |  __setstate__(self, state: 'DictAny') -> None\n",
      "     |  \n",
      "     |  copy(self: 'Model', *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, update: Optional[ForwardRef('DictStrAny')] = None, deep: bool = False) -> 'Model'\n",
      "     |      Duplicate a model, optionally choose which fields to include, exclude and change.\n",
      "     |      \n",
      "     |      :param include: fields to include in new model\n",
      "     |      :param exclude: fields to exclude from new model, as with values this takes precedence over include\n",
      "     |      :param update: values to change/add in the new model. Note: the data is not validated before creating\n",
      "     |          the new model: you should trust this data\n",
      "     |      :param deep: set to `True` to make a deep copy of the model\n",
      "     |      :return: new model instance\n",
      "     |  \n",
      "     |  json(self, *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) -> 'unicode'\n",
      "     |      Generate a JSON representation of the model, `include` and `exclude` arguments as per `dict()`.\n",
      "     |      \n",
      "     |      `encoder` is an optional function to supply as `default` to json.dumps(), other arguments as per `json.dumps()`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __get_validators__() -> 'CallableGenerator' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  __try_update_forward_refs__(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      "     |      Same as update_forward_refs but will not raise exception\n",
      "     |      when forward references are not defined.\n",
      "     |  \n",
      "     |  construct(_fields_set: Optional[ForwardRef('SetStr')] = None, **values: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |      Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data.\n",
      "     |      Default values are respected, but no other validation is performed.\n",
      "     |      Behaves as if `Config.extra = 'allow'` was set since it adds all passed values\n",
      "     |  \n",
      "     |  from_orm(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_file(path: Union[str, pathlib.Path], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_obj(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_raw(b: Union[str, bytes], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema(by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}') -> 'DictStrAny' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema_json(*, by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}', **dumps_kwargs: Any) -> 'unicode' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  update_forward_refs(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      "     |      Try to update ForwardRefs on fields based on this Model, globalns and localns.\n",
      "     |  \n",
      "     |  validate(value: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __fields_set__\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.utils.Representation:\n",
      "     |  \n",
      "     |  __pretty__(self, fmt: Callable[[Any], Any], **kwargs: Any) -> Generator[Any, NoneType, NoneType]\n",
      "     |      Used by devtools (https://python-devtools.helpmanual.io/) to provide a human readable representations of objects\n",
      "     |  \n",
      "     |  __repr__(self) -> 'unicode'\n",
      "     |  \n",
      "     |  __repr_name__(self) -> 'unicode'\n",
      "     |      Name of the instance's class, used in __repr__.\n",
      "     |  \n",
      "     |  __repr_str__(self, join_str: 'unicode') -> 'unicode'\n",
      "     |  \n",
      "     |  __rich_repr__(self) -> 'RichReprResult'\n",
      "     |      Get fields for Rich library\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.schema.runnable.base.Runnable:\n",
      "     |  \n",
      "     |  __or__(self, other: 'Union[Runnable[Any, Other], Callable[[Any], Other], Mapping[str, Union[Runnable[Any, Other], Callable[[Any], Other], Any]]]') -> 'RunnableSequence[Input, Other]'\n",
      "     |      Return self|value.\n",
      "     |  \n",
      "     |  __ror__(self, other: 'Union[Runnable[Other, Any], Callable[[Any], Other], Mapping[str, Union[Runnable[Other, Any], Callable[[Other], Any], Any]]]') -> 'RunnableSequence[Other, Output]'\n",
      "     |      Return value|self.\n",
      "     |  \n",
      "     |  async atransform(self, input: 'AsyncIterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'AsyncIterator[Output]'\n",
      "     |      Default implementation of atransform, which buffers input and calls astream.\n",
      "     |      Subclasses should override this method if they can start producing output while\n",
      "     |      input is still being generated.\n",
      "     |  \n",
      "     |  bind(self, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      "     |      Bind arguments to a Runnable, returning a new Runnable.\n",
      "     |  \n",
      "     |  map(self) -> 'Runnable[List[Input], List[Output]]'\n",
      "     |      Return a new Runnable that maps a list of inputs to a list of outputs,\n",
      "     |      by calling invoke() with each input.\n",
      "     |  \n",
      "     |  transform(self, input: 'Iterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'Iterator[Output]'\n",
      "     |      Default implementation of transform, which buffers input and then calls stream.\n",
      "     |      Subclasses should override this method if they can start producing output while\n",
      "     |      input is still being generated.\n",
      "     |  \n",
      "     |  with_config(self, config: 'Optional[RunnableConfig]' = None, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      "     |      Bind config to a Runnable, returning a new Runnable.\n",
      "     |  \n",
      "     |  with_fallbacks(self, fallbacks: 'Sequence[Runnable[Input, Output]]', *, exceptions_to_handle: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,)) -> 'RunnableWithFallbacks[Input, Output]'\n",
      "     |  \n",
      "     |  with_retry(self, *, retry_if_exception_type: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,), wait_exponential_jitter: 'bool' = True, stop_after_attempt: 'int' = 3) -> 'Runnable[Input, Output]'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __class_getitem__(params) from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  __init_subclass__(*args, **kwargs) from pydantic.main.ModelMetaclass\n",
      "     |      This method is called when a class is subclassed.\n",
      "     |      \n",
      "     |      The default implementation does nothing. It may be\n",
      "     |      overridden to extend subclasses.\n",
      "    \n",
      "    class HuggingFaceTextGenInference(langchain.llms.base.LLM)\n",
      "     |  HuggingFaceTextGenInference(*, cache: Optional[bool] = None, verbose: bool = None, callbacks: Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager, NoneType] = None, callback_manager: Optional[langchain.callbacks.base.BaseCallbackManager] = None, tags: Optional[List[str]] = None, metadata: Optional[Dict[str, Any]] = None, max_new_tokens: int = 512, top_k: Optional[int] = None, top_p: Optional[float] = 0.95, typical_p: Optional[float] = 0.95, temperature: Optional[float] = 0.8, repetition_penalty: Optional[float] = None, return_full_text: bool = False, truncate: Optional[int] = None, stop_sequences: List[str] = None, seed: Optional[int] = None, inference_server_url: str = '', timeout: int = 120, streaming: bool = False, do_sample: bool = False, watermark: bool = False, server_kwargs: Dict[str, Any] = None, model_kwargs: Dict[str, Any] = None, client: Any = None, async_client: Any = None) -> None\n",
      "     |  \n",
      "     |  HuggingFace text generation API.\n",
      "     |  \n",
      "     |  To use, you should have the `text-generation` python package installed and\n",
      "     |  a text-generation server running.\n",
      "     |  \n",
      "     |  Example:\n",
      "     |      .. code-block:: python\n",
      "     |  \n",
      "     |          # Basic Example (no streaming)\n",
      "     |          llm = HuggingFaceTextGenInference(\n",
      "     |              inference_server_url=\"http://localhost:8010/\",\n",
      "     |              max_new_tokens=512,\n",
      "     |              top_k=10,\n",
      "     |              top_p=0.95,\n",
      "     |              typical_p=0.95,\n",
      "     |              temperature=0.01,\n",
      "     |              repetition_penalty=1.03,\n",
      "     |          )\n",
      "     |          print(llm(\"What is Deep Learning?\"))\n",
      "     |  \n",
      "     |          # Streaming response example\n",
      "     |          from langchain.callbacks import streaming_stdout\n",
      "     |  \n",
      "     |          callbacks = [streaming_stdout.StreamingStdOutCallbackHandler()]\n",
      "     |          llm = HuggingFaceTextGenInference(\n",
      "     |              inference_server_url=\"http://localhost:8010/\",\n",
      "     |              max_new_tokens=512,\n",
      "     |              top_k=10,\n",
      "     |              top_p=0.95,\n",
      "     |              typical_p=0.95,\n",
      "     |              temperature=0.01,\n",
      "     |              repetition_penalty=1.03,\n",
      "     |              callbacks=callbacks,\n",
      "     |              streaming=True\n",
      "     |          )\n",
      "     |          print(llm(\"What is Deep Learning?\"))\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      HuggingFaceTextGenInference\n",
      "     |      langchain.llms.base.LLM\n",
      "     |      langchain.llms.base.BaseLLM\n",
      "     |      langchain.schema.language_model.BaseLanguageModel\n",
      "     |      langchain.load.serializable.Serializable\n",
      "     |      pydantic.main.BaseModel\n",
      "     |      pydantic.utils.Representation\n",
      "     |      langchain.schema.runnable.base.Runnable\n",
      "     |      typing.Generic\n",
      "     |      abc.ABC\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  build_extra(values: Dict[str, Any]) -> Dict[str, Any] from pydantic.main.ModelMetaclass\n",
      "     |      Build extra kwargs from additional params that were passed in.\n",
      "     |  \n",
      "     |  validate_environment(values: Dict) -> Dict from pydantic.main.ModelMetaclass\n",
      "     |      Validate that python package exists in environment.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __json_encoder__ = pydantic_encoder(obj: Any) -> Any\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  Config = <class 'langchain.llms.huggingface_text_gen_inference.Hugging...\n",
      "     |      Configuration for this pydantic object.\n",
      "     |  \n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {'async_client': typing.Any, 'client': typing.Any, '...\n",
      "     |  \n",
      "     |  __class_vars__ = set()\n",
      "     |  \n",
      "     |  __config__ = <class 'langchain.llms.huggingface_text_gen_inference.Con...\n",
      "     |  \n",
      "     |  __custom_root_type__ = False\n",
      "     |  \n",
      "     |  __exclude_fields__ = {'callback_manager': True, 'callbacks': True, 'me...\n",
      "     |  \n",
      "     |  __fields__ = {'async_client': ModelField(name='async_client', type=Opt...\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "     |  \n",
      "     |  __include_fields__ = None\n",
      "     |  \n",
      "     |  __parameters__ = ()\n",
      "     |  \n",
      "     |  __post_root_validators__ = [(False, <function BaseLLM.raise_deprecatio...\n",
      "     |  \n",
      "     |  __pre_root_validators__ = [<function HuggingFaceTextGenInference.build...\n",
      "     |  \n",
      "     |  __private_attributes__ = {'_lc_kwargs': ModelPrivateAttr(default=Pydan...\n",
      "     |  \n",
      "     |  __schema_cache__ = {}\n",
      "     |  \n",
      "     |  __signature__ = <Signature (*, cache: Optional[bool] = None, ver...t: ...\n",
      "     |  \n",
      "     |  __validators__ = {'verbose': [<pydantic.class_validators.Validator obj...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  __call__(self, prompt: 'str', stop: 'Optional[List[str]]' = None, callbacks: 'Callbacks' = None, *, tags: 'Optional[List[str]]' = None, metadata: 'Optional[Dict[str, Any]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Check Cache and run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  __str__(self) -> 'str'\n",
      "     |      Get a string representation of the object for printing.\n",
      "     |  \n",
      "     |  async abatch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      "     |      Default implementation of abatch, which calls ainvoke N times.\n",
      "     |      Subclasses should override this method if they can batch more efficiently.\n",
      "     |  \n",
      "     |  async agenerate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  async agenerate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Asynchronously pass a sequence of prompts and return model generations.\n",
      "     |      \n",
      "     |      This method should make use of batched calls for models that expose a batched\n",
      "     |      API.\n",
      "     |      \n",
      "     |      Use this method when you want to:\n",
      "     |          1. take advantage of batched calls,\n",
      "     |          2. need more output from the model than just the top generated value,\n",
      "     |          3. are building chains that are agnostic to the underlying language model\n",
      "     |              type (e.g., pure text completion models vs chat models).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      "     |              converted to match the format of any language model (string for pure\n",
      "     |              text generation models and BaseMessages for chat models).\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          callbacks: Callbacks to pass through. Used for executing additional\n",
      "     |              functionality, such as logging or streaming, throughout generation.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          An LLMResult, which contains a list of candidate Generations for each input\n",
      "     |              prompt and additional model provider-specific output.\n",
      "     |  \n",
      "     |  async ainvoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Default implementation of ainvoke, which calls invoke in a thread pool.\n",
      "     |      Subclasses should override this method if they can run asynchronously.\n",
      "     |  \n",
      "     |  async apredict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Asynchronously pass a string to the model and return a string prediction.\n",
      "     |      \n",
      "     |      Use this method when calling pure text generation models and only the top\n",
      "     |          candidate generation is needed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: String input to pass to the model.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a string.\n",
      "     |  \n",
      "     |  async apredict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      "     |      Asynchronously pass messages to the model and return a message prediction.\n",
      "     |      \n",
      "     |      Use this method when calling chat models and only the top\n",
      "     |          candidate generation is needed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: A sequence of chat messages corresponding to a single model input.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a message.\n",
      "     |  \n",
      "     |  async astream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'AsyncIterator[str]'\n",
      "     |      Default implementation of astream, which calls ainvoke.\n",
      "     |      Subclasses should override this method if they support streaming output.\n",
      "     |  \n",
      "     |  batch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      "     |      Default implementation of batch, which calls invoke N times.\n",
      "     |      Subclasses should override this method if they can batch more efficiently.\n",
      "     |  \n",
      "     |  dict(self, **kwargs: 'Any') -> 'Dict'\n",
      "     |      Return a dictionary of the LLM.\n",
      "     |  \n",
      "     |  generate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  generate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Pass a sequence of prompts to the model and return model generations.\n",
      "     |      \n",
      "     |      This method should make use of batched calls for models that expose a batched\n",
      "     |      API.\n",
      "     |      \n",
      "     |      Use this method when you want to:\n",
      "     |          1. take advantage of batched calls,\n",
      "     |          2. need more output from the model than just the top generated value,\n",
      "     |          3. are building chains that are agnostic to the underlying language model\n",
      "     |              type (e.g., pure text completion models vs chat models).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      "     |              converted to match the format of any language model (string for pure\n",
      "     |              text generation models and BaseMessages for chat models).\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          callbacks: Callbacks to pass through. Used for executing additional\n",
      "     |              functionality, such as logging or streaming, throughout generation.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          An LLMResult, which contains a list of candidate Generations for each input\n",
      "     |              prompt and additional model provider-specific output.\n",
      "     |  \n",
      "     |  invoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |  \n",
      "     |  predict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Pass a single string input to the model and return a string prediction.\n",
      "     |      \n",
      "     |       Use this method when passing in raw text. If you want to pass in specific\n",
      "     |          types of chat messages, use predict_messages.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: String input to pass to the model.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a string.\n",
      "     |  \n",
      "     |  predict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      "     |      Pass a message sequence to the model and return a message prediction.\n",
      "     |      \n",
      "     |      Use this method when passing in chat messages. If you want to pass in raw text,\n",
      "     |          use predict.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: A sequence of chat messages corresponding to a single model input.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a message.\n",
      "     |  \n",
      "     |  save(self, file_path: 'Union[Path, str]') -> 'None'\n",
      "     |      Save the LLM.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          file_path: Path to file to save the LLM to.\n",
      "     |      \n",
      "     |      Example:\n",
      "     |      .. code-block:: python\n",
      "     |      \n",
      "     |          llm.save(file_path=\"path/llm.yaml\")\n",
      "     |  \n",
      "     |  stream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'Iterator[str]'\n",
      "     |      Default implementation of stream, which calls invoke.\n",
      "     |      Subclasses should override this method if they support streaming output.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  raise_deprecation(values: 'Dict') -> 'Dict' from pydantic.main.ModelMetaclass\n",
      "     |      Raise deprecation warning if callback_manager is used.\n",
      "     |  \n",
      "     |  set_verbose(verbose: 'Optional[bool]') -> 'bool' from pydantic.main.ModelMetaclass\n",
      "     |      If verbose is None, set it.\n",
      "     |      \n",
      "     |      This allows users to pass in None as verbose to access the global setting.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  __orig_bases__ = (langchain.schema.language_model.BaseLanguageModel[st...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.schema.language_model.BaseLanguageModel:\n",
      "     |  \n",
      "     |  get_num_tokens(self, text: 'str') -> 'int'\n",
      "     |      Get the number of tokens present in the text.\n",
      "     |      \n",
      "     |      Useful for checking if an input will fit in a model's context window.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: The string input to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The integer number of tokens in the text.\n",
      "     |  \n",
      "     |  get_num_tokens_from_messages(self, messages: 'List[BaseMessage]') -> 'int'\n",
      "     |      Get the number of tokens in the messages.\n",
      "     |      \n",
      "     |      Useful for checking if an input will fit in a model's context window.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: The message inputs to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The sum of the number of tokens across the messages.\n",
      "     |  \n",
      "     |  get_token_ids(self, text: 'str') -> 'List[int]'\n",
      "     |      Return the ordered ids of the tokens in a text.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: The string input to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          A list of ids corresponding to the tokens in the text, in order they occur\n",
      "     |              in the text.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from langchain.schema.language_model.BaseLanguageModel:\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.load.serializable.Serializable:\n",
      "     |  \n",
      "     |  __init__(self, **kwargs: Any) -> None\n",
      "     |      Create a new model by parsing and validating input data from keyword arguments.\n",
      "     |      \n",
      "     |      Raises ValidationError if the input data cannot be parsed to form a valid model.\n",
      "     |  \n",
      "     |  to_json(self) -> Union[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\n",
      "     |  \n",
      "     |  to_json_not_implemented(self) -> langchain.load.serializable.SerializedNotImplemented\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from langchain.load.serializable.Serializable:\n",
      "     |  \n",
      "     |  lc_attributes\n",
      "     |      Return a list of attribute names that should be included in the\n",
      "     |      serialized kwargs. These attributes must be accepted by the\n",
      "     |      constructor.\n",
      "     |  \n",
      "     |  lc_namespace\n",
      "     |      Return the namespace of the langchain object.\n",
      "     |      eg. [\"langchain\", \"llms\", \"openai\"]\n",
      "     |  \n",
      "     |  lc_secrets\n",
      "     |      Return a map of constructor argument names to secret ids.\n",
      "     |      eg. {\"openai_api_key\": \"OPENAI_API_KEY\"}\n",
      "     |  \n",
      "     |  lc_serializable\n",
      "     |      Return whether or not the class is serializable.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __eq__(self, other: Any) -> bool\n",
      "     |  \n",
      "     |  __getstate__(self) -> 'DictAny'\n",
      "     |  \n",
      "     |  __iter__(self) -> 'TupleGenerator'\n",
      "     |      so `dict(model)` works\n",
      "     |  \n",
      "     |  __repr_args__(self) -> 'ReprArgs'\n",
      "     |  \n",
      "     |  __setattr__(self, name, value)\n",
      "     |  \n",
      "     |  __setstate__(self, state: 'DictAny') -> None\n",
      "     |  \n",
      "     |  copy(self: 'Model', *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, update: Optional[ForwardRef('DictStrAny')] = None, deep: bool = False) -> 'Model'\n",
      "     |      Duplicate a model, optionally choose which fields to include, exclude and change.\n",
      "     |      \n",
      "     |      :param include: fields to include in new model\n",
      "     |      :param exclude: fields to exclude from new model, as with values this takes precedence over include\n",
      "     |      :param update: values to change/add in the new model. Note: the data is not validated before creating\n",
      "     |          the new model: you should trust this data\n",
      "     |      :param deep: set to `True` to make a deep copy of the model\n",
      "     |      :return: new model instance\n",
      "     |  \n",
      "     |  json(self, *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) -> 'unicode'\n",
      "     |      Generate a JSON representation of the model, `include` and `exclude` arguments as per `dict()`.\n",
      "     |      \n",
      "     |      `encoder` is an optional function to supply as `default` to json.dumps(), other arguments as per `json.dumps()`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __get_validators__() -> 'CallableGenerator' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  __try_update_forward_refs__(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      "     |      Same as update_forward_refs but will not raise exception\n",
      "     |      when forward references are not defined.\n",
      "     |  \n",
      "     |  construct(_fields_set: Optional[ForwardRef('SetStr')] = None, **values: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |      Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data.\n",
      "     |      Default values are respected, but no other validation is performed.\n",
      "     |      Behaves as if `Config.extra = 'allow'` was set since it adds all passed values\n",
      "     |  \n",
      "     |  from_orm(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_file(path: Union[str, pathlib.Path], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_obj(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_raw(b: Union[str, bytes], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema(by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}') -> 'DictStrAny' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema_json(*, by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}', **dumps_kwargs: Any) -> 'unicode' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  update_forward_refs(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      "     |      Try to update ForwardRefs on fields based on this Model, globalns and localns.\n",
      "     |  \n",
      "     |  validate(value: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __fields_set__\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.utils.Representation:\n",
      "     |  \n",
      "     |  __pretty__(self, fmt: Callable[[Any], Any], **kwargs: Any) -> Generator[Any, NoneType, NoneType]\n",
      "     |      Used by devtools (https://python-devtools.helpmanual.io/) to provide a human readable representations of objects\n",
      "     |  \n",
      "     |  __repr__(self) -> 'unicode'\n",
      "     |  \n",
      "     |  __repr_name__(self) -> 'unicode'\n",
      "     |      Name of the instance's class, used in __repr__.\n",
      "     |  \n",
      "     |  __repr_str__(self, join_str: 'unicode') -> 'unicode'\n",
      "     |  \n",
      "     |  __rich_repr__(self) -> 'RichReprResult'\n",
      "     |      Get fields for Rich library\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.schema.runnable.base.Runnable:\n",
      "     |  \n",
      "     |  __or__(self, other: 'Union[Runnable[Any, Other], Callable[[Any], Other], Mapping[str, Union[Runnable[Any, Other], Callable[[Any], Other], Any]]]') -> 'RunnableSequence[Input, Other]'\n",
      "     |      Return self|value.\n",
      "     |  \n",
      "     |  __ror__(self, other: 'Union[Runnable[Other, Any], Callable[[Any], Other], Mapping[str, Union[Runnable[Other, Any], Callable[[Other], Any], Any]]]') -> 'RunnableSequence[Other, Output]'\n",
      "     |      Return value|self.\n",
      "     |  \n",
      "     |  async atransform(self, input: 'AsyncIterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'AsyncIterator[Output]'\n",
      "     |      Default implementation of atransform, which buffers input and calls astream.\n",
      "     |      Subclasses should override this method if they can start producing output while\n",
      "     |      input is still being generated.\n",
      "     |  \n",
      "     |  bind(self, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      "     |      Bind arguments to a Runnable, returning a new Runnable.\n",
      "     |  \n",
      "     |  map(self) -> 'Runnable[List[Input], List[Output]]'\n",
      "     |      Return a new Runnable that maps a list of inputs to a list of outputs,\n",
      "     |      by calling invoke() with each input.\n",
      "     |  \n",
      "     |  transform(self, input: 'Iterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'Iterator[Output]'\n",
      "     |      Default implementation of transform, which buffers input and then calls stream.\n",
      "     |      Subclasses should override this method if they can start producing output while\n",
      "     |      input is still being generated.\n",
      "     |  \n",
      "     |  with_config(self, config: 'Optional[RunnableConfig]' = None, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      "     |      Bind config to a Runnable, returning a new Runnable.\n",
      "     |  \n",
      "     |  with_fallbacks(self, fallbacks: 'Sequence[Runnable[Input, Output]]', *, exceptions_to_handle: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,)) -> 'RunnableWithFallbacks[Input, Output]'\n",
      "     |  \n",
      "     |  with_retry(self, *, retry_if_exception_type: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,), wait_exponential_jitter: 'bool' = True, stop_after_attempt: 'int' = 3) -> 'Runnable[Input, Output]'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __class_getitem__(params) from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  __init_subclass__(*args, **kwargs) from pydantic.main.ModelMetaclass\n",
      "     |      This method is called when a class is subclassed.\n",
      "     |      \n",
      "     |      The default implementation does nothing. It may be\n",
      "     |      overridden to extend subclasses.\n",
      "    \n",
      "    class HumanInputLLM(langchain.llms.base.LLM)\n",
      "     |  HumanInputLLM(*, cache: Optional[bool] = None, verbose: bool = None, callbacks: Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager, NoneType] = None, callback_manager: Optional[langchain.callbacks.base.BaseCallbackManager] = None, tags: Optional[List[str]] = None, metadata: Optional[Dict[str, Any]] = None, input_func: Callable = None, prompt_func: Callable[[str], NoneType] = None, separator: str = '\\n', input_kwargs: Mapping[str, Any] = {}, prompt_kwargs: Mapping[str, Any] = {}) -> None\n",
      "     |  \n",
      "     |  It returns user input as the response.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      HumanInputLLM\n",
      "     |      langchain.llms.base.LLM\n",
      "     |      langchain.llms.base.BaseLLM\n",
      "     |      langchain.schema.language_model.BaseLanguageModel\n",
      "     |      langchain.load.serializable.Serializable\n",
      "     |      pydantic.main.BaseModel\n",
      "     |      pydantic.utils.Representation\n",
      "     |      langchain.schema.runnable.base.Runnable\n",
      "     |      typing.Generic\n",
      "     |      abc.ABC\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __json_encoder__ = pydantic_encoder(obj: Any) -> Any\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {'input_func': typing.Callable, 'input_kwargs': typi...\n",
      "     |  \n",
      "     |  __class_vars__ = set()\n",
      "     |  \n",
      "     |  __config__ = <class 'langchain.llms.human.Config'>\n",
      "     |  \n",
      "     |  __custom_root_type__ = False\n",
      "     |  \n",
      "     |  __exclude_fields__ = {'callback_manager': True, 'callbacks': True, 'me...\n",
      "     |  \n",
      "     |  __fields__ = {'cache': ModelField(name='cache', type=Optional[bool], r...\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "     |  \n",
      "     |  __include_fields__ = None\n",
      "     |  \n",
      "     |  __parameters__ = ()\n",
      "     |  \n",
      "     |  __post_root_validators__ = [(False, <function BaseLLM.raise_deprecatio...\n",
      "     |  \n",
      "     |  __pre_root_validators__ = []\n",
      "     |  \n",
      "     |  __private_attributes__ = {'_lc_kwargs': ModelPrivateAttr(default=Pydan...\n",
      "     |  \n",
      "     |  __schema_cache__ = {}\n",
      "     |  \n",
      "     |  __signature__ = <Signature (*, cache: Optional[bool] = None, ver..., p...\n",
      "     |  \n",
      "     |  __validators__ = {'verbose': [<pydantic.class_validators.Validator obj...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  __call__(self, prompt: 'str', stop: 'Optional[List[str]]' = None, callbacks: 'Callbacks' = None, *, tags: 'Optional[List[str]]' = None, metadata: 'Optional[Dict[str, Any]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Check Cache and run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  __str__(self) -> 'str'\n",
      "     |      Get a string representation of the object for printing.\n",
      "     |  \n",
      "     |  async abatch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      "     |      Default implementation of abatch, which calls ainvoke N times.\n",
      "     |      Subclasses should override this method if they can batch more efficiently.\n",
      "     |  \n",
      "     |  async agenerate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  async agenerate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Asynchronously pass a sequence of prompts and return model generations.\n",
      "     |      \n",
      "     |      This method should make use of batched calls for models that expose a batched\n",
      "     |      API.\n",
      "     |      \n",
      "     |      Use this method when you want to:\n",
      "     |          1. take advantage of batched calls,\n",
      "     |          2. need more output from the model than just the top generated value,\n",
      "     |          3. are building chains that are agnostic to the underlying language model\n",
      "     |              type (e.g., pure text completion models vs chat models).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      "     |              converted to match the format of any language model (string for pure\n",
      "     |              text generation models and BaseMessages for chat models).\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          callbacks: Callbacks to pass through. Used for executing additional\n",
      "     |              functionality, such as logging or streaming, throughout generation.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          An LLMResult, which contains a list of candidate Generations for each input\n",
      "     |              prompt and additional model provider-specific output.\n",
      "     |  \n",
      "     |  async ainvoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Default implementation of ainvoke, which calls invoke in a thread pool.\n",
      "     |      Subclasses should override this method if they can run asynchronously.\n",
      "     |  \n",
      "     |  async apredict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Asynchronously pass a string to the model and return a string prediction.\n",
      "     |      \n",
      "     |      Use this method when calling pure text generation models and only the top\n",
      "     |          candidate generation is needed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: String input to pass to the model.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a string.\n",
      "     |  \n",
      "     |  async apredict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      "     |      Asynchronously pass messages to the model and return a message prediction.\n",
      "     |      \n",
      "     |      Use this method when calling chat models and only the top\n",
      "     |          candidate generation is needed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: A sequence of chat messages corresponding to a single model input.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a message.\n",
      "     |  \n",
      "     |  async astream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'AsyncIterator[str]'\n",
      "     |      Default implementation of astream, which calls ainvoke.\n",
      "     |      Subclasses should override this method if they support streaming output.\n",
      "     |  \n",
      "     |  batch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      "     |      Default implementation of batch, which calls invoke N times.\n",
      "     |      Subclasses should override this method if they can batch more efficiently.\n",
      "     |  \n",
      "     |  dict(self, **kwargs: 'Any') -> 'Dict'\n",
      "     |      Return a dictionary of the LLM.\n",
      "     |  \n",
      "     |  generate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  generate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Pass a sequence of prompts to the model and return model generations.\n",
      "     |      \n",
      "     |      This method should make use of batched calls for models that expose a batched\n",
      "     |      API.\n",
      "     |      \n",
      "     |      Use this method when you want to:\n",
      "     |          1. take advantage of batched calls,\n",
      "     |          2. need more output from the model than just the top generated value,\n",
      "     |          3. are building chains that are agnostic to the underlying language model\n",
      "     |              type (e.g., pure text completion models vs chat models).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      "     |              converted to match the format of any language model (string for pure\n",
      "     |              text generation models and BaseMessages for chat models).\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          callbacks: Callbacks to pass through. Used for executing additional\n",
      "     |              functionality, such as logging or streaming, throughout generation.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          An LLMResult, which contains a list of candidate Generations for each input\n",
      "     |              prompt and additional model provider-specific output.\n",
      "     |  \n",
      "     |  invoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |  \n",
      "     |  predict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Pass a single string input to the model and return a string prediction.\n",
      "     |      \n",
      "     |       Use this method when passing in raw text. If you want to pass in specific\n",
      "     |          types of chat messages, use predict_messages.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: String input to pass to the model.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a string.\n",
      "     |  \n",
      "     |  predict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      "     |      Pass a message sequence to the model and return a message prediction.\n",
      "     |      \n",
      "     |      Use this method when passing in chat messages. If you want to pass in raw text,\n",
      "     |          use predict.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: A sequence of chat messages corresponding to a single model input.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a message.\n",
      "     |  \n",
      "     |  save(self, file_path: 'Union[Path, str]') -> 'None'\n",
      "     |      Save the LLM.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          file_path: Path to file to save the LLM to.\n",
      "     |      \n",
      "     |      Example:\n",
      "     |      .. code-block:: python\n",
      "     |      \n",
      "     |          llm.save(file_path=\"path/llm.yaml\")\n",
      "     |  \n",
      "     |  stream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'Iterator[str]'\n",
      "     |      Default implementation of stream, which calls invoke.\n",
      "     |      Subclasses should override this method if they support streaming output.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  raise_deprecation(values: 'Dict') -> 'Dict' from pydantic.main.ModelMetaclass\n",
      "     |      Raise deprecation warning if callback_manager is used.\n",
      "     |  \n",
      "     |  set_verbose(verbose: 'Optional[bool]') -> 'bool' from pydantic.main.ModelMetaclass\n",
      "     |      If verbose is None, set it.\n",
      "     |      \n",
      "     |      This allows users to pass in None as verbose to access the global setting.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  Config = <class 'langchain.llms.base.BaseLLM.Config'>\n",
      "     |      Configuration for this pydantic object.\n",
      "     |  \n",
      "     |  \n",
      "     |  __orig_bases__ = (langchain.schema.language_model.BaseLanguageModel[st...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.schema.language_model.BaseLanguageModel:\n",
      "     |  \n",
      "     |  get_num_tokens(self, text: 'str') -> 'int'\n",
      "     |      Get the number of tokens present in the text.\n",
      "     |      \n",
      "     |      Useful for checking if an input will fit in a model's context window.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: The string input to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The integer number of tokens in the text.\n",
      "     |  \n",
      "     |  get_num_tokens_from_messages(self, messages: 'List[BaseMessage]') -> 'int'\n",
      "     |      Get the number of tokens in the messages.\n",
      "     |      \n",
      "     |      Useful for checking if an input will fit in a model's context window.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: The message inputs to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The sum of the number of tokens across the messages.\n",
      "     |  \n",
      "     |  get_token_ids(self, text: 'str') -> 'List[int]'\n",
      "     |      Return the ordered ids of the tokens in a text.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: The string input to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          A list of ids corresponding to the tokens in the text, in order they occur\n",
      "     |              in the text.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from langchain.schema.language_model.BaseLanguageModel:\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.load.serializable.Serializable:\n",
      "     |  \n",
      "     |  __init__(self, **kwargs: Any) -> None\n",
      "     |      Create a new model by parsing and validating input data from keyword arguments.\n",
      "     |      \n",
      "     |      Raises ValidationError if the input data cannot be parsed to form a valid model.\n",
      "     |  \n",
      "     |  to_json(self) -> Union[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\n",
      "     |  \n",
      "     |  to_json_not_implemented(self) -> langchain.load.serializable.SerializedNotImplemented\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from langchain.load.serializable.Serializable:\n",
      "     |  \n",
      "     |  lc_attributes\n",
      "     |      Return a list of attribute names that should be included in the\n",
      "     |      serialized kwargs. These attributes must be accepted by the\n",
      "     |      constructor.\n",
      "     |  \n",
      "     |  lc_namespace\n",
      "     |      Return the namespace of the langchain object.\n",
      "     |      eg. [\"langchain\", \"llms\", \"openai\"]\n",
      "     |  \n",
      "     |  lc_secrets\n",
      "     |      Return a map of constructor argument names to secret ids.\n",
      "     |      eg. {\"openai_api_key\": \"OPENAI_API_KEY\"}\n",
      "     |  \n",
      "     |  lc_serializable\n",
      "     |      Return whether or not the class is serializable.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __eq__(self, other: Any) -> bool\n",
      "     |  \n",
      "     |  __getstate__(self) -> 'DictAny'\n",
      "     |  \n",
      "     |  __iter__(self) -> 'TupleGenerator'\n",
      "     |      so `dict(model)` works\n",
      "     |  \n",
      "     |  __repr_args__(self) -> 'ReprArgs'\n",
      "     |  \n",
      "     |  __setattr__(self, name, value)\n",
      "     |  \n",
      "     |  __setstate__(self, state: 'DictAny') -> None\n",
      "     |  \n",
      "     |  copy(self: 'Model', *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, update: Optional[ForwardRef('DictStrAny')] = None, deep: bool = False) -> 'Model'\n",
      "     |      Duplicate a model, optionally choose which fields to include, exclude and change.\n",
      "     |      \n",
      "     |      :param include: fields to include in new model\n",
      "     |      :param exclude: fields to exclude from new model, as with values this takes precedence over include\n",
      "     |      :param update: values to change/add in the new model. Note: the data is not validated before creating\n",
      "     |          the new model: you should trust this data\n",
      "     |      :param deep: set to `True` to make a deep copy of the model\n",
      "     |      :return: new model instance\n",
      "     |  \n",
      "     |  json(self, *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) -> 'unicode'\n",
      "     |      Generate a JSON representation of the model, `include` and `exclude` arguments as per `dict()`.\n",
      "     |      \n",
      "     |      `encoder` is an optional function to supply as `default` to json.dumps(), other arguments as per `json.dumps()`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __get_validators__() -> 'CallableGenerator' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  __try_update_forward_refs__(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      "     |      Same as update_forward_refs but will not raise exception\n",
      "     |      when forward references are not defined.\n",
      "     |  \n",
      "     |  construct(_fields_set: Optional[ForwardRef('SetStr')] = None, **values: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |      Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data.\n",
      "     |      Default values are respected, but no other validation is performed.\n",
      "     |      Behaves as if `Config.extra = 'allow'` was set since it adds all passed values\n",
      "     |  \n",
      "     |  from_orm(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_file(path: Union[str, pathlib.Path], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_obj(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_raw(b: Union[str, bytes], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema(by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}') -> 'DictStrAny' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema_json(*, by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}', **dumps_kwargs: Any) -> 'unicode' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  update_forward_refs(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      "     |      Try to update ForwardRefs on fields based on this Model, globalns and localns.\n",
      "     |  \n",
      "     |  validate(value: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __fields_set__\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.utils.Representation:\n",
      "     |  \n",
      "     |  __pretty__(self, fmt: Callable[[Any], Any], **kwargs: Any) -> Generator[Any, NoneType, NoneType]\n",
      "     |      Used by devtools (https://python-devtools.helpmanual.io/) to provide a human readable representations of objects\n",
      "     |  \n",
      "     |  __repr__(self) -> 'unicode'\n",
      "     |  \n",
      "     |  __repr_name__(self) -> 'unicode'\n",
      "     |      Name of the instance's class, used in __repr__.\n",
      "     |  \n",
      "     |  __repr_str__(self, join_str: 'unicode') -> 'unicode'\n",
      "     |  \n",
      "     |  __rich_repr__(self) -> 'RichReprResult'\n",
      "     |      Get fields for Rich library\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.schema.runnable.base.Runnable:\n",
      "     |  \n",
      "     |  __or__(self, other: 'Union[Runnable[Any, Other], Callable[[Any], Other], Mapping[str, Union[Runnable[Any, Other], Callable[[Any], Other], Any]]]') -> 'RunnableSequence[Input, Other]'\n",
      "     |      Return self|value.\n",
      "     |  \n",
      "     |  __ror__(self, other: 'Union[Runnable[Other, Any], Callable[[Any], Other], Mapping[str, Union[Runnable[Other, Any], Callable[[Other], Any], Any]]]') -> 'RunnableSequence[Other, Output]'\n",
      "     |      Return value|self.\n",
      "     |  \n",
      "     |  async atransform(self, input: 'AsyncIterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'AsyncIterator[Output]'\n",
      "     |      Default implementation of atransform, which buffers input and calls astream.\n",
      "     |      Subclasses should override this method if they can start producing output while\n",
      "     |      input is still being generated.\n",
      "     |  \n",
      "     |  bind(self, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      "     |      Bind arguments to a Runnable, returning a new Runnable.\n",
      "     |  \n",
      "     |  map(self) -> 'Runnable[List[Input], List[Output]]'\n",
      "     |      Return a new Runnable that maps a list of inputs to a list of outputs,\n",
      "     |      by calling invoke() with each input.\n",
      "     |  \n",
      "     |  transform(self, input: 'Iterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'Iterator[Output]'\n",
      "     |      Default implementation of transform, which buffers input and then calls stream.\n",
      "     |      Subclasses should override this method if they can start producing output while\n",
      "     |      input is still being generated.\n",
      "     |  \n",
      "     |  with_config(self, config: 'Optional[RunnableConfig]' = None, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      "     |      Bind config to a Runnable, returning a new Runnable.\n",
      "     |  \n",
      "     |  with_fallbacks(self, fallbacks: 'Sequence[Runnable[Input, Output]]', *, exceptions_to_handle: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,)) -> 'RunnableWithFallbacks[Input, Output]'\n",
      "     |  \n",
      "     |  with_retry(self, *, retry_if_exception_type: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,), wait_exponential_jitter: 'bool' = True, stop_after_attempt: 'int' = 3) -> 'Runnable[Input, Output]'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __class_getitem__(params) from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  __init_subclass__(*args, **kwargs) from pydantic.main.ModelMetaclass\n",
      "     |      This method is called when a class is subclassed.\n",
      "     |      \n",
      "     |      The default implementation does nothing. It may be\n",
      "     |      overridden to extend subclasses.\n",
      "    \n",
      "    class KoboldApiLLM(langchain.llms.base.LLM)\n",
      "     |  KoboldApiLLM(*, cache: Optional[bool] = None, verbose: bool = None, callbacks: Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager, NoneType] = None, callback_manager: Optional[langchain.callbacks.base.BaseCallbackManager] = None, tags: Optional[List[str]] = None, metadata: Optional[Dict[str, Any]] = None, endpoint: str, use_story: Optional[bool] = False, use_authors_note: Optional[bool] = False, use_world_info: Optional[bool] = False, use_memory: Optional[bool] = False, max_context_length: Optional[int] = 1600, max_length: Optional[int] = 80, rep_pen: Optional[float] = 1.12, rep_pen_range: Optional[int] = 1024, rep_pen_slope: Optional[float] = 0.9, temperature: Optional[float] = 0.6, tfs: Optional[float] = 0.9, top_a: Optional[float] = 0.9, top_p: Optional[float] = 0.95, top_k: Optional[int] = 0, typical: Optional[float] = 0.5) -> None\n",
      "     |  \n",
      "     |  Kobold API language model.\n",
      "     |  \n",
      "     |  It includes several fields that can be used to control the text generation process.\n",
      "     |  \n",
      "     |  To use this class, instantiate it with the required parameters and call it with a\n",
      "     |  prompt to generate text. For example:\n",
      "     |  \n",
      "     |      kobold = KoboldApiLLM(endpoint=\"http://localhost:5000\")\n",
      "     |      result = kobold(\"Write a story about a dragon.\")\n",
      "     |  \n",
      "     |  This will send a POST request to the Kobold API with the provided prompt and\n",
      "     |  generate text.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      KoboldApiLLM\n",
      "     |      langchain.llms.base.LLM\n",
      "     |      langchain.llms.base.BaseLLM\n",
      "     |      langchain.schema.language_model.BaseLanguageModel\n",
      "     |      langchain.load.serializable.Serializable\n",
      "     |      pydantic.main.BaseModel\n",
      "     |      pydantic.utils.Representation\n",
      "     |      langchain.schema.runnable.base.Runnable\n",
      "     |      typing.Generic\n",
      "     |      abc.ABC\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __json_encoder__ = pydantic_encoder(obj: Any) -> Any\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {'endpoint': <class 'str'>, 'max_context_length': ty...\n",
      "     |  \n",
      "     |  __class_vars__ = set()\n",
      "     |  \n",
      "     |  __config__ = <class 'langchain.llms.koboldai.Config'>\n",
      "     |  \n",
      "     |  __custom_root_type__ = False\n",
      "     |  \n",
      "     |  __exclude_fields__ = {'callback_manager': True, 'callbacks': True, 'me...\n",
      "     |  \n",
      "     |  __fields__ = {'cache': ModelField(name='cache', type=Optional[bool], r...\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "     |  \n",
      "     |  __include_fields__ = None\n",
      "     |  \n",
      "     |  __parameters__ = ()\n",
      "     |  \n",
      "     |  __post_root_validators__ = [(False, <function BaseLLM.raise_deprecatio...\n",
      "     |  \n",
      "     |  __pre_root_validators__ = []\n",
      "     |  \n",
      "     |  __private_attributes__ = {'_lc_kwargs': ModelPrivateAttr(default=Pydan...\n",
      "     |  \n",
      "     |  __schema_cache__ = {}\n",
      "     |  \n",
      "     |  __signature__ = <Signature (*, cache: Optional[bool] = None, ver...nt]...\n",
      "     |  \n",
      "     |  __validators__ = {'verbose': [<pydantic.class_validators.Validator obj...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  __call__(self, prompt: 'str', stop: 'Optional[List[str]]' = None, callbacks: 'Callbacks' = None, *, tags: 'Optional[List[str]]' = None, metadata: 'Optional[Dict[str, Any]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Check Cache and run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  __str__(self) -> 'str'\n",
      "     |      Get a string representation of the object for printing.\n",
      "     |  \n",
      "     |  async abatch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      "     |      Default implementation of abatch, which calls ainvoke N times.\n",
      "     |      Subclasses should override this method if they can batch more efficiently.\n",
      "     |  \n",
      "     |  async agenerate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  async agenerate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Asynchronously pass a sequence of prompts and return model generations.\n",
      "     |      \n",
      "     |      This method should make use of batched calls for models that expose a batched\n",
      "     |      API.\n",
      "     |      \n",
      "     |      Use this method when you want to:\n",
      "     |          1. take advantage of batched calls,\n",
      "     |          2. need more output from the model than just the top generated value,\n",
      "     |          3. are building chains that are agnostic to the underlying language model\n",
      "     |              type (e.g., pure text completion models vs chat models).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      "     |              converted to match the format of any language model (string for pure\n",
      "     |              text generation models and BaseMessages for chat models).\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          callbacks: Callbacks to pass through. Used for executing additional\n",
      "     |              functionality, such as logging or streaming, throughout generation.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          An LLMResult, which contains a list of candidate Generations for each input\n",
      "     |              prompt and additional model provider-specific output.\n",
      "     |  \n",
      "     |  async ainvoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Default implementation of ainvoke, which calls invoke in a thread pool.\n",
      "     |      Subclasses should override this method if they can run asynchronously.\n",
      "     |  \n",
      "     |  async apredict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Asynchronously pass a string to the model and return a string prediction.\n",
      "     |      \n",
      "     |      Use this method when calling pure text generation models and only the top\n",
      "     |          candidate generation is needed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: String input to pass to the model.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a string.\n",
      "     |  \n",
      "     |  async apredict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      "     |      Asynchronously pass messages to the model and return a message prediction.\n",
      "     |      \n",
      "     |      Use this method when calling chat models and only the top\n",
      "     |          candidate generation is needed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: A sequence of chat messages corresponding to a single model input.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a message.\n",
      "     |  \n",
      "     |  async astream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'AsyncIterator[str]'\n",
      "     |      Default implementation of astream, which calls ainvoke.\n",
      "     |      Subclasses should override this method if they support streaming output.\n",
      "     |  \n",
      "     |  batch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      "     |      Default implementation of batch, which calls invoke N times.\n",
      "     |      Subclasses should override this method if they can batch more efficiently.\n",
      "     |  \n",
      "     |  dict(self, **kwargs: 'Any') -> 'Dict'\n",
      "     |      Return a dictionary of the LLM.\n",
      "     |  \n",
      "     |  generate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  generate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Pass a sequence of prompts to the model and return model generations.\n",
      "     |      \n",
      "     |      This method should make use of batched calls for models that expose a batched\n",
      "     |      API.\n",
      "     |      \n",
      "     |      Use this method when you want to:\n",
      "     |          1. take advantage of batched calls,\n",
      "     |          2. need more output from the model than just the top generated value,\n",
      "     |          3. are building chains that are agnostic to the underlying language model\n",
      "     |              type (e.g., pure text completion models vs chat models).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      "     |              converted to match the format of any language model (string for pure\n",
      "     |              text generation models and BaseMessages for chat models).\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          callbacks: Callbacks to pass through. Used for executing additional\n",
      "     |              functionality, such as logging or streaming, throughout generation.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          An LLMResult, which contains a list of candidate Generations for each input\n",
      "     |              prompt and additional model provider-specific output.\n",
      "     |  \n",
      "     |  invoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |  \n",
      "     |  predict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Pass a single string input to the model and return a string prediction.\n",
      "     |      \n",
      "     |       Use this method when passing in raw text. If you want to pass in specific\n",
      "     |          types of chat messages, use predict_messages.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: String input to pass to the model.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a string.\n",
      "     |  \n",
      "     |  predict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      "     |      Pass a message sequence to the model and return a message prediction.\n",
      "     |      \n",
      "     |      Use this method when passing in chat messages. If you want to pass in raw text,\n",
      "     |          use predict.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: A sequence of chat messages corresponding to a single model input.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a message.\n",
      "     |  \n",
      "     |  save(self, file_path: 'Union[Path, str]') -> 'None'\n",
      "     |      Save the LLM.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          file_path: Path to file to save the LLM to.\n",
      "     |      \n",
      "     |      Example:\n",
      "     |      .. code-block:: python\n",
      "     |      \n",
      "     |          llm.save(file_path=\"path/llm.yaml\")\n",
      "     |  \n",
      "     |  stream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'Iterator[str]'\n",
      "     |      Default implementation of stream, which calls invoke.\n",
      "     |      Subclasses should override this method if they support streaming output.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  raise_deprecation(values: 'Dict') -> 'Dict' from pydantic.main.ModelMetaclass\n",
      "     |      Raise deprecation warning if callback_manager is used.\n",
      "     |  \n",
      "     |  set_verbose(verbose: 'Optional[bool]') -> 'bool' from pydantic.main.ModelMetaclass\n",
      "     |      If verbose is None, set it.\n",
      "     |      \n",
      "     |      This allows users to pass in None as verbose to access the global setting.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  Config = <class 'langchain.llms.base.BaseLLM.Config'>\n",
      "     |      Configuration for this pydantic object.\n",
      "     |  \n",
      "     |  \n",
      "     |  __orig_bases__ = (langchain.schema.language_model.BaseLanguageModel[st...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.schema.language_model.BaseLanguageModel:\n",
      "     |  \n",
      "     |  get_num_tokens(self, text: 'str') -> 'int'\n",
      "     |      Get the number of tokens present in the text.\n",
      "     |      \n",
      "     |      Useful for checking if an input will fit in a model's context window.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: The string input to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The integer number of tokens in the text.\n",
      "     |  \n",
      "     |  get_num_tokens_from_messages(self, messages: 'List[BaseMessage]') -> 'int'\n",
      "     |      Get the number of tokens in the messages.\n",
      "     |      \n",
      "     |      Useful for checking if an input will fit in a model's context window.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: The message inputs to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The sum of the number of tokens across the messages.\n",
      "     |  \n",
      "     |  get_token_ids(self, text: 'str') -> 'List[int]'\n",
      "     |      Return the ordered ids of the tokens in a text.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: The string input to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          A list of ids corresponding to the tokens in the text, in order they occur\n",
      "     |              in the text.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from langchain.schema.language_model.BaseLanguageModel:\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.load.serializable.Serializable:\n",
      "     |  \n",
      "     |  __init__(self, **kwargs: Any) -> None\n",
      "     |      Create a new model by parsing and validating input data from keyword arguments.\n",
      "     |      \n",
      "     |      Raises ValidationError if the input data cannot be parsed to form a valid model.\n",
      "     |  \n",
      "     |  to_json(self) -> Union[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\n",
      "     |  \n",
      "     |  to_json_not_implemented(self) -> langchain.load.serializable.SerializedNotImplemented\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from langchain.load.serializable.Serializable:\n",
      "     |  \n",
      "     |  lc_attributes\n",
      "     |      Return a list of attribute names that should be included in the\n",
      "     |      serialized kwargs. These attributes must be accepted by the\n",
      "     |      constructor.\n",
      "     |  \n",
      "     |  lc_namespace\n",
      "     |      Return the namespace of the langchain object.\n",
      "     |      eg. [\"langchain\", \"llms\", \"openai\"]\n",
      "     |  \n",
      "     |  lc_secrets\n",
      "     |      Return a map of constructor argument names to secret ids.\n",
      "     |      eg. {\"openai_api_key\": \"OPENAI_API_KEY\"}\n",
      "     |  \n",
      "     |  lc_serializable\n",
      "     |      Return whether or not the class is serializable.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __eq__(self, other: Any) -> bool\n",
      "     |  \n",
      "     |  __getstate__(self) -> 'DictAny'\n",
      "     |  \n",
      "     |  __iter__(self) -> 'TupleGenerator'\n",
      "     |      so `dict(model)` works\n",
      "     |  \n",
      "     |  __repr_args__(self) -> 'ReprArgs'\n",
      "     |  \n",
      "     |  __setattr__(self, name, value)\n",
      "     |  \n",
      "     |  __setstate__(self, state: 'DictAny') -> None\n",
      "     |  \n",
      "     |  copy(self: 'Model', *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, update: Optional[ForwardRef('DictStrAny')] = None, deep: bool = False) -> 'Model'\n",
      "     |      Duplicate a model, optionally choose which fields to include, exclude and change.\n",
      "     |      \n",
      "     |      :param include: fields to include in new model\n",
      "     |      :param exclude: fields to exclude from new model, as with values this takes precedence over include\n",
      "     |      :param update: values to change/add in the new model. Note: the data is not validated before creating\n",
      "     |          the new model: you should trust this data\n",
      "     |      :param deep: set to `True` to make a deep copy of the model\n",
      "     |      :return: new model instance\n",
      "     |  \n",
      "     |  json(self, *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) -> 'unicode'\n",
      "     |      Generate a JSON representation of the model, `include` and `exclude` arguments as per `dict()`.\n",
      "     |      \n",
      "     |      `encoder` is an optional function to supply as `default` to json.dumps(), other arguments as per `json.dumps()`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __get_validators__() -> 'CallableGenerator' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  __try_update_forward_refs__(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      "     |      Same as update_forward_refs but will not raise exception\n",
      "     |      when forward references are not defined.\n",
      "     |  \n",
      "     |  construct(_fields_set: Optional[ForwardRef('SetStr')] = None, **values: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |      Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data.\n",
      "     |      Default values are respected, but no other validation is performed.\n",
      "     |      Behaves as if `Config.extra = 'allow'` was set since it adds all passed values\n",
      "     |  \n",
      "     |  from_orm(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_file(path: Union[str, pathlib.Path], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_obj(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_raw(b: Union[str, bytes], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema(by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}') -> 'DictStrAny' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema_json(*, by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}', **dumps_kwargs: Any) -> 'unicode' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  update_forward_refs(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      "     |      Try to update ForwardRefs on fields based on this Model, globalns and localns.\n",
      "     |  \n",
      "     |  validate(value: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __fields_set__\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.utils.Representation:\n",
      "     |  \n",
      "     |  __pretty__(self, fmt: Callable[[Any], Any], **kwargs: Any) -> Generator[Any, NoneType, NoneType]\n",
      "     |      Used by devtools (https://python-devtools.helpmanual.io/) to provide a human readable representations of objects\n",
      "     |  \n",
      "     |  __repr__(self) -> 'unicode'\n",
      "     |  \n",
      "     |  __repr_name__(self) -> 'unicode'\n",
      "     |      Name of the instance's class, used in __repr__.\n",
      "     |  \n",
      "     |  __repr_str__(self, join_str: 'unicode') -> 'unicode'\n",
      "     |  \n",
      "     |  __rich_repr__(self) -> 'RichReprResult'\n",
      "     |      Get fields for Rich library\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.schema.runnable.base.Runnable:\n",
      "     |  \n",
      "     |  __or__(self, other: 'Union[Runnable[Any, Other], Callable[[Any], Other], Mapping[str, Union[Runnable[Any, Other], Callable[[Any], Other], Any]]]') -> 'RunnableSequence[Input, Other]'\n",
      "     |      Return self|value.\n",
      "     |  \n",
      "     |  __ror__(self, other: 'Union[Runnable[Other, Any], Callable[[Any], Other], Mapping[str, Union[Runnable[Other, Any], Callable[[Other], Any], Any]]]') -> 'RunnableSequence[Other, Output]'\n",
      "     |      Return value|self.\n",
      "     |  \n",
      "     |  async atransform(self, input: 'AsyncIterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'AsyncIterator[Output]'\n",
      "     |      Default implementation of atransform, which buffers input and calls astream.\n",
      "     |      Subclasses should override this method if they can start producing output while\n",
      "     |      input is still being generated.\n",
      "     |  \n",
      "     |  bind(self, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      "     |      Bind arguments to a Runnable, returning a new Runnable.\n",
      "     |  \n",
      "     |  map(self) -> 'Runnable[List[Input], List[Output]]'\n",
      "     |      Return a new Runnable that maps a list of inputs to a list of outputs,\n",
      "     |      by calling invoke() with each input.\n",
      "     |  \n",
      "     |  transform(self, input: 'Iterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'Iterator[Output]'\n",
      "     |      Default implementation of transform, which buffers input and then calls stream.\n",
      "     |      Subclasses should override this method if they can start producing output while\n",
      "     |      input is still being generated.\n",
      "     |  \n",
      "     |  with_config(self, config: 'Optional[RunnableConfig]' = None, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      "     |      Bind config to a Runnable, returning a new Runnable.\n",
      "     |  \n",
      "     |  with_fallbacks(self, fallbacks: 'Sequence[Runnable[Input, Output]]', *, exceptions_to_handle: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,)) -> 'RunnableWithFallbacks[Input, Output]'\n",
      "     |  \n",
      "     |  with_retry(self, *, retry_if_exception_type: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,), wait_exponential_jitter: 'bool' = True, stop_after_attempt: 'int' = 3) -> 'Runnable[Input, Output]'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __class_getitem__(params) from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  __init_subclass__(*args, **kwargs) from pydantic.main.ModelMetaclass\n",
      "     |      This method is called when a class is subclassed.\n",
      "     |      \n",
      "     |      The default implementation does nothing. It may be\n",
      "     |      overridden to extend subclasses.\n",
      "    \n",
      "    class LlamaCpp(langchain.llms.base.LLM)\n",
      "     |  LlamaCpp(*, cache: Optional[bool] = None, verbose: bool = True, callbacks: Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager, NoneType] = None, callback_manager: Optional[langchain.callbacks.base.BaseCallbackManager] = None, tags: Optional[List[str]] = None, metadata: Optional[Dict[str, Any]] = None, client: Any = None, model_path: str, lora_base: Optional[str] = None, lora_path: Optional[str] = None, n_ctx: int = 512, n_parts: int = -1, seed: int = -1, f16_kv: bool = True, logits_all: bool = False, vocab_only: bool = False, use_mlock: bool = False, n_threads: Optional[int] = None, n_batch: Optional[int] = 8, n_gpu_layers: Optional[int] = None, suffix: Optional[str] = None, max_tokens: Optional[int] = 256, temperature: Optional[float] = 0.8, top_p: Optional[float] = 0.95, logprobs: Optional[int] = None, echo: Optional[bool] = False, stop: Optional[List[str]] = [], repeat_penalty: Optional[float] = 1.1, top_k: Optional[int] = 40, last_n_tokens_size: Optional[int] = 64, use_mmap: Optional[bool] = True, rope_freq_scale: float = 1.0, rope_freq_base: float = 10000.0, model_kwargs: Dict[str, Any] = None, streaming: bool = True, grammar_path: Union[str, pathlib.Path, NoneType] = None, grammar: ForwardRef('Optional[Union[str, LlamaGrammar]]') = None) -> None\n",
      "     |  \n",
      "     |  llama.cpp model.\n",
      "     |  \n",
      "     |  To use, you should have the llama-cpp-python library installed, and provide the\n",
      "     |  path to the Llama model as a named parameter to the constructor.\n",
      "     |  Check out: https://github.com/abetlen/llama-cpp-python\n",
      "     |  \n",
      "     |  Example:\n",
      "     |      .. code-block:: python\n",
      "     |  \n",
      "     |          from langchain.llms import LlamaCpp\n",
      "     |          llm = LlamaCpp(model_path=\"/path/to/llama/model\")\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      LlamaCpp\n",
      "     |      langchain.llms.base.LLM\n",
      "     |      langchain.llms.base.BaseLLM\n",
      "     |      langchain.schema.language_model.BaseLanguageModel\n",
      "     |      langchain.load.serializable.Serializable\n",
      "     |      pydantic.main.BaseModel\n",
      "     |      pydantic.utils.Representation\n",
      "     |      langchain.schema.runnable.base.Runnable\n",
      "     |      typing.Generic\n",
      "     |      abc.ABC\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  get_num_tokens(self, text: 'str') -> 'int'\n",
      "     |      Get the number of tokens present in the text.\n",
      "     |      \n",
      "     |      Useful for checking if an input will fit in a model's context window.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: The string input to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The integer number of tokens in the text.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  build_model_kwargs(values: 'Dict[str, Any]') -> 'Dict[str, Any]' from pydantic.main.ModelMetaclass\n",
      "     |      Build extra kwargs from additional params that were passed in.\n",
      "     |  \n",
      "     |  validate_environment(values: 'Dict') -> 'Dict' from pydantic.main.ModelMetaclass\n",
      "     |      Validate that llama-cpp-python library is installed.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __json_encoder__ = pydantic_encoder(obj: Any) -> Any\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {'client': 'Any', 'echo': 'Optional[bool]', 'f16_kv'...\n",
      "     |  \n",
      "     |  __class_vars__ = set()\n",
      "     |  \n",
      "     |  __config__ = <class 'langchain.llms.llamacpp.Config'>\n",
      "     |  \n",
      "     |  __custom_root_type__ = False\n",
      "     |  \n",
      "     |  __exclude_fields__ = {'callback_manager': True, 'callbacks': True, 'me...\n",
      "     |  \n",
      "     |  __fields__ = {'cache': ModelField(name='cache', type=Optional[bool], r...\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "     |  \n",
      "     |  __include_fields__ = None\n",
      "     |  \n",
      "     |  __parameters__ = ()\n",
      "     |  \n",
      "     |  __post_root_validators__ = [(False, <function BaseLLM.raise_deprecatio...\n",
      "     |  \n",
      "     |  __pre_root_validators__ = [<function LlamaCpp.build_model_kwargs>]\n",
      "     |  \n",
      "     |  __private_attributes__ = {'_lc_kwargs': ModelPrivateAttr(default=Pydan...\n",
      "     |  \n",
      "     |  __schema_cache__ = {}\n",
      "     |  \n",
      "     |  __signature__ = <Signature (*, cache: Optional[bool] = None, ver...ona...\n",
      "     |  \n",
      "     |  __validators__ = {'verbose': [<pydantic.class_validators.Validator obj...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  __call__(self, prompt: 'str', stop: 'Optional[List[str]]' = None, callbacks: 'Callbacks' = None, *, tags: 'Optional[List[str]]' = None, metadata: 'Optional[Dict[str, Any]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Check Cache and run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  __str__(self) -> 'str'\n",
      "     |      Get a string representation of the object for printing.\n",
      "     |  \n",
      "     |  async abatch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      "     |      Default implementation of abatch, which calls ainvoke N times.\n",
      "     |      Subclasses should override this method if they can batch more efficiently.\n",
      "     |  \n",
      "     |  async agenerate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  async agenerate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Asynchronously pass a sequence of prompts and return model generations.\n",
      "     |      \n",
      "     |      This method should make use of batched calls for models that expose a batched\n",
      "     |      API.\n",
      "     |      \n",
      "     |      Use this method when you want to:\n",
      "     |          1. take advantage of batched calls,\n",
      "     |          2. need more output from the model than just the top generated value,\n",
      "     |          3. are building chains that are agnostic to the underlying language model\n",
      "     |              type (e.g., pure text completion models vs chat models).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      "     |              converted to match the format of any language model (string for pure\n",
      "     |              text generation models and BaseMessages for chat models).\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          callbacks: Callbacks to pass through. Used for executing additional\n",
      "     |              functionality, such as logging or streaming, throughout generation.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          An LLMResult, which contains a list of candidate Generations for each input\n",
      "     |              prompt and additional model provider-specific output.\n",
      "     |  \n",
      "     |  async ainvoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Default implementation of ainvoke, which calls invoke in a thread pool.\n",
      "     |      Subclasses should override this method if they can run asynchronously.\n",
      "     |  \n",
      "     |  async apredict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Asynchronously pass a string to the model and return a string prediction.\n",
      "     |      \n",
      "     |      Use this method when calling pure text generation models and only the top\n",
      "     |          candidate generation is needed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: String input to pass to the model.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a string.\n",
      "     |  \n",
      "     |  async apredict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      "     |      Asynchronously pass messages to the model and return a message prediction.\n",
      "     |      \n",
      "     |      Use this method when calling chat models and only the top\n",
      "     |          candidate generation is needed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: A sequence of chat messages corresponding to a single model input.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a message.\n",
      "     |  \n",
      "     |  async astream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'AsyncIterator[str]'\n",
      "     |      Default implementation of astream, which calls ainvoke.\n",
      "     |      Subclasses should override this method if they support streaming output.\n",
      "     |  \n",
      "     |  batch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      "     |      Default implementation of batch, which calls invoke N times.\n",
      "     |      Subclasses should override this method if they can batch more efficiently.\n",
      "     |  \n",
      "     |  dict(self, **kwargs: 'Any') -> 'Dict'\n",
      "     |      Return a dictionary of the LLM.\n",
      "     |  \n",
      "     |  generate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  generate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Pass a sequence of prompts to the model and return model generations.\n",
      "     |      \n",
      "     |      This method should make use of batched calls for models that expose a batched\n",
      "     |      API.\n",
      "     |      \n",
      "     |      Use this method when you want to:\n",
      "     |          1. take advantage of batched calls,\n",
      "     |          2. need more output from the model than just the top generated value,\n",
      "     |          3. are building chains that are agnostic to the underlying language model\n",
      "     |              type (e.g., pure text completion models vs chat models).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      "     |              converted to match the format of any language model (string for pure\n",
      "     |              text generation models and BaseMessages for chat models).\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          callbacks: Callbacks to pass through. Used for executing additional\n",
      "     |              functionality, such as logging or streaming, throughout generation.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          An LLMResult, which contains a list of candidate Generations for each input\n",
      "     |              prompt and additional model provider-specific output.\n",
      "     |  \n",
      "     |  invoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |  \n",
      "     |  predict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Pass a single string input to the model and return a string prediction.\n",
      "     |      \n",
      "     |       Use this method when passing in raw text. If you want to pass in specific\n",
      "     |          types of chat messages, use predict_messages.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: String input to pass to the model.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a string.\n",
      "     |  \n",
      "     |  predict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      "     |      Pass a message sequence to the model and return a message prediction.\n",
      "     |      \n",
      "     |      Use this method when passing in chat messages. If you want to pass in raw text,\n",
      "     |          use predict.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: A sequence of chat messages corresponding to a single model input.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a message.\n",
      "     |  \n",
      "     |  save(self, file_path: 'Union[Path, str]') -> 'None'\n",
      "     |      Save the LLM.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          file_path: Path to file to save the LLM to.\n",
      "     |      \n",
      "     |      Example:\n",
      "     |      .. code-block:: python\n",
      "     |      \n",
      "     |          llm.save(file_path=\"path/llm.yaml\")\n",
      "     |  \n",
      "     |  stream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'Iterator[str]'\n",
      "     |      Default implementation of stream, which calls invoke.\n",
      "     |      Subclasses should override this method if they support streaming output.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  raise_deprecation(values: 'Dict') -> 'Dict' from pydantic.main.ModelMetaclass\n",
      "     |      Raise deprecation warning if callback_manager is used.\n",
      "     |  \n",
      "     |  set_verbose(verbose: 'Optional[bool]') -> 'bool' from pydantic.main.ModelMetaclass\n",
      "     |      If verbose is None, set it.\n",
      "     |      \n",
      "     |      This allows users to pass in None as verbose to access the global setting.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  Config = <class 'langchain.llms.base.BaseLLM.Config'>\n",
      "     |      Configuration for this pydantic object.\n",
      "     |  \n",
      "     |  \n",
      "     |  __orig_bases__ = (langchain.schema.language_model.BaseLanguageModel[st...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.schema.language_model.BaseLanguageModel:\n",
      "     |  \n",
      "     |  get_num_tokens_from_messages(self, messages: 'List[BaseMessage]') -> 'int'\n",
      "     |      Get the number of tokens in the messages.\n",
      "     |      \n",
      "     |      Useful for checking if an input will fit in a model's context window.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: The message inputs to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The sum of the number of tokens across the messages.\n",
      "     |  \n",
      "     |  get_token_ids(self, text: 'str') -> 'List[int]'\n",
      "     |      Return the ordered ids of the tokens in a text.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: The string input to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          A list of ids corresponding to the tokens in the text, in order they occur\n",
      "     |              in the text.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from langchain.schema.language_model.BaseLanguageModel:\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.load.serializable.Serializable:\n",
      "     |  \n",
      "     |  __init__(self, **kwargs: Any) -> None\n",
      "     |      Create a new model by parsing and validating input data from keyword arguments.\n",
      "     |      \n",
      "     |      Raises ValidationError if the input data cannot be parsed to form a valid model.\n",
      "     |  \n",
      "     |  to_json(self) -> Union[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\n",
      "     |  \n",
      "     |  to_json_not_implemented(self) -> langchain.load.serializable.SerializedNotImplemented\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from langchain.load.serializable.Serializable:\n",
      "     |  \n",
      "     |  lc_attributes\n",
      "     |      Return a list of attribute names that should be included in the\n",
      "     |      serialized kwargs. These attributes must be accepted by the\n",
      "     |      constructor.\n",
      "     |  \n",
      "     |  lc_namespace\n",
      "     |      Return the namespace of the langchain object.\n",
      "     |      eg. [\"langchain\", \"llms\", \"openai\"]\n",
      "     |  \n",
      "     |  lc_secrets\n",
      "     |      Return a map of constructor argument names to secret ids.\n",
      "     |      eg. {\"openai_api_key\": \"OPENAI_API_KEY\"}\n",
      "     |  \n",
      "     |  lc_serializable\n",
      "     |      Return whether or not the class is serializable.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __eq__(self, other: Any) -> bool\n",
      "     |  \n",
      "     |  __getstate__(self) -> 'DictAny'\n",
      "     |  \n",
      "     |  __iter__(self) -> 'TupleGenerator'\n",
      "     |      so `dict(model)` works\n",
      "     |  \n",
      "     |  __repr_args__(self) -> 'ReprArgs'\n",
      "     |  \n",
      "     |  __setattr__(self, name, value)\n",
      "     |  \n",
      "     |  __setstate__(self, state: 'DictAny') -> None\n",
      "     |  \n",
      "     |  copy(self: 'Model', *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, update: Optional[ForwardRef('DictStrAny')] = None, deep: bool = False) -> 'Model'\n",
      "     |      Duplicate a model, optionally choose which fields to include, exclude and change.\n",
      "     |      \n",
      "     |      :param include: fields to include in new model\n",
      "     |      :param exclude: fields to exclude from new model, as with values this takes precedence over include\n",
      "     |      :param update: values to change/add in the new model. Note: the data is not validated before creating\n",
      "     |          the new model: you should trust this data\n",
      "     |      :param deep: set to `True` to make a deep copy of the model\n",
      "     |      :return: new model instance\n",
      "     |  \n",
      "     |  json(self, *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) -> 'unicode'\n",
      "     |      Generate a JSON representation of the model, `include` and `exclude` arguments as per `dict()`.\n",
      "     |      \n",
      "     |      `encoder` is an optional function to supply as `default` to json.dumps(), other arguments as per `json.dumps()`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __get_validators__() -> 'CallableGenerator' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  __try_update_forward_refs__(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      "     |      Same as update_forward_refs but will not raise exception\n",
      "     |      when forward references are not defined.\n",
      "     |  \n",
      "     |  construct(_fields_set: Optional[ForwardRef('SetStr')] = None, **values: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |      Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data.\n",
      "     |      Default values are respected, but no other validation is performed.\n",
      "     |      Behaves as if `Config.extra = 'allow'` was set since it adds all passed values\n",
      "     |  \n",
      "     |  from_orm(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_file(path: Union[str, pathlib.Path], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_obj(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_raw(b: Union[str, bytes], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema(by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}') -> 'DictStrAny' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema_json(*, by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}', **dumps_kwargs: Any) -> 'unicode' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  update_forward_refs(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      "     |      Try to update ForwardRefs on fields based on this Model, globalns and localns.\n",
      "     |  \n",
      "     |  validate(value: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __fields_set__\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.utils.Representation:\n",
      "     |  \n",
      "     |  __pretty__(self, fmt: Callable[[Any], Any], **kwargs: Any) -> Generator[Any, NoneType, NoneType]\n",
      "     |      Used by devtools (https://python-devtools.helpmanual.io/) to provide a human readable representations of objects\n",
      "     |  \n",
      "     |  __repr__(self) -> 'unicode'\n",
      "     |  \n",
      "     |  __repr_name__(self) -> 'unicode'\n",
      "     |      Name of the instance's class, used in __repr__.\n",
      "     |  \n",
      "     |  __repr_str__(self, join_str: 'unicode') -> 'unicode'\n",
      "     |  \n",
      "     |  __rich_repr__(self) -> 'RichReprResult'\n",
      "     |      Get fields for Rich library\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.schema.runnable.base.Runnable:\n",
      "     |  \n",
      "     |  __or__(self, other: 'Union[Runnable[Any, Other], Callable[[Any], Other], Mapping[str, Union[Runnable[Any, Other], Callable[[Any], Other], Any]]]') -> 'RunnableSequence[Input, Other]'\n",
      "     |      Return self|value.\n",
      "     |  \n",
      "     |  __ror__(self, other: 'Union[Runnable[Other, Any], Callable[[Any], Other], Mapping[str, Union[Runnable[Other, Any], Callable[[Other], Any], Any]]]') -> 'RunnableSequence[Other, Output]'\n",
      "     |      Return value|self.\n",
      "     |  \n",
      "     |  async atransform(self, input: 'AsyncIterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'AsyncIterator[Output]'\n",
      "     |      Default implementation of atransform, which buffers input and calls astream.\n",
      "     |      Subclasses should override this method if they can start producing output while\n",
      "     |      input is still being generated.\n",
      "     |  \n",
      "     |  bind(self, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      "     |      Bind arguments to a Runnable, returning a new Runnable.\n",
      "     |  \n",
      "     |  map(self) -> 'Runnable[List[Input], List[Output]]'\n",
      "     |      Return a new Runnable that maps a list of inputs to a list of outputs,\n",
      "     |      by calling invoke() with each input.\n",
      "     |  \n",
      "     |  transform(self, input: 'Iterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'Iterator[Output]'\n",
      "     |      Default implementation of transform, which buffers input and then calls stream.\n",
      "     |      Subclasses should override this method if they can start producing output while\n",
      "     |      input is still being generated.\n",
      "     |  \n",
      "     |  with_config(self, config: 'Optional[RunnableConfig]' = None, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      "     |      Bind config to a Runnable, returning a new Runnable.\n",
      "     |  \n",
      "     |  with_fallbacks(self, fallbacks: 'Sequence[Runnable[Input, Output]]', *, exceptions_to_handle: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,)) -> 'RunnableWithFallbacks[Input, Output]'\n",
      "     |  \n",
      "     |  with_retry(self, *, retry_if_exception_type: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,), wait_exponential_jitter: 'bool' = True, stop_after_attempt: 'int' = 3) -> 'Runnable[Input, Output]'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __class_getitem__(params) from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  __init_subclass__(*args, **kwargs) from pydantic.main.ModelMetaclass\n",
      "     |      This method is called when a class is subclassed.\n",
      "     |      \n",
      "     |      The default implementation does nothing. It may be\n",
      "     |      overridden to extend subclasses.\n",
      "    \n",
      "    class ManifestWrapper(langchain.llms.base.LLM)\n",
      "     |  ManifestWrapper(*, cache: Optional[bool] = None, verbose: bool = None, callbacks: Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager, NoneType] = None, callback_manager: Optional[langchain.callbacks.base.BaseCallbackManager] = None, tags: Optional[List[str]] = None, metadata: Optional[Dict[str, Any]] = None, client: Any = None, llm_kwargs: Optional[Dict] = None) -> None\n",
      "     |  \n",
      "     |  HazyResearch's Manifest library.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      ManifestWrapper\n",
      "     |      langchain.llms.base.LLM\n",
      "     |      langchain.llms.base.BaseLLM\n",
      "     |      langchain.schema.language_model.BaseLanguageModel\n",
      "     |      langchain.load.serializable.Serializable\n",
      "     |      pydantic.main.BaseModel\n",
      "     |      pydantic.utils.Representation\n",
      "     |      langchain.schema.runnable.base.Runnable\n",
      "     |      typing.Generic\n",
      "     |      abc.ABC\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  validate_environment(values: Dict) -> Dict from pydantic.main.ModelMetaclass\n",
      "     |      Validate that python package exists in environment.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __json_encoder__ = pydantic_encoder(obj: Any) -> Any\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  Config = <class 'langchain.llms.manifest.ManifestWrapper.Config'>\n",
      "     |      Configuration for this pydantic object.\n",
      "     |  \n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {'client': typing.Any, 'llm_kwargs': typing.Optional...\n",
      "     |  \n",
      "     |  __class_vars__ = set()\n",
      "     |  \n",
      "     |  __config__ = <class 'langchain.llms.manifest.Config'>\n",
      "     |  \n",
      "     |  __custom_root_type__ = False\n",
      "     |  \n",
      "     |  __exclude_fields__ = {'callback_manager': True, 'callbacks': True, 'me...\n",
      "     |  \n",
      "     |  __fields__ = {'cache': ModelField(name='cache', type=Optional[bool], r...\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "     |  \n",
      "     |  __include_fields__ = None\n",
      "     |  \n",
      "     |  __parameters__ = ()\n",
      "     |  \n",
      "     |  __post_root_validators__ = [(False, <function BaseLLM.raise_deprecatio...\n",
      "     |  \n",
      "     |  __pre_root_validators__ = []\n",
      "     |  \n",
      "     |  __private_attributes__ = {'_lc_kwargs': ModelPrivateAttr(default=Pydan...\n",
      "     |  \n",
      "     |  __schema_cache__ = {}\n",
      "     |  \n",
      "     |  __signature__ = <Signature (*, cache: Optional[bool] = None, ver...Non...\n",
      "     |  \n",
      "     |  __validators__ = {'verbose': [<pydantic.class_validators.Validator obj...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  __call__(self, prompt: 'str', stop: 'Optional[List[str]]' = None, callbacks: 'Callbacks' = None, *, tags: 'Optional[List[str]]' = None, metadata: 'Optional[Dict[str, Any]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Check Cache and run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  __str__(self) -> 'str'\n",
      "     |      Get a string representation of the object for printing.\n",
      "     |  \n",
      "     |  async abatch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      "     |      Default implementation of abatch, which calls ainvoke N times.\n",
      "     |      Subclasses should override this method if they can batch more efficiently.\n",
      "     |  \n",
      "     |  async agenerate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  async agenerate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Asynchronously pass a sequence of prompts and return model generations.\n",
      "     |      \n",
      "     |      This method should make use of batched calls for models that expose a batched\n",
      "     |      API.\n",
      "     |      \n",
      "     |      Use this method when you want to:\n",
      "     |          1. take advantage of batched calls,\n",
      "     |          2. need more output from the model than just the top generated value,\n",
      "     |          3. are building chains that are agnostic to the underlying language model\n",
      "     |              type (e.g., pure text completion models vs chat models).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      "     |              converted to match the format of any language model (string for pure\n",
      "     |              text generation models and BaseMessages for chat models).\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          callbacks: Callbacks to pass through. Used for executing additional\n",
      "     |              functionality, such as logging or streaming, throughout generation.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          An LLMResult, which contains a list of candidate Generations for each input\n",
      "     |              prompt and additional model provider-specific output.\n",
      "     |  \n",
      "     |  async ainvoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Default implementation of ainvoke, which calls invoke in a thread pool.\n",
      "     |      Subclasses should override this method if they can run asynchronously.\n",
      "     |  \n",
      "     |  async apredict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Asynchronously pass a string to the model and return a string prediction.\n",
      "     |      \n",
      "     |      Use this method when calling pure text generation models and only the top\n",
      "     |          candidate generation is needed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: String input to pass to the model.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a string.\n",
      "     |  \n",
      "     |  async apredict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      "     |      Asynchronously pass messages to the model and return a message prediction.\n",
      "     |      \n",
      "     |      Use this method when calling chat models and only the top\n",
      "     |          candidate generation is needed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: A sequence of chat messages corresponding to a single model input.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a message.\n",
      "     |  \n",
      "     |  async astream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'AsyncIterator[str]'\n",
      "     |      Default implementation of astream, which calls ainvoke.\n",
      "     |      Subclasses should override this method if they support streaming output.\n",
      "     |  \n",
      "     |  batch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      "     |      Default implementation of batch, which calls invoke N times.\n",
      "     |      Subclasses should override this method if they can batch more efficiently.\n",
      "     |  \n",
      "     |  dict(self, **kwargs: 'Any') -> 'Dict'\n",
      "     |      Return a dictionary of the LLM.\n",
      "     |  \n",
      "     |  generate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  generate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Pass a sequence of prompts to the model and return model generations.\n",
      "     |      \n",
      "     |      This method should make use of batched calls for models that expose a batched\n",
      "     |      API.\n",
      "     |      \n",
      "     |      Use this method when you want to:\n",
      "     |          1. take advantage of batched calls,\n",
      "     |          2. need more output from the model than just the top generated value,\n",
      "     |          3. are building chains that are agnostic to the underlying language model\n",
      "     |              type (e.g., pure text completion models vs chat models).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      "     |              converted to match the format of any language model (string for pure\n",
      "     |              text generation models and BaseMessages for chat models).\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          callbacks: Callbacks to pass through. Used for executing additional\n",
      "     |              functionality, such as logging or streaming, throughout generation.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          An LLMResult, which contains a list of candidate Generations for each input\n",
      "     |              prompt and additional model provider-specific output.\n",
      "     |  \n",
      "     |  invoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |  \n",
      "     |  predict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Pass a single string input to the model and return a string prediction.\n",
      "     |      \n",
      "     |       Use this method when passing in raw text. If you want to pass in specific\n",
      "     |          types of chat messages, use predict_messages.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: String input to pass to the model.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a string.\n",
      "     |  \n",
      "     |  predict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      "     |      Pass a message sequence to the model and return a message prediction.\n",
      "     |      \n",
      "     |      Use this method when passing in chat messages. If you want to pass in raw text,\n",
      "     |          use predict.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: A sequence of chat messages corresponding to a single model input.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a message.\n",
      "     |  \n",
      "     |  save(self, file_path: 'Union[Path, str]') -> 'None'\n",
      "     |      Save the LLM.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          file_path: Path to file to save the LLM to.\n",
      "     |      \n",
      "     |      Example:\n",
      "     |      .. code-block:: python\n",
      "     |      \n",
      "     |          llm.save(file_path=\"path/llm.yaml\")\n",
      "     |  \n",
      "     |  stream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'Iterator[str]'\n",
      "     |      Default implementation of stream, which calls invoke.\n",
      "     |      Subclasses should override this method if they support streaming output.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  raise_deprecation(values: 'Dict') -> 'Dict' from pydantic.main.ModelMetaclass\n",
      "     |      Raise deprecation warning if callback_manager is used.\n",
      "     |  \n",
      "     |  set_verbose(verbose: 'Optional[bool]') -> 'bool' from pydantic.main.ModelMetaclass\n",
      "     |      If verbose is None, set it.\n",
      "     |      \n",
      "     |      This allows users to pass in None as verbose to access the global setting.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  __orig_bases__ = (langchain.schema.language_model.BaseLanguageModel[st...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.schema.language_model.BaseLanguageModel:\n",
      "     |  \n",
      "     |  get_num_tokens(self, text: 'str') -> 'int'\n",
      "     |      Get the number of tokens present in the text.\n",
      "     |      \n",
      "     |      Useful for checking if an input will fit in a model's context window.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: The string input to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The integer number of tokens in the text.\n",
      "     |  \n",
      "     |  get_num_tokens_from_messages(self, messages: 'List[BaseMessage]') -> 'int'\n",
      "     |      Get the number of tokens in the messages.\n",
      "     |      \n",
      "     |      Useful for checking if an input will fit in a model's context window.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: The message inputs to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The sum of the number of tokens across the messages.\n",
      "     |  \n",
      "     |  get_token_ids(self, text: 'str') -> 'List[int]'\n",
      "     |      Return the ordered ids of the tokens in a text.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: The string input to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          A list of ids corresponding to the tokens in the text, in order they occur\n",
      "     |              in the text.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from langchain.schema.language_model.BaseLanguageModel:\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.load.serializable.Serializable:\n",
      "     |  \n",
      "     |  __init__(self, **kwargs: Any) -> None\n",
      "     |      Create a new model by parsing and validating input data from keyword arguments.\n",
      "     |      \n",
      "     |      Raises ValidationError if the input data cannot be parsed to form a valid model.\n",
      "     |  \n",
      "     |  to_json(self) -> Union[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\n",
      "     |  \n",
      "     |  to_json_not_implemented(self) -> langchain.load.serializable.SerializedNotImplemented\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from langchain.load.serializable.Serializable:\n",
      "     |  \n",
      "     |  lc_attributes\n",
      "     |      Return a list of attribute names that should be included in the\n",
      "     |      serialized kwargs. These attributes must be accepted by the\n",
      "     |      constructor.\n",
      "     |  \n",
      "     |  lc_namespace\n",
      "     |      Return the namespace of the langchain object.\n",
      "     |      eg. [\"langchain\", \"llms\", \"openai\"]\n",
      "     |  \n",
      "     |  lc_secrets\n",
      "     |      Return a map of constructor argument names to secret ids.\n",
      "     |      eg. {\"openai_api_key\": \"OPENAI_API_KEY\"}\n",
      "     |  \n",
      "     |  lc_serializable\n",
      "     |      Return whether or not the class is serializable.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __eq__(self, other: Any) -> bool\n",
      "     |  \n",
      "     |  __getstate__(self) -> 'DictAny'\n",
      "     |  \n",
      "     |  __iter__(self) -> 'TupleGenerator'\n",
      "     |      so `dict(model)` works\n",
      "     |  \n",
      "     |  __repr_args__(self) -> 'ReprArgs'\n",
      "     |  \n",
      "     |  __setattr__(self, name, value)\n",
      "     |  \n",
      "     |  __setstate__(self, state: 'DictAny') -> None\n",
      "     |  \n",
      "     |  copy(self: 'Model', *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, update: Optional[ForwardRef('DictStrAny')] = None, deep: bool = False) -> 'Model'\n",
      "     |      Duplicate a model, optionally choose which fields to include, exclude and change.\n",
      "     |      \n",
      "     |      :param include: fields to include in new model\n",
      "     |      :param exclude: fields to exclude from new model, as with values this takes precedence over include\n",
      "     |      :param update: values to change/add in the new model. Note: the data is not validated before creating\n",
      "     |          the new model: you should trust this data\n",
      "     |      :param deep: set to `True` to make a deep copy of the model\n",
      "     |      :return: new model instance\n",
      "     |  \n",
      "     |  json(self, *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) -> 'unicode'\n",
      "     |      Generate a JSON representation of the model, `include` and `exclude` arguments as per `dict()`.\n",
      "     |      \n",
      "     |      `encoder` is an optional function to supply as `default` to json.dumps(), other arguments as per `json.dumps()`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __get_validators__() -> 'CallableGenerator' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  __try_update_forward_refs__(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      "     |      Same as update_forward_refs but will not raise exception\n",
      "     |      when forward references are not defined.\n",
      "     |  \n",
      "     |  construct(_fields_set: Optional[ForwardRef('SetStr')] = None, **values: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |      Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data.\n",
      "     |      Default values are respected, but no other validation is performed.\n",
      "     |      Behaves as if `Config.extra = 'allow'` was set since it adds all passed values\n",
      "     |  \n",
      "     |  from_orm(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_file(path: Union[str, pathlib.Path], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_obj(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_raw(b: Union[str, bytes], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema(by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}') -> 'DictStrAny' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema_json(*, by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}', **dumps_kwargs: Any) -> 'unicode' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  update_forward_refs(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      "     |      Try to update ForwardRefs on fields based on this Model, globalns and localns.\n",
      "     |  \n",
      "     |  validate(value: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __fields_set__\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.utils.Representation:\n",
      "     |  \n",
      "     |  __pretty__(self, fmt: Callable[[Any], Any], **kwargs: Any) -> Generator[Any, NoneType, NoneType]\n",
      "     |      Used by devtools (https://python-devtools.helpmanual.io/) to provide a human readable representations of objects\n",
      "     |  \n",
      "     |  __repr__(self) -> 'unicode'\n",
      "     |  \n",
      "     |  __repr_name__(self) -> 'unicode'\n",
      "     |      Name of the instance's class, used in __repr__.\n",
      "     |  \n",
      "     |  __repr_str__(self, join_str: 'unicode') -> 'unicode'\n",
      "     |  \n",
      "     |  __rich_repr__(self) -> 'RichReprResult'\n",
      "     |      Get fields for Rich library\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.schema.runnable.base.Runnable:\n",
      "     |  \n",
      "     |  __or__(self, other: 'Union[Runnable[Any, Other], Callable[[Any], Other], Mapping[str, Union[Runnable[Any, Other], Callable[[Any], Other], Any]]]') -> 'RunnableSequence[Input, Other]'\n",
      "     |      Return self|value.\n",
      "     |  \n",
      "     |  __ror__(self, other: 'Union[Runnable[Other, Any], Callable[[Any], Other], Mapping[str, Union[Runnable[Other, Any], Callable[[Other], Any], Any]]]') -> 'RunnableSequence[Other, Output]'\n",
      "     |      Return value|self.\n",
      "     |  \n",
      "     |  async atransform(self, input: 'AsyncIterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'AsyncIterator[Output]'\n",
      "     |      Default implementation of atransform, which buffers input and calls astream.\n",
      "     |      Subclasses should override this method if they can start producing output while\n",
      "     |      input is still being generated.\n",
      "     |  \n",
      "     |  bind(self, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      "     |      Bind arguments to a Runnable, returning a new Runnable.\n",
      "     |  \n",
      "     |  map(self) -> 'Runnable[List[Input], List[Output]]'\n",
      "     |      Return a new Runnable that maps a list of inputs to a list of outputs,\n",
      "     |      by calling invoke() with each input.\n",
      "     |  \n",
      "     |  transform(self, input: 'Iterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'Iterator[Output]'\n",
      "     |      Default implementation of transform, which buffers input and then calls stream.\n",
      "     |      Subclasses should override this method if they can start producing output while\n",
      "     |      input is still being generated.\n",
      "     |  \n",
      "     |  with_config(self, config: 'Optional[RunnableConfig]' = None, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      "     |      Bind config to a Runnable, returning a new Runnable.\n",
      "     |  \n",
      "     |  with_fallbacks(self, fallbacks: 'Sequence[Runnable[Input, Output]]', *, exceptions_to_handle: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,)) -> 'RunnableWithFallbacks[Input, Output]'\n",
      "     |  \n",
      "     |  with_retry(self, *, retry_if_exception_type: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,), wait_exponential_jitter: 'bool' = True, stop_after_attempt: 'int' = 3) -> 'Runnable[Input, Output]'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __class_getitem__(params) from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  __init_subclass__(*args, **kwargs) from pydantic.main.ModelMetaclass\n",
      "     |      This method is called when a class is subclassed.\n",
      "     |      \n",
      "     |      The default implementation does nothing. It may be\n",
      "     |      overridden to extend subclasses.\n",
      "    \n",
      "    class Minimax(langchain.llms.base.LLM)\n",
      "     |  Minimax(*, cache: Optional[bool] = None, verbose: bool = None, callbacks: Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager, NoneType] = None, callback_manager: Optional[langchain.callbacks.base.BaseCallbackManager] = None, tags: Optional[List[str]] = None, metadata: Optional[Dict[str, Any]] = None, model: str = 'abab5.5-chat', max_tokens: int = 256, temperature: float = 0.7, top_p: float = 0.95, model_kwargs: Dict[str, Any] = None, minimax_api_host: Optional[str] = None, minimax_group_id: Optional[str] = None, minimax_api_key: Optional[str] = None) -> None\n",
      "     |  \n",
      "     |  Wrapper around Minimax large language models.\n",
      "     |  To use, you should have the environment variable\n",
      "     |  ``MINIMAX_API_KEY`` and ``MINIMAX_GROUP_ID`` set with your API key,\n",
      "     |  or pass them as a named parameter to the constructor.\n",
      "     |  Example:\n",
      "     |   .. code-block:: python\n",
      "     |       from langchain.llms.minimax import Minimax\n",
      "     |       minimax = Minimax(model=\"<model_name>\", minimax_api_key=\"my-api-key\",\n",
      "     |        minimax_group_id=\"my-group-id\")\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Minimax\n",
      "     |      langchain.llms.base.LLM\n",
      "     |      langchain.llms.base.BaseLLM\n",
      "     |      langchain.schema.language_model.BaseLanguageModel\n",
      "     |      langchain.load.serializable.Serializable\n",
      "     |      pydantic.main.BaseModel\n",
      "     |      pydantic.utils.Representation\n",
      "     |      langchain.schema.runnable.base.Runnable\n",
      "     |      typing.Generic\n",
      "     |      abc.ABC\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, **data: 'Any')\n",
      "     |      Create a new model by parsing and validating input data from keyword arguments.\n",
      "     |      \n",
      "     |      Raises ValidationError if the input data cannot be parsed to form a valid model.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  validate_environment(values: 'Dict') -> 'Dict' from pydantic.main.ModelMetaclass\n",
      "     |      Validate that api key and python package exists in environment.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __json_encoder__ = pydantic_encoder(obj: Any) -> Any\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  Config = <class 'langchain.llms.minimax.Minimax.Config'>\n",
      "     |      Configuration for this pydantic object.\n",
      "     |  \n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {'_client': '_MinimaxEndpointClient', 'max_tokens': ...\n",
      "     |  \n",
      "     |  __class_vars__ = set()\n",
      "     |  \n",
      "     |  __config__ = <class 'langchain.llms.minimax.Config'>\n",
      "     |  \n",
      "     |  __custom_root_type__ = False\n",
      "     |  \n",
      "     |  __exclude_fields__ = {'callback_manager': True, 'callbacks': True, 'me...\n",
      "     |  \n",
      "     |  __fields__ = {'cache': ModelField(name='cache', type=Optional[bool], r...\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "     |  \n",
      "     |  __include_fields__ = None\n",
      "     |  \n",
      "     |  __parameters__ = ()\n",
      "     |  \n",
      "     |  __post_root_validators__ = [(False, <function BaseLLM.raise_deprecatio...\n",
      "     |  \n",
      "     |  __pre_root_validators__ = []\n",
      "     |  \n",
      "     |  __private_attributes__ = {'_client': ModelPrivateAttr(default=Pydantic...\n",
      "     |  \n",
      "     |  __schema_cache__ = {}\n",
      "     |  \n",
      "     |  __signature__ = <Signature (*, cache: Optional[bool] = None, ver..., m...\n",
      "     |  \n",
      "     |  __validators__ = {'verbose': [<pydantic.class_validators.Validator obj...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  __call__(self, prompt: 'str', stop: 'Optional[List[str]]' = None, callbacks: 'Callbacks' = None, *, tags: 'Optional[List[str]]' = None, metadata: 'Optional[Dict[str, Any]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Check Cache and run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  __str__(self) -> 'str'\n",
      "     |      Get a string representation of the object for printing.\n",
      "     |  \n",
      "     |  async abatch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      "     |      Default implementation of abatch, which calls ainvoke N times.\n",
      "     |      Subclasses should override this method if they can batch more efficiently.\n",
      "     |  \n",
      "     |  async agenerate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  async agenerate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Asynchronously pass a sequence of prompts and return model generations.\n",
      "     |      \n",
      "     |      This method should make use of batched calls for models that expose a batched\n",
      "     |      API.\n",
      "     |      \n",
      "     |      Use this method when you want to:\n",
      "     |          1. take advantage of batched calls,\n",
      "     |          2. need more output from the model than just the top generated value,\n",
      "     |          3. are building chains that are agnostic to the underlying language model\n",
      "     |              type (e.g., pure text completion models vs chat models).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      "     |              converted to match the format of any language model (string for pure\n",
      "     |              text generation models and BaseMessages for chat models).\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          callbacks: Callbacks to pass through. Used for executing additional\n",
      "     |              functionality, such as logging or streaming, throughout generation.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          An LLMResult, which contains a list of candidate Generations for each input\n",
      "     |              prompt and additional model provider-specific output.\n",
      "     |  \n",
      "     |  async ainvoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Default implementation of ainvoke, which calls invoke in a thread pool.\n",
      "     |      Subclasses should override this method if they can run asynchronously.\n",
      "     |  \n",
      "     |  async apredict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Asynchronously pass a string to the model and return a string prediction.\n",
      "     |      \n",
      "     |      Use this method when calling pure text generation models and only the top\n",
      "     |          candidate generation is needed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: String input to pass to the model.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a string.\n",
      "     |  \n",
      "     |  async apredict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      "     |      Asynchronously pass messages to the model and return a message prediction.\n",
      "     |      \n",
      "     |      Use this method when calling chat models and only the top\n",
      "     |          candidate generation is needed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: A sequence of chat messages corresponding to a single model input.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a message.\n",
      "     |  \n",
      "     |  async astream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'AsyncIterator[str]'\n",
      "     |      Default implementation of astream, which calls ainvoke.\n",
      "     |      Subclasses should override this method if they support streaming output.\n",
      "     |  \n",
      "     |  batch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      "     |      Default implementation of batch, which calls invoke N times.\n",
      "     |      Subclasses should override this method if they can batch more efficiently.\n",
      "     |  \n",
      "     |  dict(self, **kwargs: 'Any') -> 'Dict'\n",
      "     |      Return a dictionary of the LLM.\n",
      "     |  \n",
      "     |  generate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  generate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Pass a sequence of prompts to the model and return model generations.\n",
      "     |      \n",
      "     |      This method should make use of batched calls for models that expose a batched\n",
      "     |      API.\n",
      "     |      \n",
      "     |      Use this method when you want to:\n",
      "     |          1. take advantage of batched calls,\n",
      "     |          2. need more output from the model than just the top generated value,\n",
      "     |          3. are building chains that are agnostic to the underlying language model\n",
      "     |              type (e.g., pure text completion models vs chat models).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      "     |              converted to match the format of any language model (string for pure\n",
      "     |              text generation models and BaseMessages for chat models).\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          callbacks: Callbacks to pass through. Used for executing additional\n",
      "     |              functionality, such as logging or streaming, throughout generation.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          An LLMResult, which contains a list of candidate Generations for each input\n",
      "     |              prompt and additional model provider-specific output.\n",
      "     |  \n",
      "     |  invoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |  \n",
      "     |  predict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Pass a single string input to the model and return a string prediction.\n",
      "     |      \n",
      "     |       Use this method when passing in raw text. If you want to pass in specific\n",
      "     |          types of chat messages, use predict_messages.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: String input to pass to the model.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a string.\n",
      "     |  \n",
      "     |  predict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      "     |      Pass a message sequence to the model and return a message prediction.\n",
      "     |      \n",
      "     |      Use this method when passing in chat messages. If you want to pass in raw text,\n",
      "     |          use predict.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: A sequence of chat messages corresponding to a single model input.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a message.\n",
      "     |  \n",
      "     |  save(self, file_path: 'Union[Path, str]') -> 'None'\n",
      "     |      Save the LLM.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          file_path: Path to file to save the LLM to.\n",
      "     |      \n",
      "     |      Example:\n",
      "     |      .. code-block:: python\n",
      "     |      \n",
      "     |          llm.save(file_path=\"path/llm.yaml\")\n",
      "     |  \n",
      "     |  stream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'Iterator[str]'\n",
      "     |      Default implementation of stream, which calls invoke.\n",
      "     |      Subclasses should override this method if they support streaming output.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  raise_deprecation(values: 'Dict') -> 'Dict' from pydantic.main.ModelMetaclass\n",
      "     |      Raise deprecation warning if callback_manager is used.\n",
      "     |  \n",
      "     |  set_verbose(verbose: 'Optional[bool]') -> 'bool' from pydantic.main.ModelMetaclass\n",
      "     |      If verbose is None, set it.\n",
      "     |      \n",
      "     |      This allows users to pass in None as verbose to access the global setting.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  __orig_bases__ = (langchain.schema.language_model.BaseLanguageModel[st...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.schema.language_model.BaseLanguageModel:\n",
      "     |  \n",
      "     |  get_num_tokens(self, text: 'str') -> 'int'\n",
      "     |      Get the number of tokens present in the text.\n",
      "     |      \n",
      "     |      Useful for checking if an input will fit in a model's context window.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: The string input to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The integer number of tokens in the text.\n",
      "     |  \n",
      "     |  get_num_tokens_from_messages(self, messages: 'List[BaseMessage]') -> 'int'\n",
      "     |      Get the number of tokens in the messages.\n",
      "     |      \n",
      "     |      Useful for checking if an input will fit in a model's context window.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: The message inputs to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The sum of the number of tokens across the messages.\n",
      "     |  \n",
      "     |  get_token_ids(self, text: 'str') -> 'List[int]'\n",
      "     |      Return the ordered ids of the tokens in a text.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: The string input to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          A list of ids corresponding to the tokens in the text, in order they occur\n",
      "     |              in the text.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from langchain.schema.language_model.BaseLanguageModel:\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.load.serializable.Serializable:\n",
      "     |  \n",
      "     |  to_json(self) -> Union[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\n",
      "     |  \n",
      "     |  to_json_not_implemented(self) -> langchain.load.serializable.SerializedNotImplemented\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from langchain.load.serializable.Serializable:\n",
      "     |  \n",
      "     |  lc_attributes\n",
      "     |      Return a list of attribute names that should be included in the\n",
      "     |      serialized kwargs. These attributes must be accepted by the\n",
      "     |      constructor.\n",
      "     |  \n",
      "     |  lc_namespace\n",
      "     |      Return the namespace of the langchain object.\n",
      "     |      eg. [\"langchain\", \"llms\", \"openai\"]\n",
      "     |  \n",
      "     |  lc_secrets\n",
      "     |      Return a map of constructor argument names to secret ids.\n",
      "     |      eg. {\"openai_api_key\": \"OPENAI_API_KEY\"}\n",
      "     |  \n",
      "     |  lc_serializable\n",
      "     |      Return whether or not the class is serializable.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __eq__(self, other: Any) -> bool\n",
      "     |  \n",
      "     |  __getstate__(self) -> 'DictAny'\n",
      "     |  \n",
      "     |  __iter__(self) -> 'TupleGenerator'\n",
      "     |      so `dict(model)` works\n",
      "     |  \n",
      "     |  __repr_args__(self) -> 'ReprArgs'\n",
      "     |  \n",
      "     |  __setattr__(self, name, value)\n",
      "     |  \n",
      "     |  __setstate__(self, state: 'DictAny') -> None\n",
      "     |  \n",
      "     |  copy(self: 'Model', *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, update: Optional[ForwardRef('DictStrAny')] = None, deep: bool = False) -> 'Model'\n",
      "     |      Duplicate a model, optionally choose which fields to include, exclude and change.\n",
      "     |      \n",
      "     |      :param include: fields to include in new model\n",
      "     |      :param exclude: fields to exclude from new model, as with values this takes precedence over include\n",
      "     |      :param update: values to change/add in the new model. Note: the data is not validated before creating\n",
      "     |          the new model: you should trust this data\n",
      "     |      :param deep: set to `True` to make a deep copy of the model\n",
      "     |      :return: new model instance\n",
      "     |  \n",
      "     |  json(self, *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) -> 'unicode'\n",
      "     |      Generate a JSON representation of the model, `include` and `exclude` arguments as per `dict()`.\n",
      "     |      \n",
      "     |      `encoder` is an optional function to supply as `default` to json.dumps(), other arguments as per `json.dumps()`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __get_validators__() -> 'CallableGenerator' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  __try_update_forward_refs__(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      "     |      Same as update_forward_refs but will not raise exception\n",
      "     |      when forward references are not defined.\n",
      "     |  \n",
      "     |  construct(_fields_set: Optional[ForwardRef('SetStr')] = None, **values: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |      Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data.\n",
      "     |      Default values are respected, but no other validation is performed.\n",
      "     |      Behaves as if `Config.extra = 'allow'` was set since it adds all passed values\n",
      "     |  \n",
      "     |  from_orm(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_file(path: Union[str, pathlib.Path], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_obj(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_raw(b: Union[str, bytes], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema(by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}') -> 'DictStrAny' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema_json(*, by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}', **dumps_kwargs: Any) -> 'unicode' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  update_forward_refs(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      "     |      Try to update ForwardRefs on fields based on this Model, globalns and localns.\n",
      "     |  \n",
      "     |  validate(value: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __fields_set__\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.utils.Representation:\n",
      "     |  \n",
      "     |  __pretty__(self, fmt: Callable[[Any], Any], **kwargs: Any) -> Generator[Any, NoneType, NoneType]\n",
      "     |      Used by devtools (https://python-devtools.helpmanual.io/) to provide a human readable representations of objects\n",
      "     |  \n",
      "     |  __repr__(self) -> 'unicode'\n",
      "     |  \n",
      "     |  __repr_name__(self) -> 'unicode'\n",
      "     |      Name of the instance's class, used in __repr__.\n",
      "     |  \n",
      "     |  __repr_str__(self, join_str: 'unicode') -> 'unicode'\n",
      "     |  \n",
      "     |  __rich_repr__(self) -> 'RichReprResult'\n",
      "     |      Get fields for Rich library\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.schema.runnable.base.Runnable:\n",
      "     |  \n",
      "     |  __or__(self, other: 'Union[Runnable[Any, Other], Callable[[Any], Other], Mapping[str, Union[Runnable[Any, Other], Callable[[Any], Other], Any]]]') -> 'RunnableSequence[Input, Other]'\n",
      "     |      Return self|value.\n",
      "     |  \n",
      "     |  __ror__(self, other: 'Union[Runnable[Other, Any], Callable[[Any], Other], Mapping[str, Union[Runnable[Other, Any], Callable[[Other], Any], Any]]]') -> 'RunnableSequence[Other, Output]'\n",
      "     |      Return value|self.\n",
      "     |  \n",
      "     |  async atransform(self, input: 'AsyncIterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'AsyncIterator[Output]'\n",
      "     |      Default implementation of atransform, which buffers input and calls astream.\n",
      "     |      Subclasses should override this method if they can start producing output while\n",
      "     |      input is still being generated.\n",
      "     |  \n",
      "     |  bind(self, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      "     |      Bind arguments to a Runnable, returning a new Runnable.\n",
      "     |  \n",
      "     |  map(self) -> 'Runnable[List[Input], List[Output]]'\n",
      "     |      Return a new Runnable that maps a list of inputs to a list of outputs,\n",
      "     |      by calling invoke() with each input.\n",
      "     |  \n",
      "     |  transform(self, input: 'Iterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'Iterator[Output]'\n",
      "     |      Default implementation of transform, which buffers input and then calls stream.\n",
      "     |      Subclasses should override this method if they can start producing output while\n",
      "     |      input is still being generated.\n",
      "     |  \n",
      "     |  with_config(self, config: 'Optional[RunnableConfig]' = None, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      "     |      Bind config to a Runnable, returning a new Runnable.\n",
      "     |  \n",
      "     |  with_fallbacks(self, fallbacks: 'Sequence[Runnable[Input, Output]]', *, exceptions_to_handle: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,)) -> 'RunnableWithFallbacks[Input, Output]'\n",
      "     |  \n",
      "     |  with_retry(self, *, retry_if_exception_type: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,), wait_exponential_jitter: 'bool' = True, stop_after_attempt: 'int' = 3) -> 'Runnable[Input, Output]'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __class_getitem__(params) from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  __init_subclass__(*args, **kwargs) from pydantic.main.ModelMetaclass\n",
      "     |      This method is called when a class is subclassed.\n",
      "     |      \n",
      "     |      The default implementation does nothing. It may be\n",
      "     |      overridden to extend subclasses.\n",
      "    \n",
      "    class MlflowAIGateway(langchain.llms.base.LLM)\n",
      "     |  MlflowAIGateway(*, cache: Optional[bool] = None, verbose: bool = None, callbacks: Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager, NoneType] = None, callback_manager: Optional[langchain.callbacks.base.BaseCallbackManager] = None, tags: Optional[List[str]] = None, metadata: Optional[Dict[str, Any]] = None, route: str, gateway_uri: Optional[str] = None, params: Optional[langchain.llms.mlflow_ai_gateway.Params] = None) -> None\n",
      "     |  \n",
      "     |  Wrapper around completions LLMs in the MLflow AI Gateway.\n",
      "     |  \n",
      "     |  To use, you should have the ``mlflow[gateway]`` python package installed.\n",
      "     |  For more information, see https://mlflow.org/docs/latest/gateway/index.html.\n",
      "     |  \n",
      "     |  Example:\n",
      "     |      .. code-block:: python\n",
      "     |  \n",
      "     |          from langchain.llms import MlflowAIGateway\n",
      "     |  \n",
      "     |          completions = MlflowAIGateway(\n",
      "     |              gateway_uri=\"<your-mlflow-ai-gateway-uri>\",\n",
      "     |              route=\"<your-mlflow-ai-gateway-completions-route>\",\n",
      "     |              params={\n",
      "     |                  \"temperature\": 0.1\n",
      "     |              }\n",
      "     |          )\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      MlflowAIGateway\n",
      "     |      langchain.llms.base.LLM\n",
      "     |      langchain.llms.base.BaseLLM\n",
      "     |      langchain.schema.language_model.BaseLanguageModel\n",
      "     |      langchain.load.serializable.Serializable\n",
      "     |      pydantic.main.BaseModel\n",
      "     |      pydantic.utils.Representation\n",
      "     |      langchain.schema.runnable.base.Runnable\n",
      "     |      typing.Generic\n",
      "     |      abc.ABC\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, **kwargs: 'Any')\n",
      "     |      Create a new model by parsing and validating input data from keyword arguments.\n",
      "     |      \n",
      "     |      Raises ValidationError if the input data cannot be parsed to form a valid model.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __json_encoder__ = pydantic_encoder(obj: Any) -> Any\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {'gateway_uri': 'Optional[str]', 'params': 'Optional...\n",
      "     |  \n",
      "     |  __class_vars__ = set()\n",
      "     |  \n",
      "     |  __config__ = <class 'langchain.llms.mlflow_ai_gateway.Config'>\n",
      "     |  \n",
      "     |  __custom_root_type__ = False\n",
      "     |  \n",
      "     |  __exclude_fields__ = {'callback_manager': True, 'callbacks': True, 'me...\n",
      "     |  \n",
      "     |  __fields__ = {'cache': ModelField(name='cache', type=Optional[bool], r...\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "     |  \n",
      "     |  __include_fields__ = None\n",
      "     |  \n",
      "     |  __parameters__ = ()\n",
      "     |  \n",
      "     |  __post_root_validators__ = [(False, <function BaseLLM.raise_deprecatio...\n",
      "     |  \n",
      "     |  __pre_root_validators__ = []\n",
      "     |  \n",
      "     |  __private_attributes__ = {'_lc_kwargs': ModelPrivateAttr(default=Pydan...\n",
      "     |  \n",
      "     |  __schema_cache__ = {}\n",
      "     |  \n",
      "     |  __signature__ = <Signature (*, cache: Optional[bool] = None, ver...n.l...\n",
      "     |  \n",
      "     |  __validators__ = {'verbose': [<pydantic.class_validators.Validator obj...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  __call__(self, prompt: 'str', stop: 'Optional[List[str]]' = None, callbacks: 'Callbacks' = None, *, tags: 'Optional[List[str]]' = None, metadata: 'Optional[Dict[str, Any]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Check Cache and run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  __str__(self) -> 'str'\n",
      "     |      Get a string representation of the object for printing.\n",
      "     |  \n",
      "     |  async abatch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      "     |      Default implementation of abatch, which calls ainvoke N times.\n",
      "     |      Subclasses should override this method if they can batch more efficiently.\n",
      "     |  \n",
      "     |  async agenerate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  async agenerate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Asynchronously pass a sequence of prompts and return model generations.\n",
      "     |      \n",
      "     |      This method should make use of batched calls for models that expose a batched\n",
      "     |      API.\n",
      "     |      \n",
      "     |      Use this method when you want to:\n",
      "     |          1. take advantage of batched calls,\n",
      "     |          2. need more output from the model than just the top generated value,\n",
      "     |          3. are building chains that are agnostic to the underlying language model\n",
      "     |              type (e.g., pure text completion models vs chat models).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      "     |              converted to match the format of any language model (string for pure\n",
      "     |              text generation models and BaseMessages for chat models).\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          callbacks: Callbacks to pass through. Used for executing additional\n",
      "     |              functionality, such as logging or streaming, throughout generation.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          An LLMResult, which contains a list of candidate Generations for each input\n",
      "     |              prompt and additional model provider-specific output.\n",
      "     |  \n",
      "     |  async ainvoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Default implementation of ainvoke, which calls invoke in a thread pool.\n",
      "     |      Subclasses should override this method if they can run asynchronously.\n",
      "     |  \n",
      "     |  async apredict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Asynchronously pass a string to the model and return a string prediction.\n",
      "     |      \n",
      "     |      Use this method when calling pure text generation models and only the top\n",
      "     |          candidate generation is needed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: String input to pass to the model.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a string.\n",
      "     |  \n",
      "     |  async apredict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      "     |      Asynchronously pass messages to the model and return a message prediction.\n",
      "     |      \n",
      "     |      Use this method when calling chat models and only the top\n",
      "     |          candidate generation is needed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: A sequence of chat messages corresponding to a single model input.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a message.\n",
      "     |  \n",
      "     |  async astream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'AsyncIterator[str]'\n",
      "     |      Default implementation of astream, which calls ainvoke.\n",
      "     |      Subclasses should override this method if they support streaming output.\n",
      "     |  \n",
      "     |  batch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      "     |      Default implementation of batch, which calls invoke N times.\n",
      "     |      Subclasses should override this method if they can batch more efficiently.\n",
      "     |  \n",
      "     |  dict(self, **kwargs: 'Any') -> 'Dict'\n",
      "     |      Return a dictionary of the LLM.\n",
      "     |  \n",
      "     |  generate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  generate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Pass a sequence of prompts to the model and return model generations.\n",
      "     |      \n",
      "     |      This method should make use of batched calls for models that expose a batched\n",
      "     |      API.\n",
      "     |      \n",
      "     |      Use this method when you want to:\n",
      "     |          1. take advantage of batched calls,\n",
      "     |          2. need more output from the model than just the top generated value,\n",
      "     |          3. are building chains that are agnostic to the underlying language model\n",
      "     |              type (e.g., pure text completion models vs chat models).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      "     |              converted to match the format of any language model (string for pure\n",
      "     |              text generation models and BaseMessages for chat models).\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          callbacks: Callbacks to pass through. Used for executing additional\n",
      "     |              functionality, such as logging or streaming, throughout generation.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          An LLMResult, which contains a list of candidate Generations for each input\n",
      "     |              prompt and additional model provider-specific output.\n",
      "     |  \n",
      "     |  invoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |  \n",
      "     |  predict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Pass a single string input to the model and return a string prediction.\n",
      "     |      \n",
      "     |       Use this method when passing in raw text. If you want to pass in specific\n",
      "     |          types of chat messages, use predict_messages.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: String input to pass to the model.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a string.\n",
      "     |  \n",
      "     |  predict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      "     |      Pass a message sequence to the model and return a message prediction.\n",
      "     |      \n",
      "     |      Use this method when passing in chat messages. If you want to pass in raw text,\n",
      "     |          use predict.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: A sequence of chat messages corresponding to a single model input.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a message.\n",
      "     |  \n",
      "     |  save(self, file_path: 'Union[Path, str]') -> 'None'\n",
      "     |      Save the LLM.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          file_path: Path to file to save the LLM to.\n",
      "     |      \n",
      "     |      Example:\n",
      "     |      .. code-block:: python\n",
      "     |      \n",
      "     |          llm.save(file_path=\"path/llm.yaml\")\n",
      "     |  \n",
      "     |  stream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'Iterator[str]'\n",
      "     |      Default implementation of stream, which calls invoke.\n",
      "     |      Subclasses should override this method if they support streaming output.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  raise_deprecation(values: 'Dict') -> 'Dict' from pydantic.main.ModelMetaclass\n",
      "     |      Raise deprecation warning if callback_manager is used.\n",
      "     |  \n",
      "     |  set_verbose(verbose: 'Optional[bool]') -> 'bool' from pydantic.main.ModelMetaclass\n",
      "     |      If verbose is None, set it.\n",
      "     |      \n",
      "     |      This allows users to pass in None as verbose to access the global setting.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  Config = <class 'langchain.llms.base.BaseLLM.Config'>\n",
      "     |      Configuration for this pydantic object.\n",
      "     |  \n",
      "     |  \n",
      "     |  __orig_bases__ = (langchain.schema.language_model.BaseLanguageModel[st...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.schema.language_model.BaseLanguageModel:\n",
      "     |  \n",
      "     |  get_num_tokens(self, text: 'str') -> 'int'\n",
      "     |      Get the number of tokens present in the text.\n",
      "     |      \n",
      "     |      Useful for checking if an input will fit in a model's context window.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: The string input to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The integer number of tokens in the text.\n",
      "     |  \n",
      "     |  get_num_tokens_from_messages(self, messages: 'List[BaseMessage]') -> 'int'\n",
      "     |      Get the number of tokens in the messages.\n",
      "     |      \n",
      "     |      Useful for checking if an input will fit in a model's context window.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: The message inputs to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The sum of the number of tokens across the messages.\n",
      "     |  \n",
      "     |  get_token_ids(self, text: 'str') -> 'List[int]'\n",
      "     |      Return the ordered ids of the tokens in a text.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: The string input to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          A list of ids corresponding to the tokens in the text, in order they occur\n",
      "     |              in the text.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from langchain.schema.language_model.BaseLanguageModel:\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.load.serializable.Serializable:\n",
      "     |  \n",
      "     |  to_json(self) -> Union[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\n",
      "     |  \n",
      "     |  to_json_not_implemented(self) -> langchain.load.serializable.SerializedNotImplemented\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from langchain.load.serializable.Serializable:\n",
      "     |  \n",
      "     |  lc_attributes\n",
      "     |      Return a list of attribute names that should be included in the\n",
      "     |      serialized kwargs. These attributes must be accepted by the\n",
      "     |      constructor.\n",
      "     |  \n",
      "     |  lc_namespace\n",
      "     |      Return the namespace of the langchain object.\n",
      "     |      eg. [\"langchain\", \"llms\", \"openai\"]\n",
      "     |  \n",
      "     |  lc_secrets\n",
      "     |      Return a map of constructor argument names to secret ids.\n",
      "     |      eg. {\"openai_api_key\": \"OPENAI_API_KEY\"}\n",
      "     |  \n",
      "     |  lc_serializable\n",
      "     |      Return whether or not the class is serializable.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __eq__(self, other: Any) -> bool\n",
      "     |  \n",
      "     |  __getstate__(self) -> 'DictAny'\n",
      "     |  \n",
      "     |  __iter__(self) -> 'TupleGenerator'\n",
      "     |      so `dict(model)` works\n",
      "     |  \n",
      "     |  __repr_args__(self) -> 'ReprArgs'\n",
      "     |  \n",
      "     |  __setattr__(self, name, value)\n",
      "     |  \n",
      "     |  __setstate__(self, state: 'DictAny') -> None\n",
      "     |  \n",
      "     |  copy(self: 'Model', *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, update: Optional[ForwardRef('DictStrAny')] = None, deep: bool = False) -> 'Model'\n",
      "     |      Duplicate a model, optionally choose which fields to include, exclude and change.\n",
      "     |      \n",
      "     |      :param include: fields to include in new model\n",
      "     |      :param exclude: fields to exclude from new model, as with values this takes precedence over include\n",
      "     |      :param update: values to change/add in the new model. Note: the data is not validated before creating\n",
      "     |          the new model: you should trust this data\n",
      "     |      :param deep: set to `True` to make a deep copy of the model\n",
      "     |      :return: new model instance\n",
      "     |  \n",
      "     |  json(self, *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) -> 'unicode'\n",
      "     |      Generate a JSON representation of the model, `include` and `exclude` arguments as per `dict()`.\n",
      "     |      \n",
      "     |      `encoder` is an optional function to supply as `default` to json.dumps(), other arguments as per `json.dumps()`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __get_validators__() -> 'CallableGenerator' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  __try_update_forward_refs__(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      "     |      Same as update_forward_refs but will not raise exception\n",
      "     |      when forward references are not defined.\n",
      "     |  \n",
      "     |  construct(_fields_set: Optional[ForwardRef('SetStr')] = None, **values: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |      Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data.\n",
      "     |      Default values are respected, but no other validation is performed.\n",
      "     |      Behaves as if `Config.extra = 'allow'` was set since it adds all passed values\n",
      "     |  \n",
      "     |  from_orm(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_file(path: Union[str, pathlib.Path], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_obj(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_raw(b: Union[str, bytes], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema(by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}') -> 'DictStrAny' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema_json(*, by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}', **dumps_kwargs: Any) -> 'unicode' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  update_forward_refs(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      "     |      Try to update ForwardRefs on fields based on this Model, globalns and localns.\n",
      "     |  \n",
      "     |  validate(value: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __fields_set__\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.utils.Representation:\n",
      "     |  \n",
      "     |  __pretty__(self, fmt: Callable[[Any], Any], **kwargs: Any) -> Generator[Any, NoneType, NoneType]\n",
      "     |      Used by devtools (https://python-devtools.helpmanual.io/) to provide a human readable representations of objects\n",
      "     |  \n",
      "     |  __repr__(self) -> 'unicode'\n",
      "     |  \n",
      "     |  __repr_name__(self) -> 'unicode'\n",
      "     |      Name of the instance's class, used in __repr__.\n",
      "     |  \n",
      "     |  __repr_str__(self, join_str: 'unicode') -> 'unicode'\n",
      "     |  \n",
      "     |  __rich_repr__(self) -> 'RichReprResult'\n",
      "     |      Get fields for Rich library\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.schema.runnable.base.Runnable:\n",
      "     |  \n",
      "     |  __or__(self, other: 'Union[Runnable[Any, Other], Callable[[Any], Other], Mapping[str, Union[Runnable[Any, Other], Callable[[Any], Other], Any]]]') -> 'RunnableSequence[Input, Other]'\n",
      "     |      Return self|value.\n",
      "     |  \n",
      "     |  __ror__(self, other: 'Union[Runnable[Other, Any], Callable[[Any], Other], Mapping[str, Union[Runnable[Other, Any], Callable[[Other], Any], Any]]]') -> 'RunnableSequence[Other, Output]'\n",
      "     |      Return value|self.\n",
      "     |  \n",
      "     |  async atransform(self, input: 'AsyncIterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'AsyncIterator[Output]'\n",
      "     |      Default implementation of atransform, which buffers input and calls astream.\n",
      "     |      Subclasses should override this method if they can start producing output while\n",
      "     |      input is still being generated.\n",
      "     |  \n",
      "     |  bind(self, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      "     |      Bind arguments to a Runnable, returning a new Runnable.\n",
      "     |  \n",
      "     |  map(self) -> 'Runnable[List[Input], List[Output]]'\n",
      "     |      Return a new Runnable that maps a list of inputs to a list of outputs,\n",
      "     |      by calling invoke() with each input.\n",
      "     |  \n",
      "     |  transform(self, input: 'Iterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'Iterator[Output]'\n",
      "     |      Default implementation of transform, which buffers input and then calls stream.\n",
      "     |      Subclasses should override this method if they can start producing output while\n",
      "     |      input is still being generated.\n",
      "     |  \n",
      "     |  with_config(self, config: 'Optional[RunnableConfig]' = None, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      "     |      Bind config to a Runnable, returning a new Runnable.\n",
      "     |  \n",
      "     |  with_fallbacks(self, fallbacks: 'Sequence[Runnable[Input, Output]]', *, exceptions_to_handle: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,)) -> 'RunnableWithFallbacks[Input, Output]'\n",
      "     |  \n",
      "     |  with_retry(self, *, retry_if_exception_type: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,), wait_exponential_jitter: 'bool' = True, stop_after_attempt: 'int' = 3) -> 'Runnable[Input, Output]'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __class_getitem__(params) from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  __init_subclass__(*args, **kwargs) from pydantic.main.ModelMetaclass\n",
      "     |      This method is called when a class is subclassed.\n",
      "     |      \n",
      "     |      The default implementation does nothing. It may be\n",
      "     |      overridden to extend subclasses.\n",
      "    \n",
      "    class Modal(langchain.llms.base.LLM)\n",
      "     |  Modal(*, cache: Optional[bool] = None, verbose: bool = None, callbacks: Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager, NoneType] = None, callback_manager: Optional[langchain.callbacks.base.BaseCallbackManager] = None, tags: Optional[List[str]] = None, metadata: Optional[Dict[str, Any]] = None, endpoint_url: str = '', model_kwargs: Dict[str, Any] = None) -> None\n",
      "     |  \n",
      "     |  Modal large language models.\n",
      "     |  \n",
      "     |  To use, you should have the ``modal-client`` python package installed.\n",
      "     |  \n",
      "     |  Any parameters that are valid to be passed to the call can be passed\n",
      "     |  in, even if not explicitly saved on this class.\n",
      "     |  \n",
      "     |  Example:\n",
      "     |      .. code-block:: python\n",
      "     |  \n",
      "     |          from langchain.llms import Modal\n",
      "     |          modal = Modal(endpoint_url=\"\")\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Modal\n",
      "     |      langchain.llms.base.LLM\n",
      "     |      langchain.llms.base.BaseLLM\n",
      "     |      langchain.schema.language_model.BaseLanguageModel\n",
      "     |      langchain.load.serializable.Serializable\n",
      "     |      pydantic.main.BaseModel\n",
      "     |      pydantic.utils.Representation\n",
      "     |      langchain.schema.runnable.base.Runnable\n",
      "     |      typing.Generic\n",
      "     |      abc.ABC\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  build_extra(values: Dict[str, Any]) -> Dict[str, Any] from pydantic.main.ModelMetaclass\n",
      "     |      Build extra kwargs from additional params that were passed in.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __json_encoder__ = pydantic_encoder(obj: Any) -> Any\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  Config = <class 'langchain.llms.modal.Modal.Config'>\n",
      "     |      Configuration for this pydantic config.\n",
      "     |  \n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {'endpoint_url': <class 'str'>, 'model_kwargs': typi...\n",
      "     |  \n",
      "     |  __class_vars__ = set()\n",
      "     |  \n",
      "     |  __config__ = <class 'langchain.llms.modal.Config'>\n",
      "     |  \n",
      "     |  __custom_root_type__ = False\n",
      "     |  \n",
      "     |  __exclude_fields__ = {'callback_manager': True, 'callbacks': True, 'me...\n",
      "     |  \n",
      "     |  __fields__ = {'cache': ModelField(name='cache', type=Optional[bool], r...\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "     |  \n",
      "     |  __include_fields__ = None\n",
      "     |  \n",
      "     |  __parameters__ = ()\n",
      "     |  \n",
      "     |  __post_root_validators__ = [(False, <function BaseLLM.raise_deprecatio...\n",
      "     |  \n",
      "     |  __pre_root_validators__ = [<function Modal.build_extra>]\n",
      "     |  \n",
      "     |  __private_attributes__ = {'_lc_kwargs': ModelPrivateAttr(default=Pydan...\n",
      "     |  \n",
      "     |  __schema_cache__ = {}\n",
      "     |  \n",
      "     |  __signature__ = <Signature (*, cache: Optional[bool] = None, ver...'',...\n",
      "     |  \n",
      "     |  __validators__ = {'verbose': [<pydantic.class_validators.Validator obj...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  __call__(self, prompt: 'str', stop: 'Optional[List[str]]' = None, callbacks: 'Callbacks' = None, *, tags: 'Optional[List[str]]' = None, metadata: 'Optional[Dict[str, Any]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Check Cache and run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  __str__(self) -> 'str'\n",
      "     |      Get a string representation of the object for printing.\n",
      "     |  \n",
      "     |  async abatch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      "     |      Default implementation of abatch, which calls ainvoke N times.\n",
      "     |      Subclasses should override this method if they can batch more efficiently.\n",
      "     |  \n",
      "     |  async agenerate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  async agenerate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Asynchronously pass a sequence of prompts and return model generations.\n",
      "     |      \n",
      "     |      This method should make use of batched calls for models that expose a batched\n",
      "     |      API.\n",
      "     |      \n",
      "     |      Use this method when you want to:\n",
      "     |          1. take advantage of batched calls,\n",
      "     |          2. need more output from the model than just the top generated value,\n",
      "     |          3. are building chains that are agnostic to the underlying language model\n",
      "     |              type (e.g., pure text completion models vs chat models).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      "     |              converted to match the format of any language model (string for pure\n",
      "     |              text generation models and BaseMessages for chat models).\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          callbacks: Callbacks to pass through. Used for executing additional\n",
      "     |              functionality, such as logging or streaming, throughout generation.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          An LLMResult, which contains a list of candidate Generations for each input\n",
      "     |              prompt and additional model provider-specific output.\n",
      "     |  \n",
      "     |  async ainvoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Default implementation of ainvoke, which calls invoke in a thread pool.\n",
      "     |      Subclasses should override this method if they can run asynchronously.\n",
      "     |  \n",
      "     |  async apredict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Asynchronously pass a string to the model and return a string prediction.\n",
      "     |      \n",
      "     |      Use this method when calling pure text generation models and only the top\n",
      "     |          candidate generation is needed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: String input to pass to the model.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a string.\n",
      "     |  \n",
      "     |  async apredict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      "     |      Asynchronously pass messages to the model and return a message prediction.\n",
      "     |      \n",
      "     |      Use this method when calling chat models and only the top\n",
      "     |          candidate generation is needed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: A sequence of chat messages corresponding to a single model input.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a message.\n",
      "     |  \n",
      "     |  async astream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'AsyncIterator[str]'\n",
      "     |      Default implementation of astream, which calls ainvoke.\n",
      "     |      Subclasses should override this method if they support streaming output.\n",
      "     |  \n",
      "     |  batch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      "     |      Default implementation of batch, which calls invoke N times.\n",
      "     |      Subclasses should override this method if they can batch more efficiently.\n",
      "     |  \n",
      "     |  dict(self, **kwargs: 'Any') -> 'Dict'\n",
      "     |      Return a dictionary of the LLM.\n",
      "     |  \n",
      "     |  generate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  generate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Pass a sequence of prompts to the model and return model generations.\n",
      "     |      \n",
      "     |      This method should make use of batched calls for models that expose a batched\n",
      "     |      API.\n",
      "     |      \n",
      "     |      Use this method when you want to:\n",
      "     |          1. take advantage of batched calls,\n",
      "     |          2. need more output from the model than just the top generated value,\n",
      "     |          3. are building chains that are agnostic to the underlying language model\n",
      "     |              type (e.g., pure text completion models vs chat models).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      "     |              converted to match the format of any language model (string for pure\n",
      "     |              text generation models and BaseMessages for chat models).\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          callbacks: Callbacks to pass through. Used for executing additional\n",
      "     |              functionality, such as logging or streaming, throughout generation.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          An LLMResult, which contains a list of candidate Generations for each input\n",
      "     |              prompt and additional model provider-specific output.\n",
      "     |  \n",
      "     |  invoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |  \n",
      "     |  predict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Pass a single string input to the model and return a string prediction.\n",
      "     |      \n",
      "     |       Use this method when passing in raw text. If you want to pass in specific\n",
      "     |          types of chat messages, use predict_messages.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: String input to pass to the model.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a string.\n",
      "     |  \n",
      "     |  predict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      "     |      Pass a message sequence to the model and return a message prediction.\n",
      "     |      \n",
      "     |      Use this method when passing in chat messages. If you want to pass in raw text,\n",
      "     |          use predict.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: A sequence of chat messages corresponding to a single model input.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a message.\n",
      "     |  \n",
      "     |  save(self, file_path: 'Union[Path, str]') -> 'None'\n",
      "     |      Save the LLM.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          file_path: Path to file to save the LLM to.\n",
      "     |      \n",
      "     |      Example:\n",
      "     |      .. code-block:: python\n",
      "     |      \n",
      "     |          llm.save(file_path=\"path/llm.yaml\")\n",
      "     |  \n",
      "     |  stream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'Iterator[str]'\n",
      "     |      Default implementation of stream, which calls invoke.\n",
      "     |      Subclasses should override this method if they support streaming output.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  raise_deprecation(values: 'Dict') -> 'Dict' from pydantic.main.ModelMetaclass\n",
      "     |      Raise deprecation warning if callback_manager is used.\n",
      "     |  \n",
      "     |  set_verbose(verbose: 'Optional[bool]') -> 'bool' from pydantic.main.ModelMetaclass\n",
      "     |      If verbose is None, set it.\n",
      "     |      \n",
      "     |      This allows users to pass in None as verbose to access the global setting.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  __orig_bases__ = (langchain.schema.language_model.BaseLanguageModel[st...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.schema.language_model.BaseLanguageModel:\n",
      "     |  \n",
      "     |  get_num_tokens(self, text: 'str') -> 'int'\n",
      "     |      Get the number of tokens present in the text.\n",
      "     |      \n",
      "     |      Useful for checking if an input will fit in a model's context window.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: The string input to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The integer number of tokens in the text.\n",
      "     |  \n",
      "     |  get_num_tokens_from_messages(self, messages: 'List[BaseMessage]') -> 'int'\n",
      "     |      Get the number of tokens in the messages.\n",
      "     |      \n",
      "     |      Useful for checking if an input will fit in a model's context window.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: The message inputs to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The sum of the number of tokens across the messages.\n",
      "     |  \n",
      "     |  get_token_ids(self, text: 'str') -> 'List[int]'\n",
      "     |      Return the ordered ids of the tokens in a text.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: The string input to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          A list of ids corresponding to the tokens in the text, in order they occur\n",
      "     |              in the text.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from langchain.schema.language_model.BaseLanguageModel:\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.load.serializable.Serializable:\n",
      "     |  \n",
      "     |  __init__(self, **kwargs: Any) -> None\n",
      "     |      Create a new model by parsing and validating input data from keyword arguments.\n",
      "     |      \n",
      "     |      Raises ValidationError if the input data cannot be parsed to form a valid model.\n",
      "     |  \n",
      "     |  to_json(self) -> Union[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\n",
      "     |  \n",
      "     |  to_json_not_implemented(self) -> langchain.load.serializable.SerializedNotImplemented\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from langchain.load.serializable.Serializable:\n",
      "     |  \n",
      "     |  lc_attributes\n",
      "     |      Return a list of attribute names that should be included in the\n",
      "     |      serialized kwargs. These attributes must be accepted by the\n",
      "     |      constructor.\n",
      "     |  \n",
      "     |  lc_namespace\n",
      "     |      Return the namespace of the langchain object.\n",
      "     |      eg. [\"langchain\", \"llms\", \"openai\"]\n",
      "     |  \n",
      "     |  lc_secrets\n",
      "     |      Return a map of constructor argument names to secret ids.\n",
      "     |      eg. {\"openai_api_key\": \"OPENAI_API_KEY\"}\n",
      "     |  \n",
      "     |  lc_serializable\n",
      "     |      Return whether or not the class is serializable.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __eq__(self, other: Any) -> bool\n",
      "     |  \n",
      "     |  __getstate__(self) -> 'DictAny'\n",
      "     |  \n",
      "     |  __iter__(self) -> 'TupleGenerator'\n",
      "     |      so `dict(model)` works\n",
      "     |  \n",
      "     |  __repr_args__(self) -> 'ReprArgs'\n",
      "     |  \n",
      "     |  __setattr__(self, name, value)\n",
      "     |  \n",
      "     |  __setstate__(self, state: 'DictAny') -> None\n",
      "     |  \n",
      "     |  copy(self: 'Model', *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, update: Optional[ForwardRef('DictStrAny')] = None, deep: bool = False) -> 'Model'\n",
      "     |      Duplicate a model, optionally choose which fields to include, exclude and change.\n",
      "     |      \n",
      "     |      :param include: fields to include in new model\n",
      "     |      :param exclude: fields to exclude from new model, as with values this takes precedence over include\n",
      "     |      :param update: values to change/add in the new model. Note: the data is not validated before creating\n",
      "     |          the new model: you should trust this data\n",
      "     |      :param deep: set to `True` to make a deep copy of the model\n",
      "     |      :return: new model instance\n",
      "     |  \n",
      "     |  json(self, *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) -> 'unicode'\n",
      "     |      Generate a JSON representation of the model, `include` and `exclude` arguments as per `dict()`.\n",
      "     |      \n",
      "     |      `encoder` is an optional function to supply as `default` to json.dumps(), other arguments as per `json.dumps()`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __get_validators__() -> 'CallableGenerator' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  __try_update_forward_refs__(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      "     |      Same as update_forward_refs but will not raise exception\n",
      "     |      when forward references are not defined.\n",
      "     |  \n",
      "     |  construct(_fields_set: Optional[ForwardRef('SetStr')] = None, **values: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |      Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data.\n",
      "     |      Default values are respected, but no other validation is performed.\n",
      "     |      Behaves as if `Config.extra = 'allow'` was set since it adds all passed values\n",
      "     |  \n",
      "     |  from_orm(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_file(path: Union[str, pathlib.Path], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_obj(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_raw(b: Union[str, bytes], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema(by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}') -> 'DictStrAny' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema_json(*, by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}', **dumps_kwargs: Any) -> 'unicode' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  update_forward_refs(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      "     |      Try to update ForwardRefs on fields based on this Model, globalns and localns.\n",
      "     |  \n",
      "     |  validate(value: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __fields_set__\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.utils.Representation:\n",
      "     |  \n",
      "     |  __pretty__(self, fmt: Callable[[Any], Any], **kwargs: Any) -> Generator[Any, NoneType, NoneType]\n",
      "     |      Used by devtools (https://python-devtools.helpmanual.io/) to provide a human readable representations of objects\n",
      "     |  \n",
      "     |  __repr__(self) -> 'unicode'\n",
      "     |  \n",
      "     |  __repr_name__(self) -> 'unicode'\n",
      "     |      Name of the instance's class, used in __repr__.\n",
      "     |  \n",
      "     |  __repr_str__(self, join_str: 'unicode') -> 'unicode'\n",
      "     |  \n",
      "     |  __rich_repr__(self) -> 'RichReprResult'\n",
      "     |      Get fields for Rich library\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.schema.runnable.base.Runnable:\n",
      "     |  \n",
      "     |  __or__(self, other: 'Union[Runnable[Any, Other], Callable[[Any], Other], Mapping[str, Union[Runnable[Any, Other], Callable[[Any], Other], Any]]]') -> 'RunnableSequence[Input, Other]'\n",
      "     |      Return self|value.\n",
      "     |  \n",
      "     |  __ror__(self, other: 'Union[Runnable[Other, Any], Callable[[Any], Other], Mapping[str, Union[Runnable[Other, Any], Callable[[Other], Any], Any]]]') -> 'RunnableSequence[Other, Output]'\n",
      "     |      Return value|self.\n",
      "     |  \n",
      "     |  async atransform(self, input: 'AsyncIterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'AsyncIterator[Output]'\n",
      "     |      Default implementation of atransform, which buffers input and calls astream.\n",
      "     |      Subclasses should override this method if they can start producing output while\n",
      "     |      input is still being generated.\n",
      "     |  \n",
      "     |  bind(self, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      "     |      Bind arguments to a Runnable, returning a new Runnable.\n",
      "     |  \n",
      "     |  map(self) -> 'Runnable[List[Input], List[Output]]'\n",
      "     |      Return a new Runnable that maps a list of inputs to a list of outputs,\n",
      "     |      by calling invoke() with each input.\n",
      "     |  \n",
      "     |  transform(self, input: 'Iterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'Iterator[Output]'\n",
      "     |      Default implementation of transform, which buffers input and then calls stream.\n",
      "     |      Subclasses should override this method if they can start producing output while\n",
      "     |      input is still being generated.\n",
      "     |  \n",
      "     |  with_config(self, config: 'Optional[RunnableConfig]' = None, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      "     |      Bind config to a Runnable, returning a new Runnable.\n",
      "     |  \n",
      "     |  with_fallbacks(self, fallbacks: 'Sequence[Runnable[Input, Output]]', *, exceptions_to_handle: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,)) -> 'RunnableWithFallbacks[Input, Output]'\n",
      "     |  \n",
      "     |  with_retry(self, *, retry_if_exception_type: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,), wait_exponential_jitter: 'bool' = True, stop_after_attempt: 'int' = 3) -> 'Runnable[Input, Output]'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __class_getitem__(params) from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  __init_subclass__(*args, **kwargs) from pydantic.main.ModelMetaclass\n",
      "     |      This method is called when a class is subclassed.\n",
      "     |      \n",
      "     |      The default implementation does nothing. It may be\n",
      "     |      overridden to extend subclasses.\n",
      "    \n",
      "    class MosaicML(langchain.llms.base.LLM)\n",
      "     |  MosaicML(*, cache: Optional[bool] = None, verbose: bool = None, callbacks: Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager, NoneType] = None, callback_manager: Optional[langchain.callbacks.base.BaseCallbackManager] = None, tags: Optional[List[str]] = None, metadata: Optional[Dict[str, Any]] = None, endpoint_url: str = 'https://models.hosted-on.mosaicml.hosting/mpt-7b-instruct/v1/predict', inject_instruction_format: bool = False, model_kwargs: Optional[dict] = None, retry_sleep: float = 1.0, mosaicml_api_token: Optional[str] = None) -> None\n",
      "     |  \n",
      "     |  MosaicML LLM service.\n",
      "     |  \n",
      "     |  To use, you should have the\n",
      "     |  environment variable ``MOSAICML_API_TOKEN`` set with your API token, or pass\n",
      "     |  it as a named parameter to the constructor.\n",
      "     |  \n",
      "     |  Example:\n",
      "     |      .. code-block:: python\n",
      "     |  \n",
      "     |          from langchain.llms import MosaicML\n",
      "     |          endpoint_url = (\n",
      "     |              \"https://models.hosted-on.mosaicml.hosting/mpt-7b-instruct/v1/predict\"\n",
      "     |          )\n",
      "     |          mosaic_llm = MosaicML(\n",
      "     |              endpoint_url=endpoint_url,\n",
      "     |              mosaicml_api_token=\"my-api-key\"\n",
      "     |          )\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      MosaicML\n",
      "     |      langchain.llms.base.LLM\n",
      "     |      langchain.llms.base.BaseLLM\n",
      "     |      langchain.schema.language_model.BaseLanguageModel\n",
      "     |      langchain.load.serializable.Serializable\n",
      "     |      pydantic.main.BaseModel\n",
      "     |      pydantic.utils.Representation\n",
      "     |      langchain.schema.runnable.base.Runnable\n",
      "     |      typing.Generic\n",
      "     |      abc.ABC\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  validate_environment(values: Dict) -> Dict from pydantic.main.ModelMetaclass\n",
      "     |      Validate that api key and python package exists in environment.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __json_encoder__ = pydantic_encoder(obj: Any) -> Any\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  Config = <class 'langchain.llms.mosaicml.MosaicML.Config'>\n",
      "     |      Configuration for this pydantic object.\n",
      "     |  \n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {'endpoint_url': <class 'str'>, 'inject_instruction_...\n",
      "     |  \n",
      "     |  __class_vars__ = set()\n",
      "     |  \n",
      "     |  __config__ = <class 'langchain.llms.mosaicml.Config'>\n",
      "     |  \n",
      "     |  __custom_root_type__ = False\n",
      "     |  \n",
      "     |  __exclude_fields__ = {'callback_manager': True, 'callbacks': True, 'me...\n",
      "     |  \n",
      "     |  __fields__ = {'cache': ModelField(name='cache', type=Optional[bool], r...\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "     |  \n",
      "     |  __include_fields__ = None\n",
      "     |  \n",
      "     |  __parameters__ = ()\n",
      "     |  \n",
      "     |  __post_root_validators__ = [(False, <function BaseLLM.raise_deprecatio...\n",
      "     |  \n",
      "     |  __pre_root_validators__ = []\n",
      "     |  \n",
      "     |  __private_attributes__ = {'_lc_kwargs': ModelPrivateAttr(default=Pydan...\n",
      "     |  \n",
      "     |  __schema_cache__ = {}\n",
      "     |  \n",
      "     |  __signature__ = <Signature (*, cache: Optional[bool] = None, ver...osa...\n",
      "     |  \n",
      "     |  __validators__ = {'verbose': [<pydantic.class_validators.Validator obj...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  __call__(self, prompt: 'str', stop: 'Optional[List[str]]' = None, callbacks: 'Callbacks' = None, *, tags: 'Optional[List[str]]' = None, metadata: 'Optional[Dict[str, Any]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Check Cache and run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  __str__(self) -> 'str'\n",
      "     |      Get a string representation of the object for printing.\n",
      "     |  \n",
      "     |  async abatch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      "     |      Default implementation of abatch, which calls ainvoke N times.\n",
      "     |      Subclasses should override this method if they can batch more efficiently.\n",
      "     |  \n",
      "     |  async agenerate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  async agenerate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Asynchronously pass a sequence of prompts and return model generations.\n",
      "     |      \n",
      "     |      This method should make use of batched calls for models that expose a batched\n",
      "     |      API.\n",
      "     |      \n",
      "     |      Use this method when you want to:\n",
      "     |          1. take advantage of batched calls,\n",
      "     |          2. need more output from the model than just the top generated value,\n",
      "     |          3. are building chains that are agnostic to the underlying language model\n",
      "     |              type (e.g., pure text completion models vs chat models).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      "     |              converted to match the format of any language model (string for pure\n",
      "     |              text generation models and BaseMessages for chat models).\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          callbacks: Callbacks to pass through. Used for executing additional\n",
      "     |              functionality, such as logging or streaming, throughout generation.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          An LLMResult, which contains a list of candidate Generations for each input\n",
      "     |              prompt and additional model provider-specific output.\n",
      "     |  \n",
      "     |  async ainvoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Default implementation of ainvoke, which calls invoke in a thread pool.\n",
      "     |      Subclasses should override this method if they can run asynchronously.\n",
      "     |  \n",
      "     |  async apredict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Asynchronously pass a string to the model and return a string prediction.\n",
      "     |      \n",
      "     |      Use this method when calling pure text generation models and only the top\n",
      "     |          candidate generation is needed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: String input to pass to the model.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a string.\n",
      "     |  \n",
      "     |  async apredict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      "     |      Asynchronously pass messages to the model and return a message prediction.\n",
      "     |      \n",
      "     |      Use this method when calling chat models and only the top\n",
      "     |          candidate generation is needed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: A sequence of chat messages corresponding to a single model input.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a message.\n",
      "     |  \n",
      "     |  async astream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'AsyncIterator[str]'\n",
      "     |      Default implementation of astream, which calls ainvoke.\n",
      "     |      Subclasses should override this method if they support streaming output.\n",
      "     |  \n",
      "     |  batch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      "     |      Default implementation of batch, which calls invoke N times.\n",
      "     |      Subclasses should override this method if they can batch more efficiently.\n",
      "     |  \n",
      "     |  dict(self, **kwargs: 'Any') -> 'Dict'\n",
      "     |      Return a dictionary of the LLM.\n",
      "     |  \n",
      "     |  generate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  generate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Pass a sequence of prompts to the model and return model generations.\n",
      "     |      \n",
      "     |      This method should make use of batched calls for models that expose a batched\n",
      "     |      API.\n",
      "     |      \n",
      "     |      Use this method when you want to:\n",
      "     |          1. take advantage of batched calls,\n",
      "     |          2. need more output from the model than just the top generated value,\n",
      "     |          3. are building chains that are agnostic to the underlying language model\n",
      "     |              type (e.g., pure text completion models vs chat models).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      "     |              converted to match the format of any language model (string for pure\n",
      "     |              text generation models and BaseMessages for chat models).\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          callbacks: Callbacks to pass through. Used for executing additional\n",
      "     |              functionality, such as logging or streaming, throughout generation.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          An LLMResult, which contains a list of candidate Generations for each input\n",
      "     |              prompt and additional model provider-specific output.\n",
      "     |  \n",
      "     |  invoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |  \n",
      "     |  predict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Pass a single string input to the model and return a string prediction.\n",
      "     |      \n",
      "     |       Use this method when passing in raw text. If you want to pass in specific\n",
      "     |          types of chat messages, use predict_messages.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: String input to pass to the model.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a string.\n",
      "     |  \n",
      "     |  predict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      "     |      Pass a message sequence to the model and return a message prediction.\n",
      "     |      \n",
      "     |      Use this method when passing in chat messages. If you want to pass in raw text,\n",
      "     |          use predict.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: A sequence of chat messages corresponding to a single model input.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a message.\n",
      "     |  \n",
      "     |  save(self, file_path: 'Union[Path, str]') -> 'None'\n",
      "     |      Save the LLM.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          file_path: Path to file to save the LLM to.\n",
      "     |      \n",
      "     |      Example:\n",
      "     |      .. code-block:: python\n",
      "     |      \n",
      "     |          llm.save(file_path=\"path/llm.yaml\")\n",
      "     |  \n",
      "     |  stream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'Iterator[str]'\n",
      "     |      Default implementation of stream, which calls invoke.\n",
      "     |      Subclasses should override this method if they support streaming output.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  raise_deprecation(values: 'Dict') -> 'Dict' from pydantic.main.ModelMetaclass\n",
      "     |      Raise deprecation warning if callback_manager is used.\n",
      "     |  \n",
      "     |  set_verbose(verbose: 'Optional[bool]') -> 'bool' from pydantic.main.ModelMetaclass\n",
      "     |      If verbose is None, set it.\n",
      "     |      \n",
      "     |      This allows users to pass in None as verbose to access the global setting.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  __orig_bases__ = (langchain.schema.language_model.BaseLanguageModel[st...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.schema.language_model.BaseLanguageModel:\n",
      "     |  \n",
      "     |  get_num_tokens(self, text: 'str') -> 'int'\n",
      "     |      Get the number of tokens present in the text.\n",
      "     |      \n",
      "     |      Useful for checking if an input will fit in a model's context window.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: The string input to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The integer number of tokens in the text.\n",
      "     |  \n",
      "     |  get_num_tokens_from_messages(self, messages: 'List[BaseMessage]') -> 'int'\n",
      "     |      Get the number of tokens in the messages.\n",
      "     |      \n",
      "     |      Useful for checking if an input will fit in a model's context window.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: The message inputs to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The sum of the number of tokens across the messages.\n",
      "     |  \n",
      "     |  get_token_ids(self, text: 'str') -> 'List[int]'\n",
      "     |      Return the ordered ids of the tokens in a text.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: The string input to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          A list of ids corresponding to the tokens in the text, in order they occur\n",
      "     |              in the text.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from langchain.schema.language_model.BaseLanguageModel:\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.load.serializable.Serializable:\n",
      "     |  \n",
      "     |  __init__(self, **kwargs: Any) -> None\n",
      "     |      Create a new model by parsing and validating input data from keyword arguments.\n",
      "     |      \n",
      "     |      Raises ValidationError if the input data cannot be parsed to form a valid model.\n",
      "     |  \n",
      "     |  to_json(self) -> Union[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\n",
      "     |  \n",
      "     |  to_json_not_implemented(self) -> langchain.load.serializable.SerializedNotImplemented\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from langchain.load.serializable.Serializable:\n",
      "     |  \n",
      "     |  lc_attributes\n",
      "     |      Return a list of attribute names that should be included in the\n",
      "     |      serialized kwargs. These attributes must be accepted by the\n",
      "     |      constructor.\n",
      "     |  \n",
      "     |  lc_namespace\n",
      "     |      Return the namespace of the langchain object.\n",
      "     |      eg. [\"langchain\", \"llms\", \"openai\"]\n",
      "     |  \n",
      "     |  lc_secrets\n",
      "     |      Return a map of constructor argument names to secret ids.\n",
      "     |      eg. {\"openai_api_key\": \"OPENAI_API_KEY\"}\n",
      "     |  \n",
      "     |  lc_serializable\n",
      "     |      Return whether or not the class is serializable.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __eq__(self, other: Any) -> bool\n",
      "     |  \n",
      "     |  __getstate__(self) -> 'DictAny'\n",
      "     |  \n",
      "     |  __iter__(self) -> 'TupleGenerator'\n",
      "     |      so `dict(model)` works\n",
      "     |  \n",
      "     |  __repr_args__(self) -> 'ReprArgs'\n",
      "     |  \n",
      "     |  __setattr__(self, name, value)\n",
      "     |  \n",
      "     |  __setstate__(self, state: 'DictAny') -> None\n",
      "     |  \n",
      "     |  copy(self: 'Model', *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, update: Optional[ForwardRef('DictStrAny')] = None, deep: bool = False) -> 'Model'\n",
      "     |      Duplicate a model, optionally choose which fields to include, exclude and change.\n",
      "     |      \n",
      "     |      :param include: fields to include in new model\n",
      "     |      :param exclude: fields to exclude from new model, as with values this takes precedence over include\n",
      "     |      :param update: values to change/add in the new model. Note: the data is not validated before creating\n",
      "     |          the new model: you should trust this data\n",
      "     |      :param deep: set to `True` to make a deep copy of the model\n",
      "     |      :return: new model instance\n",
      "     |  \n",
      "     |  json(self, *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) -> 'unicode'\n",
      "     |      Generate a JSON representation of the model, `include` and `exclude` arguments as per `dict()`.\n",
      "     |      \n",
      "     |      `encoder` is an optional function to supply as `default` to json.dumps(), other arguments as per `json.dumps()`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __get_validators__() -> 'CallableGenerator' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  __try_update_forward_refs__(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      "     |      Same as update_forward_refs but will not raise exception\n",
      "     |      when forward references are not defined.\n",
      "     |  \n",
      "     |  construct(_fields_set: Optional[ForwardRef('SetStr')] = None, **values: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |      Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data.\n",
      "     |      Default values are respected, but no other validation is performed.\n",
      "     |      Behaves as if `Config.extra = 'allow'` was set since it adds all passed values\n",
      "     |  \n",
      "     |  from_orm(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_file(path: Union[str, pathlib.Path], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_obj(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_raw(b: Union[str, bytes], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema(by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}') -> 'DictStrAny' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema_json(*, by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}', **dumps_kwargs: Any) -> 'unicode' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  update_forward_refs(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      "     |      Try to update ForwardRefs on fields based on this Model, globalns and localns.\n",
      "     |  \n",
      "     |  validate(value: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __fields_set__\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.utils.Representation:\n",
      "     |  \n",
      "     |  __pretty__(self, fmt: Callable[[Any], Any], **kwargs: Any) -> Generator[Any, NoneType, NoneType]\n",
      "     |      Used by devtools (https://python-devtools.helpmanual.io/) to provide a human readable representations of objects\n",
      "     |  \n",
      "     |  __repr__(self) -> 'unicode'\n",
      "     |  \n",
      "     |  __repr_name__(self) -> 'unicode'\n",
      "     |      Name of the instance's class, used in __repr__.\n",
      "     |  \n",
      "     |  __repr_str__(self, join_str: 'unicode') -> 'unicode'\n",
      "     |  \n",
      "     |  __rich_repr__(self) -> 'RichReprResult'\n",
      "     |      Get fields for Rich library\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.schema.runnable.base.Runnable:\n",
      "     |  \n",
      "     |  __or__(self, other: 'Union[Runnable[Any, Other], Callable[[Any], Other], Mapping[str, Union[Runnable[Any, Other], Callable[[Any], Other], Any]]]') -> 'RunnableSequence[Input, Other]'\n",
      "     |      Return self|value.\n",
      "     |  \n",
      "     |  __ror__(self, other: 'Union[Runnable[Other, Any], Callable[[Any], Other], Mapping[str, Union[Runnable[Other, Any], Callable[[Other], Any], Any]]]') -> 'RunnableSequence[Other, Output]'\n",
      "     |      Return value|self.\n",
      "     |  \n",
      "     |  async atransform(self, input: 'AsyncIterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'AsyncIterator[Output]'\n",
      "     |      Default implementation of atransform, which buffers input and calls astream.\n",
      "     |      Subclasses should override this method if they can start producing output while\n",
      "     |      input is still being generated.\n",
      "     |  \n",
      "     |  bind(self, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      "     |      Bind arguments to a Runnable, returning a new Runnable.\n",
      "     |  \n",
      "     |  map(self) -> 'Runnable[List[Input], List[Output]]'\n",
      "     |      Return a new Runnable that maps a list of inputs to a list of outputs,\n",
      "     |      by calling invoke() with each input.\n",
      "     |  \n",
      "     |  transform(self, input: 'Iterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'Iterator[Output]'\n",
      "     |      Default implementation of transform, which buffers input and then calls stream.\n",
      "     |      Subclasses should override this method if they can start producing output while\n",
      "     |      input is still being generated.\n",
      "     |  \n",
      "     |  with_config(self, config: 'Optional[RunnableConfig]' = None, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      "     |      Bind config to a Runnable, returning a new Runnable.\n",
      "     |  \n",
      "     |  with_fallbacks(self, fallbacks: 'Sequence[Runnable[Input, Output]]', *, exceptions_to_handle: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,)) -> 'RunnableWithFallbacks[Input, Output]'\n",
      "     |  \n",
      "     |  with_retry(self, *, retry_if_exception_type: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,), wait_exponential_jitter: 'bool' = True, stop_after_attempt: 'int' = 3) -> 'Runnable[Input, Output]'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __class_getitem__(params) from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  __init_subclass__(*args, **kwargs) from pydantic.main.ModelMetaclass\n",
      "     |      This method is called when a class is subclassed.\n",
      "     |      \n",
      "     |      The default implementation does nothing. It may be\n",
      "     |      overridden to extend subclasses.\n",
      "    \n",
      "    class NIBittensorLLM(langchain.llms.base.LLM)\n",
      "     |  NIBittensorLLM(*, cache: Optional[bool] = None, verbose: bool = None, callbacks: Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager, NoneType] = None, callback_manager: Optional[langchain.callbacks.base.BaseCallbackManager] = None, tags: Optional[List[str]] = None, metadata: Optional[Dict[str, Any]] = None, system_prompt: Optional[str] = None, top_responses: Optional[int] = 0) -> None\n",
      "     |  \n",
      "     |  NIBittensorLLM is created by Neural Internet (https://neuralinternet.ai/),\n",
      "     |  powered by Bittensor, a decentralized network full of different AI models.\n",
      "     |  \n",
      "     |  To analyze API_KEYS and logs of your usage visit\n",
      "     |      https://api.neuralinternet.ai/api-keys\n",
      "     |      https://api.neuralinternet.ai/logs\n",
      "     |  \n",
      "     |  Example:\n",
      "     |      .. code-block:: python\n",
      "     |  \n",
      "     |          from langchain.llms import NIBittensorLLM\n",
      "     |          llm = NIBittensorLLM()\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      NIBittensorLLM\n",
      "     |      langchain.llms.base.LLM\n",
      "     |      langchain.llms.base.BaseLLM\n",
      "     |      langchain.schema.language_model.BaseLanguageModel\n",
      "     |      langchain.load.serializable.Serializable\n",
      "     |      pydantic.main.BaseModel\n",
      "     |      pydantic.utils.Representation\n",
      "     |      langchain.schema.runnable.base.Runnable\n",
      "     |      typing.Generic\n",
      "     |      abc.ABC\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __json_encoder__ = pydantic_encoder(obj: Any) -> Any\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {'system_prompt': typing.Optional[str], 'top_respons...\n",
      "     |  \n",
      "     |  __class_vars__ = set()\n",
      "     |  \n",
      "     |  __config__ = <class 'langchain.llms.bittensor.Config'>\n",
      "     |  \n",
      "     |  __custom_root_type__ = False\n",
      "     |  \n",
      "     |  __exclude_fields__ = {'callback_manager': True, 'callbacks': True, 'me...\n",
      "     |  \n",
      "     |  __fields__ = {'cache': ModelField(name='cache', type=Optional[bool], r...\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "     |  \n",
      "     |  __include_fields__ = None\n",
      "     |  \n",
      "     |  __parameters__ = ()\n",
      "     |  \n",
      "     |  __post_root_validators__ = [(False, <function BaseLLM.raise_deprecatio...\n",
      "     |  \n",
      "     |  __pre_root_validators__ = []\n",
      "     |  \n",
      "     |  __private_attributes__ = {'_lc_kwargs': ModelPrivateAttr(default=Pydan...\n",
      "     |  \n",
      "     |  __schema_cache__ = {}\n",
      "     |  \n",
      "     |  __signature__ = <Signature (*, cache: Optional[bool] = None, ver... No...\n",
      "     |  \n",
      "     |  __validators__ = {'verbose': [<pydantic.class_validators.Validator obj...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  __call__(self, prompt: 'str', stop: 'Optional[List[str]]' = None, callbacks: 'Callbacks' = None, *, tags: 'Optional[List[str]]' = None, metadata: 'Optional[Dict[str, Any]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Check Cache and run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  __str__(self) -> 'str'\n",
      "     |      Get a string representation of the object for printing.\n",
      "     |  \n",
      "     |  async abatch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      "     |      Default implementation of abatch, which calls ainvoke N times.\n",
      "     |      Subclasses should override this method if they can batch more efficiently.\n",
      "     |  \n",
      "     |  async agenerate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  async agenerate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Asynchronously pass a sequence of prompts and return model generations.\n",
      "     |      \n",
      "     |      This method should make use of batched calls for models that expose a batched\n",
      "     |      API.\n",
      "     |      \n",
      "     |      Use this method when you want to:\n",
      "     |          1. take advantage of batched calls,\n",
      "     |          2. need more output from the model than just the top generated value,\n",
      "     |          3. are building chains that are agnostic to the underlying language model\n",
      "     |              type (e.g., pure text completion models vs chat models).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      "     |              converted to match the format of any language model (string for pure\n",
      "     |              text generation models and BaseMessages for chat models).\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          callbacks: Callbacks to pass through. Used for executing additional\n",
      "     |              functionality, such as logging or streaming, throughout generation.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          An LLMResult, which contains a list of candidate Generations for each input\n",
      "     |              prompt and additional model provider-specific output.\n",
      "     |  \n",
      "     |  async ainvoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Default implementation of ainvoke, which calls invoke in a thread pool.\n",
      "     |      Subclasses should override this method if they can run asynchronously.\n",
      "     |  \n",
      "     |  async apredict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Asynchronously pass a string to the model and return a string prediction.\n",
      "     |      \n",
      "     |      Use this method when calling pure text generation models and only the top\n",
      "     |          candidate generation is needed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: String input to pass to the model.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a string.\n",
      "     |  \n",
      "     |  async apredict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      "     |      Asynchronously pass messages to the model and return a message prediction.\n",
      "     |      \n",
      "     |      Use this method when calling chat models and only the top\n",
      "     |          candidate generation is needed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: A sequence of chat messages corresponding to a single model input.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a message.\n",
      "     |  \n",
      "     |  async astream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'AsyncIterator[str]'\n",
      "     |      Default implementation of astream, which calls ainvoke.\n",
      "     |      Subclasses should override this method if they support streaming output.\n",
      "     |  \n",
      "     |  batch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      "     |      Default implementation of batch, which calls invoke N times.\n",
      "     |      Subclasses should override this method if they can batch more efficiently.\n",
      "     |  \n",
      "     |  dict(self, **kwargs: 'Any') -> 'Dict'\n",
      "     |      Return a dictionary of the LLM.\n",
      "     |  \n",
      "     |  generate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  generate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Pass a sequence of prompts to the model and return model generations.\n",
      "     |      \n",
      "     |      This method should make use of batched calls for models that expose a batched\n",
      "     |      API.\n",
      "     |      \n",
      "     |      Use this method when you want to:\n",
      "     |          1. take advantage of batched calls,\n",
      "     |          2. need more output from the model than just the top generated value,\n",
      "     |          3. are building chains that are agnostic to the underlying language model\n",
      "     |              type (e.g., pure text completion models vs chat models).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      "     |              converted to match the format of any language model (string for pure\n",
      "     |              text generation models and BaseMessages for chat models).\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          callbacks: Callbacks to pass through. Used for executing additional\n",
      "     |              functionality, such as logging or streaming, throughout generation.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          An LLMResult, which contains a list of candidate Generations for each input\n",
      "     |              prompt and additional model provider-specific output.\n",
      "     |  \n",
      "     |  invoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |  \n",
      "     |  predict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Pass a single string input to the model and return a string prediction.\n",
      "     |      \n",
      "     |       Use this method when passing in raw text. If you want to pass in specific\n",
      "     |          types of chat messages, use predict_messages.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: String input to pass to the model.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a string.\n",
      "     |  \n",
      "     |  predict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      "     |      Pass a message sequence to the model and return a message prediction.\n",
      "     |      \n",
      "     |      Use this method when passing in chat messages. If you want to pass in raw text,\n",
      "     |          use predict.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: A sequence of chat messages corresponding to a single model input.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a message.\n",
      "     |  \n",
      "     |  save(self, file_path: 'Union[Path, str]') -> 'None'\n",
      "     |      Save the LLM.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          file_path: Path to file to save the LLM to.\n",
      "     |      \n",
      "     |      Example:\n",
      "     |      .. code-block:: python\n",
      "     |      \n",
      "     |          llm.save(file_path=\"path/llm.yaml\")\n",
      "     |  \n",
      "     |  stream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'Iterator[str]'\n",
      "     |      Default implementation of stream, which calls invoke.\n",
      "     |      Subclasses should override this method if they support streaming output.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  raise_deprecation(values: 'Dict') -> 'Dict' from pydantic.main.ModelMetaclass\n",
      "     |      Raise deprecation warning if callback_manager is used.\n",
      "     |  \n",
      "     |  set_verbose(verbose: 'Optional[bool]') -> 'bool' from pydantic.main.ModelMetaclass\n",
      "     |      If verbose is None, set it.\n",
      "     |      \n",
      "     |      This allows users to pass in None as verbose to access the global setting.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  Config = <class 'langchain.llms.base.BaseLLM.Config'>\n",
      "     |      Configuration for this pydantic object.\n",
      "     |  \n",
      "     |  \n",
      "     |  __orig_bases__ = (langchain.schema.language_model.BaseLanguageModel[st...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.schema.language_model.BaseLanguageModel:\n",
      "     |  \n",
      "     |  get_num_tokens(self, text: 'str') -> 'int'\n",
      "     |      Get the number of tokens present in the text.\n",
      "     |      \n",
      "     |      Useful for checking if an input will fit in a model's context window.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: The string input to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The integer number of tokens in the text.\n",
      "     |  \n",
      "     |  get_num_tokens_from_messages(self, messages: 'List[BaseMessage]') -> 'int'\n",
      "     |      Get the number of tokens in the messages.\n",
      "     |      \n",
      "     |      Useful for checking if an input will fit in a model's context window.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: The message inputs to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The sum of the number of tokens across the messages.\n",
      "     |  \n",
      "     |  get_token_ids(self, text: 'str') -> 'List[int]'\n",
      "     |      Return the ordered ids of the tokens in a text.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: The string input to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          A list of ids corresponding to the tokens in the text, in order they occur\n",
      "     |              in the text.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from langchain.schema.language_model.BaseLanguageModel:\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.load.serializable.Serializable:\n",
      "     |  \n",
      "     |  __init__(self, **kwargs: Any) -> None\n",
      "     |      Create a new model by parsing and validating input data from keyword arguments.\n",
      "     |      \n",
      "     |      Raises ValidationError if the input data cannot be parsed to form a valid model.\n",
      "     |  \n",
      "     |  to_json(self) -> Union[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\n",
      "     |  \n",
      "     |  to_json_not_implemented(self) -> langchain.load.serializable.SerializedNotImplemented\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from langchain.load.serializable.Serializable:\n",
      "     |  \n",
      "     |  lc_attributes\n",
      "     |      Return a list of attribute names that should be included in the\n",
      "     |      serialized kwargs. These attributes must be accepted by the\n",
      "     |      constructor.\n",
      "     |  \n",
      "     |  lc_namespace\n",
      "     |      Return the namespace of the langchain object.\n",
      "     |      eg. [\"langchain\", \"llms\", \"openai\"]\n",
      "     |  \n",
      "     |  lc_secrets\n",
      "     |      Return a map of constructor argument names to secret ids.\n",
      "     |      eg. {\"openai_api_key\": \"OPENAI_API_KEY\"}\n",
      "     |  \n",
      "     |  lc_serializable\n",
      "     |      Return whether or not the class is serializable.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __eq__(self, other: Any) -> bool\n",
      "     |  \n",
      "     |  __getstate__(self) -> 'DictAny'\n",
      "     |  \n",
      "     |  __iter__(self) -> 'TupleGenerator'\n",
      "     |      so `dict(model)` works\n",
      "     |  \n",
      "     |  __repr_args__(self) -> 'ReprArgs'\n",
      "     |  \n",
      "     |  __setattr__(self, name, value)\n",
      "     |  \n",
      "     |  __setstate__(self, state: 'DictAny') -> None\n",
      "     |  \n",
      "     |  copy(self: 'Model', *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, update: Optional[ForwardRef('DictStrAny')] = None, deep: bool = False) -> 'Model'\n",
      "     |      Duplicate a model, optionally choose which fields to include, exclude and change.\n",
      "     |      \n",
      "     |      :param include: fields to include in new model\n",
      "     |      :param exclude: fields to exclude from new model, as with values this takes precedence over include\n",
      "     |      :param update: values to change/add in the new model. Note: the data is not validated before creating\n",
      "     |          the new model: you should trust this data\n",
      "     |      :param deep: set to `True` to make a deep copy of the model\n",
      "     |      :return: new model instance\n",
      "     |  \n",
      "     |  json(self, *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) -> 'unicode'\n",
      "     |      Generate a JSON representation of the model, `include` and `exclude` arguments as per `dict()`.\n",
      "     |      \n",
      "     |      `encoder` is an optional function to supply as `default` to json.dumps(), other arguments as per `json.dumps()`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __get_validators__() -> 'CallableGenerator' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  __try_update_forward_refs__(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      "     |      Same as update_forward_refs but will not raise exception\n",
      "     |      when forward references are not defined.\n",
      "     |  \n",
      "     |  construct(_fields_set: Optional[ForwardRef('SetStr')] = None, **values: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |      Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data.\n",
      "     |      Default values are respected, but no other validation is performed.\n",
      "     |      Behaves as if `Config.extra = 'allow'` was set since it adds all passed values\n",
      "     |  \n",
      "     |  from_orm(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_file(path: Union[str, pathlib.Path], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_obj(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_raw(b: Union[str, bytes], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema(by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}') -> 'DictStrAny' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema_json(*, by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}', **dumps_kwargs: Any) -> 'unicode' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  update_forward_refs(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      "     |      Try to update ForwardRefs on fields based on this Model, globalns and localns.\n",
      "     |  \n",
      "     |  validate(value: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __fields_set__\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.utils.Representation:\n",
      "     |  \n",
      "     |  __pretty__(self, fmt: Callable[[Any], Any], **kwargs: Any) -> Generator[Any, NoneType, NoneType]\n",
      "     |      Used by devtools (https://python-devtools.helpmanual.io/) to provide a human readable representations of objects\n",
      "     |  \n",
      "     |  __repr__(self) -> 'unicode'\n",
      "     |  \n",
      "     |  __repr_name__(self) -> 'unicode'\n",
      "     |      Name of the instance's class, used in __repr__.\n",
      "     |  \n",
      "     |  __repr_str__(self, join_str: 'unicode') -> 'unicode'\n",
      "     |  \n",
      "     |  __rich_repr__(self) -> 'RichReprResult'\n",
      "     |      Get fields for Rich library\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.schema.runnable.base.Runnable:\n",
      "     |  \n",
      "     |  __or__(self, other: 'Union[Runnable[Any, Other], Callable[[Any], Other], Mapping[str, Union[Runnable[Any, Other], Callable[[Any], Other], Any]]]') -> 'RunnableSequence[Input, Other]'\n",
      "     |      Return self|value.\n",
      "     |  \n",
      "     |  __ror__(self, other: 'Union[Runnable[Other, Any], Callable[[Any], Other], Mapping[str, Union[Runnable[Other, Any], Callable[[Other], Any], Any]]]') -> 'RunnableSequence[Other, Output]'\n",
      "     |      Return value|self.\n",
      "     |  \n",
      "     |  async atransform(self, input: 'AsyncIterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'AsyncIterator[Output]'\n",
      "     |      Default implementation of atransform, which buffers input and calls astream.\n",
      "     |      Subclasses should override this method if they can start producing output while\n",
      "     |      input is still being generated.\n",
      "     |  \n",
      "     |  bind(self, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      "     |      Bind arguments to a Runnable, returning a new Runnable.\n",
      "     |  \n",
      "     |  map(self) -> 'Runnable[List[Input], List[Output]]'\n",
      "     |      Return a new Runnable that maps a list of inputs to a list of outputs,\n",
      "     |      by calling invoke() with each input.\n",
      "     |  \n",
      "     |  transform(self, input: 'Iterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'Iterator[Output]'\n",
      "     |      Default implementation of transform, which buffers input and then calls stream.\n",
      "     |      Subclasses should override this method if they can start producing output while\n",
      "     |      input is still being generated.\n",
      "     |  \n",
      "     |  with_config(self, config: 'Optional[RunnableConfig]' = None, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      "     |      Bind config to a Runnable, returning a new Runnable.\n",
      "     |  \n",
      "     |  with_fallbacks(self, fallbacks: 'Sequence[Runnable[Input, Output]]', *, exceptions_to_handle: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,)) -> 'RunnableWithFallbacks[Input, Output]'\n",
      "     |  \n",
      "     |  with_retry(self, *, retry_if_exception_type: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,), wait_exponential_jitter: 'bool' = True, stop_after_attempt: 'int' = 3) -> 'Runnable[Input, Output]'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __class_getitem__(params) from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  __init_subclass__(*args, **kwargs) from pydantic.main.ModelMetaclass\n",
      "     |      This method is called when a class is subclassed.\n",
      "     |      \n",
      "     |      The default implementation does nothing. It may be\n",
      "     |      overridden to extend subclasses.\n",
      "    \n",
      "    class NLPCloud(langchain.llms.base.LLM)\n",
      "     |  NLPCloud(*, cache: Optional[bool] = None, verbose: bool = None, callbacks: Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager, NoneType] = None, callback_manager: Optional[langchain.callbacks.base.BaseCallbackManager] = None, tags: Optional[List[str]] = None, metadata: Optional[Dict[str, Any]] = None, client: Any = None, model_name: str = 'finetuned-gpt-neox-20b', gpu: bool = True, lang: str = 'en', temperature: float = 0.7, max_length: int = 256, length_no_input: bool = True, remove_input: bool = True, remove_end_sequence: bool = True, bad_words: List[str] = [], top_p: int = 1, top_k: int = 50, repetition_penalty: float = 1.0, num_beams: int = 1, num_return_sequences: int = 1, nlpcloud_api_key: Optional[str] = None) -> None\n",
      "     |  \n",
      "     |  NLPCloud large language models.\n",
      "     |  \n",
      "     |  To use, you should have the ``nlpcloud`` python package installed, and the\n",
      "     |  environment variable ``NLPCLOUD_API_KEY`` set with your API key.\n",
      "     |  \n",
      "     |  Example:\n",
      "     |      .. code-block:: python\n",
      "     |  \n",
      "     |          from langchain.llms import NLPCloud\n",
      "     |          nlpcloud = NLPCloud(model=\"finetuned-gpt-neox-20b\")\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      NLPCloud\n",
      "     |      langchain.llms.base.LLM\n",
      "     |      langchain.llms.base.BaseLLM\n",
      "     |      langchain.schema.language_model.BaseLanguageModel\n",
      "     |      langchain.load.serializable.Serializable\n",
      "     |      pydantic.main.BaseModel\n",
      "     |      pydantic.utils.Representation\n",
      "     |      langchain.schema.runnable.base.Runnable\n",
      "     |      typing.Generic\n",
      "     |      abc.ABC\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  validate_environment(values: Dict) -> Dict from pydantic.main.ModelMetaclass\n",
      "     |      Validate that api key and python package exists in environment.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __json_encoder__ = pydantic_encoder(obj: Any) -> Any\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  Config = <class 'langchain.llms.nlpcloud.NLPCloud.Config'>\n",
      "     |      Configuration for this pydantic object.\n",
      "     |  \n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {'bad_words': typing.List[str], 'client': typing.Any...\n",
      "     |  \n",
      "     |  __class_vars__ = set()\n",
      "     |  \n",
      "     |  __config__ = <class 'langchain.llms.nlpcloud.Config'>\n",
      "     |  \n",
      "     |  __custom_root_type__ = False\n",
      "     |  \n",
      "     |  __exclude_fields__ = {'callback_manager': True, 'callbacks': True, 'me...\n",
      "     |  \n",
      "     |  __fields__ = {'bad_words': ModelField(name='bad_words', type=List[str]...\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "     |  \n",
      "     |  __include_fields__ = None\n",
      "     |  \n",
      "     |  __parameters__ = ()\n",
      "     |  \n",
      "     |  __post_root_validators__ = [(False, <function BaseLLM.raise_deprecatio...\n",
      "     |  \n",
      "     |  __pre_root_validators__ = []\n",
      "     |  \n",
      "     |  __private_attributes__ = {'_lc_kwargs': ModelPrivateAttr(default=Pydan...\n",
      "     |  \n",
      "     |  __schema_cache__ = {}\n",
      "     |  \n",
      "     |  __signature__ = <Signature (*, cache: Optional[bool] = None, ver... nl...\n",
      "     |  \n",
      "     |  __validators__ = {'verbose': [<pydantic.class_validators.Validator obj...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  __call__(self, prompt: 'str', stop: 'Optional[List[str]]' = None, callbacks: 'Callbacks' = None, *, tags: 'Optional[List[str]]' = None, metadata: 'Optional[Dict[str, Any]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Check Cache and run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  __str__(self) -> 'str'\n",
      "     |      Get a string representation of the object for printing.\n",
      "     |  \n",
      "     |  async abatch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      "     |      Default implementation of abatch, which calls ainvoke N times.\n",
      "     |      Subclasses should override this method if they can batch more efficiently.\n",
      "     |  \n",
      "     |  async agenerate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  async agenerate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Asynchronously pass a sequence of prompts and return model generations.\n",
      "     |      \n",
      "     |      This method should make use of batched calls for models that expose a batched\n",
      "     |      API.\n",
      "     |      \n",
      "     |      Use this method when you want to:\n",
      "     |          1. take advantage of batched calls,\n",
      "     |          2. need more output from the model than just the top generated value,\n",
      "     |          3. are building chains that are agnostic to the underlying language model\n",
      "     |              type (e.g., pure text completion models vs chat models).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      "     |              converted to match the format of any language model (string for pure\n",
      "     |              text generation models and BaseMessages for chat models).\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          callbacks: Callbacks to pass through. Used for executing additional\n",
      "     |              functionality, such as logging or streaming, throughout generation.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          An LLMResult, which contains a list of candidate Generations for each input\n",
      "     |              prompt and additional model provider-specific output.\n",
      "     |  \n",
      "     |  async ainvoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Default implementation of ainvoke, which calls invoke in a thread pool.\n",
      "     |      Subclasses should override this method if they can run asynchronously.\n",
      "     |  \n",
      "     |  async apredict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Asynchronously pass a string to the model and return a string prediction.\n",
      "     |      \n",
      "     |      Use this method when calling pure text generation models and only the top\n",
      "     |          candidate generation is needed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: String input to pass to the model.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a string.\n",
      "     |  \n",
      "     |  async apredict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      "     |      Asynchronously pass messages to the model and return a message prediction.\n",
      "     |      \n",
      "     |      Use this method when calling chat models and only the top\n",
      "     |          candidate generation is needed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: A sequence of chat messages corresponding to a single model input.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a message.\n",
      "     |  \n",
      "     |  async astream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'AsyncIterator[str]'\n",
      "     |      Default implementation of astream, which calls ainvoke.\n",
      "     |      Subclasses should override this method if they support streaming output.\n",
      "     |  \n",
      "     |  batch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      "     |      Default implementation of batch, which calls invoke N times.\n",
      "     |      Subclasses should override this method if they can batch more efficiently.\n",
      "     |  \n",
      "     |  dict(self, **kwargs: 'Any') -> 'Dict'\n",
      "     |      Return a dictionary of the LLM.\n",
      "     |  \n",
      "     |  generate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  generate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Pass a sequence of prompts to the model and return model generations.\n",
      "     |      \n",
      "     |      This method should make use of batched calls for models that expose a batched\n",
      "     |      API.\n",
      "     |      \n",
      "     |      Use this method when you want to:\n",
      "     |          1. take advantage of batched calls,\n",
      "     |          2. need more output from the model than just the top generated value,\n",
      "     |          3. are building chains that are agnostic to the underlying language model\n",
      "     |              type (e.g., pure text completion models vs chat models).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      "     |              converted to match the format of any language model (string for pure\n",
      "     |              text generation models and BaseMessages for chat models).\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          callbacks: Callbacks to pass through. Used for executing additional\n",
      "     |              functionality, such as logging or streaming, throughout generation.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          An LLMResult, which contains a list of candidate Generations for each input\n",
      "     |              prompt and additional model provider-specific output.\n",
      "     |  \n",
      "     |  invoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |  \n",
      "     |  predict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Pass a single string input to the model and return a string prediction.\n",
      "     |      \n",
      "     |       Use this method when passing in raw text. If you want to pass in specific\n",
      "     |          types of chat messages, use predict_messages.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: String input to pass to the model.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a string.\n",
      "     |  \n",
      "     |  predict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      "     |      Pass a message sequence to the model and return a message prediction.\n",
      "     |      \n",
      "     |      Use this method when passing in chat messages. If you want to pass in raw text,\n",
      "     |          use predict.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: A sequence of chat messages corresponding to a single model input.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a message.\n",
      "     |  \n",
      "     |  save(self, file_path: 'Union[Path, str]') -> 'None'\n",
      "     |      Save the LLM.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          file_path: Path to file to save the LLM to.\n",
      "     |      \n",
      "     |      Example:\n",
      "     |      .. code-block:: python\n",
      "     |      \n",
      "     |          llm.save(file_path=\"path/llm.yaml\")\n",
      "     |  \n",
      "     |  stream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'Iterator[str]'\n",
      "     |      Default implementation of stream, which calls invoke.\n",
      "     |      Subclasses should override this method if they support streaming output.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  raise_deprecation(values: 'Dict') -> 'Dict' from pydantic.main.ModelMetaclass\n",
      "     |      Raise deprecation warning if callback_manager is used.\n",
      "     |  \n",
      "     |  set_verbose(verbose: 'Optional[bool]') -> 'bool' from pydantic.main.ModelMetaclass\n",
      "     |      If verbose is None, set it.\n",
      "     |      \n",
      "     |      This allows users to pass in None as verbose to access the global setting.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  __orig_bases__ = (langchain.schema.language_model.BaseLanguageModel[st...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.schema.language_model.BaseLanguageModel:\n",
      "     |  \n",
      "     |  get_num_tokens(self, text: 'str') -> 'int'\n",
      "     |      Get the number of tokens present in the text.\n",
      "     |      \n",
      "     |      Useful for checking if an input will fit in a model's context window.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: The string input to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The integer number of tokens in the text.\n",
      "     |  \n",
      "     |  get_num_tokens_from_messages(self, messages: 'List[BaseMessage]') -> 'int'\n",
      "     |      Get the number of tokens in the messages.\n",
      "     |      \n",
      "     |      Useful for checking if an input will fit in a model's context window.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: The message inputs to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The sum of the number of tokens across the messages.\n",
      "     |  \n",
      "     |  get_token_ids(self, text: 'str') -> 'List[int]'\n",
      "     |      Return the ordered ids of the tokens in a text.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: The string input to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          A list of ids corresponding to the tokens in the text, in order they occur\n",
      "     |              in the text.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from langchain.schema.language_model.BaseLanguageModel:\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.load.serializable.Serializable:\n",
      "     |  \n",
      "     |  __init__(self, **kwargs: Any) -> None\n",
      "     |      Create a new model by parsing and validating input data from keyword arguments.\n",
      "     |      \n",
      "     |      Raises ValidationError if the input data cannot be parsed to form a valid model.\n",
      "     |  \n",
      "     |  to_json(self) -> Union[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\n",
      "     |  \n",
      "     |  to_json_not_implemented(self) -> langchain.load.serializable.SerializedNotImplemented\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from langchain.load.serializable.Serializable:\n",
      "     |  \n",
      "     |  lc_attributes\n",
      "     |      Return a list of attribute names that should be included in the\n",
      "     |      serialized kwargs. These attributes must be accepted by the\n",
      "     |      constructor.\n",
      "     |  \n",
      "     |  lc_namespace\n",
      "     |      Return the namespace of the langchain object.\n",
      "     |      eg. [\"langchain\", \"llms\", \"openai\"]\n",
      "     |  \n",
      "     |  lc_secrets\n",
      "     |      Return a map of constructor argument names to secret ids.\n",
      "     |      eg. {\"openai_api_key\": \"OPENAI_API_KEY\"}\n",
      "     |  \n",
      "     |  lc_serializable\n",
      "     |      Return whether or not the class is serializable.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __eq__(self, other: Any) -> bool\n",
      "     |  \n",
      "     |  __getstate__(self) -> 'DictAny'\n",
      "     |  \n",
      "     |  __iter__(self) -> 'TupleGenerator'\n",
      "     |      so `dict(model)` works\n",
      "     |  \n",
      "     |  __repr_args__(self) -> 'ReprArgs'\n",
      "     |  \n",
      "     |  __setattr__(self, name, value)\n",
      "     |  \n",
      "     |  __setstate__(self, state: 'DictAny') -> None\n",
      "     |  \n",
      "     |  copy(self: 'Model', *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, update: Optional[ForwardRef('DictStrAny')] = None, deep: bool = False) -> 'Model'\n",
      "     |      Duplicate a model, optionally choose which fields to include, exclude and change.\n",
      "     |      \n",
      "     |      :param include: fields to include in new model\n",
      "     |      :param exclude: fields to exclude from new model, as with values this takes precedence over include\n",
      "     |      :param update: values to change/add in the new model. Note: the data is not validated before creating\n",
      "     |          the new model: you should trust this data\n",
      "     |      :param deep: set to `True` to make a deep copy of the model\n",
      "     |      :return: new model instance\n",
      "     |  \n",
      "     |  json(self, *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) -> 'unicode'\n",
      "     |      Generate a JSON representation of the model, `include` and `exclude` arguments as per `dict()`.\n",
      "     |      \n",
      "     |      `encoder` is an optional function to supply as `default` to json.dumps(), other arguments as per `json.dumps()`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __get_validators__() -> 'CallableGenerator' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  __try_update_forward_refs__(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      "     |      Same as update_forward_refs but will not raise exception\n",
      "     |      when forward references are not defined.\n",
      "     |  \n",
      "     |  construct(_fields_set: Optional[ForwardRef('SetStr')] = None, **values: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |      Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data.\n",
      "     |      Default values are respected, but no other validation is performed.\n",
      "     |      Behaves as if `Config.extra = 'allow'` was set since it adds all passed values\n",
      "     |  \n",
      "     |  from_orm(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_file(path: Union[str, pathlib.Path], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_obj(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_raw(b: Union[str, bytes], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema(by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}') -> 'DictStrAny' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema_json(*, by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}', **dumps_kwargs: Any) -> 'unicode' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  update_forward_refs(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      "     |      Try to update ForwardRefs on fields based on this Model, globalns and localns.\n",
      "     |  \n",
      "     |  validate(value: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __fields_set__\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.utils.Representation:\n",
      "     |  \n",
      "     |  __pretty__(self, fmt: Callable[[Any], Any], **kwargs: Any) -> Generator[Any, NoneType, NoneType]\n",
      "     |      Used by devtools (https://python-devtools.helpmanual.io/) to provide a human readable representations of objects\n",
      "     |  \n",
      "     |  __repr__(self) -> 'unicode'\n",
      "     |  \n",
      "     |  __repr_name__(self) -> 'unicode'\n",
      "     |      Name of the instance's class, used in __repr__.\n",
      "     |  \n",
      "     |  __repr_str__(self, join_str: 'unicode') -> 'unicode'\n",
      "     |  \n",
      "     |  __rich_repr__(self) -> 'RichReprResult'\n",
      "     |      Get fields for Rich library\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.schema.runnable.base.Runnable:\n",
      "     |  \n",
      "     |  __or__(self, other: 'Union[Runnable[Any, Other], Callable[[Any], Other], Mapping[str, Union[Runnable[Any, Other], Callable[[Any], Other], Any]]]') -> 'RunnableSequence[Input, Other]'\n",
      "     |      Return self|value.\n",
      "     |  \n",
      "     |  __ror__(self, other: 'Union[Runnable[Other, Any], Callable[[Any], Other], Mapping[str, Union[Runnable[Other, Any], Callable[[Other], Any], Any]]]') -> 'RunnableSequence[Other, Output]'\n",
      "     |      Return value|self.\n",
      "     |  \n",
      "     |  async atransform(self, input: 'AsyncIterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'AsyncIterator[Output]'\n",
      "     |      Default implementation of atransform, which buffers input and calls astream.\n",
      "     |      Subclasses should override this method if they can start producing output while\n",
      "     |      input is still being generated.\n",
      "     |  \n",
      "     |  bind(self, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      "     |      Bind arguments to a Runnable, returning a new Runnable.\n",
      "     |  \n",
      "     |  map(self) -> 'Runnable[List[Input], List[Output]]'\n",
      "     |      Return a new Runnable that maps a list of inputs to a list of outputs,\n",
      "     |      by calling invoke() with each input.\n",
      "     |  \n",
      "     |  transform(self, input: 'Iterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'Iterator[Output]'\n",
      "     |      Default implementation of transform, which buffers input and then calls stream.\n",
      "     |      Subclasses should override this method if they can start producing output while\n",
      "     |      input is still being generated.\n",
      "     |  \n",
      "     |  with_config(self, config: 'Optional[RunnableConfig]' = None, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      "     |      Bind config to a Runnable, returning a new Runnable.\n",
      "     |  \n",
      "     |  with_fallbacks(self, fallbacks: 'Sequence[Runnable[Input, Output]]', *, exceptions_to_handle: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,)) -> 'RunnableWithFallbacks[Input, Output]'\n",
      "     |  \n",
      "     |  with_retry(self, *, retry_if_exception_type: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,), wait_exponential_jitter: 'bool' = True, stop_after_attempt: 'int' = 3) -> 'Runnable[Input, Output]'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __class_getitem__(params) from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  __init_subclass__(*args, **kwargs) from pydantic.main.ModelMetaclass\n",
      "     |      This method is called when a class is subclassed.\n",
      "     |      \n",
      "     |      The default implementation does nothing. It may be\n",
      "     |      overridden to extend subclasses.\n",
      "    \n",
      "    class Nebula(langchain.llms.base.LLM)\n",
      "     |  Nebula(*, cache: Optional[bool] = None, verbose: bool = None, callbacks: Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager, NoneType] = None, callback_manager: Optional[langchain.callbacks.base.BaseCallbackManager] = None, tags: Optional[List[str]] = None, metadata: Optional[Dict[str, Any]] = None, model_kwargs: Optional[dict] = None, nebula_service_url: Optional[str] = None, nebula_service_path: Optional[str] = None, nebula_api_key: Optional[str] = None, model: Optional[str] = None, max_new_tokens: Optional[int] = 128, temperature: Optional[float] = 0.6, top_p: Optional[float] = 0.95, repetition_penalty: Optional[float] = 1.0, top_k: Optional[int] = 0, penalty_alpha: Optional[float] = 0.0, stop_sequences: Optional[List[str]] = None, max_retries: Optional[int] = 10) -> None\n",
      "     |  \n",
      "     |  Nebula Service models.\n",
      "     |  \n",
      "     |  To use, you should have the environment variable ``NEBULA_SERVICE_URL``,\n",
      "     |  ``NEBULA_SERVICE_PATH`` and ``NEBULA_API_KEY`` set with your Nebula\n",
      "     |  Service, or pass it as a named parameter to the constructor.\n",
      "     |  \n",
      "     |  Example:\n",
      "     |      .. code-block:: python\n",
      "     |  \n",
      "     |          from langchain.llms import Nebula\n",
      "     |  \n",
      "     |          nebula = Nebula(\n",
      "     |              nebula_service_url=\"NEBULA_SERVICE_URL\",\n",
      "     |              nebula_service_path=\"NEBULA_SERVICE_PATH\",\n",
      "     |              nebula_api_key=\"NEBULA_API_KEY\",\n",
      "     |          )\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Nebula\n",
      "     |      langchain.llms.base.LLM\n",
      "     |      langchain.llms.base.BaseLLM\n",
      "     |      langchain.schema.language_model.BaseLanguageModel\n",
      "     |      langchain.load.serializable.Serializable\n",
      "     |      pydantic.main.BaseModel\n",
      "     |      pydantic.utils.Representation\n",
      "     |      langchain.schema.runnable.base.Runnable\n",
      "     |      typing.Generic\n",
      "     |      abc.ABC\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  validate_environment(values: Dict) -> Dict from pydantic.main.ModelMetaclass\n",
      "     |      Validate that api key and python package exists in environment.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __json_encoder__ = pydantic_encoder(obj: Any) -> Any\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  Config = <class 'langchain.llms.symblai_nebula.Nebula.Config'>\n",
      "     |      Configuration for this pydantic object.\n",
      "     |  \n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {'max_new_tokens': typing.Optional[int], 'max_retrie...\n",
      "     |  \n",
      "     |  __class_vars__ = set()\n",
      "     |  \n",
      "     |  __config__ = <class 'langchain.llms.symblai_nebula.Config'>\n",
      "     |  \n",
      "     |  __custom_root_type__ = False\n",
      "     |  \n",
      "     |  __exclude_fields__ = {'callback_manager': True, 'callbacks': True, 'me...\n",
      "     |  \n",
      "     |  __fields__ = {'cache': ModelField(name='cache', type=Optional[bool], r...\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "     |  \n",
      "     |  __include_fields__ = None\n",
      "     |  \n",
      "     |  __parameters__ = ()\n",
      "     |  \n",
      "     |  __post_root_validators__ = [(False, <function BaseLLM.raise_deprecatio...\n",
      "     |  \n",
      "     |  __pre_root_validators__ = []\n",
      "     |  \n",
      "     |  __private_attributes__ = {'_lc_kwargs': ModelPrivateAttr(default=Pydan...\n",
      "     |  \n",
      "     |  __schema_cache__ = {}\n",
      "     |  \n",
      "     |  __signature__ = <Signature (*, cache: Optional[bool] = None, ver...= N...\n",
      "     |  \n",
      "     |  __validators__ = {'verbose': [<pydantic.class_validators.Validator obj...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  __call__(self, prompt: 'str', stop: 'Optional[List[str]]' = None, callbacks: 'Callbacks' = None, *, tags: 'Optional[List[str]]' = None, metadata: 'Optional[Dict[str, Any]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Check Cache and run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  __str__(self) -> 'str'\n",
      "     |      Get a string representation of the object for printing.\n",
      "     |  \n",
      "     |  async abatch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      "     |      Default implementation of abatch, which calls ainvoke N times.\n",
      "     |      Subclasses should override this method if they can batch more efficiently.\n",
      "     |  \n",
      "     |  async agenerate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  async agenerate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Asynchronously pass a sequence of prompts and return model generations.\n",
      "     |      \n",
      "     |      This method should make use of batched calls for models that expose a batched\n",
      "     |      API.\n",
      "     |      \n",
      "     |      Use this method when you want to:\n",
      "     |          1. take advantage of batched calls,\n",
      "     |          2. need more output from the model than just the top generated value,\n",
      "     |          3. are building chains that are agnostic to the underlying language model\n",
      "     |              type (e.g., pure text completion models vs chat models).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      "     |              converted to match the format of any language model (string for pure\n",
      "     |              text generation models and BaseMessages for chat models).\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          callbacks: Callbacks to pass through. Used for executing additional\n",
      "     |              functionality, such as logging or streaming, throughout generation.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          An LLMResult, which contains a list of candidate Generations for each input\n",
      "     |              prompt and additional model provider-specific output.\n",
      "     |  \n",
      "     |  async ainvoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Default implementation of ainvoke, which calls invoke in a thread pool.\n",
      "     |      Subclasses should override this method if they can run asynchronously.\n",
      "     |  \n",
      "     |  async apredict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Asynchronously pass a string to the model and return a string prediction.\n",
      "     |      \n",
      "     |      Use this method when calling pure text generation models and only the top\n",
      "     |          candidate generation is needed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: String input to pass to the model.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a string.\n",
      "     |  \n",
      "     |  async apredict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      "     |      Asynchronously pass messages to the model and return a message prediction.\n",
      "     |      \n",
      "     |      Use this method when calling chat models and only the top\n",
      "     |          candidate generation is needed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: A sequence of chat messages corresponding to a single model input.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a message.\n",
      "     |  \n",
      "     |  async astream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'AsyncIterator[str]'\n",
      "     |      Default implementation of astream, which calls ainvoke.\n",
      "     |      Subclasses should override this method if they support streaming output.\n",
      "     |  \n",
      "     |  batch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      "     |      Default implementation of batch, which calls invoke N times.\n",
      "     |      Subclasses should override this method if they can batch more efficiently.\n",
      "     |  \n",
      "     |  dict(self, **kwargs: 'Any') -> 'Dict'\n",
      "     |      Return a dictionary of the LLM.\n",
      "     |  \n",
      "     |  generate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  generate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Pass a sequence of prompts to the model and return model generations.\n",
      "     |      \n",
      "     |      This method should make use of batched calls for models that expose a batched\n",
      "     |      API.\n",
      "     |      \n",
      "     |      Use this method when you want to:\n",
      "     |          1. take advantage of batched calls,\n",
      "     |          2. need more output from the model than just the top generated value,\n",
      "     |          3. are building chains that are agnostic to the underlying language model\n",
      "     |              type (e.g., pure text completion models vs chat models).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      "     |              converted to match the format of any language model (string for pure\n",
      "     |              text generation models and BaseMessages for chat models).\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          callbacks: Callbacks to pass through. Used for executing additional\n",
      "     |              functionality, such as logging or streaming, throughout generation.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          An LLMResult, which contains a list of candidate Generations for each input\n",
      "     |              prompt and additional model provider-specific output.\n",
      "     |  \n",
      "     |  invoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |  \n",
      "     |  predict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Pass a single string input to the model and return a string prediction.\n",
      "     |      \n",
      "     |       Use this method when passing in raw text. If you want to pass in specific\n",
      "     |          types of chat messages, use predict_messages.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: String input to pass to the model.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a string.\n",
      "     |  \n",
      "     |  predict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      "     |      Pass a message sequence to the model and return a message prediction.\n",
      "     |      \n",
      "     |      Use this method when passing in chat messages. If you want to pass in raw text,\n",
      "     |          use predict.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: A sequence of chat messages corresponding to a single model input.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a message.\n",
      "     |  \n",
      "     |  save(self, file_path: 'Union[Path, str]') -> 'None'\n",
      "     |      Save the LLM.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          file_path: Path to file to save the LLM to.\n",
      "     |      \n",
      "     |      Example:\n",
      "     |      .. code-block:: python\n",
      "     |      \n",
      "     |          llm.save(file_path=\"path/llm.yaml\")\n",
      "     |  \n",
      "     |  stream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'Iterator[str]'\n",
      "     |      Default implementation of stream, which calls invoke.\n",
      "     |      Subclasses should override this method if they support streaming output.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  raise_deprecation(values: 'Dict') -> 'Dict' from pydantic.main.ModelMetaclass\n",
      "     |      Raise deprecation warning if callback_manager is used.\n",
      "     |  \n",
      "     |  set_verbose(verbose: 'Optional[bool]') -> 'bool' from pydantic.main.ModelMetaclass\n",
      "     |      If verbose is None, set it.\n",
      "     |      \n",
      "     |      This allows users to pass in None as verbose to access the global setting.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  __orig_bases__ = (langchain.schema.language_model.BaseLanguageModel[st...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.schema.language_model.BaseLanguageModel:\n",
      "     |  \n",
      "     |  get_num_tokens(self, text: 'str') -> 'int'\n",
      "     |      Get the number of tokens present in the text.\n",
      "     |      \n",
      "     |      Useful for checking if an input will fit in a model's context window.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: The string input to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The integer number of tokens in the text.\n",
      "     |  \n",
      "     |  get_num_tokens_from_messages(self, messages: 'List[BaseMessage]') -> 'int'\n",
      "     |      Get the number of tokens in the messages.\n",
      "     |      \n",
      "     |      Useful for checking if an input will fit in a model's context window.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: The message inputs to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The sum of the number of tokens across the messages.\n",
      "     |  \n",
      "     |  get_token_ids(self, text: 'str') -> 'List[int]'\n",
      "     |      Return the ordered ids of the tokens in a text.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: The string input to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          A list of ids corresponding to the tokens in the text, in order they occur\n",
      "     |              in the text.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from langchain.schema.language_model.BaseLanguageModel:\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.load.serializable.Serializable:\n",
      "     |  \n",
      "     |  __init__(self, **kwargs: Any) -> None\n",
      "     |      Create a new model by parsing and validating input data from keyword arguments.\n",
      "     |      \n",
      "     |      Raises ValidationError if the input data cannot be parsed to form a valid model.\n",
      "     |  \n",
      "     |  to_json(self) -> Union[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\n",
      "     |  \n",
      "     |  to_json_not_implemented(self) -> langchain.load.serializable.SerializedNotImplemented\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from langchain.load.serializable.Serializable:\n",
      "     |  \n",
      "     |  lc_attributes\n",
      "     |      Return a list of attribute names that should be included in the\n",
      "     |      serialized kwargs. These attributes must be accepted by the\n",
      "     |      constructor.\n",
      "     |  \n",
      "     |  lc_namespace\n",
      "     |      Return the namespace of the langchain object.\n",
      "     |      eg. [\"langchain\", \"llms\", \"openai\"]\n",
      "     |  \n",
      "     |  lc_secrets\n",
      "     |      Return a map of constructor argument names to secret ids.\n",
      "     |      eg. {\"openai_api_key\": \"OPENAI_API_KEY\"}\n",
      "     |  \n",
      "     |  lc_serializable\n",
      "     |      Return whether or not the class is serializable.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __eq__(self, other: Any) -> bool\n",
      "     |  \n",
      "     |  __getstate__(self) -> 'DictAny'\n",
      "     |  \n",
      "     |  __iter__(self) -> 'TupleGenerator'\n",
      "     |      so `dict(model)` works\n",
      "     |  \n",
      "     |  __repr_args__(self) -> 'ReprArgs'\n",
      "     |  \n",
      "     |  __setattr__(self, name, value)\n",
      "     |  \n",
      "     |  __setstate__(self, state: 'DictAny') -> None\n",
      "     |  \n",
      "     |  copy(self: 'Model', *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, update: Optional[ForwardRef('DictStrAny')] = None, deep: bool = False) -> 'Model'\n",
      "     |      Duplicate a model, optionally choose which fields to include, exclude and change.\n",
      "     |      \n",
      "     |      :param include: fields to include in new model\n",
      "     |      :param exclude: fields to exclude from new model, as with values this takes precedence over include\n",
      "     |      :param update: values to change/add in the new model. Note: the data is not validated before creating\n",
      "     |          the new model: you should trust this data\n",
      "     |      :param deep: set to `True` to make a deep copy of the model\n",
      "     |      :return: new model instance\n",
      "     |  \n",
      "     |  json(self, *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) -> 'unicode'\n",
      "     |      Generate a JSON representation of the model, `include` and `exclude` arguments as per `dict()`.\n",
      "     |      \n",
      "     |      `encoder` is an optional function to supply as `default` to json.dumps(), other arguments as per `json.dumps()`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __get_validators__() -> 'CallableGenerator' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  __try_update_forward_refs__(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      "     |      Same as update_forward_refs but will not raise exception\n",
      "     |      when forward references are not defined.\n",
      "     |  \n",
      "     |  construct(_fields_set: Optional[ForwardRef('SetStr')] = None, **values: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |      Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data.\n",
      "     |      Default values are respected, but no other validation is performed.\n",
      "     |      Behaves as if `Config.extra = 'allow'` was set since it adds all passed values\n",
      "     |  \n",
      "     |  from_orm(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_file(path: Union[str, pathlib.Path], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_obj(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_raw(b: Union[str, bytes], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema(by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}') -> 'DictStrAny' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema_json(*, by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}', **dumps_kwargs: Any) -> 'unicode' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  update_forward_refs(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      "     |      Try to update ForwardRefs on fields based on this Model, globalns and localns.\n",
      "     |  \n",
      "     |  validate(value: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __fields_set__\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.utils.Representation:\n",
      "     |  \n",
      "     |  __pretty__(self, fmt: Callable[[Any], Any], **kwargs: Any) -> Generator[Any, NoneType, NoneType]\n",
      "     |      Used by devtools (https://python-devtools.helpmanual.io/) to provide a human readable representations of objects\n",
      "     |  \n",
      "     |  __repr__(self) -> 'unicode'\n",
      "     |  \n",
      "     |  __repr_name__(self) -> 'unicode'\n",
      "     |      Name of the instance's class, used in __repr__.\n",
      "     |  \n",
      "     |  __repr_str__(self, join_str: 'unicode') -> 'unicode'\n",
      "     |  \n",
      "     |  __rich_repr__(self) -> 'RichReprResult'\n",
      "     |      Get fields for Rich library\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.schema.runnable.base.Runnable:\n",
      "     |  \n",
      "     |  __or__(self, other: 'Union[Runnable[Any, Other], Callable[[Any], Other], Mapping[str, Union[Runnable[Any, Other], Callable[[Any], Other], Any]]]') -> 'RunnableSequence[Input, Other]'\n",
      "     |      Return self|value.\n",
      "     |  \n",
      "     |  __ror__(self, other: 'Union[Runnable[Other, Any], Callable[[Any], Other], Mapping[str, Union[Runnable[Other, Any], Callable[[Other], Any], Any]]]') -> 'RunnableSequence[Other, Output]'\n",
      "     |      Return value|self.\n",
      "     |  \n",
      "     |  async atransform(self, input: 'AsyncIterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'AsyncIterator[Output]'\n",
      "     |      Default implementation of atransform, which buffers input and calls astream.\n",
      "     |      Subclasses should override this method if they can start producing output while\n",
      "     |      input is still being generated.\n",
      "     |  \n",
      "     |  bind(self, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      "     |      Bind arguments to a Runnable, returning a new Runnable.\n",
      "     |  \n",
      "     |  map(self) -> 'Runnable[List[Input], List[Output]]'\n",
      "     |      Return a new Runnable that maps a list of inputs to a list of outputs,\n",
      "     |      by calling invoke() with each input.\n",
      "     |  \n",
      "     |  transform(self, input: 'Iterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'Iterator[Output]'\n",
      "     |      Default implementation of transform, which buffers input and then calls stream.\n",
      "     |      Subclasses should override this method if they can start producing output while\n",
      "     |      input is still being generated.\n",
      "     |  \n",
      "     |  with_config(self, config: 'Optional[RunnableConfig]' = None, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      "     |      Bind config to a Runnable, returning a new Runnable.\n",
      "     |  \n",
      "     |  with_fallbacks(self, fallbacks: 'Sequence[Runnable[Input, Output]]', *, exceptions_to_handle: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,)) -> 'RunnableWithFallbacks[Input, Output]'\n",
      "     |  \n",
      "     |  with_retry(self, *, retry_if_exception_type: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,), wait_exponential_jitter: 'bool' = True, stop_after_attempt: 'int' = 3) -> 'Runnable[Input, Output]'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __class_getitem__(params) from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  __init_subclass__(*args, **kwargs) from pydantic.main.ModelMetaclass\n",
      "     |      This method is called when a class is subclassed.\n",
      "     |      \n",
      "     |      The default implementation does nothing. It may be\n",
      "     |      overridden to extend subclasses.\n",
      "    \n",
      "    class OctoAIEndpoint(langchain.llms.base.LLM)\n",
      "     |  OctoAIEndpoint(*, cache: Optional[bool] = None, verbose: bool = None, callbacks: Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager, NoneType] = None, callback_manager: Optional[langchain.callbacks.base.BaseCallbackManager] = None, tags: Optional[List[str]] = None, metadata: Optional[Dict[str, Any]] = None, endpoint_url: Optional[str] = None, model_kwargs: Optional[dict] = None, octoai_api_token: Optional[str] = None) -> None\n",
      "     |  \n",
      "     |  OctoAI LLM Endpoints.\n",
      "     |  \n",
      "     |  OctoAIEndpoint is a class to interact with OctoAI\n",
      "     |   Compute Service large language model endpoints.\n",
      "     |  \n",
      "     |  To use, you should have the ``octoai`` python package installed, and the\n",
      "     |  environment variable ``OCTOAI_API_TOKEN`` set with your API token, or pass\n",
      "     |  it as a named parameter to the constructor.\n",
      "     |  \n",
      "     |  Example:\n",
      "     |      .. code-block:: python\n",
      "     |  \n",
      "     |          from langchain.llms.octoai_endpoint  import OctoAIEndpoint\n",
      "     |          OctoAIEndpoint(\n",
      "     |              octoai_api_token=\"octoai-api-key\",\n",
      "     |              endpoint_url=\"https://mpt-7b-demo-kk0powt97tmb.octoai.cloud/generate\",\n",
      "     |              model_kwargs={\n",
      "     |                  \"max_new_tokens\": 200,\n",
      "     |                  \"temperature\": 0.75,\n",
      "     |                  \"top_p\": 0.95,\n",
      "     |                  \"repetition_penalty\": 1,\n",
      "     |                  \"seed\": None,\n",
      "     |                  \"stop\": [],\n",
      "     |              },\n",
      "     |          )\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      OctoAIEndpoint\n",
      "     |      langchain.llms.base.LLM\n",
      "     |      langchain.llms.base.BaseLLM\n",
      "     |      langchain.schema.language_model.BaseLanguageModel\n",
      "     |      langchain.load.serializable.Serializable\n",
      "     |      pydantic.main.BaseModel\n",
      "     |      pydantic.utils.Representation\n",
      "     |      langchain.schema.runnable.base.Runnable\n",
      "     |      typing.Generic\n",
      "     |      abc.ABC\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  validate_environment(values: Dict) -> Dict from pydantic.main.ModelMetaclass\n",
      "     |      Validate that api key and python package exists in environment.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __json_encoder__ = pydantic_encoder(obj: Any) -> Any\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  Config = <class 'langchain.llms.octoai_endpoint.OctoAIEndpoint.Config'...\n",
      "     |      Configuration for this pydantic object.\n",
      "     |  \n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {'endpoint_url': typing.Optional[str], 'model_kwargs...\n",
      "     |  \n",
      "     |  __class_vars__ = set()\n",
      "     |  \n",
      "     |  __config__ = <class 'langchain.llms.octoai_endpoint.Config'>\n",
      "     |  \n",
      "     |  __custom_root_type__ = False\n",
      "     |  \n",
      "     |  __exclude_fields__ = {'callback_manager': True, 'callbacks': True, 'me...\n",
      "     |  \n",
      "     |  __fields__ = {'cache': ModelField(name='cache', type=Optional[bool], r...\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "     |  \n",
      "     |  __include_fields__ = None\n",
      "     |  \n",
      "     |  __parameters__ = ()\n",
      "     |  \n",
      "     |  __post_root_validators__ = [(False, <function BaseLLM.raise_deprecatio...\n",
      "     |  \n",
      "     |  __pre_root_validators__ = []\n",
      "     |  \n",
      "     |  __private_attributes__ = {'_lc_kwargs': ModelPrivateAttr(default=Pydan...\n",
      "     |  \n",
      "     |  __schema_cache__ = {}\n",
      "     |  \n",
      "     |  __signature__ = <Signature (*, cache: Optional[bool] = None, ver... oc...\n",
      "     |  \n",
      "     |  __validators__ = {'verbose': [<pydantic.class_validators.Validator obj...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  __call__(self, prompt: 'str', stop: 'Optional[List[str]]' = None, callbacks: 'Callbacks' = None, *, tags: 'Optional[List[str]]' = None, metadata: 'Optional[Dict[str, Any]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Check Cache and run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  __str__(self) -> 'str'\n",
      "     |      Get a string representation of the object for printing.\n",
      "     |  \n",
      "     |  async abatch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      "     |      Default implementation of abatch, which calls ainvoke N times.\n",
      "     |      Subclasses should override this method if they can batch more efficiently.\n",
      "     |  \n",
      "     |  async agenerate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  async agenerate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Asynchronously pass a sequence of prompts and return model generations.\n",
      "     |      \n",
      "     |      This method should make use of batched calls for models that expose a batched\n",
      "     |      API.\n",
      "     |      \n",
      "     |      Use this method when you want to:\n",
      "     |          1. take advantage of batched calls,\n",
      "     |          2. need more output from the model than just the top generated value,\n",
      "     |          3. are building chains that are agnostic to the underlying language model\n",
      "     |              type (e.g., pure text completion models vs chat models).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      "     |              converted to match the format of any language model (string for pure\n",
      "     |              text generation models and BaseMessages for chat models).\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          callbacks: Callbacks to pass through. Used for executing additional\n",
      "     |              functionality, such as logging or streaming, throughout generation.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          An LLMResult, which contains a list of candidate Generations for each input\n",
      "     |              prompt and additional model provider-specific output.\n",
      "     |  \n",
      "     |  async ainvoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Default implementation of ainvoke, which calls invoke in a thread pool.\n",
      "     |      Subclasses should override this method if they can run asynchronously.\n",
      "     |  \n",
      "     |  async apredict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Asynchronously pass a string to the model and return a string prediction.\n",
      "     |      \n",
      "     |      Use this method when calling pure text generation models and only the top\n",
      "     |          candidate generation is needed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: String input to pass to the model.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a string.\n",
      "     |  \n",
      "     |  async apredict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      "     |      Asynchronously pass messages to the model and return a message prediction.\n",
      "     |      \n",
      "     |      Use this method when calling chat models and only the top\n",
      "     |          candidate generation is needed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: A sequence of chat messages corresponding to a single model input.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a message.\n",
      "     |  \n",
      "     |  async astream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'AsyncIterator[str]'\n",
      "     |      Default implementation of astream, which calls ainvoke.\n",
      "     |      Subclasses should override this method if they support streaming output.\n",
      "     |  \n",
      "     |  batch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      "     |      Default implementation of batch, which calls invoke N times.\n",
      "     |      Subclasses should override this method if they can batch more efficiently.\n",
      "     |  \n",
      "     |  dict(self, **kwargs: 'Any') -> 'Dict'\n",
      "     |      Return a dictionary of the LLM.\n",
      "     |  \n",
      "     |  generate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  generate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Pass a sequence of prompts to the model and return model generations.\n",
      "     |      \n",
      "     |      This method should make use of batched calls for models that expose a batched\n",
      "     |      API.\n",
      "     |      \n",
      "     |      Use this method when you want to:\n",
      "     |          1. take advantage of batched calls,\n",
      "     |          2. need more output from the model than just the top generated value,\n",
      "     |          3. are building chains that are agnostic to the underlying language model\n",
      "     |              type (e.g., pure text completion models vs chat models).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      "     |              converted to match the format of any language model (string for pure\n",
      "     |              text generation models and BaseMessages for chat models).\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          callbacks: Callbacks to pass through. Used for executing additional\n",
      "     |              functionality, such as logging or streaming, throughout generation.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          An LLMResult, which contains a list of candidate Generations for each input\n",
      "     |              prompt and additional model provider-specific output.\n",
      "     |  \n",
      "     |  invoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |  \n",
      "     |  predict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Pass a single string input to the model and return a string prediction.\n",
      "     |      \n",
      "     |       Use this method when passing in raw text. If you want to pass in specific\n",
      "     |          types of chat messages, use predict_messages.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: String input to pass to the model.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a string.\n",
      "     |  \n",
      "     |  predict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      "     |      Pass a message sequence to the model and return a message prediction.\n",
      "     |      \n",
      "     |      Use this method when passing in chat messages. If you want to pass in raw text,\n",
      "     |          use predict.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: A sequence of chat messages corresponding to a single model input.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a message.\n",
      "     |  \n",
      "     |  save(self, file_path: 'Union[Path, str]') -> 'None'\n",
      "     |      Save the LLM.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          file_path: Path to file to save the LLM to.\n",
      "     |      \n",
      "     |      Example:\n",
      "     |      .. code-block:: python\n",
      "     |      \n",
      "     |          llm.save(file_path=\"path/llm.yaml\")\n",
      "     |  \n",
      "     |  stream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'Iterator[str]'\n",
      "     |      Default implementation of stream, which calls invoke.\n",
      "     |      Subclasses should override this method if they support streaming output.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  raise_deprecation(values: 'Dict') -> 'Dict' from pydantic.main.ModelMetaclass\n",
      "     |      Raise deprecation warning if callback_manager is used.\n",
      "     |  \n",
      "     |  set_verbose(verbose: 'Optional[bool]') -> 'bool' from pydantic.main.ModelMetaclass\n",
      "     |      If verbose is None, set it.\n",
      "     |      \n",
      "     |      This allows users to pass in None as verbose to access the global setting.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  __orig_bases__ = (langchain.schema.language_model.BaseLanguageModel[st...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.schema.language_model.BaseLanguageModel:\n",
      "     |  \n",
      "     |  get_num_tokens(self, text: 'str') -> 'int'\n",
      "     |      Get the number of tokens present in the text.\n",
      "     |      \n",
      "     |      Useful for checking if an input will fit in a model's context window.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: The string input to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The integer number of tokens in the text.\n",
      "     |  \n",
      "     |  get_num_tokens_from_messages(self, messages: 'List[BaseMessage]') -> 'int'\n",
      "     |      Get the number of tokens in the messages.\n",
      "     |      \n",
      "     |      Useful for checking if an input will fit in a model's context window.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: The message inputs to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The sum of the number of tokens across the messages.\n",
      "     |  \n",
      "     |  get_token_ids(self, text: 'str') -> 'List[int]'\n",
      "     |      Return the ordered ids of the tokens in a text.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: The string input to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          A list of ids corresponding to the tokens in the text, in order they occur\n",
      "     |              in the text.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from langchain.schema.language_model.BaseLanguageModel:\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.load.serializable.Serializable:\n",
      "     |  \n",
      "     |  __init__(self, **kwargs: Any) -> None\n",
      "     |      Create a new model by parsing and validating input data from keyword arguments.\n",
      "     |      \n",
      "     |      Raises ValidationError if the input data cannot be parsed to form a valid model.\n",
      "     |  \n",
      "     |  to_json(self) -> Union[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\n",
      "     |  \n",
      "     |  to_json_not_implemented(self) -> langchain.load.serializable.SerializedNotImplemented\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from langchain.load.serializable.Serializable:\n",
      "     |  \n",
      "     |  lc_attributes\n",
      "     |      Return a list of attribute names that should be included in the\n",
      "     |      serialized kwargs. These attributes must be accepted by the\n",
      "     |      constructor.\n",
      "     |  \n",
      "     |  lc_namespace\n",
      "     |      Return the namespace of the langchain object.\n",
      "     |      eg. [\"langchain\", \"llms\", \"openai\"]\n",
      "     |  \n",
      "     |  lc_secrets\n",
      "     |      Return a map of constructor argument names to secret ids.\n",
      "     |      eg. {\"openai_api_key\": \"OPENAI_API_KEY\"}\n",
      "     |  \n",
      "     |  lc_serializable\n",
      "     |      Return whether or not the class is serializable.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __eq__(self, other: Any) -> bool\n",
      "     |  \n",
      "     |  __getstate__(self) -> 'DictAny'\n",
      "     |  \n",
      "     |  __iter__(self) -> 'TupleGenerator'\n",
      "     |      so `dict(model)` works\n",
      "     |  \n",
      "     |  __repr_args__(self) -> 'ReprArgs'\n",
      "     |  \n",
      "     |  __setattr__(self, name, value)\n",
      "     |  \n",
      "     |  __setstate__(self, state: 'DictAny') -> None\n",
      "     |  \n",
      "     |  copy(self: 'Model', *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, update: Optional[ForwardRef('DictStrAny')] = None, deep: bool = False) -> 'Model'\n",
      "     |      Duplicate a model, optionally choose which fields to include, exclude and change.\n",
      "     |      \n",
      "     |      :param include: fields to include in new model\n",
      "     |      :param exclude: fields to exclude from new model, as with values this takes precedence over include\n",
      "     |      :param update: values to change/add in the new model. Note: the data is not validated before creating\n",
      "     |          the new model: you should trust this data\n",
      "     |      :param deep: set to `True` to make a deep copy of the model\n",
      "     |      :return: new model instance\n",
      "     |  \n",
      "     |  json(self, *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) -> 'unicode'\n",
      "     |      Generate a JSON representation of the model, `include` and `exclude` arguments as per `dict()`.\n",
      "     |      \n",
      "     |      `encoder` is an optional function to supply as `default` to json.dumps(), other arguments as per `json.dumps()`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __get_validators__() -> 'CallableGenerator' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  __try_update_forward_refs__(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      "     |      Same as update_forward_refs but will not raise exception\n",
      "     |      when forward references are not defined.\n",
      "     |  \n",
      "     |  construct(_fields_set: Optional[ForwardRef('SetStr')] = None, **values: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |      Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data.\n",
      "     |      Default values are respected, but no other validation is performed.\n",
      "     |      Behaves as if `Config.extra = 'allow'` was set since it adds all passed values\n",
      "     |  \n",
      "     |  from_orm(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_file(path: Union[str, pathlib.Path], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_obj(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_raw(b: Union[str, bytes], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema(by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}') -> 'DictStrAny' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema_json(*, by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}', **dumps_kwargs: Any) -> 'unicode' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  update_forward_refs(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      "     |      Try to update ForwardRefs on fields based on this Model, globalns and localns.\n",
      "     |  \n",
      "     |  validate(value: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __fields_set__\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.utils.Representation:\n",
      "     |  \n",
      "     |  __pretty__(self, fmt: Callable[[Any], Any], **kwargs: Any) -> Generator[Any, NoneType, NoneType]\n",
      "     |      Used by devtools (https://python-devtools.helpmanual.io/) to provide a human readable representations of objects\n",
      "     |  \n",
      "     |  __repr__(self) -> 'unicode'\n",
      "     |  \n",
      "     |  __repr_name__(self) -> 'unicode'\n",
      "     |      Name of the instance's class, used in __repr__.\n",
      "     |  \n",
      "     |  __repr_str__(self, join_str: 'unicode') -> 'unicode'\n",
      "     |  \n",
      "     |  __rich_repr__(self) -> 'RichReprResult'\n",
      "     |      Get fields for Rich library\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.schema.runnable.base.Runnable:\n",
      "     |  \n",
      "     |  __or__(self, other: 'Union[Runnable[Any, Other], Callable[[Any], Other], Mapping[str, Union[Runnable[Any, Other], Callable[[Any], Other], Any]]]') -> 'RunnableSequence[Input, Other]'\n",
      "     |      Return self|value.\n",
      "     |  \n",
      "     |  __ror__(self, other: 'Union[Runnable[Other, Any], Callable[[Any], Other], Mapping[str, Union[Runnable[Other, Any], Callable[[Other], Any], Any]]]') -> 'RunnableSequence[Other, Output]'\n",
      "     |      Return value|self.\n",
      "     |  \n",
      "     |  async atransform(self, input: 'AsyncIterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'AsyncIterator[Output]'\n",
      "     |      Default implementation of atransform, which buffers input and calls astream.\n",
      "     |      Subclasses should override this method if they can start producing output while\n",
      "     |      input is still being generated.\n",
      "     |  \n",
      "     |  bind(self, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      "     |      Bind arguments to a Runnable, returning a new Runnable.\n",
      "     |  \n",
      "     |  map(self) -> 'Runnable[List[Input], List[Output]]'\n",
      "     |      Return a new Runnable that maps a list of inputs to a list of outputs,\n",
      "     |      by calling invoke() with each input.\n",
      "     |  \n",
      "     |  transform(self, input: 'Iterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'Iterator[Output]'\n",
      "     |      Default implementation of transform, which buffers input and then calls stream.\n",
      "     |      Subclasses should override this method if they can start producing output while\n",
      "     |      input is still being generated.\n",
      "     |  \n",
      "     |  with_config(self, config: 'Optional[RunnableConfig]' = None, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      "     |      Bind config to a Runnable, returning a new Runnable.\n",
      "     |  \n",
      "     |  with_fallbacks(self, fallbacks: 'Sequence[Runnable[Input, Output]]', *, exceptions_to_handle: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,)) -> 'RunnableWithFallbacks[Input, Output]'\n",
      "     |  \n",
      "     |  with_retry(self, *, retry_if_exception_type: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,), wait_exponential_jitter: 'bool' = True, stop_after_attempt: 'int' = 3) -> 'Runnable[Input, Output]'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __class_getitem__(params) from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  __init_subclass__(*args, **kwargs) from pydantic.main.ModelMetaclass\n",
      "     |      This method is called when a class is subclassed.\n",
      "     |      \n",
      "     |      The default implementation does nothing. It may be\n",
      "     |      overridden to extend subclasses.\n",
      "    \n",
      "    class Ollama(langchain.llms.base.BaseLLM, _OllamaCommon)\n",
      "     |  Ollama(*, base_url: str = 'http://localhost:11434', model: str = 'llama2', mirostat: Optional[int] = None, mirostat_eta: Optional[float] = None, mirostat_tau: Optional[float] = None, num_ctx: Optional[int] = None, num_gpu: Optional[int] = None, num_thread: Optional[int] = None, repeat_last_n: Optional[int] = None, repeat_penalty: Optional[float] = None, temperature: Optional[float] = None, stop: Optional[List[str]] = None, tfs_z: Optional[float] = None, top_k: Optional[int] = None, top_p: Optional[int] = None, cache: Optional[bool] = None, verbose: bool = None, callbacks: Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager, NoneType] = None, callback_manager: Optional[langchain.callbacks.base.BaseCallbackManager] = None, tags: Optional[List[str]] = None, metadata: Optional[Dict[str, Any]] = None) -> None\n",
      "     |  \n",
      "     |  Ollama locally runs large language models.\n",
      "     |  \n",
      "     |  To use, follow the instructions at https://ollama.ai/.\n",
      "     |  \n",
      "     |  Example:\n",
      "     |      .. code-block:: python\n",
      "     |  \n",
      "     |          from langchain.llms import Ollama\n",
      "     |          ollama = Ollama(model=\"llama2\")\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Ollama\n",
      "     |      langchain.llms.base.BaseLLM\n",
      "     |      _OllamaCommon\n",
      "     |      langchain.schema.language_model.BaseLanguageModel\n",
      "     |      langchain.load.serializable.Serializable\n",
      "     |      pydantic.main.BaseModel\n",
      "     |      pydantic.utils.Representation\n",
      "     |      langchain.schema.runnable.base.Runnable\n",
      "     |      typing.Generic\n",
      "     |      abc.ABC\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __json_encoder__ = pydantic_encoder(obj: Any) -> Any\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  Config = <class 'langchain.llms.ollama.Ollama.Config'>\n",
      "     |      Configuration for this pydantic object.\n",
      "     |  \n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {}\n",
      "     |  \n",
      "     |  __class_vars__ = set()\n",
      "     |  \n",
      "     |  __config__ = <class 'langchain.llms.ollama.Config'>\n",
      "     |  \n",
      "     |  __custom_root_type__ = False\n",
      "     |  \n",
      "     |  __exclude_fields__ = {'callback_manager': True, 'callbacks': True, 'me...\n",
      "     |  \n",
      "     |  __fields__ = {'base_url': ModelField(name='base_url', type=str, requir...\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "     |  \n",
      "     |  __include_fields__ = None\n",
      "     |  \n",
      "     |  __parameters__ = ()\n",
      "     |  \n",
      "     |  __post_root_validators__ = [(False, <function BaseLLM.raise_deprecatio...\n",
      "     |  \n",
      "     |  __pre_root_validators__ = []\n",
      "     |  \n",
      "     |  __private_attributes__ = {'_lc_kwargs': ModelPrivateAttr(default=Pydan...\n",
      "     |  \n",
      "     |  __schema_cache__ = {}\n",
      "     |  \n",
      "     |  __signature__ = <Signature (*, base_url: str = 'http://localhost...tad...\n",
      "     |  \n",
      "     |  __validators__ = {'verbose': [<pydantic.class_validators.Validator obj...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  __call__(self, prompt: 'str', stop: 'Optional[List[str]]' = None, callbacks: 'Callbacks' = None, *, tags: 'Optional[List[str]]' = None, metadata: 'Optional[Dict[str, Any]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Check Cache and run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  __str__(self) -> 'str'\n",
      "     |      Get a string representation of the object for printing.\n",
      "     |  \n",
      "     |  async abatch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      "     |      Default implementation of abatch, which calls ainvoke N times.\n",
      "     |      Subclasses should override this method if they can batch more efficiently.\n",
      "     |  \n",
      "     |  async agenerate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  async agenerate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Asynchronously pass a sequence of prompts and return model generations.\n",
      "     |      \n",
      "     |      This method should make use of batched calls for models that expose a batched\n",
      "     |      API.\n",
      "     |      \n",
      "     |      Use this method when you want to:\n",
      "     |          1. take advantage of batched calls,\n",
      "     |          2. need more output from the model than just the top generated value,\n",
      "     |          3. are building chains that are agnostic to the underlying language model\n",
      "     |              type (e.g., pure text completion models vs chat models).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      "     |              converted to match the format of any language model (string for pure\n",
      "     |              text generation models and BaseMessages for chat models).\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          callbacks: Callbacks to pass through. Used for executing additional\n",
      "     |              functionality, such as logging or streaming, throughout generation.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          An LLMResult, which contains a list of candidate Generations for each input\n",
      "     |              prompt and additional model provider-specific output.\n",
      "     |  \n",
      "     |  async ainvoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Default implementation of ainvoke, which calls invoke in a thread pool.\n",
      "     |      Subclasses should override this method if they can run asynchronously.\n",
      "     |  \n",
      "     |  async apredict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Asynchronously pass a string to the model and return a string prediction.\n",
      "     |      \n",
      "     |      Use this method when calling pure text generation models and only the top\n",
      "     |          candidate generation is needed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: String input to pass to the model.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a string.\n",
      "     |  \n",
      "     |  async apredict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      "     |      Asynchronously pass messages to the model and return a message prediction.\n",
      "     |      \n",
      "     |      Use this method when calling chat models and only the top\n",
      "     |          candidate generation is needed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: A sequence of chat messages corresponding to a single model input.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a message.\n",
      "     |  \n",
      "     |  async astream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'AsyncIterator[str]'\n",
      "     |      Default implementation of astream, which calls ainvoke.\n",
      "     |      Subclasses should override this method if they support streaming output.\n",
      "     |  \n",
      "     |  batch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      "     |      Default implementation of batch, which calls invoke N times.\n",
      "     |      Subclasses should override this method if they can batch more efficiently.\n",
      "     |  \n",
      "     |  dict(self, **kwargs: 'Any') -> 'Dict'\n",
      "     |      Return a dictionary of the LLM.\n",
      "     |  \n",
      "     |  generate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  generate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Pass a sequence of prompts to the model and return model generations.\n",
      "     |      \n",
      "     |      This method should make use of batched calls for models that expose a batched\n",
      "     |      API.\n",
      "     |      \n",
      "     |      Use this method when you want to:\n",
      "     |          1. take advantage of batched calls,\n",
      "     |          2. need more output from the model than just the top generated value,\n",
      "     |          3. are building chains that are agnostic to the underlying language model\n",
      "     |              type (e.g., pure text completion models vs chat models).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      "     |              converted to match the format of any language model (string for pure\n",
      "     |              text generation models and BaseMessages for chat models).\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          callbacks: Callbacks to pass through. Used for executing additional\n",
      "     |              functionality, such as logging or streaming, throughout generation.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          An LLMResult, which contains a list of candidate Generations for each input\n",
      "     |              prompt and additional model provider-specific output.\n",
      "     |  \n",
      "     |  invoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |  \n",
      "     |  predict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Pass a single string input to the model and return a string prediction.\n",
      "     |      \n",
      "     |       Use this method when passing in raw text. If you want to pass in specific\n",
      "     |          types of chat messages, use predict_messages.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: String input to pass to the model.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a string.\n",
      "     |  \n",
      "     |  predict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      "     |      Pass a message sequence to the model and return a message prediction.\n",
      "     |      \n",
      "     |      Use this method when passing in chat messages. If you want to pass in raw text,\n",
      "     |          use predict.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: A sequence of chat messages corresponding to a single model input.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a message.\n",
      "     |  \n",
      "     |  save(self, file_path: 'Union[Path, str]') -> 'None'\n",
      "     |      Save the LLM.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          file_path: Path to file to save the LLM to.\n",
      "     |      \n",
      "     |      Example:\n",
      "     |      .. code-block:: python\n",
      "     |      \n",
      "     |          llm.save(file_path=\"path/llm.yaml\")\n",
      "     |  \n",
      "     |  stream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'Iterator[str]'\n",
      "     |      Default implementation of stream, which calls invoke.\n",
      "     |      Subclasses should override this method if they support streaming output.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  raise_deprecation(values: 'Dict') -> 'Dict' from pydantic.main.ModelMetaclass\n",
      "     |      Raise deprecation warning if callback_manager is used.\n",
      "     |  \n",
      "     |  set_verbose(verbose: 'Optional[bool]') -> 'bool' from pydantic.main.ModelMetaclass\n",
      "     |      If verbose is None, set it.\n",
      "     |      \n",
      "     |      This allows users to pass in None as verbose to access the global setting.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  __orig_bases__ = (langchain.schema.language_model.BaseLanguageModel[st...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.schema.language_model.BaseLanguageModel:\n",
      "     |  \n",
      "     |  get_num_tokens(self, text: 'str') -> 'int'\n",
      "     |      Get the number of tokens present in the text.\n",
      "     |      \n",
      "     |      Useful for checking if an input will fit in a model's context window.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: The string input to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The integer number of tokens in the text.\n",
      "     |  \n",
      "     |  get_num_tokens_from_messages(self, messages: 'List[BaseMessage]') -> 'int'\n",
      "     |      Get the number of tokens in the messages.\n",
      "     |      \n",
      "     |      Useful for checking if an input will fit in a model's context window.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: The message inputs to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The sum of the number of tokens across the messages.\n",
      "     |  \n",
      "     |  get_token_ids(self, text: 'str') -> 'List[int]'\n",
      "     |      Return the ordered ids of the tokens in a text.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: The string input to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          A list of ids corresponding to the tokens in the text, in order they occur\n",
      "     |              in the text.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from langchain.schema.language_model.BaseLanguageModel:\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.load.serializable.Serializable:\n",
      "     |  \n",
      "     |  __init__(self, **kwargs: Any) -> None\n",
      "     |      Create a new model by parsing and validating input data from keyword arguments.\n",
      "     |      \n",
      "     |      Raises ValidationError if the input data cannot be parsed to form a valid model.\n",
      "     |  \n",
      "     |  to_json(self) -> Union[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\n",
      "     |  \n",
      "     |  to_json_not_implemented(self) -> langchain.load.serializable.SerializedNotImplemented\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from langchain.load.serializable.Serializable:\n",
      "     |  \n",
      "     |  lc_attributes\n",
      "     |      Return a list of attribute names that should be included in the\n",
      "     |      serialized kwargs. These attributes must be accepted by the\n",
      "     |      constructor.\n",
      "     |  \n",
      "     |  lc_namespace\n",
      "     |      Return the namespace of the langchain object.\n",
      "     |      eg. [\"langchain\", \"llms\", \"openai\"]\n",
      "     |  \n",
      "     |  lc_secrets\n",
      "     |      Return a map of constructor argument names to secret ids.\n",
      "     |      eg. {\"openai_api_key\": \"OPENAI_API_KEY\"}\n",
      "     |  \n",
      "     |  lc_serializable\n",
      "     |      Return whether or not the class is serializable.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __eq__(self, other: Any) -> bool\n",
      "     |  \n",
      "     |  __getstate__(self) -> 'DictAny'\n",
      "     |  \n",
      "     |  __iter__(self) -> 'TupleGenerator'\n",
      "     |      so `dict(model)` works\n",
      "     |  \n",
      "     |  __repr_args__(self) -> 'ReprArgs'\n",
      "     |  \n",
      "     |  __setattr__(self, name, value)\n",
      "     |  \n",
      "     |  __setstate__(self, state: 'DictAny') -> None\n",
      "     |  \n",
      "     |  copy(self: 'Model', *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, update: Optional[ForwardRef('DictStrAny')] = None, deep: bool = False) -> 'Model'\n",
      "     |      Duplicate a model, optionally choose which fields to include, exclude and change.\n",
      "     |      \n",
      "     |      :param include: fields to include in new model\n",
      "     |      :param exclude: fields to exclude from new model, as with values this takes precedence over include\n",
      "     |      :param update: values to change/add in the new model. Note: the data is not validated before creating\n",
      "     |          the new model: you should trust this data\n",
      "     |      :param deep: set to `True` to make a deep copy of the model\n",
      "     |      :return: new model instance\n",
      "     |  \n",
      "     |  json(self, *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) -> 'unicode'\n",
      "     |      Generate a JSON representation of the model, `include` and `exclude` arguments as per `dict()`.\n",
      "     |      \n",
      "     |      `encoder` is an optional function to supply as `default` to json.dumps(), other arguments as per `json.dumps()`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __get_validators__() -> 'CallableGenerator' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  __try_update_forward_refs__(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      "     |      Same as update_forward_refs but will not raise exception\n",
      "     |      when forward references are not defined.\n",
      "     |  \n",
      "     |  construct(_fields_set: Optional[ForwardRef('SetStr')] = None, **values: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |      Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data.\n",
      "     |      Default values are respected, but no other validation is performed.\n",
      "     |      Behaves as if `Config.extra = 'allow'` was set since it adds all passed values\n",
      "     |  \n",
      "     |  from_orm(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_file(path: Union[str, pathlib.Path], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_obj(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_raw(b: Union[str, bytes], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema(by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}') -> 'DictStrAny' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema_json(*, by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}', **dumps_kwargs: Any) -> 'unicode' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  update_forward_refs(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      "     |      Try to update ForwardRefs on fields based on this Model, globalns and localns.\n",
      "     |  \n",
      "     |  validate(value: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __fields_set__\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.utils.Representation:\n",
      "     |  \n",
      "     |  __pretty__(self, fmt: Callable[[Any], Any], **kwargs: Any) -> Generator[Any, NoneType, NoneType]\n",
      "     |      Used by devtools (https://python-devtools.helpmanual.io/) to provide a human readable representations of objects\n",
      "     |  \n",
      "     |  __repr__(self) -> 'unicode'\n",
      "     |  \n",
      "     |  __repr_name__(self) -> 'unicode'\n",
      "     |      Name of the instance's class, used in __repr__.\n",
      "     |  \n",
      "     |  __repr_str__(self, join_str: 'unicode') -> 'unicode'\n",
      "     |  \n",
      "     |  __rich_repr__(self) -> 'RichReprResult'\n",
      "     |      Get fields for Rich library\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.schema.runnable.base.Runnable:\n",
      "     |  \n",
      "     |  __or__(self, other: 'Union[Runnable[Any, Other], Callable[[Any], Other], Mapping[str, Union[Runnable[Any, Other], Callable[[Any], Other], Any]]]') -> 'RunnableSequence[Input, Other]'\n",
      "     |      Return self|value.\n",
      "     |  \n",
      "     |  __ror__(self, other: 'Union[Runnable[Other, Any], Callable[[Any], Other], Mapping[str, Union[Runnable[Other, Any], Callable[[Other], Any], Any]]]') -> 'RunnableSequence[Other, Output]'\n",
      "     |      Return value|self.\n",
      "     |  \n",
      "     |  async atransform(self, input: 'AsyncIterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'AsyncIterator[Output]'\n",
      "     |      Default implementation of atransform, which buffers input and calls astream.\n",
      "     |      Subclasses should override this method if they can start producing output while\n",
      "     |      input is still being generated.\n",
      "     |  \n",
      "     |  bind(self, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      "     |      Bind arguments to a Runnable, returning a new Runnable.\n",
      "     |  \n",
      "     |  map(self) -> 'Runnable[List[Input], List[Output]]'\n",
      "     |      Return a new Runnable that maps a list of inputs to a list of outputs,\n",
      "     |      by calling invoke() with each input.\n",
      "     |  \n",
      "     |  transform(self, input: 'Iterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'Iterator[Output]'\n",
      "     |      Default implementation of transform, which buffers input and then calls stream.\n",
      "     |      Subclasses should override this method if they can start producing output while\n",
      "     |      input is still being generated.\n",
      "     |  \n",
      "     |  with_config(self, config: 'Optional[RunnableConfig]' = None, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      "     |      Bind config to a Runnable, returning a new Runnable.\n",
      "     |  \n",
      "     |  with_fallbacks(self, fallbacks: 'Sequence[Runnable[Input, Output]]', *, exceptions_to_handle: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,)) -> 'RunnableWithFallbacks[Input, Output]'\n",
      "     |  \n",
      "     |  with_retry(self, *, retry_if_exception_type: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,), wait_exponential_jitter: 'bool' = True, stop_after_attempt: 'int' = 3) -> 'Runnable[Input, Output]'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __class_getitem__(params) from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  __init_subclass__(*args, **kwargs) from pydantic.main.ModelMetaclass\n",
      "     |      This method is called when a class is subclassed.\n",
      "     |      \n",
      "     |      The default implementation does nothing. It may be\n",
      "     |      overridden to extend subclasses.\n",
      "    \n",
      "    class OpaquePrompts(langchain.llms.base.LLM)\n",
      "     |  OpaquePrompts(*, cache: Optional[bool] = None, verbose: bool = None, callbacks: Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager, NoneType] = None, callback_manager: Optional[langchain.callbacks.base.BaseCallbackManager] = None, tags: Optional[List[str]] = None, metadata: Optional[Dict[str, Any]] = None, base_llm: langchain.schema.language_model.BaseLanguageModel) -> None\n",
      "     |  \n",
      "     |  An LLM wrapper that uses OpaquePrompts to sanitize prompts.\n",
      "     |  \n",
      "     |  Wraps another LLM and sanitizes prompts before passing it to the LLM, then\n",
      "     |      de-sanitizes the response.\n",
      "     |  \n",
      "     |  To use, you should have the ``opaqueprompts`` python package installed,\n",
      "     |  and the environment variable ``OPAQUEPROMPTS_API_KEY`` set with\n",
      "     |  your API key, or pass it as a named parameter to the constructor.\n",
      "     |  \n",
      "     |  Example:\n",
      "     |      .. code-block:: python\n",
      "     |  \n",
      "     |          from langchain.llms import OpaquePrompts\n",
      "     |          from langchain.chat_models import ChatOpenAI\n",
      "     |  \n",
      "     |          op_llm = OpaquePrompts(base_llm=ChatOpenAI())\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      OpaquePrompts\n",
      "     |      langchain.llms.base.LLM\n",
      "     |      langchain.llms.base.BaseLLM\n",
      "     |      langchain.schema.language_model.BaseLanguageModel\n",
      "     |      langchain.load.serializable.Serializable\n",
      "     |      pydantic.main.BaseModel\n",
      "     |      pydantic.utils.Representation\n",
      "     |      langchain.schema.runnable.base.Runnable\n",
      "     |      typing.Generic\n",
      "     |      abc.ABC\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  validate_environment(values: Dict) -> Dict from pydantic.main.ModelMetaclass\n",
      "     |      Validates that the OpaquePrompts API key and the Python package exist.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __json_encoder__ = pydantic_encoder(obj: Any) -> Any\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  Config = <class 'langchain.llms.opaqueprompts.OpaquePrompts.Config'>\n",
      "     |      Configuration for this pydantic object.\n",
      "     |  \n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {'base_llm': <class 'langchain.schema.language_model...\n",
      "     |  \n",
      "     |  __class_vars__ = set()\n",
      "     |  \n",
      "     |  __config__ = <class 'langchain.llms.opaqueprompts.Config'>\n",
      "     |  \n",
      "     |  __custom_root_type__ = False\n",
      "     |  \n",
      "     |  __exclude_fields__ = {'callback_manager': True, 'callbacks': True, 'me...\n",
      "     |  \n",
      "     |  __fields__ = {'base_llm': ModelField(name='base_llm', type=BaseLanguag...\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "     |  \n",
      "     |  __include_fields__ = None\n",
      "     |  \n",
      "     |  __parameters__ = ()\n",
      "     |  \n",
      "     |  __post_root_validators__ = [(False, <function BaseLLM.raise_deprecatio...\n",
      "     |  \n",
      "     |  __pre_root_validators__ = []\n",
      "     |  \n",
      "     |  __private_attributes__ = {'_lc_kwargs': ModelPrivateAttr(default=Pydan...\n",
      "     |  \n",
      "     |  __schema_cache__ = {}\n",
      "     |  \n",
      "     |  __signature__ = <Signature (*, cache: Optional[bool] = None, ver...sch...\n",
      "     |  \n",
      "     |  __validators__ = {'verbose': [<pydantic.class_validators.Validator obj...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  __call__(self, prompt: 'str', stop: 'Optional[List[str]]' = None, callbacks: 'Callbacks' = None, *, tags: 'Optional[List[str]]' = None, metadata: 'Optional[Dict[str, Any]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Check Cache and run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  __str__(self) -> 'str'\n",
      "     |      Get a string representation of the object for printing.\n",
      "     |  \n",
      "     |  async abatch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      "     |      Default implementation of abatch, which calls ainvoke N times.\n",
      "     |      Subclasses should override this method if they can batch more efficiently.\n",
      "     |  \n",
      "     |  async agenerate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  async agenerate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Asynchronously pass a sequence of prompts and return model generations.\n",
      "     |      \n",
      "     |      This method should make use of batched calls for models that expose a batched\n",
      "     |      API.\n",
      "     |      \n",
      "     |      Use this method when you want to:\n",
      "     |          1. take advantage of batched calls,\n",
      "     |          2. need more output from the model than just the top generated value,\n",
      "     |          3. are building chains that are agnostic to the underlying language model\n",
      "     |              type (e.g., pure text completion models vs chat models).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      "     |              converted to match the format of any language model (string for pure\n",
      "     |              text generation models and BaseMessages for chat models).\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          callbacks: Callbacks to pass through. Used for executing additional\n",
      "     |              functionality, such as logging or streaming, throughout generation.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          An LLMResult, which contains a list of candidate Generations for each input\n",
      "     |              prompt and additional model provider-specific output.\n",
      "     |  \n",
      "     |  async ainvoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Default implementation of ainvoke, which calls invoke in a thread pool.\n",
      "     |      Subclasses should override this method if they can run asynchronously.\n",
      "     |  \n",
      "     |  async apredict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Asynchronously pass a string to the model and return a string prediction.\n",
      "     |      \n",
      "     |      Use this method when calling pure text generation models and only the top\n",
      "     |          candidate generation is needed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: String input to pass to the model.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a string.\n",
      "     |  \n",
      "     |  async apredict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      "     |      Asynchronously pass messages to the model and return a message prediction.\n",
      "     |      \n",
      "     |      Use this method when calling chat models and only the top\n",
      "     |          candidate generation is needed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: A sequence of chat messages corresponding to a single model input.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a message.\n",
      "     |  \n",
      "     |  async astream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'AsyncIterator[str]'\n",
      "     |      Default implementation of astream, which calls ainvoke.\n",
      "     |      Subclasses should override this method if they support streaming output.\n",
      "     |  \n",
      "     |  batch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      "     |      Default implementation of batch, which calls invoke N times.\n",
      "     |      Subclasses should override this method if they can batch more efficiently.\n",
      "     |  \n",
      "     |  dict(self, **kwargs: 'Any') -> 'Dict'\n",
      "     |      Return a dictionary of the LLM.\n",
      "     |  \n",
      "     |  generate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  generate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Pass a sequence of prompts to the model and return model generations.\n",
      "     |      \n",
      "     |      This method should make use of batched calls for models that expose a batched\n",
      "     |      API.\n",
      "     |      \n",
      "     |      Use this method when you want to:\n",
      "     |          1. take advantage of batched calls,\n",
      "     |          2. need more output from the model than just the top generated value,\n",
      "     |          3. are building chains that are agnostic to the underlying language model\n",
      "     |              type (e.g., pure text completion models vs chat models).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      "     |              converted to match the format of any language model (string for pure\n",
      "     |              text generation models and BaseMessages for chat models).\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          callbacks: Callbacks to pass through. Used for executing additional\n",
      "     |              functionality, such as logging or streaming, throughout generation.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          An LLMResult, which contains a list of candidate Generations for each input\n",
      "     |              prompt and additional model provider-specific output.\n",
      "     |  \n",
      "     |  invoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |  \n",
      "     |  predict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Pass a single string input to the model and return a string prediction.\n",
      "     |      \n",
      "     |       Use this method when passing in raw text. If you want to pass in specific\n",
      "     |          types of chat messages, use predict_messages.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: String input to pass to the model.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a string.\n",
      "     |  \n",
      "     |  predict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      "     |      Pass a message sequence to the model and return a message prediction.\n",
      "     |      \n",
      "     |      Use this method when passing in chat messages. If you want to pass in raw text,\n",
      "     |          use predict.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: A sequence of chat messages corresponding to a single model input.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a message.\n",
      "     |  \n",
      "     |  save(self, file_path: 'Union[Path, str]') -> 'None'\n",
      "     |      Save the LLM.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          file_path: Path to file to save the LLM to.\n",
      "     |      \n",
      "     |      Example:\n",
      "     |      .. code-block:: python\n",
      "     |      \n",
      "     |          llm.save(file_path=\"path/llm.yaml\")\n",
      "     |  \n",
      "     |  stream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'Iterator[str]'\n",
      "     |      Default implementation of stream, which calls invoke.\n",
      "     |      Subclasses should override this method if they support streaming output.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  raise_deprecation(values: 'Dict') -> 'Dict' from pydantic.main.ModelMetaclass\n",
      "     |      Raise deprecation warning if callback_manager is used.\n",
      "     |  \n",
      "     |  set_verbose(verbose: 'Optional[bool]') -> 'bool' from pydantic.main.ModelMetaclass\n",
      "     |      If verbose is None, set it.\n",
      "     |      \n",
      "     |      This allows users to pass in None as verbose to access the global setting.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  __orig_bases__ = (langchain.schema.language_model.BaseLanguageModel[st...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.schema.language_model.BaseLanguageModel:\n",
      "     |  \n",
      "     |  get_num_tokens(self, text: 'str') -> 'int'\n",
      "     |      Get the number of tokens present in the text.\n",
      "     |      \n",
      "     |      Useful for checking if an input will fit in a model's context window.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: The string input to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The integer number of tokens in the text.\n",
      "     |  \n",
      "     |  get_num_tokens_from_messages(self, messages: 'List[BaseMessage]') -> 'int'\n",
      "     |      Get the number of tokens in the messages.\n",
      "     |      \n",
      "     |      Useful for checking if an input will fit in a model's context window.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: The message inputs to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The sum of the number of tokens across the messages.\n",
      "     |  \n",
      "     |  get_token_ids(self, text: 'str') -> 'List[int]'\n",
      "     |      Return the ordered ids of the tokens in a text.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: The string input to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          A list of ids corresponding to the tokens in the text, in order they occur\n",
      "     |              in the text.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from langchain.schema.language_model.BaseLanguageModel:\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.load.serializable.Serializable:\n",
      "     |  \n",
      "     |  __init__(self, **kwargs: Any) -> None\n",
      "     |      Create a new model by parsing and validating input data from keyword arguments.\n",
      "     |      \n",
      "     |      Raises ValidationError if the input data cannot be parsed to form a valid model.\n",
      "     |  \n",
      "     |  to_json(self) -> Union[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\n",
      "     |  \n",
      "     |  to_json_not_implemented(self) -> langchain.load.serializable.SerializedNotImplemented\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from langchain.load.serializable.Serializable:\n",
      "     |  \n",
      "     |  lc_attributes\n",
      "     |      Return a list of attribute names that should be included in the\n",
      "     |      serialized kwargs. These attributes must be accepted by the\n",
      "     |      constructor.\n",
      "     |  \n",
      "     |  lc_namespace\n",
      "     |      Return the namespace of the langchain object.\n",
      "     |      eg. [\"langchain\", \"llms\", \"openai\"]\n",
      "     |  \n",
      "     |  lc_secrets\n",
      "     |      Return a map of constructor argument names to secret ids.\n",
      "     |      eg. {\"openai_api_key\": \"OPENAI_API_KEY\"}\n",
      "     |  \n",
      "     |  lc_serializable\n",
      "     |      Return whether or not the class is serializable.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __eq__(self, other: Any) -> bool\n",
      "     |  \n",
      "     |  __getstate__(self) -> 'DictAny'\n",
      "     |  \n",
      "     |  __iter__(self) -> 'TupleGenerator'\n",
      "     |      so `dict(model)` works\n",
      "     |  \n",
      "     |  __repr_args__(self) -> 'ReprArgs'\n",
      "     |  \n",
      "     |  __setattr__(self, name, value)\n",
      "     |  \n",
      "     |  __setstate__(self, state: 'DictAny') -> None\n",
      "     |  \n",
      "     |  copy(self: 'Model', *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, update: Optional[ForwardRef('DictStrAny')] = None, deep: bool = False) -> 'Model'\n",
      "     |      Duplicate a model, optionally choose which fields to include, exclude and change.\n",
      "     |      \n",
      "     |      :param include: fields to include in new model\n",
      "     |      :param exclude: fields to exclude from new model, as with values this takes precedence over include\n",
      "     |      :param update: values to change/add in the new model. Note: the data is not validated before creating\n",
      "     |          the new model: you should trust this data\n",
      "     |      :param deep: set to `True` to make a deep copy of the model\n",
      "     |      :return: new model instance\n",
      "     |  \n",
      "     |  json(self, *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) -> 'unicode'\n",
      "     |      Generate a JSON representation of the model, `include` and `exclude` arguments as per `dict()`.\n",
      "     |      \n",
      "     |      `encoder` is an optional function to supply as `default` to json.dumps(), other arguments as per `json.dumps()`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __get_validators__() -> 'CallableGenerator' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  __try_update_forward_refs__(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      "     |      Same as update_forward_refs but will not raise exception\n",
      "     |      when forward references are not defined.\n",
      "     |  \n",
      "     |  construct(_fields_set: Optional[ForwardRef('SetStr')] = None, **values: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |      Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data.\n",
      "     |      Default values are respected, but no other validation is performed.\n",
      "     |      Behaves as if `Config.extra = 'allow'` was set since it adds all passed values\n",
      "     |  \n",
      "     |  from_orm(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_file(path: Union[str, pathlib.Path], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_obj(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_raw(b: Union[str, bytes], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema(by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}') -> 'DictStrAny' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema_json(*, by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}', **dumps_kwargs: Any) -> 'unicode' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  update_forward_refs(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      "     |      Try to update ForwardRefs on fields based on this Model, globalns and localns.\n",
      "     |  \n",
      "     |  validate(value: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __fields_set__\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.utils.Representation:\n",
      "     |  \n",
      "     |  __pretty__(self, fmt: Callable[[Any], Any], **kwargs: Any) -> Generator[Any, NoneType, NoneType]\n",
      "     |      Used by devtools (https://python-devtools.helpmanual.io/) to provide a human readable representations of objects\n",
      "     |  \n",
      "     |  __repr__(self) -> 'unicode'\n",
      "     |  \n",
      "     |  __repr_name__(self) -> 'unicode'\n",
      "     |      Name of the instance's class, used in __repr__.\n",
      "     |  \n",
      "     |  __repr_str__(self, join_str: 'unicode') -> 'unicode'\n",
      "     |  \n",
      "     |  __rich_repr__(self) -> 'RichReprResult'\n",
      "     |      Get fields for Rich library\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.schema.runnable.base.Runnable:\n",
      "     |  \n",
      "     |  __or__(self, other: 'Union[Runnable[Any, Other], Callable[[Any], Other], Mapping[str, Union[Runnable[Any, Other], Callable[[Any], Other], Any]]]') -> 'RunnableSequence[Input, Other]'\n",
      "     |      Return self|value.\n",
      "     |  \n",
      "     |  __ror__(self, other: 'Union[Runnable[Other, Any], Callable[[Any], Other], Mapping[str, Union[Runnable[Other, Any], Callable[[Other], Any], Any]]]') -> 'RunnableSequence[Other, Output]'\n",
      "     |      Return value|self.\n",
      "     |  \n",
      "     |  async atransform(self, input: 'AsyncIterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'AsyncIterator[Output]'\n",
      "     |      Default implementation of atransform, which buffers input and calls astream.\n",
      "     |      Subclasses should override this method if they can start producing output while\n",
      "     |      input is still being generated.\n",
      "     |  \n",
      "     |  bind(self, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      "     |      Bind arguments to a Runnable, returning a new Runnable.\n",
      "     |  \n",
      "     |  map(self) -> 'Runnable[List[Input], List[Output]]'\n",
      "     |      Return a new Runnable that maps a list of inputs to a list of outputs,\n",
      "     |      by calling invoke() with each input.\n",
      "     |  \n",
      "     |  transform(self, input: 'Iterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'Iterator[Output]'\n",
      "     |      Default implementation of transform, which buffers input and then calls stream.\n",
      "     |      Subclasses should override this method if they can start producing output while\n",
      "     |      input is still being generated.\n",
      "     |  \n",
      "     |  with_config(self, config: 'Optional[RunnableConfig]' = None, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      "     |      Bind config to a Runnable, returning a new Runnable.\n",
      "     |  \n",
      "     |  with_fallbacks(self, fallbacks: 'Sequence[Runnable[Input, Output]]', *, exceptions_to_handle: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,)) -> 'RunnableWithFallbacks[Input, Output]'\n",
      "     |  \n",
      "     |  with_retry(self, *, retry_if_exception_type: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,), wait_exponential_jitter: 'bool' = True, stop_after_attempt: 'int' = 3) -> 'Runnable[Input, Output]'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __class_getitem__(params) from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  __init_subclass__(*args, **kwargs) from pydantic.main.ModelMetaclass\n",
      "     |      This method is called when a class is subclassed.\n",
      "     |      \n",
      "     |      The default implementation does nothing. It may be\n",
      "     |      overridden to extend subclasses.\n",
      "    \n",
      "    class OpenAI(BaseOpenAI)\n",
      "     |  OpenAI(*, cache: Optional[bool] = None, verbose: bool = None, callbacks: Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager, NoneType] = None, callback_manager: Optional[langchain.callbacks.base.BaseCallbackManager] = None, tags: Optional[List[str]] = None, metadata: Optional[Dict[str, Any]] = None, client: Any = None, model: str = 'text-davinci-003', temperature: float = 0.7, max_tokens: int = 256, top_p: float = 1, frequency_penalty: float = 0, presence_penalty: float = 0, n: int = 1, best_of: int = 1, model_kwargs: Dict[str, Any] = None, openai_api_key: Optional[str] = None, openai_api_base: Optional[str] = None, openai_organization: Optional[str] = None, openai_proxy: Optional[str] = None, batch_size: int = 20, request_timeout: Union[float, Tuple[float, float], NoneType] = None, logit_bias: Optional[Dict[str, float]] = None, max_retries: int = 6, streaming: bool = False, allowed_special: Union[Literal['all'], AbstractSet[str]] = set(), disallowed_special: Union[Literal['all'], Collection[str]] = 'all', tiktoken_model_name: Optional[str] = None) -> None\n",
      "     |  \n",
      "     |  OpenAI large language models.\n",
      "     |  \n",
      "     |  To use, you should have the ``openai`` python package installed, and the\n",
      "     |  environment variable ``OPENAI_API_KEY`` set with your API key.\n",
      "     |  \n",
      "     |  Any parameters that are valid to be passed to the openai.create call can be passed\n",
      "     |  in, even if not explicitly saved on this class.\n",
      "     |  \n",
      "     |  Example:\n",
      "     |      .. code-block:: python\n",
      "     |  \n",
      "     |          from langchain.llms import OpenAI\n",
      "     |          openai = OpenAI(model_name=\"text-davinci-003\")\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      OpenAI\n",
      "     |      BaseOpenAI\n",
      "     |      langchain.llms.base.BaseLLM\n",
      "     |      langchain.schema.language_model.BaseLanguageModel\n",
      "     |      langchain.load.serializable.Serializable\n",
      "     |      pydantic.main.BaseModel\n",
      "     |      pydantic.utils.Representation\n",
      "     |      langchain.schema.runnable.base.Runnable\n",
      "     |      typing.Generic\n",
      "     |      abc.ABC\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __json_encoder__ = pydantic_encoder(obj: Any) -> Any\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {}\n",
      "     |  \n",
      "     |  __class_vars__ = set()\n",
      "     |  \n",
      "     |  __config__ = <class 'langchain.llms.openai.Config'>\n",
      "     |  \n",
      "     |  __custom_root_type__ = False\n",
      "     |  \n",
      "     |  __exclude_fields__ = {'callback_manager': True, 'callbacks': True, 'me...\n",
      "     |  \n",
      "     |  __fields__ = {'allowed_special': ModelField(name='allowed_special', ty...\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "     |  \n",
      "     |  __include_fields__ = None\n",
      "     |  \n",
      "     |  __parameters__ = ()\n",
      "     |  \n",
      "     |  __post_root_validators__ = [(False, <function BaseLLM.raise_deprecatio...\n",
      "     |  \n",
      "     |  __pre_root_validators__ = [<function BaseOpenAI.build_extra>]\n",
      "     |  \n",
      "     |  __private_attributes__ = {'_lc_kwargs': ModelPrivateAttr(default=Pydan...\n",
      "     |  \n",
      "     |  __schema_cache__ = {}\n",
      "     |  \n",
      "     |  __signature__ = <Signature (*, cache: Optional[bool] = None, ver...kto...\n",
      "     |  \n",
      "     |  __validators__ = {'verbose': [<pydantic.class_validators.Validator obj...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BaseOpenAI:\n",
      "     |  \n",
      "     |  create_llm_result(self, choices: 'Any', prompts: 'List[str]', token_usage: 'Dict[str, int]') -> 'LLMResult'\n",
      "     |      Create the LLMResult from the choices and prompts.\n",
      "     |  \n",
      "     |  get_sub_prompts(self, params: 'Dict[str, Any]', prompts: 'List[str]', stop: 'Optional[List[str]]' = None) -> 'List[List[str]]'\n",
      "     |      Get the sub prompts for llm call.\n",
      "     |  \n",
      "     |  get_token_ids(self, text: 'str') -> 'List[int]'\n",
      "     |      Get the token IDs using the tiktoken package.\n",
      "     |  \n",
      "     |  max_tokens_for_prompt(self, prompt: 'str') -> 'int'\n",
      "     |      Calculate the maximum number of tokens possible to generate for a prompt.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompt: The prompt to pass into the model.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The maximum number of tokens to generate for a prompt.\n",
      "     |      \n",
      "     |      Example:\n",
      "     |          .. code-block:: python\n",
      "     |      \n",
      "     |              max_tokens = openai.max_token_for_prompt(\"Tell me a joke.\")\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from BaseOpenAI:\n",
      "     |  \n",
      "     |  build_extra(values: 'Dict[str, Any]') -> 'Dict[str, Any]' from pydantic.main.ModelMetaclass\n",
      "     |      Build extra kwargs from additional params that were passed in.\n",
      "     |  \n",
      "     |  validate_environment(values: 'Dict') -> 'Dict' from pydantic.main.ModelMetaclass\n",
      "     |      Validate that api key and python package exists in environment.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from BaseOpenAI:\n",
      "     |  \n",
      "     |  __new__(cls, **data: 'Any') -> 'Union[OpenAIChat, BaseOpenAI]'\n",
      "     |      Initialize the OpenAI object.\n",
      "     |  \n",
      "     |  modelname_to_contextsize(modelname: 'str') -> 'int'\n",
      "     |      Calculate the maximum number of tokens possible to generate for a model.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          modelname: The modelname we want to know the context size for.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The maximum context size\n",
      "     |      \n",
      "     |      Example:\n",
      "     |          .. code-block:: python\n",
      "     |      \n",
      "     |              max_tokens = openai.modelname_to_contextsize(\"text-davinci-003\")\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from BaseOpenAI:\n",
      "     |  \n",
      "     |  lc_secrets\n",
      "     |      Return a map of constructor argument names to secret ids.\n",
      "     |      eg. {\"openai_api_key\": \"OPENAI_API_KEY\"}\n",
      "     |  \n",
      "     |  lc_serializable\n",
      "     |      Return whether or not the class is serializable.\n",
      "     |  \n",
      "     |  max_context_size\n",
      "     |      Get max context size for this model.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from BaseOpenAI:\n",
      "     |  \n",
      "     |  Config = <class 'langchain.llms.openai.BaseOpenAI.Config'>\n",
      "     |      Configuration for this pydantic object.\n",
      "     |  \n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  __call__(self, prompt: 'str', stop: 'Optional[List[str]]' = None, callbacks: 'Callbacks' = None, *, tags: 'Optional[List[str]]' = None, metadata: 'Optional[Dict[str, Any]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Check Cache and run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  __str__(self) -> 'str'\n",
      "     |      Get a string representation of the object for printing.\n",
      "     |  \n",
      "     |  async abatch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      "     |      Default implementation of abatch, which calls ainvoke N times.\n",
      "     |      Subclasses should override this method if they can batch more efficiently.\n",
      "     |  \n",
      "     |  async agenerate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  async agenerate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Asynchronously pass a sequence of prompts and return model generations.\n",
      "     |      \n",
      "     |      This method should make use of batched calls for models that expose a batched\n",
      "     |      API.\n",
      "     |      \n",
      "     |      Use this method when you want to:\n",
      "     |          1. take advantage of batched calls,\n",
      "     |          2. need more output from the model than just the top generated value,\n",
      "     |          3. are building chains that are agnostic to the underlying language model\n",
      "     |              type (e.g., pure text completion models vs chat models).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      "     |              converted to match the format of any language model (string for pure\n",
      "     |              text generation models and BaseMessages for chat models).\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          callbacks: Callbacks to pass through. Used for executing additional\n",
      "     |              functionality, such as logging or streaming, throughout generation.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          An LLMResult, which contains a list of candidate Generations for each input\n",
      "     |              prompt and additional model provider-specific output.\n",
      "     |  \n",
      "     |  async ainvoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Default implementation of ainvoke, which calls invoke in a thread pool.\n",
      "     |      Subclasses should override this method if they can run asynchronously.\n",
      "     |  \n",
      "     |  async apredict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Asynchronously pass a string to the model and return a string prediction.\n",
      "     |      \n",
      "     |      Use this method when calling pure text generation models and only the top\n",
      "     |          candidate generation is needed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: String input to pass to the model.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a string.\n",
      "     |  \n",
      "     |  async apredict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      "     |      Asynchronously pass messages to the model and return a message prediction.\n",
      "     |      \n",
      "     |      Use this method when calling chat models and only the top\n",
      "     |          candidate generation is needed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: A sequence of chat messages corresponding to a single model input.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a message.\n",
      "     |  \n",
      "     |  async astream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'AsyncIterator[str]'\n",
      "     |      Default implementation of astream, which calls ainvoke.\n",
      "     |      Subclasses should override this method if they support streaming output.\n",
      "     |  \n",
      "     |  batch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      "     |      Default implementation of batch, which calls invoke N times.\n",
      "     |      Subclasses should override this method if they can batch more efficiently.\n",
      "     |  \n",
      "     |  dict(self, **kwargs: 'Any') -> 'Dict'\n",
      "     |      Return a dictionary of the LLM.\n",
      "     |  \n",
      "     |  generate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  generate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Pass a sequence of prompts to the model and return model generations.\n",
      "     |      \n",
      "     |      This method should make use of batched calls for models that expose a batched\n",
      "     |      API.\n",
      "     |      \n",
      "     |      Use this method when you want to:\n",
      "     |          1. take advantage of batched calls,\n",
      "     |          2. need more output from the model than just the top generated value,\n",
      "     |          3. are building chains that are agnostic to the underlying language model\n",
      "     |              type (e.g., pure text completion models vs chat models).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      "     |              converted to match the format of any language model (string for pure\n",
      "     |              text generation models and BaseMessages for chat models).\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          callbacks: Callbacks to pass through. Used for executing additional\n",
      "     |              functionality, such as logging or streaming, throughout generation.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          An LLMResult, which contains a list of candidate Generations for each input\n",
      "     |              prompt and additional model provider-specific output.\n",
      "     |  \n",
      "     |  invoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |  \n",
      "     |  predict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Pass a single string input to the model and return a string prediction.\n",
      "     |      \n",
      "     |       Use this method when passing in raw text. If you want to pass in specific\n",
      "     |          types of chat messages, use predict_messages.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: String input to pass to the model.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a string.\n",
      "     |  \n",
      "     |  predict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      "     |      Pass a message sequence to the model and return a message prediction.\n",
      "     |      \n",
      "     |      Use this method when passing in chat messages. If you want to pass in raw text,\n",
      "     |          use predict.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: A sequence of chat messages corresponding to a single model input.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a message.\n",
      "     |  \n",
      "     |  save(self, file_path: 'Union[Path, str]') -> 'None'\n",
      "     |      Save the LLM.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          file_path: Path to file to save the LLM to.\n",
      "     |      \n",
      "     |      Example:\n",
      "     |      .. code-block:: python\n",
      "     |      \n",
      "     |          llm.save(file_path=\"path/llm.yaml\")\n",
      "     |  \n",
      "     |  stream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'Iterator[str]'\n",
      "     |      Default implementation of stream, which calls invoke.\n",
      "     |      Subclasses should override this method if they support streaming output.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  raise_deprecation(values: 'Dict') -> 'Dict' from pydantic.main.ModelMetaclass\n",
      "     |      Raise deprecation warning if callback_manager is used.\n",
      "     |  \n",
      "     |  set_verbose(verbose: 'Optional[bool]') -> 'bool' from pydantic.main.ModelMetaclass\n",
      "     |      If verbose is None, set it.\n",
      "     |      \n",
      "     |      This allows users to pass in None as verbose to access the global setting.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  __orig_bases__ = (langchain.schema.language_model.BaseLanguageModel[st...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.schema.language_model.BaseLanguageModel:\n",
      "     |  \n",
      "     |  get_num_tokens(self, text: 'str') -> 'int'\n",
      "     |      Get the number of tokens present in the text.\n",
      "     |      \n",
      "     |      Useful for checking if an input will fit in a model's context window.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: The string input to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The integer number of tokens in the text.\n",
      "     |  \n",
      "     |  get_num_tokens_from_messages(self, messages: 'List[BaseMessage]') -> 'int'\n",
      "     |      Get the number of tokens in the messages.\n",
      "     |      \n",
      "     |      Useful for checking if an input will fit in a model's context window.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: The message inputs to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The sum of the number of tokens across the messages.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from langchain.schema.language_model.BaseLanguageModel:\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.load.serializable.Serializable:\n",
      "     |  \n",
      "     |  __init__(self, **kwargs: Any) -> None\n",
      "     |      Create a new model by parsing and validating input data from keyword arguments.\n",
      "     |      \n",
      "     |      Raises ValidationError if the input data cannot be parsed to form a valid model.\n",
      "     |  \n",
      "     |  to_json(self) -> Union[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\n",
      "     |  \n",
      "     |  to_json_not_implemented(self) -> langchain.load.serializable.SerializedNotImplemented\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from langchain.load.serializable.Serializable:\n",
      "     |  \n",
      "     |  lc_attributes\n",
      "     |      Return a list of attribute names that should be included in the\n",
      "     |      serialized kwargs. These attributes must be accepted by the\n",
      "     |      constructor.\n",
      "     |  \n",
      "     |  lc_namespace\n",
      "     |      Return the namespace of the langchain object.\n",
      "     |      eg. [\"langchain\", \"llms\", \"openai\"]\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __eq__(self, other: Any) -> bool\n",
      "     |  \n",
      "     |  __getstate__(self) -> 'DictAny'\n",
      "     |  \n",
      "     |  __iter__(self) -> 'TupleGenerator'\n",
      "     |      so `dict(model)` works\n",
      "     |  \n",
      "     |  __repr_args__(self) -> 'ReprArgs'\n",
      "     |  \n",
      "     |  __setattr__(self, name, value)\n",
      "     |  \n",
      "     |  __setstate__(self, state: 'DictAny') -> None\n",
      "     |  \n",
      "     |  copy(self: 'Model', *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, update: Optional[ForwardRef('DictStrAny')] = None, deep: bool = False) -> 'Model'\n",
      "     |      Duplicate a model, optionally choose which fields to include, exclude and change.\n",
      "     |      \n",
      "     |      :param include: fields to include in new model\n",
      "     |      :param exclude: fields to exclude from new model, as with values this takes precedence over include\n",
      "     |      :param update: values to change/add in the new model. Note: the data is not validated before creating\n",
      "     |          the new model: you should trust this data\n",
      "     |      :param deep: set to `True` to make a deep copy of the model\n",
      "     |      :return: new model instance\n",
      "     |  \n",
      "     |  json(self, *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) -> 'unicode'\n",
      "     |      Generate a JSON representation of the model, `include` and `exclude` arguments as per `dict()`.\n",
      "     |      \n",
      "     |      `encoder` is an optional function to supply as `default` to json.dumps(), other arguments as per `json.dumps()`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __get_validators__() -> 'CallableGenerator' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  __try_update_forward_refs__(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      "     |      Same as update_forward_refs but will not raise exception\n",
      "     |      when forward references are not defined.\n",
      "     |  \n",
      "     |  construct(_fields_set: Optional[ForwardRef('SetStr')] = None, **values: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |      Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data.\n",
      "     |      Default values are respected, but no other validation is performed.\n",
      "     |      Behaves as if `Config.extra = 'allow'` was set since it adds all passed values\n",
      "     |  \n",
      "     |  from_orm(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_file(path: Union[str, pathlib.Path], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_obj(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_raw(b: Union[str, bytes], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema(by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}') -> 'DictStrAny' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema_json(*, by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}', **dumps_kwargs: Any) -> 'unicode' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  update_forward_refs(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      "     |      Try to update ForwardRefs on fields based on this Model, globalns and localns.\n",
      "     |  \n",
      "     |  validate(value: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __fields_set__\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.utils.Representation:\n",
      "     |  \n",
      "     |  __pretty__(self, fmt: Callable[[Any], Any], **kwargs: Any) -> Generator[Any, NoneType, NoneType]\n",
      "     |      Used by devtools (https://python-devtools.helpmanual.io/) to provide a human readable representations of objects\n",
      "     |  \n",
      "     |  __repr__(self) -> 'unicode'\n",
      "     |  \n",
      "     |  __repr_name__(self) -> 'unicode'\n",
      "     |      Name of the instance's class, used in __repr__.\n",
      "     |  \n",
      "     |  __repr_str__(self, join_str: 'unicode') -> 'unicode'\n",
      "     |  \n",
      "     |  __rich_repr__(self) -> 'RichReprResult'\n",
      "     |      Get fields for Rich library\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.schema.runnable.base.Runnable:\n",
      "     |  \n",
      "     |  __or__(self, other: 'Union[Runnable[Any, Other], Callable[[Any], Other], Mapping[str, Union[Runnable[Any, Other], Callable[[Any], Other], Any]]]') -> 'RunnableSequence[Input, Other]'\n",
      "     |      Return self|value.\n",
      "     |  \n",
      "     |  __ror__(self, other: 'Union[Runnable[Other, Any], Callable[[Any], Other], Mapping[str, Union[Runnable[Other, Any], Callable[[Other], Any], Any]]]') -> 'RunnableSequence[Other, Output]'\n",
      "     |      Return value|self.\n",
      "     |  \n",
      "     |  async atransform(self, input: 'AsyncIterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'AsyncIterator[Output]'\n",
      "     |      Default implementation of atransform, which buffers input and calls astream.\n",
      "     |      Subclasses should override this method if they can start producing output while\n",
      "     |      input is still being generated.\n",
      "     |  \n",
      "     |  bind(self, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      "     |      Bind arguments to a Runnable, returning a new Runnable.\n",
      "     |  \n",
      "     |  map(self) -> 'Runnable[List[Input], List[Output]]'\n",
      "     |      Return a new Runnable that maps a list of inputs to a list of outputs,\n",
      "     |      by calling invoke() with each input.\n",
      "     |  \n",
      "     |  transform(self, input: 'Iterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'Iterator[Output]'\n",
      "     |      Default implementation of transform, which buffers input and then calls stream.\n",
      "     |      Subclasses should override this method if they can start producing output while\n",
      "     |      input is still being generated.\n",
      "     |  \n",
      "     |  with_config(self, config: 'Optional[RunnableConfig]' = None, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      "     |      Bind config to a Runnable, returning a new Runnable.\n",
      "     |  \n",
      "     |  with_fallbacks(self, fallbacks: 'Sequence[Runnable[Input, Output]]', *, exceptions_to_handle: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,)) -> 'RunnableWithFallbacks[Input, Output]'\n",
      "     |  \n",
      "     |  with_retry(self, *, retry_if_exception_type: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,), wait_exponential_jitter: 'bool' = True, stop_after_attempt: 'int' = 3) -> 'Runnable[Input, Output]'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __class_getitem__(params) from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  __init_subclass__(*args, **kwargs) from pydantic.main.ModelMetaclass\n",
      "     |      This method is called when a class is subclassed.\n",
      "     |      \n",
      "     |      The default implementation does nothing. It may be\n",
      "     |      overridden to extend subclasses.\n",
      "    \n",
      "    class OpenAIChat(langchain.llms.base.BaseLLM)\n",
      "     |  OpenAIChat(*, cache: Optional[bool] = None, verbose: bool = None, callbacks: Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager, NoneType] = None, callback_manager: Optional[langchain.callbacks.base.BaseCallbackManager] = None, tags: Optional[List[str]] = None, metadata: Optional[Dict[str, Any]] = None, client: Any = None, model_name: str = 'gpt-3.5-turbo', model_kwargs: Dict[str, Any] = None, openai_api_key: Optional[str] = None, openai_api_base: Optional[str] = None, openai_proxy: Optional[str] = None, max_retries: int = 6, prefix_messages: List = None, streaming: bool = False, allowed_special: Union[Literal['all'], AbstractSet[str]] = set(), disallowed_special: Union[Literal['all'], Collection[str]] = 'all') -> None\n",
      "     |  \n",
      "     |  OpenAI Chat large language models.\n",
      "     |  \n",
      "     |  To use, you should have the ``openai`` python package installed, and the\n",
      "     |  environment variable ``OPENAI_API_KEY`` set with your API key.\n",
      "     |  \n",
      "     |  Any parameters that are valid to be passed to the openai.create call can be passed\n",
      "     |  in, even if not explicitly saved on this class.\n",
      "     |  \n",
      "     |  Example:\n",
      "     |      .. code-block:: python\n",
      "     |  \n",
      "     |          from langchain.llms import OpenAIChat\n",
      "     |          openaichat = OpenAIChat(model_name=\"gpt-3.5-turbo\")\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      OpenAIChat\n",
      "     |      langchain.llms.base.BaseLLM\n",
      "     |      langchain.schema.language_model.BaseLanguageModel\n",
      "     |      langchain.load.serializable.Serializable\n",
      "     |      pydantic.main.BaseModel\n",
      "     |      pydantic.utils.Representation\n",
      "     |      langchain.schema.runnable.base.Runnable\n",
      "     |      typing.Generic\n",
      "     |      abc.ABC\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  get_token_ids(self, text: 'str') -> 'List[int]'\n",
      "     |      Get the token IDs using the tiktoken package.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  build_extra(values: 'Dict[str, Any]') -> 'Dict[str, Any]' from pydantic.main.ModelMetaclass\n",
      "     |      Build extra kwargs from additional params that were passed in.\n",
      "     |  \n",
      "     |  validate_environment(values: 'Dict') -> 'Dict' from pydantic.main.ModelMetaclass\n",
      "     |      Validate that api key and python package exists in environment.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __json_encoder__ = pydantic_encoder(obj: Any) -> Any\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {'allowed_special': \"Union[Literal['all'], AbstractS...\n",
      "     |  \n",
      "     |  __class_vars__ = set()\n",
      "     |  \n",
      "     |  __config__ = <class 'langchain.llms.openai.Config'>\n",
      "     |  \n",
      "     |  __custom_root_type__ = False\n",
      "     |  \n",
      "     |  __exclude_fields__ = {'callback_manager': True, 'callbacks': True, 'me...\n",
      "     |  \n",
      "     |  __fields__ = {'allowed_special': ModelField(name='allowed_special', ty...\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "     |  \n",
      "     |  __include_fields__ = None\n",
      "     |  \n",
      "     |  __parameters__ = ()\n",
      "     |  \n",
      "     |  __post_root_validators__ = [(False, <function BaseLLM.raise_deprecatio...\n",
      "     |  \n",
      "     |  __pre_root_validators__ = [<function OpenAIChat.build_extra>]\n",
      "     |  \n",
      "     |  __private_attributes__ = {'_lc_kwargs': ModelPrivateAttr(default=Pydan...\n",
      "     |  \n",
      "     |  __schema_cache__ = {}\n",
      "     |  \n",
      "     |  __signature__ = <Signature (*, cache: Optional[bool] = None, ver...ite...\n",
      "     |  \n",
      "     |  __validators__ = {'verbose': [<pydantic.class_validators.Validator obj...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  __call__(self, prompt: 'str', stop: 'Optional[List[str]]' = None, callbacks: 'Callbacks' = None, *, tags: 'Optional[List[str]]' = None, metadata: 'Optional[Dict[str, Any]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Check Cache and run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  __str__(self) -> 'str'\n",
      "     |      Get a string representation of the object for printing.\n",
      "     |  \n",
      "     |  async abatch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      "     |      Default implementation of abatch, which calls ainvoke N times.\n",
      "     |      Subclasses should override this method if they can batch more efficiently.\n",
      "     |  \n",
      "     |  async agenerate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  async agenerate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Asynchronously pass a sequence of prompts and return model generations.\n",
      "     |      \n",
      "     |      This method should make use of batched calls for models that expose a batched\n",
      "     |      API.\n",
      "     |      \n",
      "     |      Use this method when you want to:\n",
      "     |          1. take advantage of batched calls,\n",
      "     |          2. need more output from the model than just the top generated value,\n",
      "     |          3. are building chains that are agnostic to the underlying language model\n",
      "     |              type (e.g., pure text completion models vs chat models).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      "     |              converted to match the format of any language model (string for pure\n",
      "     |              text generation models and BaseMessages for chat models).\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          callbacks: Callbacks to pass through. Used for executing additional\n",
      "     |              functionality, such as logging or streaming, throughout generation.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          An LLMResult, which contains a list of candidate Generations for each input\n",
      "     |              prompt and additional model provider-specific output.\n",
      "     |  \n",
      "     |  async ainvoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Default implementation of ainvoke, which calls invoke in a thread pool.\n",
      "     |      Subclasses should override this method if they can run asynchronously.\n",
      "     |  \n",
      "     |  async apredict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Asynchronously pass a string to the model and return a string prediction.\n",
      "     |      \n",
      "     |      Use this method when calling pure text generation models and only the top\n",
      "     |          candidate generation is needed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: String input to pass to the model.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a string.\n",
      "     |  \n",
      "     |  async apredict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      "     |      Asynchronously pass messages to the model and return a message prediction.\n",
      "     |      \n",
      "     |      Use this method when calling chat models and only the top\n",
      "     |          candidate generation is needed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: A sequence of chat messages corresponding to a single model input.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a message.\n",
      "     |  \n",
      "     |  async astream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'AsyncIterator[str]'\n",
      "     |      Default implementation of astream, which calls ainvoke.\n",
      "     |      Subclasses should override this method if they support streaming output.\n",
      "     |  \n",
      "     |  batch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      "     |      Default implementation of batch, which calls invoke N times.\n",
      "     |      Subclasses should override this method if they can batch more efficiently.\n",
      "     |  \n",
      "     |  dict(self, **kwargs: 'Any') -> 'Dict'\n",
      "     |      Return a dictionary of the LLM.\n",
      "     |  \n",
      "     |  generate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  generate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Pass a sequence of prompts to the model and return model generations.\n",
      "     |      \n",
      "     |      This method should make use of batched calls for models that expose a batched\n",
      "     |      API.\n",
      "     |      \n",
      "     |      Use this method when you want to:\n",
      "     |          1. take advantage of batched calls,\n",
      "     |          2. need more output from the model than just the top generated value,\n",
      "     |          3. are building chains that are agnostic to the underlying language model\n",
      "     |              type (e.g., pure text completion models vs chat models).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      "     |              converted to match the format of any language model (string for pure\n",
      "     |              text generation models and BaseMessages for chat models).\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          callbacks: Callbacks to pass through. Used for executing additional\n",
      "     |              functionality, such as logging or streaming, throughout generation.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          An LLMResult, which contains a list of candidate Generations for each input\n",
      "     |              prompt and additional model provider-specific output.\n",
      "     |  \n",
      "     |  invoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |  \n",
      "     |  predict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Pass a single string input to the model and return a string prediction.\n",
      "     |      \n",
      "     |       Use this method when passing in raw text. If you want to pass in specific\n",
      "     |          types of chat messages, use predict_messages.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: String input to pass to the model.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a string.\n",
      "     |  \n",
      "     |  predict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      "     |      Pass a message sequence to the model and return a message prediction.\n",
      "     |      \n",
      "     |      Use this method when passing in chat messages. If you want to pass in raw text,\n",
      "     |          use predict.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: A sequence of chat messages corresponding to a single model input.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a message.\n",
      "     |  \n",
      "     |  save(self, file_path: 'Union[Path, str]') -> 'None'\n",
      "     |      Save the LLM.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          file_path: Path to file to save the LLM to.\n",
      "     |      \n",
      "     |      Example:\n",
      "     |      .. code-block:: python\n",
      "     |      \n",
      "     |          llm.save(file_path=\"path/llm.yaml\")\n",
      "     |  \n",
      "     |  stream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'Iterator[str]'\n",
      "     |      Default implementation of stream, which calls invoke.\n",
      "     |      Subclasses should override this method if they support streaming output.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  raise_deprecation(values: 'Dict') -> 'Dict' from pydantic.main.ModelMetaclass\n",
      "     |      Raise deprecation warning if callback_manager is used.\n",
      "     |  \n",
      "     |  set_verbose(verbose: 'Optional[bool]') -> 'bool' from pydantic.main.ModelMetaclass\n",
      "     |      If verbose is None, set it.\n",
      "     |      \n",
      "     |      This allows users to pass in None as verbose to access the global setting.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  Config = <class 'langchain.llms.base.BaseLLM.Config'>\n",
      "     |      Configuration for this pydantic object.\n",
      "     |  \n",
      "     |  \n",
      "     |  __orig_bases__ = (langchain.schema.language_model.BaseLanguageModel[st...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.schema.language_model.BaseLanguageModel:\n",
      "     |  \n",
      "     |  get_num_tokens(self, text: 'str') -> 'int'\n",
      "     |      Get the number of tokens present in the text.\n",
      "     |      \n",
      "     |      Useful for checking if an input will fit in a model's context window.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: The string input to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The integer number of tokens in the text.\n",
      "     |  \n",
      "     |  get_num_tokens_from_messages(self, messages: 'List[BaseMessage]') -> 'int'\n",
      "     |      Get the number of tokens in the messages.\n",
      "     |      \n",
      "     |      Useful for checking if an input will fit in a model's context window.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: The message inputs to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The sum of the number of tokens across the messages.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from langchain.schema.language_model.BaseLanguageModel:\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.load.serializable.Serializable:\n",
      "     |  \n",
      "     |  __init__(self, **kwargs: Any) -> None\n",
      "     |      Create a new model by parsing and validating input data from keyword arguments.\n",
      "     |      \n",
      "     |      Raises ValidationError if the input data cannot be parsed to form a valid model.\n",
      "     |  \n",
      "     |  to_json(self) -> Union[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\n",
      "     |  \n",
      "     |  to_json_not_implemented(self) -> langchain.load.serializable.SerializedNotImplemented\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from langchain.load.serializable.Serializable:\n",
      "     |  \n",
      "     |  lc_attributes\n",
      "     |      Return a list of attribute names that should be included in the\n",
      "     |      serialized kwargs. These attributes must be accepted by the\n",
      "     |      constructor.\n",
      "     |  \n",
      "     |  lc_namespace\n",
      "     |      Return the namespace of the langchain object.\n",
      "     |      eg. [\"langchain\", \"llms\", \"openai\"]\n",
      "     |  \n",
      "     |  lc_secrets\n",
      "     |      Return a map of constructor argument names to secret ids.\n",
      "     |      eg. {\"openai_api_key\": \"OPENAI_API_KEY\"}\n",
      "     |  \n",
      "     |  lc_serializable\n",
      "     |      Return whether or not the class is serializable.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __eq__(self, other: Any) -> bool\n",
      "     |  \n",
      "     |  __getstate__(self) -> 'DictAny'\n",
      "     |  \n",
      "     |  __iter__(self) -> 'TupleGenerator'\n",
      "     |      so `dict(model)` works\n",
      "     |  \n",
      "     |  __repr_args__(self) -> 'ReprArgs'\n",
      "     |  \n",
      "     |  __setattr__(self, name, value)\n",
      "     |  \n",
      "     |  __setstate__(self, state: 'DictAny') -> None\n",
      "     |  \n",
      "     |  copy(self: 'Model', *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, update: Optional[ForwardRef('DictStrAny')] = None, deep: bool = False) -> 'Model'\n",
      "     |      Duplicate a model, optionally choose which fields to include, exclude and change.\n",
      "     |      \n",
      "     |      :param include: fields to include in new model\n",
      "     |      :param exclude: fields to exclude from new model, as with values this takes precedence over include\n",
      "     |      :param update: values to change/add in the new model. Note: the data is not validated before creating\n",
      "     |          the new model: you should trust this data\n",
      "     |      :param deep: set to `True` to make a deep copy of the model\n",
      "     |      :return: new model instance\n",
      "     |  \n",
      "     |  json(self, *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) -> 'unicode'\n",
      "     |      Generate a JSON representation of the model, `include` and `exclude` arguments as per `dict()`.\n",
      "     |      \n",
      "     |      `encoder` is an optional function to supply as `default` to json.dumps(), other arguments as per `json.dumps()`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __get_validators__() -> 'CallableGenerator' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  __try_update_forward_refs__(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      "     |      Same as update_forward_refs but will not raise exception\n",
      "     |      when forward references are not defined.\n",
      "     |  \n",
      "     |  construct(_fields_set: Optional[ForwardRef('SetStr')] = None, **values: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |      Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data.\n",
      "     |      Default values are respected, but no other validation is performed.\n",
      "     |      Behaves as if `Config.extra = 'allow'` was set since it adds all passed values\n",
      "     |  \n",
      "     |  from_orm(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_file(path: Union[str, pathlib.Path], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_obj(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_raw(b: Union[str, bytes], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema(by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}') -> 'DictStrAny' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema_json(*, by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}', **dumps_kwargs: Any) -> 'unicode' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  update_forward_refs(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      "     |      Try to update ForwardRefs on fields based on this Model, globalns and localns.\n",
      "     |  \n",
      "     |  validate(value: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __fields_set__\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.utils.Representation:\n",
      "     |  \n",
      "     |  __pretty__(self, fmt: Callable[[Any], Any], **kwargs: Any) -> Generator[Any, NoneType, NoneType]\n",
      "     |      Used by devtools (https://python-devtools.helpmanual.io/) to provide a human readable representations of objects\n",
      "     |  \n",
      "     |  __repr__(self) -> 'unicode'\n",
      "     |  \n",
      "     |  __repr_name__(self) -> 'unicode'\n",
      "     |      Name of the instance's class, used in __repr__.\n",
      "     |  \n",
      "     |  __repr_str__(self, join_str: 'unicode') -> 'unicode'\n",
      "     |  \n",
      "     |  __rich_repr__(self) -> 'RichReprResult'\n",
      "     |      Get fields for Rich library\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.schema.runnable.base.Runnable:\n",
      "     |  \n",
      "     |  __or__(self, other: 'Union[Runnable[Any, Other], Callable[[Any], Other], Mapping[str, Union[Runnable[Any, Other], Callable[[Any], Other], Any]]]') -> 'RunnableSequence[Input, Other]'\n",
      "     |      Return self|value.\n",
      "     |  \n",
      "     |  __ror__(self, other: 'Union[Runnable[Other, Any], Callable[[Any], Other], Mapping[str, Union[Runnable[Other, Any], Callable[[Other], Any], Any]]]') -> 'RunnableSequence[Other, Output]'\n",
      "     |      Return value|self.\n",
      "     |  \n",
      "     |  async atransform(self, input: 'AsyncIterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'AsyncIterator[Output]'\n",
      "     |      Default implementation of atransform, which buffers input and calls astream.\n",
      "     |      Subclasses should override this method if they can start producing output while\n",
      "     |      input is still being generated.\n",
      "     |  \n",
      "     |  bind(self, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      "     |      Bind arguments to a Runnable, returning a new Runnable.\n",
      "     |  \n",
      "     |  map(self) -> 'Runnable[List[Input], List[Output]]'\n",
      "     |      Return a new Runnable that maps a list of inputs to a list of outputs,\n",
      "     |      by calling invoke() with each input.\n",
      "     |  \n",
      "     |  transform(self, input: 'Iterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'Iterator[Output]'\n",
      "     |      Default implementation of transform, which buffers input and then calls stream.\n",
      "     |      Subclasses should override this method if they can start producing output while\n",
      "     |      input is still being generated.\n",
      "     |  \n",
      "     |  with_config(self, config: 'Optional[RunnableConfig]' = None, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      "     |      Bind config to a Runnable, returning a new Runnable.\n",
      "     |  \n",
      "     |  with_fallbacks(self, fallbacks: 'Sequence[Runnable[Input, Output]]', *, exceptions_to_handle: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,)) -> 'RunnableWithFallbacks[Input, Output]'\n",
      "     |  \n",
      "     |  with_retry(self, *, retry_if_exception_type: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,), wait_exponential_jitter: 'bool' = True, stop_after_attempt: 'int' = 3) -> 'Runnable[Input, Output]'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __class_getitem__(params) from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  __init_subclass__(*args, **kwargs) from pydantic.main.ModelMetaclass\n",
      "     |      This method is called when a class is subclassed.\n",
      "     |      \n",
      "     |      The default implementation does nothing. It may be\n",
      "     |      overridden to extend subclasses.\n",
      "    \n",
      "    class OpenLLM(langchain.llms.base.LLM)\n",
      "     |  OpenLLM(model_name: 'Optional[str]' = None, *, model_id: 'Optional[str]' = None, server_url: 'Optional[str]' = None, server_type: \"Literal['grpc', 'http']\" = 'http', embedded: 'bool' = True, cache: Optional[bool] = None, verbose: bool = None, callbacks: Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager, NoneType] = None, callback_manager: Optional[langchain.callbacks.base.BaseCallbackManager] = None, tags: Optional[List[str]] = None, metadata: Optional[Dict[str, Any]] = None, llm_kwargs: Dict[str, Any]) -> None\n",
      "     |  \n",
      "     |  OpenLLM, supporting both in-process model\n",
      "     |  instance and remote OpenLLM servers.\n",
      "     |  \n",
      "     |  To use, you should have the openllm library installed:\n",
      "     |  \n",
      "     |  .. code-block:: bash\n",
      "     |  \n",
      "     |      pip install openllm\n",
      "     |  \n",
      "     |  Learn more at: https://github.com/bentoml/openllm\n",
      "     |  \n",
      "     |  Example running an LLM model locally managed by OpenLLM:\n",
      "     |      .. code-block:: python\n",
      "     |  \n",
      "     |          from langchain.llms import OpenLLM\n",
      "     |          llm = OpenLLM(\n",
      "     |              model_name='flan-t5',\n",
      "     |              model_id='google/flan-t5-large',\n",
      "     |          )\n",
      "     |          llm(\"What is the difference between a duck and a goose?\")\n",
      "     |  \n",
      "     |  For all available supported models, you can run 'openllm models'.\n",
      "     |  \n",
      "     |  If you have a OpenLLM server running, you can also use it remotely:\n",
      "     |      .. code-block:: python\n",
      "     |  \n",
      "     |          from langchain.llms import OpenLLM\n",
      "     |          llm = OpenLLM(server_url='http://localhost:3000')\n",
      "     |          llm(\"What is the difference between a duck and a goose?\")\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      OpenLLM\n",
      "     |      langchain.llms.base.LLM\n",
      "     |      langchain.llms.base.BaseLLM\n",
      "     |      langchain.schema.language_model.BaseLanguageModel\n",
      "     |      langchain.load.serializable.Serializable\n",
      "     |      pydantic.main.BaseModel\n",
      "     |      pydantic.utils.Representation\n",
      "     |      langchain.schema.runnable.base.Runnable\n",
      "     |      typing.Generic\n",
      "     |      abc.ABC\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, model_name: 'Optional[str]' = None, *, model_id: 'Optional[str]' = None, server_url: 'Optional[str]' = None, server_type: \"Literal['grpc', 'http']\" = 'http', embedded: 'bool' = True, **llm_kwargs: 'Any')\n",
      "     |      Create a new model by parsing and validating input data from keyword arguments.\n",
      "     |      \n",
      "     |      Raises ValidationError if the input data cannot be parsed to form a valid model.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __json_encoder__ = pydantic_encoder(obj: Any) -> Any\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  runner\n",
      "     |      Get the underlying openllm.LLMRunner instance for integration with BentoML.\n",
      "     |      \n",
      "     |      Example:\n",
      "     |      .. code-block:: python\n",
      "     |      \n",
      "     |          llm = OpenLLM(\n",
      "     |              model_name='flan-t5',\n",
      "     |              model_id='google/flan-t5-large',\n",
      "     |              embedded=False,\n",
      "     |          )\n",
      "     |          tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n",
      "     |          agent = initialize_agent(\n",
      "     |              tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION\n",
      "     |          )\n",
      "     |          svc = bentoml.Service(\"langchain-openllm\", runners=[llm.runner])\n",
      "     |      \n",
      "     |          @svc.api(input=Text(), output=Text())\n",
      "     |          def chat(input_text: str):\n",
      "     |              return agent.run(input_text)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  Config = <class 'langchain.llms.openllm.OpenLLM.Config'>\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {'_client': 'Union[openllm.client.HTTPClient, openll...\n",
      "     |  \n",
      "     |  __class_vars__ = set()\n",
      "     |  \n",
      "     |  __config__ = <class 'langchain.llms.openllm.Config'>\n",
      "     |  \n",
      "     |  __custom_root_type__ = False\n",
      "     |  \n",
      "     |  __exclude_fields__ = {'callback_manager': True, 'callbacks': True, 'me...\n",
      "     |  \n",
      "     |  __fields__ = {'cache': ModelField(name='cache', type=Optional[bool], r...\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "     |  \n",
      "     |  __include_fields__ = None\n",
      "     |  \n",
      "     |  __parameters__ = ()\n",
      "     |  \n",
      "     |  __post_root_validators__ = [(False, <function BaseLLM.raise_deprecatio...\n",
      "     |  \n",
      "     |  __pre_root_validators__ = []\n",
      "     |  \n",
      "     |  __private_attributes__ = {'_client': ModelPrivateAttr(), '_lc_kwargs':...\n",
      "     |  \n",
      "     |  __schema_cache__ = {}\n",
      "     |  \n",
      "     |  __signature__ = <Signature (model_name: 'Optional[str]' = None, ...ny]...\n",
      "     |  \n",
      "     |  __validators__ = {'verbose': [<pydantic.class_validators.Validator obj...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  __call__(self, prompt: 'str', stop: 'Optional[List[str]]' = None, callbacks: 'Callbacks' = None, *, tags: 'Optional[List[str]]' = None, metadata: 'Optional[Dict[str, Any]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Check Cache and run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  __str__(self) -> 'str'\n",
      "     |      Get a string representation of the object for printing.\n",
      "     |  \n",
      "     |  async abatch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      "     |      Default implementation of abatch, which calls ainvoke N times.\n",
      "     |      Subclasses should override this method if they can batch more efficiently.\n",
      "     |  \n",
      "     |  async agenerate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  async agenerate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Asynchronously pass a sequence of prompts and return model generations.\n",
      "     |      \n",
      "     |      This method should make use of batched calls for models that expose a batched\n",
      "     |      API.\n",
      "     |      \n",
      "     |      Use this method when you want to:\n",
      "     |          1. take advantage of batched calls,\n",
      "     |          2. need more output from the model than just the top generated value,\n",
      "     |          3. are building chains that are agnostic to the underlying language model\n",
      "     |              type (e.g., pure text completion models vs chat models).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      "     |              converted to match the format of any language model (string for pure\n",
      "     |              text generation models and BaseMessages for chat models).\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          callbacks: Callbacks to pass through. Used for executing additional\n",
      "     |              functionality, such as logging or streaming, throughout generation.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          An LLMResult, which contains a list of candidate Generations for each input\n",
      "     |              prompt and additional model provider-specific output.\n",
      "     |  \n",
      "     |  async ainvoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Default implementation of ainvoke, which calls invoke in a thread pool.\n",
      "     |      Subclasses should override this method if they can run asynchronously.\n",
      "     |  \n",
      "     |  async apredict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Asynchronously pass a string to the model and return a string prediction.\n",
      "     |      \n",
      "     |      Use this method when calling pure text generation models and only the top\n",
      "     |          candidate generation is needed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: String input to pass to the model.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a string.\n",
      "     |  \n",
      "     |  async apredict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      "     |      Asynchronously pass messages to the model and return a message prediction.\n",
      "     |      \n",
      "     |      Use this method when calling chat models and only the top\n",
      "     |          candidate generation is needed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: A sequence of chat messages corresponding to a single model input.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a message.\n",
      "     |  \n",
      "     |  async astream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'AsyncIterator[str]'\n",
      "     |      Default implementation of astream, which calls ainvoke.\n",
      "     |      Subclasses should override this method if they support streaming output.\n",
      "     |  \n",
      "     |  batch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      "     |      Default implementation of batch, which calls invoke N times.\n",
      "     |      Subclasses should override this method if they can batch more efficiently.\n",
      "     |  \n",
      "     |  dict(self, **kwargs: 'Any') -> 'Dict'\n",
      "     |      Return a dictionary of the LLM.\n",
      "     |  \n",
      "     |  generate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  generate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Pass a sequence of prompts to the model and return model generations.\n",
      "     |      \n",
      "     |      This method should make use of batched calls for models that expose a batched\n",
      "     |      API.\n",
      "     |      \n",
      "     |      Use this method when you want to:\n",
      "     |          1. take advantage of batched calls,\n",
      "     |          2. need more output from the model than just the top generated value,\n",
      "     |          3. are building chains that are agnostic to the underlying language model\n",
      "     |              type (e.g., pure text completion models vs chat models).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      "     |              converted to match the format of any language model (string for pure\n",
      "     |              text generation models and BaseMessages for chat models).\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          callbacks: Callbacks to pass through. Used for executing additional\n",
      "     |              functionality, such as logging or streaming, throughout generation.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          An LLMResult, which contains a list of candidate Generations for each input\n",
      "     |              prompt and additional model provider-specific output.\n",
      "     |  \n",
      "     |  invoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |  \n",
      "     |  predict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Pass a single string input to the model and return a string prediction.\n",
      "     |      \n",
      "     |       Use this method when passing in raw text. If you want to pass in specific\n",
      "     |          types of chat messages, use predict_messages.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: String input to pass to the model.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a string.\n",
      "     |  \n",
      "     |  predict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      "     |      Pass a message sequence to the model and return a message prediction.\n",
      "     |      \n",
      "     |      Use this method when passing in chat messages. If you want to pass in raw text,\n",
      "     |          use predict.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: A sequence of chat messages corresponding to a single model input.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a message.\n",
      "     |  \n",
      "     |  save(self, file_path: 'Union[Path, str]') -> 'None'\n",
      "     |      Save the LLM.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          file_path: Path to file to save the LLM to.\n",
      "     |      \n",
      "     |      Example:\n",
      "     |      .. code-block:: python\n",
      "     |      \n",
      "     |          llm.save(file_path=\"path/llm.yaml\")\n",
      "     |  \n",
      "     |  stream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'Iterator[str]'\n",
      "     |      Default implementation of stream, which calls invoke.\n",
      "     |      Subclasses should override this method if they support streaming output.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  raise_deprecation(values: 'Dict') -> 'Dict' from pydantic.main.ModelMetaclass\n",
      "     |      Raise deprecation warning if callback_manager is used.\n",
      "     |  \n",
      "     |  set_verbose(verbose: 'Optional[bool]') -> 'bool' from pydantic.main.ModelMetaclass\n",
      "     |      If verbose is None, set it.\n",
      "     |      \n",
      "     |      This allows users to pass in None as verbose to access the global setting.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  __orig_bases__ = (langchain.schema.language_model.BaseLanguageModel[st...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.schema.language_model.BaseLanguageModel:\n",
      "     |  \n",
      "     |  get_num_tokens(self, text: 'str') -> 'int'\n",
      "     |      Get the number of tokens present in the text.\n",
      "     |      \n",
      "     |      Useful for checking if an input will fit in a model's context window.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: The string input to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The integer number of tokens in the text.\n",
      "     |  \n",
      "     |  get_num_tokens_from_messages(self, messages: 'List[BaseMessage]') -> 'int'\n",
      "     |      Get the number of tokens in the messages.\n",
      "     |      \n",
      "     |      Useful for checking if an input will fit in a model's context window.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: The message inputs to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The sum of the number of tokens across the messages.\n",
      "     |  \n",
      "     |  get_token_ids(self, text: 'str') -> 'List[int]'\n",
      "     |      Return the ordered ids of the tokens in a text.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: The string input to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          A list of ids corresponding to the tokens in the text, in order they occur\n",
      "     |              in the text.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from langchain.schema.language_model.BaseLanguageModel:\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.load.serializable.Serializable:\n",
      "     |  \n",
      "     |  to_json(self) -> Union[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\n",
      "     |  \n",
      "     |  to_json_not_implemented(self) -> langchain.load.serializable.SerializedNotImplemented\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from langchain.load.serializable.Serializable:\n",
      "     |  \n",
      "     |  lc_attributes\n",
      "     |      Return a list of attribute names that should be included in the\n",
      "     |      serialized kwargs. These attributes must be accepted by the\n",
      "     |      constructor.\n",
      "     |  \n",
      "     |  lc_namespace\n",
      "     |      Return the namespace of the langchain object.\n",
      "     |      eg. [\"langchain\", \"llms\", \"openai\"]\n",
      "     |  \n",
      "     |  lc_secrets\n",
      "     |      Return a map of constructor argument names to secret ids.\n",
      "     |      eg. {\"openai_api_key\": \"OPENAI_API_KEY\"}\n",
      "     |  \n",
      "     |  lc_serializable\n",
      "     |      Return whether or not the class is serializable.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __eq__(self, other: Any) -> bool\n",
      "     |  \n",
      "     |  __getstate__(self) -> 'DictAny'\n",
      "     |  \n",
      "     |  __iter__(self) -> 'TupleGenerator'\n",
      "     |      so `dict(model)` works\n",
      "     |  \n",
      "     |  __repr_args__(self) -> 'ReprArgs'\n",
      "     |  \n",
      "     |  __setattr__(self, name, value)\n",
      "     |  \n",
      "     |  __setstate__(self, state: 'DictAny') -> None\n",
      "     |  \n",
      "     |  copy(self: 'Model', *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, update: Optional[ForwardRef('DictStrAny')] = None, deep: bool = False) -> 'Model'\n",
      "     |      Duplicate a model, optionally choose which fields to include, exclude and change.\n",
      "     |      \n",
      "     |      :param include: fields to include in new model\n",
      "     |      :param exclude: fields to exclude from new model, as with values this takes precedence over include\n",
      "     |      :param update: values to change/add in the new model. Note: the data is not validated before creating\n",
      "     |          the new model: you should trust this data\n",
      "     |      :param deep: set to `True` to make a deep copy of the model\n",
      "     |      :return: new model instance\n",
      "     |  \n",
      "     |  json(self, *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) -> 'unicode'\n",
      "     |      Generate a JSON representation of the model, `include` and `exclude` arguments as per `dict()`.\n",
      "     |      \n",
      "     |      `encoder` is an optional function to supply as `default` to json.dumps(), other arguments as per `json.dumps()`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __get_validators__() -> 'CallableGenerator' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  __try_update_forward_refs__(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      "     |      Same as update_forward_refs but will not raise exception\n",
      "     |      when forward references are not defined.\n",
      "     |  \n",
      "     |  construct(_fields_set: Optional[ForwardRef('SetStr')] = None, **values: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |      Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data.\n",
      "     |      Default values are respected, but no other validation is performed.\n",
      "     |      Behaves as if `Config.extra = 'allow'` was set since it adds all passed values\n",
      "     |  \n",
      "     |  from_orm(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_file(path: Union[str, pathlib.Path], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_obj(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_raw(b: Union[str, bytes], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema(by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}') -> 'DictStrAny' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema_json(*, by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}', **dumps_kwargs: Any) -> 'unicode' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  update_forward_refs(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      "     |      Try to update ForwardRefs on fields based on this Model, globalns and localns.\n",
      "     |  \n",
      "     |  validate(value: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __fields_set__\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.utils.Representation:\n",
      "     |  \n",
      "     |  __pretty__(self, fmt: Callable[[Any], Any], **kwargs: Any) -> Generator[Any, NoneType, NoneType]\n",
      "     |      Used by devtools (https://python-devtools.helpmanual.io/) to provide a human readable representations of objects\n",
      "     |  \n",
      "     |  __repr__(self) -> 'unicode'\n",
      "     |  \n",
      "     |  __repr_name__(self) -> 'unicode'\n",
      "     |      Name of the instance's class, used in __repr__.\n",
      "     |  \n",
      "     |  __repr_str__(self, join_str: 'unicode') -> 'unicode'\n",
      "     |  \n",
      "     |  __rich_repr__(self) -> 'RichReprResult'\n",
      "     |      Get fields for Rich library\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.schema.runnable.base.Runnable:\n",
      "     |  \n",
      "     |  __or__(self, other: 'Union[Runnable[Any, Other], Callable[[Any], Other], Mapping[str, Union[Runnable[Any, Other], Callable[[Any], Other], Any]]]') -> 'RunnableSequence[Input, Other]'\n",
      "     |      Return self|value.\n",
      "     |  \n",
      "     |  __ror__(self, other: 'Union[Runnable[Other, Any], Callable[[Any], Other], Mapping[str, Union[Runnable[Other, Any], Callable[[Other], Any], Any]]]') -> 'RunnableSequence[Other, Output]'\n",
      "     |      Return value|self.\n",
      "     |  \n",
      "     |  async atransform(self, input: 'AsyncIterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'AsyncIterator[Output]'\n",
      "     |      Default implementation of atransform, which buffers input and calls astream.\n",
      "     |      Subclasses should override this method if they can start producing output while\n",
      "     |      input is still being generated.\n",
      "     |  \n",
      "     |  bind(self, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      "     |      Bind arguments to a Runnable, returning a new Runnable.\n",
      "     |  \n",
      "     |  map(self) -> 'Runnable[List[Input], List[Output]]'\n",
      "     |      Return a new Runnable that maps a list of inputs to a list of outputs,\n",
      "     |      by calling invoke() with each input.\n",
      "     |  \n",
      "     |  transform(self, input: 'Iterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'Iterator[Output]'\n",
      "     |      Default implementation of transform, which buffers input and then calls stream.\n",
      "     |      Subclasses should override this method if they can start producing output while\n",
      "     |      input is still being generated.\n",
      "     |  \n",
      "     |  with_config(self, config: 'Optional[RunnableConfig]' = None, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      "     |      Bind config to a Runnable, returning a new Runnable.\n",
      "     |  \n",
      "     |  with_fallbacks(self, fallbacks: 'Sequence[Runnable[Input, Output]]', *, exceptions_to_handle: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,)) -> 'RunnableWithFallbacks[Input, Output]'\n",
      "     |  \n",
      "     |  with_retry(self, *, retry_if_exception_type: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,), wait_exponential_jitter: 'bool' = True, stop_after_attempt: 'int' = 3) -> 'Runnable[Input, Output]'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __class_getitem__(params) from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  __init_subclass__(*args, **kwargs) from pydantic.main.ModelMetaclass\n",
      "     |      This method is called when a class is subclassed.\n",
      "     |      \n",
      "     |      The default implementation does nothing. It may be\n",
      "     |      overridden to extend subclasses.\n",
      "    \n",
      "    class OpenLM(langchain.llms.openai.BaseOpenAI)\n",
      "     |  OpenLM(*, cache: Optional[bool] = None, verbose: bool = None, callbacks: Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager, NoneType] = None, callback_manager: Optional[langchain.callbacks.base.BaseCallbackManager] = None, tags: Optional[List[str]] = None, metadata: Optional[Dict[str, Any]] = None, client: Any = None, model: str = 'text-davinci-003', temperature: float = 0.7, max_tokens: int = 256, top_p: float = 1, frequency_penalty: float = 0, presence_penalty: float = 0, n: int = 1, best_of: int = 1, model_kwargs: Dict[str, Any] = None, openai_api_key: Optional[str] = None, openai_api_base: Optional[str] = None, openai_organization: Optional[str] = None, openai_proxy: Optional[str] = None, batch_size: int = 20, request_timeout: Union[float, Tuple[float, float], NoneType] = None, logit_bias: Optional[Dict[str, float]] = None, max_retries: int = 6, streaming: bool = False, allowed_special: Union[Literal['all'], AbstractSet[str]] = set(), disallowed_special: Union[Literal['all'], Collection[str]] = 'all', tiktoken_model_name: Optional[str] = None) -> None\n",
      "     |  \n",
      "     |  OpenLM models.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      OpenLM\n",
      "     |      langchain.llms.openai.BaseOpenAI\n",
      "     |      langchain.llms.base.BaseLLM\n",
      "     |      langchain.schema.language_model.BaseLanguageModel\n",
      "     |      langchain.load.serializable.Serializable\n",
      "     |      pydantic.main.BaseModel\n",
      "     |      pydantic.utils.Representation\n",
      "     |      langchain.schema.runnable.base.Runnable\n",
      "     |      typing.Generic\n",
      "     |      abc.ABC\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  validate_environment(values: Dict) -> Dict from pydantic.main.ModelMetaclass\n",
      "     |      Validate that api key and python package exists in environment.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __json_encoder__ = pydantic_encoder(obj: Any) -> Any\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {}\n",
      "     |  \n",
      "     |  __class_vars__ = set()\n",
      "     |  \n",
      "     |  __config__ = <class 'langchain.llms.openlm.Config'>\n",
      "     |  \n",
      "     |  __custom_root_type__ = False\n",
      "     |  \n",
      "     |  __exclude_fields__ = {'callback_manager': True, 'callbacks': True, 'me...\n",
      "     |  \n",
      "     |  __fields__ = {'allowed_special': ModelField(name='allowed_special', ty...\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "     |  \n",
      "     |  __include_fields__ = None\n",
      "     |  \n",
      "     |  __parameters__ = ()\n",
      "     |  \n",
      "     |  __post_root_validators__ = [(False, <function BaseLLM.raise_deprecatio...\n",
      "     |  \n",
      "     |  __pre_root_validators__ = [<function BaseOpenAI.build_extra>]\n",
      "     |  \n",
      "     |  __private_attributes__ = {'_lc_kwargs': ModelPrivateAttr(default=Pydan...\n",
      "     |  \n",
      "     |  __schema_cache__ = {}\n",
      "     |  \n",
      "     |  __signature__ = <Signature (*, cache: Optional[bool] = None, ver...kto...\n",
      "     |  \n",
      "     |  __validators__ = {'verbose': [<pydantic.class_validators.Validator obj...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.llms.openai.BaseOpenAI:\n",
      "     |  \n",
      "     |  create_llm_result(self, choices: 'Any', prompts: 'List[str]', token_usage: 'Dict[str, int]') -> 'LLMResult'\n",
      "     |      Create the LLMResult from the choices and prompts.\n",
      "     |  \n",
      "     |  get_sub_prompts(self, params: 'Dict[str, Any]', prompts: 'List[str]', stop: 'Optional[List[str]]' = None) -> 'List[List[str]]'\n",
      "     |      Get the sub prompts for llm call.\n",
      "     |  \n",
      "     |  get_token_ids(self, text: 'str') -> 'List[int]'\n",
      "     |      Get the token IDs using the tiktoken package.\n",
      "     |  \n",
      "     |  max_tokens_for_prompt(self, prompt: 'str') -> 'int'\n",
      "     |      Calculate the maximum number of tokens possible to generate for a prompt.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompt: The prompt to pass into the model.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The maximum number of tokens to generate for a prompt.\n",
      "     |      \n",
      "     |      Example:\n",
      "     |          .. code-block:: python\n",
      "     |      \n",
      "     |              max_tokens = openai.max_token_for_prompt(\"Tell me a joke.\")\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from langchain.llms.openai.BaseOpenAI:\n",
      "     |  \n",
      "     |  build_extra(values: 'Dict[str, Any]') -> 'Dict[str, Any]' from pydantic.main.ModelMetaclass\n",
      "     |      Build extra kwargs from additional params that were passed in.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from langchain.llms.openai.BaseOpenAI:\n",
      "     |  \n",
      "     |  __new__(cls, **data: 'Any') -> 'Union[OpenAIChat, BaseOpenAI]'\n",
      "     |      Initialize the OpenAI object.\n",
      "     |  \n",
      "     |  modelname_to_contextsize(modelname: 'str') -> 'int'\n",
      "     |      Calculate the maximum number of tokens possible to generate for a model.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          modelname: The modelname we want to know the context size for.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The maximum context size\n",
      "     |      \n",
      "     |      Example:\n",
      "     |          .. code-block:: python\n",
      "     |      \n",
      "     |              max_tokens = openai.modelname_to_contextsize(\"text-davinci-003\")\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from langchain.llms.openai.BaseOpenAI:\n",
      "     |  \n",
      "     |  lc_secrets\n",
      "     |      Return a map of constructor argument names to secret ids.\n",
      "     |      eg. {\"openai_api_key\": \"OPENAI_API_KEY\"}\n",
      "     |  \n",
      "     |  lc_serializable\n",
      "     |      Return whether or not the class is serializable.\n",
      "     |  \n",
      "     |  max_context_size\n",
      "     |      Get max context size for this model.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from langchain.llms.openai.BaseOpenAI:\n",
      "     |  \n",
      "     |  Config = <class 'langchain.llms.openai.BaseOpenAI.Config'>\n",
      "     |      Configuration for this pydantic object.\n",
      "     |  \n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  __call__(self, prompt: 'str', stop: 'Optional[List[str]]' = None, callbacks: 'Callbacks' = None, *, tags: 'Optional[List[str]]' = None, metadata: 'Optional[Dict[str, Any]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Check Cache and run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  __str__(self) -> 'str'\n",
      "     |      Get a string representation of the object for printing.\n",
      "     |  \n",
      "     |  async abatch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      "     |      Default implementation of abatch, which calls ainvoke N times.\n",
      "     |      Subclasses should override this method if they can batch more efficiently.\n",
      "     |  \n",
      "     |  async agenerate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  async agenerate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Asynchronously pass a sequence of prompts and return model generations.\n",
      "     |      \n",
      "     |      This method should make use of batched calls for models that expose a batched\n",
      "     |      API.\n",
      "     |      \n",
      "     |      Use this method when you want to:\n",
      "     |          1. take advantage of batched calls,\n",
      "     |          2. need more output from the model than just the top generated value,\n",
      "     |          3. are building chains that are agnostic to the underlying language model\n",
      "     |              type (e.g., pure text completion models vs chat models).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      "     |              converted to match the format of any language model (string for pure\n",
      "     |              text generation models and BaseMessages for chat models).\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          callbacks: Callbacks to pass through. Used for executing additional\n",
      "     |              functionality, such as logging or streaming, throughout generation.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          An LLMResult, which contains a list of candidate Generations for each input\n",
      "     |              prompt and additional model provider-specific output.\n",
      "     |  \n",
      "     |  async ainvoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Default implementation of ainvoke, which calls invoke in a thread pool.\n",
      "     |      Subclasses should override this method if they can run asynchronously.\n",
      "     |  \n",
      "     |  async apredict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Asynchronously pass a string to the model and return a string prediction.\n",
      "     |      \n",
      "     |      Use this method when calling pure text generation models and only the top\n",
      "     |          candidate generation is needed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: String input to pass to the model.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a string.\n",
      "     |  \n",
      "     |  async apredict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      "     |      Asynchronously pass messages to the model and return a message prediction.\n",
      "     |      \n",
      "     |      Use this method when calling chat models and only the top\n",
      "     |          candidate generation is needed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: A sequence of chat messages corresponding to a single model input.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a message.\n",
      "     |  \n",
      "     |  async astream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'AsyncIterator[str]'\n",
      "     |      Default implementation of astream, which calls ainvoke.\n",
      "     |      Subclasses should override this method if they support streaming output.\n",
      "     |  \n",
      "     |  batch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      "     |      Default implementation of batch, which calls invoke N times.\n",
      "     |      Subclasses should override this method if they can batch more efficiently.\n",
      "     |  \n",
      "     |  dict(self, **kwargs: 'Any') -> 'Dict'\n",
      "     |      Return a dictionary of the LLM.\n",
      "     |  \n",
      "     |  generate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  generate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Pass a sequence of prompts to the model and return model generations.\n",
      "     |      \n",
      "     |      This method should make use of batched calls for models that expose a batched\n",
      "     |      API.\n",
      "     |      \n",
      "     |      Use this method when you want to:\n",
      "     |          1. take advantage of batched calls,\n",
      "     |          2. need more output from the model than just the top generated value,\n",
      "     |          3. are building chains that are agnostic to the underlying language model\n",
      "     |              type (e.g., pure text completion models vs chat models).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      "     |              converted to match the format of any language model (string for pure\n",
      "     |              text generation models and BaseMessages for chat models).\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          callbacks: Callbacks to pass through. Used for executing additional\n",
      "     |              functionality, such as logging or streaming, throughout generation.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          An LLMResult, which contains a list of candidate Generations for each input\n",
      "     |              prompt and additional model provider-specific output.\n",
      "     |  \n",
      "     |  invoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |  \n",
      "     |  predict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Pass a single string input to the model and return a string prediction.\n",
      "     |      \n",
      "     |       Use this method when passing in raw text. If you want to pass in specific\n",
      "     |          types of chat messages, use predict_messages.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: String input to pass to the model.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a string.\n",
      "     |  \n",
      "     |  predict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      "     |      Pass a message sequence to the model and return a message prediction.\n",
      "     |      \n",
      "     |      Use this method when passing in chat messages. If you want to pass in raw text,\n",
      "     |          use predict.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: A sequence of chat messages corresponding to a single model input.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a message.\n",
      "     |  \n",
      "     |  save(self, file_path: 'Union[Path, str]') -> 'None'\n",
      "     |      Save the LLM.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          file_path: Path to file to save the LLM to.\n",
      "     |      \n",
      "     |      Example:\n",
      "     |      .. code-block:: python\n",
      "     |      \n",
      "     |          llm.save(file_path=\"path/llm.yaml\")\n",
      "     |  \n",
      "     |  stream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'Iterator[str]'\n",
      "     |      Default implementation of stream, which calls invoke.\n",
      "     |      Subclasses should override this method if they support streaming output.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  raise_deprecation(values: 'Dict') -> 'Dict' from pydantic.main.ModelMetaclass\n",
      "     |      Raise deprecation warning if callback_manager is used.\n",
      "     |  \n",
      "     |  set_verbose(verbose: 'Optional[bool]') -> 'bool' from pydantic.main.ModelMetaclass\n",
      "     |      If verbose is None, set it.\n",
      "     |      \n",
      "     |      This allows users to pass in None as verbose to access the global setting.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  __orig_bases__ = (langchain.schema.language_model.BaseLanguageModel[st...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.schema.language_model.BaseLanguageModel:\n",
      "     |  \n",
      "     |  get_num_tokens(self, text: 'str') -> 'int'\n",
      "     |      Get the number of tokens present in the text.\n",
      "     |      \n",
      "     |      Useful for checking if an input will fit in a model's context window.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: The string input to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The integer number of tokens in the text.\n",
      "     |  \n",
      "     |  get_num_tokens_from_messages(self, messages: 'List[BaseMessage]') -> 'int'\n",
      "     |      Get the number of tokens in the messages.\n",
      "     |      \n",
      "     |      Useful for checking if an input will fit in a model's context window.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: The message inputs to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The sum of the number of tokens across the messages.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from langchain.schema.language_model.BaseLanguageModel:\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.load.serializable.Serializable:\n",
      "     |  \n",
      "     |  __init__(self, **kwargs: Any) -> None\n",
      "     |      Create a new model by parsing and validating input data from keyword arguments.\n",
      "     |      \n",
      "     |      Raises ValidationError if the input data cannot be parsed to form a valid model.\n",
      "     |  \n",
      "     |  to_json(self) -> Union[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\n",
      "     |  \n",
      "     |  to_json_not_implemented(self) -> langchain.load.serializable.SerializedNotImplemented\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from langchain.load.serializable.Serializable:\n",
      "     |  \n",
      "     |  lc_attributes\n",
      "     |      Return a list of attribute names that should be included in the\n",
      "     |      serialized kwargs. These attributes must be accepted by the\n",
      "     |      constructor.\n",
      "     |  \n",
      "     |  lc_namespace\n",
      "     |      Return the namespace of the langchain object.\n",
      "     |      eg. [\"langchain\", \"llms\", \"openai\"]\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __eq__(self, other: Any) -> bool\n",
      "     |  \n",
      "     |  __getstate__(self) -> 'DictAny'\n",
      "     |  \n",
      "     |  __iter__(self) -> 'TupleGenerator'\n",
      "     |      so `dict(model)` works\n",
      "     |  \n",
      "     |  __repr_args__(self) -> 'ReprArgs'\n",
      "     |  \n",
      "     |  __setattr__(self, name, value)\n",
      "     |  \n",
      "     |  __setstate__(self, state: 'DictAny') -> None\n",
      "     |  \n",
      "     |  copy(self: 'Model', *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, update: Optional[ForwardRef('DictStrAny')] = None, deep: bool = False) -> 'Model'\n",
      "     |      Duplicate a model, optionally choose which fields to include, exclude and change.\n",
      "     |      \n",
      "     |      :param include: fields to include in new model\n",
      "     |      :param exclude: fields to exclude from new model, as with values this takes precedence over include\n",
      "     |      :param update: values to change/add in the new model. Note: the data is not validated before creating\n",
      "     |          the new model: you should trust this data\n",
      "     |      :param deep: set to `True` to make a deep copy of the model\n",
      "     |      :return: new model instance\n",
      "     |  \n",
      "     |  json(self, *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) -> 'unicode'\n",
      "     |      Generate a JSON representation of the model, `include` and `exclude` arguments as per `dict()`.\n",
      "     |      \n",
      "     |      `encoder` is an optional function to supply as `default` to json.dumps(), other arguments as per `json.dumps()`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __get_validators__() -> 'CallableGenerator' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  __try_update_forward_refs__(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      "     |      Same as update_forward_refs but will not raise exception\n",
      "     |      when forward references are not defined.\n",
      "     |  \n",
      "     |  construct(_fields_set: Optional[ForwardRef('SetStr')] = None, **values: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |      Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data.\n",
      "     |      Default values are respected, but no other validation is performed.\n",
      "     |      Behaves as if `Config.extra = 'allow'` was set since it adds all passed values\n",
      "     |  \n",
      "     |  from_orm(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_file(path: Union[str, pathlib.Path], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_obj(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_raw(b: Union[str, bytes], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema(by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}') -> 'DictStrAny' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema_json(*, by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}', **dumps_kwargs: Any) -> 'unicode' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  update_forward_refs(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      "     |      Try to update ForwardRefs on fields based on this Model, globalns and localns.\n",
      "     |  \n",
      "     |  validate(value: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __fields_set__\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.utils.Representation:\n",
      "     |  \n",
      "     |  __pretty__(self, fmt: Callable[[Any], Any], **kwargs: Any) -> Generator[Any, NoneType, NoneType]\n",
      "     |      Used by devtools (https://python-devtools.helpmanual.io/) to provide a human readable representations of objects\n",
      "     |  \n",
      "     |  __repr__(self) -> 'unicode'\n",
      "     |  \n",
      "     |  __repr_name__(self) -> 'unicode'\n",
      "     |      Name of the instance's class, used in __repr__.\n",
      "     |  \n",
      "     |  __repr_str__(self, join_str: 'unicode') -> 'unicode'\n",
      "     |  \n",
      "     |  __rich_repr__(self) -> 'RichReprResult'\n",
      "     |      Get fields for Rich library\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.schema.runnable.base.Runnable:\n",
      "     |  \n",
      "     |  __or__(self, other: 'Union[Runnable[Any, Other], Callable[[Any], Other], Mapping[str, Union[Runnable[Any, Other], Callable[[Any], Other], Any]]]') -> 'RunnableSequence[Input, Other]'\n",
      "     |      Return self|value.\n",
      "     |  \n",
      "     |  __ror__(self, other: 'Union[Runnable[Other, Any], Callable[[Any], Other], Mapping[str, Union[Runnable[Other, Any], Callable[[Other], Any], Any]]]') -> 'RunnableSequence[Other, Output]'\n",
      "     |      Return value|self.\n",
      "     |  \n",
      "     |  async atransform(self, input: 'AsyncIterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'AsyncIterator[Output]'\n",
      "     |      Default implementation of atransform, which buffers input and calls astream.\n",
      "     |      Subclasses should override this method if they can start producing output while\n",
      "     |      input is still being generated.\n",
      "     |  \n",
      "     |  bind(self, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      "     |      Bind arguments to a Runnable, returning a new Runnable.\n",
      "     |  \n",
      "     |  map(self) -> 'Runnable[List[Input], List[Output]]'\n",
      "     |      Return a new Runnable that maps a list of inputs to a list of outputs,\n",
      "     |      by calling invoke() with each input.\n",
      "     |  \n",
      "     |  transform(self, input: 'Iterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'Iterator[Output]'\n",
      "     |      Default implementation of transform, which buffers input and then calls stream.\n",
      "     |      Subclasses should override this method if they can start producing output while\n",
      "     |      input is still being generated.\n",
      "     |  \n",
      "     |  with_config(self, config: 'Optional[RunnableConfig]' = None, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      "     |      Bind config to a Runnable, returning a new Runnable.\n",
      "     |  \n",
      "     |  with_fallbacks(self, fallbacks: 'Sequence[Runnable[Input, Output]]', *, exceptions_to_handle: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,)) -> 'RunnableWithFallbacks[Input, Output]'\n",
      "     |  \n",
      "     |  with_retry(self, *, retry_if_exception_type: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,), wait_exponential_jitter: 'bool' = True, stop_after_attempt: 'int' = 3) -> 'Runnable[Input, Output]'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __class_getitem__(params) from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  __init_subclass__(*args, **kwargs) from pydantic.main.ModelMetaclass\n",
      "     |      This method is called when a class is subclassed.\n",
      "     |      \n",
      "     |      The default implementation does nothing. It may be\n",
      "     |      overridden to extend subclasses.\n",
      "    \n",
      "    class Petals(langchain.llms.base.LLM)\n",
      "     |  Petals(*, cache: Optional[bool] = None, verbose: bool = None, callbacks: Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager, NoneType] = None, callback_manager: Optional[langchain.callbacks.base.BaseCallbackManager] = None, tags: Optional[List[str]] = None, metadata: Optional[Dict[str, Any]] = None, client: Any = None, tokenizer: Any = None, model_name: str = 'bigscience/bloom-petals', temperature: float = 0.7, max_new_tokens: int = 256, top_p: float = 0.9, top_k: Optional[int] = None, do_sample: bool = True, max_length: Optional[int] = None, model_kwargs: Dict[str, Any] = None, huggingface_api_key: Optional[str] = None) -> None\n",
      "     |  \n",
      "     |  Petals Bloom models.\n",
      "     |  \n",
      "     |  To use, you should have the ``petals`` python package installed, and the\n",
      "     |  environment variable ``HUGGINGFACE_API_KEY`` set with your API key.\n",
      "     |  \n",
      "     |  Any parameters that are valid to be passed to the call can be passed\n",
      "     |  in, even if not explicitly saved on this class.\n",
      "     |  \n",
      "     |  Example:\n",
      "     |      .. code-block:: python\n",
      "     |  \n",
      "     |          from langchain.llms import petals\n",
      "     |          petals = Petals()\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Petals\n",
      "     |      langchain.llms.base.LLM\n",
      "     |      langchain.llms.base.BaseLLM\n",
      "     |      langchain.schema.language_model.BaseLanguageModel\n",
      "     |      langchain.load.serializable.Serializable\n",
      "     |      pydantic.main.BaseModel\n",
      "     |      pydantic.utils.Representation\n",
      "     |      langchain.schema.runnable.base.Runnable\n",
      "     |      typing.Generic\n",
      "     |      abc.ABC\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  build_extra(values: Dict[str, Any]) -> Dict[str, Any] from pydantic.main.ModelMetaclass\n",
      "     |      Build extra kwargs from additional params that were passed in.\n",
      "     |  \n",
      "     |  validate_environment(values: Dict) -> Dict from pydantic.main.ModelMetaclass\n",
      "     |      Validate that api key and python package exists in environment.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __json_encoder__ = pydantic_encoder(obj: Any) -> Any\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  Config = <class 'langchain.llms.petals.Petals.Config'>\n",
      "     |      Configuration for this pydantic config.\n",
      "     |  \n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {'client': typing.Any, 'do_sample': <class 'bool'>, ...\n",
      "     |  \n",
      "     |  __class_vars__ = set()\n",
      "     |  \n",
      "     |  __config__ = <class 'langchain.llms.petals.Config'>\n",
      "     |  \n",
      "     |  __custom_root_type__ = False\n",
      "     |  \n",
      "     |  __exclude_fields__ = {'callback_manager': True, 'callbacks': True, 'me...\n",
      "     |  \n",
      "     |  __fields__ = {'cache': ModelField(name='cache', type=Optional[bool], r...\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "     |  \n",
      "     |  __include_fields__ = None\n",
      "     |  \n",
      "     |  __parameters__ = ()\n",
      "     |  \n",
      "     |  __post_root_validators__ = [(False, <function BaseLLM.raise_deprecatio...\n",
      "     |  \n",
      "     |  __pre_root_validators__ = [<function Petals.build_extra>]\n",
      "     |  \n",
      "     |  __private_attributes__ = {'_lc_kwargs': ModelPrivateAttr(default=Pydan...\n",
      "     |  \n",
      "     |  __schema_cache__ = {}\n",
      "     |  \n",
      "     |  __signature__ = <Signature (*, cache: Optional[bool] = None, ver...ggi...\n",
      "     |  \n",
      "     |  __validators__ = {'verbose': [<pydantic.class_validators.Validator obj...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  __call__(self, prompt: 'str', stop: 'Optional[List[str]]' = None, callbacks: 'Callbacks' = None, *, tags: 'Optional[List[str]]' = None, metadata: 'Optional[Dict[str, Any]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Check Cache and run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  __str__(self) -> 'str'\n",
      "     |      Get a string representation of the object for printing.\n",
      "     |  \n",
      "     |  async abatch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      "     |      Default implementation of abatch, which calls ainvoke N times.\n",
      "     |      Subclasses should override this method if they can batch more efficiently.\n",
      "     |  \n",
      "     |  async agenerate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  async agenerate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Asynchronously pass a sequence of prompts and return model generations.\n",
      "     |      \n",
      "     |      This method should make use of batched calls for models that expose a batched\n",
      "     |      API.\n",
      "     |      \n",
      "     |      Use this method when you want to:\n",
      "     |          1. take advantage of batched calls,\n",
      "     |          2. need more output from the model than just the top generated value,\n",
      "     |          3. are building chains that are agnostic to the underlying language model\n",
      "     |              type (e.g., pure text completion models vs chat models).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      "     |              converted to match the format of any language model (string for pure\n",
      "     |              text generation models and BaseMessages for chat models).\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          callbacks: Callbacks to pass through. Used for executing additional\n",
      "     |              functionality, such as logging or streaming, throughout generation.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          An LLMResult, which contains a list of candidate Generations for each input\n",
      "     |              prompt and additional model provider-specific output.\n",
      "     |  \n",
      "     |  async ainvoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Default implementation of ainvoke, which calls invoke in a thread pool.\n",
      "     |      Subclasses should override this method if they can run asynchronously.\n",
      "     |  \n",
      "     |  async apredict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Asynchronously pass a string to the model and return a string prediction.\n",
      "     |      \n",
      "     |      Use this method when calling pure text generation models and only the top\n",
      "     |          candidate generation is needed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: String input to pass to the model.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a string.\n",
      "     |  \n",
      "     |  async apredict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      "     |      Asynchronously pass messages to the model and return a message prediction.\n",
      "     |      \n",
      "     |      Use this method when calling chat models and only the top\n",
      "     |          candidate generation is needed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: A sequence of chat messages corresponding to a single model input.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a message.\n",
      "     |  \n",
      "     |  async astream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'AsyncIterator[str]'\n",
      "     |      Default implementation of astream, which calls ainvoke.\n",
      "     |      Subclasses should override this method if they support streaming output.\n",
      "     |  \n",
      "     |  batch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      "     |      Default implementation of batch, which calls invoke N times.\n",
      "     |      Subclasses should override this method if they can batch more efficiently.\n",
      "     |  \n",
      "     |  dict(self, **kwargs: 'Any') -> 'Dict'\n",
      "     |      Return a dictionary of the LLM.\n",
      "     |  \n",
      "     |  generate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  generate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Pass a sequence of prompts to the model and return model generations.\n",
      "     |      \n",
      "     |      This method should make use of batched calls for models that expose a batched\n",
      "     |      API.\n",
      "     |      \n",
      "     |      Use this method when you want to:\n",
      "     |          1. take advantage of batched calls,\n",
      "     |          2. need more output from the model than just the top generated value,\n",
      "     |          3. are building chains that are agnostic to the underlying language model\n",
      "     |              type (e.g., pure text completion models vs chat models).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      "     |              converted to match the format of any language model (string for pure\n",
      "     |              text generation models and BaseMessages for chat models).\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          callbacks: Callbacks to pass through. Used for executing additional\n",
      "     |              functionality, such as logging or streaming, throughout generation.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          An LLMResult, which contains a list of candidate Generations for each input\n",
      "     |              prompt and additional model provider-specific output.\n",
      "     |  \n",
      "     |  invoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |  \n",
      "     |  predict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Pass a single string input to the model and return a string prediction.\n",
      "     |      \n",
      "     |       Use this method when passing in raw text. If you want to pass in specific\n",
      "     |          types of chat messages, use predict_messages.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: String input to pass to the model.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a string.\n",
      "     |  \n",
      "     |  predict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      "     |      Pass a message sequence to the model and return a message prediction.\n",
      "     |      \n",
      "     |      Use this method when passing in chat messages. If you want to pass in raw text,\n",
      "     |          use predict.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: A sequence of chat messages corresponding to a single model input.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a message.\n",
      "     |  \n",
      "     |  save(self, file_path: 'Union[Path, str]') -> 'None'\n",
      "     |      Save the LLM.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          file_path: Path to file to save the LLM to.\n",
      "     |      \n",
      "     |      Example:\n",
      "     |      .. code-block:: python\n",
      "     |      \n",
      "     |          llm.save(file_path=\"path/llm.yaml\")\n",
      "     |  \n",
      "     |  stream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'Iterator[str]'\n",
      "     |      Default implementation of stream, which calls invoke.\n",
      "     |      Subclasses should override this method if they support streaming output.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  raise_deprecation(values: 'Dict') -> 'Dict' from pydantic.main.ModelMetaclass\n",
      "     |      Raise deprecation warning if callback_manager is used.\n",
      "     |  \n",
      "     |  set_verbose(verbose: 'Optional[bool]') -> 'bool' from pydantic.main.ModelMetaclass\n",
      "     |      If verbose is None, set it.\n",
      "     |      \n",
      "     |      This allows users to pass in None as verbose to access the global setting.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  __orig_bases__ = (langchain.schema.language_model.BaseLanguageModel[st...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.schema.language_model.BaseLanguageModel:\n",
      "     |  \n",
      "     |  get_num_tokens(self, text: 'str') -> 'int'\n",
      "     |      Get the number of tokens present in the text.\n",
      "     |      \n",
      "     |      Useful for checking if an input will fit in a model's context window.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: The string input to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The integer number of tokens in the text.\n",
      "     |  \n",
      "     |  get_num_tokens_from_messages(self, messages: 'List[BaseMessage]') -> 'int'\n",
      "     |      Get the number of tokens in the messages.\n",
      "     |      \n",
      "     |      Useful for checking if an input will fit in a model's context window.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: The message inputs to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The sum of the number of tokens across the messages.\n",
      "     |  \n",
      "     |  get_token_ids(self, text: 'str') -> 'List[int]'\n",
      "     |      Return the ordered ids of the tokens in a text.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: The string input to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          A list of ids corresponding to the tokens in the text, in order they occur\n",
      "     |              in the text.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from langchain.schema.language_model.BaseLanguageModel:\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.load.serializable.Serializable:\n",
      "     |  \n",
      "     |  __init__(self, **kwargs: Any) -> None\n",
      "     |      Create a new model by parsing and validating input data from keyword arguments.\n",
      "     |      \n",
      "     |      Raises ValidationError if the input data cannot be parsed to form a valid model.\n",
      "     |  \n",
      "     |  to_json(self) -> Union[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\n",
      "     |  \n",
      "     |  to_json_not_implemented(self) -> langchain.load.serializable.SerializedNotImplemented\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from langchain.load.serializable.Serializable:\n",
      "     |  \n",
      "     |  lc_attributes\n",
      "     |      Return a list of attribute names that should be included in the\n",
      "     |      serialized kwargs. These attributes must be accepted by the\n",
      "     |      constructor.\n",
      "     |  \n",
      "     |  lc_namespace\n",
      "     |      Return the namespace of the langchain object.\n",
      "     |      eg. [\"langchain\", \"llms\", \"openai\"]\n",
      "     |  \n",
      "     |  lc_secrets\n",
      "     |      Return a map of constructor argument names to secret ids.\n",
      "     |      eg. {\"openai_api_key\": \"OPENAI_API_KEY\"}\n",
      "     |  \n",
      "     |  lc_serializable\n",
      "     |      Return whether or not the class is serializable.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __eq__(self, other: Any) -> bool\n",
      "     |  \n",
      "     |  __getstate__(self) -> 'DictAny'\n",
      "     |  \n",
      "     |  __iter__(self) -> 'TupleGenerator'\n",
      "     |      so `dict(model)` works\n",
      "     |  \n",
      "     |  __repr_args__(self) -> 'ReprArgs'\n",
      "     |  \n",
      "     |  __setattr__(self, name, value)\n",
      "     |  \n",
      "     |  __setstate__(self, state: 'DictAny') -> None\n",
      "     |  \n",
      "     |  copy(self: 'Model', *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, update: Optional[ForwardRef('DictStrAny')] = None, deep: bool = False) -> 'Model'\n",
      "     |      Duplicate a model, optionally choose which fields to include, exclude and change.\n",
      "     |      \n",
      "     |      :param include: fields to include in new model\n",
      "     |      :param exclude: fields to exclude from new model, as with values this takes precedence over include\n",
      "     |      :param update: values to change/add in the new model. Note: the data is not validated before creating\n",
      "     |          the new model: you should trust this data\n",
      "     |      :param deep: set to `True` to make a deep copy of the model\n",
      "     |      :return: new model instance\n",
      "     |  \n",
      "     |  json(self, *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) -> 'unicode'\n",
      "     |      Generate a JSON representation of the model, `include` and `exclude` arguments as per `dict()`.\n",
      "     |      \n",
      "     |      `encoder` is an optional function to supply as `default` to json.dumps(), other arguments as per `json.dumps()`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __get_validators__() -> 'CallableGenerator' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  __try_update_forward_refs__(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      "     |      Same as update_forward_refs but will not raise exception\n",
      "     |      when forward references are not defined.\n",
      "     |  \n",
      "     |  construct(_fields_set: Optional[ForwardRef('SetStr')] = None, **values: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |      Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data.\n",
      "     |      Default values are respected, but no other validation is performed.\n",
      "     |      Behaves as if `Config.extra = 'allow'` was set since it adds all passed values\n",
      "     |  \n",
      "     |  from_orm(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_file(path: Union[str, pathlib.Path], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_obj(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_raw(b: Union[str, bytes], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema(by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}') -> 'DictStrAny' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema_json(*, by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}', **dumps_kwargs: Any) -> 'unicode' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  update_forward_refs(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      "     |      Try to update ForwardRefs on fields based on this Model, globalns and localns.\n",
      "     |  \n",
      "     |  validate(value: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __fields_set__\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.utils.Representation:\n",
      "     |  \n",
      "     |  __pretty__(self, fmt: Callable[[Any], Any], **kwargs: Any) -> Generator[Any, NoneType, NoneType]\n",
      "     |      Used by devtools (https://python-devtools.helpmanual.io/) to provide a human readable representations of objects\n",
      "     |  \n",
      "     |  __repr__(self) -> 'unicode'\n",
      "     |  \n",
      "     |  __repr_name__(self) -> 'unicode'\n",
      "     |      Name of the instance's class, used in __repr__.\n",
      "     |  \n",
      "     |  __repr_str__(self, join_str: 'unicode') -> 'unicode'\n",
      "     |  \n",
      "     |  __rich_repr__(self) -> 'RichReprResult'\n",
      "     |      Get fields for Rich library\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.schema.runnable.base.Runnable:\n",
      "     |  \n",
      "     |  __or__(self, other: 'Union[Runnable[Any, Other], Callable[[Any], Other], Mapping[str, Union[Runnable[Any, Other], Callable[[Any], Other], Any]]]') -> 'RunnableSequence[Input, Other]'\n",
      "     |      Return self|value.\n",
      "     |  \n",
      "     |  __ror__(self, other: 'Union[Runnable[Other, Any], Callable[[Any], Other], Mapping[str, Union[Runnable[Other, Any], Callable[[Other], Any], Any]]]') -> 'RunnableSequence[Other, Output]'\n",
      "     |      Return value|self.\n",
      "     |  \n",
      "     |  async atransform(self, input: 'AsyncIterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'AsyncIterator[Output]'\n",
      "     |      Default implementation of atransform, which buffers input and calls astream.\n",
      "     |      Subclasses should override this method if they can start producing output while\n",
      "     |      input is still being generated.\n",
      "     |  \n",
      "     |  bind(self, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      "     |      Bind arguments to a Runnable, returning a new Runnable.\n",
      "     |  \n",
      "     |  map(self) -> 'Runnable[List[Input], List[Output]]'\n",
      "     |      Return a new Runnable that maps a list of inputs to a list of outputs,\n",
      "     |      by calling invoke() with each input.\n",
      "     |  \n",
      "     |  transform(self, input: 'Iterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'Iterator[Output]'\n",
      "     |      Default implementation of transform, which buffers input and then calls stream.\n",
      "     |      Subclasses should override this method if they can start producing output while\n",
      "     |      input is still being generated.\n",
      "     |  \n",
      "     |  with_config(self, config: 'Optional[RunnableConfig]' = None, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      "     |      Bind config to a Runnable, returning a new Runnable.\n",
      "     |  \n",
      "     |  with_fallbacks(self, fallbacks: 'Sequence[Runnable[Input, Output]]', *, exceptions_to_handle: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,)) -> 'RunnableWithFallbacks[Input, Output]'\n",
      "     |  \n",
      "     |  with_retry(self, *, retry_if_exception_type: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,), wait_exponential_jitter: 'bool' = True, stop_after_attempt: 'int' = 3) -> 'Runnable[Input, Output]'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __class_getitem__(params) from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  __init_subclass__(*args, **kwargs) from pydantic.main.ModelMetaclass\n",
      "     |      This method is called when a class is subclassed.\n",
      "     |      \n",
      "     |      The default implementation does nothing. It may be\n",
      "     |      overridden to extend subclasses.\n",
      "    \n",
      "    class PipelineAI(langchain.llms.base.LLM, pydantic.main.BaseModel)\n",
      "     |  PipelineAI(*, cache: Optional[bool] = None, verbose: bool = None, callbacks: Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager, NoneType] = None, callback_manager: Optional[langchain.callbacks.base.BaseCallbackManager] = None, tags: Optional[List[str]] = None, metadata: Optional[Dict[str, Any]] = None, pipeline_key: str = '', pipeline_kwargs: Dict[str, Any] = None, pipeline_api_key: Optional[str] = None) -> None\n",
      "     |  \n",
      "     |  PipelineAI large language models.\n",
      "     |  \n",
      "     |  To use, you should have the ``pipeline-ai`` python package installed,\n",
      "     |  and the environment variable ``PIPELINE_API_KEY`` set with your API key.\n",
      "     |  \n",
      "     |  Any parameters that are valid to be passed to the call can be passed\n",
      "     |  in, even if not explicitly saved on this class.\n",
      "     |  \n",
      "     |  Example:\n",
      "     |      .. code-block:: python\n",
      "     |  \n",
      "     |          from langchain import PipelineAI\n",
      "     |          pipeline = PipelineAI(pipeline_key=\"\")\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      PipelineAI\n",
      "     |      langchain.llms.base.LLM\n",
      "     |      langchain.llms.base.BaseLLM\n",
      "     |      langchain.schema.language_model.BaseLanguageModel\n",
      "     |      langchain.load.serializable.Serializable\n",
      "     |      pydantic.main.BaseModel\n",
      "     |      pydantic.utils.Representation\n",
      "     |      langchain.schema.runnable.base.Runnable\n",
      "     |      typing.Generic\n",
      "     |      abc.ABC\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  build_extra(values: Dict[str, Any]) -> Dict[str, Any] from pydantic.main.ModelMetaclass\n",
      "     |      Build extra kwargs from additional params that were passed in.\n",
      "     |  \n",
      "     |  validate_environment(values: Dict) -> Dict from pydantic.main.ModelMetaclass\n",
      "     |      Validate that api key and python package exists in environment.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __json_encoder__ = pydantic_encoder(obj: Any) -> Any\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  Config = <class 'langchain.llms.pipelineai.PipelineAI.Config'>\n",
      "     |      Configuration for this pydantic config.\n",
      "     |  \n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {'pipeline_api_key': typing.Optional[str], 'pipeline...\n",
      "     |  \n",
      "     |  __class_vars__ = set()\n",
      "     |  \n",
      "     |  __config__ = <class 'langchain.llms.pipelineai.Config'>\n",
      "     |  \n",
      "     |  __custom_root_type__ = False\n",
      "     |  \n",
      "     |  __exclude_fields__ = {'callback_manager': True, 'callbacks': True, 'me...\n",
      "     |  \n",
      "     |  __fields__ = {'cache': ModelField(name='cache', type=Optional[bool], r...\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "     |  \n",
      "     |  __include_fields__ = None\n",
      "     |  \n",
      "     |  __parameters__ = ()\n",
      "     |  \n",
      "     |  __post_root_validators__ = [(False, <function BaseLLM.raise_deprecatio...\n",
      "     |  \n",
      "     |  __pre_root_validators__ = [<function PipelineAI.build_extra>]\n",
      "     |  \n",
      "     |  __private_attributes__ = {'_lc_kwargs': ModelPrivateAttr(default=Pydan...\n",
      "     |  \n",
      "     |  __schema_cache__ = {}\n",
      "     |  \n",
      "     |  __signature__ = <Signature (*, cache: Optional[bool] = None, ver... pi...\n",
      "     |  \n",
      "     |  __validators__ = {'verbose': [<pydantic.class_validators.Validator obj...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  __call__(self, prompt: 'str', stop: 'Optional[List[str]]' = None, callbacks: 'Callbacks' = None, *, tags: 'Optional[List[str]]' = None, metadata: 'Optional[Dict[str, Any]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Check Cache and run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  __str__(self) -> 'str'\n",
      "     |      Get a string representation of the object for printing.\n",
      "     |  \n",
      "     |  async abatch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      "     |      Default implementation of abatch, which calls ainvoke N times.\n",
      "     |      Subclasses should override this method if they can batch more efficiently.\n",
      "     |  \n",
      "     |  async agenerate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  async agenerate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Asynchronously pass a sequence of prompts and return model generations.\n",
      "     |      \n",
      "     |      This method should make use of batched calls for models that expose a batched\n",
      "     |      API.\n",
      "     |      \n",
      "     |      Use this method when you want to:\n",
      "     |          1. take advantage of batched calls,\n",
      "     |          2. need more output from the model than just the top generated value,\n",
      "     |          3. are building chains that are agnostic to the underlying language model\n",
      "     |              type (e.g., pure text completion models vs chat models).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      "     |              converted to match the format of any language model (string for pure\n",
      "     |              text generation models and BaseMessages for chat models).\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          callbacks: Callbacks to pass through. Used for executing additional\n",
      "     |              functionality, such as logging or streaming, throughout generation.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          An LLMResult, which contains a list of candidate Generations for each input\n",
      "     |              prompt and additional model provider-specific output.\n",
      "     |  \n",
      "     |  async ainvoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Default implementation of ainvoke, which calls invoke in a thread pool.\n",
      "     |      Subclasses should override this method if they can run asynchronously.\n",
      "     |  \n",
      "     |  async apredict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Asynchronously pass a string to the model and return a string prediction.\n",
      "     |      \n",
      "     |      Use this method when calling pure text generation models and only the top\n",
      "     |          candidate generation is needed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: String input to pass to the model.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a string.\n",
      "     |  \n",
      "     |  async apredict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      "     |      Asynchronously pass messages to the model and return a message prediction.\n",
      "     |      \n",
      "     |      Use this method when calling chat models and only the top\n",
      "     |          candidate generation is needed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: A sequence of chat messages corresponding to a single model input.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a message.\n",
      "     |  \n",
      "     |  async astream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'AsyncIterator[str]'\n",
      "     |      Default implementation of astream, which calls ainvoke.\n",
      "     |      Subclasses should override this method if they support streaming output.\n",
      "     |  \n",
      "     |  batch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      "     |      Default implementation of batch, which calls invoke N times.\n",
      "     |      Subclasses should override this method if they can batch more efficiently.\n",
      "     |  \n",
      "     |  dict(self, **kwargs: 'Any') -> 'Dict'\n",
      "     |      Return a dictionary of the LLM.\n",
      "     |  \n",
      "     |  generate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  generate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Pass a sequence of prompts to the model and return model generations.\n",
      "     |      \n",
      "     |      This method should make use of batched calls for models that expose a batched\n",
      "     |      API.\n",
      "     |      \n",
      "     |      Use this method when you want to:\n",
      "     |          1. take advantage of batched calls,\n",
      "     |          2. need more output from the model than just the top generated value,\n",
      "     |          3. are building chains that are agnostic to the underlying language model\n",
      "     |              type (e.g., pure text completion models vs chat models).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      "     |              converted to match the format of any language model (string for pure\n",
      "     |              text generation models and BaseMessages for chat models).\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          callbacks: Callbacks to pass through. Used for executing additional\n",
      "     |              functionality, such as logging or streaming, throughout generation.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          An LLMResult, which contains a list of candidate Generations for each input\n",
      "     |              prompt and additional model provider-specific output.\n",
      "     |  \n",
      "     |  invoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |  \n",
      "     |  predict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Pass a single string input to the model and return a string prediction.\n",
      "     |      \n",
      "     |       Use this method when passing in raw text. If you want to pass in specific\n",
      "     |          types of chat messages, use predict_messages.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: String input to pass to the model.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a string.\n",
      "     |  \n",
      "     |  predict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      "     |      Pass a message sequence to the model and return a message prediction.\n",
      "     |      \n",
      "     |      Use this method when passing in chat messages. If you want to pass in raw text,\n",
      "     |          use predict.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: A sequence of chat messages corresponding to a single model input.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a message.\n",
      "     |  \n",
      "     |  save(self, file_path: 'Union[Path, str]') -> 'None'\n",
      "     |      Save the LLM.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          file_path: Path to file to save the LLM to.\n",
      "     |      \n",
      "     |      Example:\n",
      "     |      .. code-block:: python\n",
      "     |      \n",
      "     |          llm.save(file_path=\"path/llm.yaml\")\n",
      "     |  \n",
      "     |  stream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'Iterator[str]'\n",
      "     |      Default implementation of stream, which calls invoke.\n",
      "     |      Subclasses should override this method if they support streaming output.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  raise_deprecation(values: 'Dict') -> 'Dict' from pydantic.main.ModelMetaclass\n",
      "     |      Raise deprecation warning if callback_manager is used.\n",
      "     |  \n",
      "     |  set_verbose(verbose: 'Optional[bool]') -> 'bool' from pydantic.main.ModelMetaclass\n",
      "     |      If verbose is None, set it.\n",
      "     |      \n",
      "     |      This allows users to pass in None as verbose to access the global setting.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  __orig_bases__ = (langchain.schema.language_model.BaseLanguageModel[st...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.schema.language_model.BaseLanguageModel:\n",
      "     |  \n",
      "     |  get_num_tokens(self, text: 'str') -> 'int'\n",
      "     |      Get the number of tokens present in the text.\n",
      "     |      \n",
      "     |      Useful for checking if an input will fit in a model's context window.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: The string input to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The integer number of tokens in the text.\n",
      "     |  \n",
      "     |  get_num_tokens_from_messages(self, messages: 'List[BaseMessage]') -> 'int'\n",
      "     |      Get the number of tokens in the messages.\n",
      "     |      \n",
      "     |      Useful for checking if an input will fit in a model's context window.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: The message inputs to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The sum of the number of tokens across the messages.\n",
      "     |  \n",
      "     |  get_token_ids(self, text: 'str') -> 'List[int]'\n",
      "     |      Return the ordered ids of the tokens in a text.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: The string input to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          A list of ids corresponding to the tokens in the text, in order they occur\n",
      "     |              in the text.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from langchain.schema.language_model.BaseLanguageModel:\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.load.serializable.Serializable:\n",
      "     |  \n",
      "     |  __init__(self, **kwargs: Any) -> None\n",
      "     |      Create a new model by parsing and validating input data from keyword arguments.\n",
      "     |      \n",
      "     |      Raises ValidationError if the input data cannot be parsed to form a valid model.\n",
      "     |  \n",
      "     |  to_json(self) -> Union[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\n",
      "     |  \n",
      "     |  to_json_not_implemented(self) -> langchain.load.serializable.SerializedNotImplemented\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from langchain.load.serializable.Serializable:\n",
      "     |  \n",
      "     |  lc_attributes\n",
      "     |      Return a list of attribute names that should be included in the\n",
      "     |      serialized kwargs. These attributes must be accepted by the\n",
      "     |      constructor.\n",
      "     |  \n",
      "     |  lc_namespace\n",
      "     |      Return the namespace of the langchain object.\n",
      "     |      eg. [\"langchain\", \"llms\", \"openai\"]\n",
      "     |  \n",
      "     |  lc_secrets\n",
      "     |      Return a map of constructor argument names to secret ids.\n",
      "     |      eg. {\"openai_api_key\": \"OPENAI_API_KEY\"}\n",
      "     |  \n",
      "     |  lc_serializable\n",
      "     |      Return whether or not the class is serializable.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __eq__(self, other: Any) -> bool\n",
      "     |  \n",
      "     |  __getstate__(self) -> 'DictAny'\n",
      "     |  \n",
      "     |  __iter__(self) -> 'TupleGenerator'\n",
      "     |      so `dict(model)` works\n",
      "     |  \n",
      "     |  __repr_args__(self) -> 'ReprArgs'\n",
      "     |  \n",
      "     |  __setattr__(self, name, value)\n",
      "     |  \n",
      "     |  __setstate__(self, state: 'DictAny') -> None\n",
      "     |  \n",
      "     |  copy(self: 'Model', *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, update: Optional[ForwardRef('DictStrAny')] = None, deep: bool = False) -> 'Model'\n",
      "     |      Duplicate a model, optionally choose which fields to include, exclude and change.\n",
      "     |      \n",
      "     |      :param include: fields to include in new model\n",
      "     |      :param exclude: fields to exclude from new model, as with values this takes precedence over include\n",
      "     |      :param update: values to change/add in the new model. Note: the data is not validated before creating\n",
      "     |          the new model: you should trust this data\n",
      "     |      :param deep: set to `True` to make a deep copy of the model\n",
      "     |      :return: new model instance\n",
      "     |  \n",
      "     |  json(self, *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) -> 'unicode'\n",
      "     |      Generate a JSON representation of the model, `include` and `exclude` arguments as per `dict()`.\n",
      "     |      \n",
      "     |      `encoder` is an optional function to supply as `default` to json.dumps(), other arguments as per `json.dumps()`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __get_validators__() -> 'CallableGenerator' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  __try_update_forward_refs__(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      "     |      Same as update_forward_refs but will not raise exception\n",
      "     |      when forward references are not defined.\n",
      "     |  \n",
      "     |  construct(_fields_set: Optional[ForwardRef('SetStr')] = None, **values: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |      Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data.\n",
      "     |      Default values are respected, but no other validation is performed.\n",
      "     |      Behaves as if `Config.extra = 'allow'` was set since it adds all passed values\n",
      "     |  \n",
      "     |  from_orm(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_file(path: Union[str, pathlib.Path], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_obj(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_raw(b: Union[str, bytes], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema(by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}') -> 'DictStrAny' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema_json(*, by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}', **dumps_kwargs: Any) -> 'unicode' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  update_forward_refs(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      "     |      Try to update ForwardRefs on fields based on this Model, globalns and localns.\n",
      "     |  \n",
      "     |  validate(value: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __fields_set__\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.utils.Representation:\n",
      "     |  \n",
      "     |  __pretty__(self, fmt: Callable[[Any], Any], **kwargs: Any) -> Generator[Any, NoneType, NoneType]\n",
      "     |      Used by devtools (https://python-devtools.helpmanual.io/) to provide a human readable representations of objects\n",
      "     |  \n",
      "     |  __repr__(self) -> 'unicode'\n",
      "     |  \n",
      "     |  __repr_name__(self) -> 'unicode'\n",
      "     |      Name of the instance's class, used in __repr__.\n",
      "     |  \n",
      "     |  __repr_str__(self, join_str: 'unicode') -> 'unicode'\n",
      "     |  \n",
      "     |  __rich_repr__(self) -> 'RichReprResult'\n",
      "     |      Get fields for Rich library\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.schema.runnable.base.Runnable:\n",
      "     |  \n",
      "     |  __or__(self, other: 'Union[Runnable[Any, Other], Callable[[Any], Other], Mapping[str, Union[Runnable[Any, Other], Callable[[Any], Other], Any]]]') -> 'RunnableSequence[Input, Other]'\n",
      "     |      Return self|value.\n",
      "     |  \n",
      "     |  __ror__(self, other: 'Union[Runnable[Other, Any], Callable[[Any], Other], Mapping[str, Union[Runnable[Other, Any], Callable[[Other], Any], Any]]]') -> 'RunnableSequence[Other, Output]'\n",
      "     |      Return value|self.\n",
      "     |  \n",
      "     |  async atransform(self, input: 'AsyncIterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'AsyncIterator[Output]'\n",
      "     |      Default implementation of atransform, which buffers input and calls astream.\n",
      "     |      Subclasses should override this method if they can start producing output while\n",
      "     |      input is still being generated.\n",
      "     |  \n",
      "     |  bind(self, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      "     |      Bind arguments to a Runnable, returning a new Runnable.\n",
      "     |  \n",
      "     |  map(self) -> 'Runnable[List[Input], List[Output]]'\n",
      "     |      Return a new Runnable that maps a list of inputs to a list of outputs,\n",
      "     |      by calling invoke() with each input.\n",
      "     |  \n",
      "     |  transform(self, input: 'Iterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'Iterator[Output]'\n",
      "     |      Default implementation of transform, which buffers input and then calls stream.\n",
      "     |      Subclasses should override this method if they can start producing output while\n",
      "     |      input is still being generated.\n",
      "     |  \n",
      "     |  with_config(self, config: 'Optional[RunnableConfig]' = None, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      "     |      Bind config to a Runnable, returning a new Runnable.\n",
      "     |  \n",
      "     |  with_fallbacks(self, fallbacks: 'Sequence[Runnable[Input, Output]]', *, exceptions_to_handle: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,)) -> 'RunnableWithFallbacks[Input, Output]'\n",
      "     |  \n",
      "     |  with_retry(self, *, retry_if_exception_type: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,), wait_exponential_jitter: 'bool' = True, stop_after_attempt: 'int' = 3) -> 'Runnable[Input, Output]'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __class_getitem__(params) from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  __init_subclass__(*args, **kwargs) from pydantic.main.ModelMetaclass\n",
      "     |      This method is called when a class is subclassed.\n",
      "     |      \n",
      "     |      The default implementation does nothing. It may be\n",
      "     |      overridden to extend subclasses.\n",
      "    \n",
      "    class Predibase(langchain.llms.base.LLM)\n",
      "     |  Predibase(*, cache: Optional[bool] = None, verbose: bool = None, callbacks: Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager, NoneType] = None, callback_manager: Optional[langchain.callbacks.base.BaseCallbackManager] = None, tags: Optional[List[str]] = None, metadata: Optional[Dict[str, Any]] = None, model: str, predibase_api_key: str, model_kwargs: Dict[str, Any] = None) -> None\n",
      "     |  \n",
      "     |  Use your Predibase models with Langchain.\n",
      "     |  \n",
      "     |  To use, you should have the ``predibase`` python package installed,\n",
      "     |  and have your Predibase API key.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Predibase\n",
      "     |      langchain.llms.base.LLM\n",
      "     |      langchain.llms.base.BaseLLM\n",
      "     |      langchain.schema.language_model.BaseLanguageModel\n",
      "     |      langchain.load.serializable.Serializable\n",
      "     |      pydantic.main.BaseModel\n",
      "     |      pydantic.utils.Representation\n",
      "     |      langchain.schema.runnable.base.Runnable\n",
      "     |      typing.Generic\n",
      "     |      abc.ABC\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __json_encoder__ = pydantic_encoder(obj: Any) -> Any\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {'model': <class 'str'>, 'model_kwargs': typing.Dict...\n",
      "     |  \n",
      "     |  __class_vars__ = set()\n",
      "     |  \n",
      "     |  __config__ = <class 'langchain.llms.predibase.Config'>\n",
      "     |  \n",
      "     |  __custom_root_type__ = False\n",
      "     |  \n",
      "     |  __exclude_fields__ = {'callback_manager': True, 'callbacks': True, 'me...\n",
      "     |  \n",
      "     |  __fields__ = {'cache': ModelField(name='cache', type=Optional[bool], r...\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "     |  \n",
      "     |  __include_fields__ = None\n",
      "     |  \n",
      "     |  __parameters__ = ()\n",
      "     |  \n",
      "     |  __post_root_validators__ = [(False, <function BaseLLM.raise_deprecatio...\n",
      "     |  \n",
      "     |  __pre_root_validators__ = []\n",
      "     |  \n",
      "     |  __private_attributes__ = {'_lc_kwargs': ModelPrivateAttr(default=Pydan...\n",
      "     |  \n",
      "     |  __schema_cache__ = {}\n",
      "     |  \n",
      "     |  __signature__ = <Signature (*, cache: Optional[bool] = None, ver...tr,...\n",
      "     |  \n",
      "     |  __validators__ = {'verbose': [<pydantic.class_validators.Validator obj...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  __call__(self, prompt: 'str', stop: 'Optional[List[str]]' = None, callbacks: 'Callbacks' = None, *, tags: 'Optional[List[str]]' = None, metadata: 'Optional[Dict[str, Any]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Check Cache and run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  __str__(self) -> 'str'\n",
      "     |      Get a string representation of the object for printing.\n",
      "     |  \n",
      "     |  async abatch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      "     |      Default implementation of abatch, which calls ainvoke N times.\n",
      "     |      Subclasses should override this method if they can batch more efficiently.\n",
      "     |  \n",
      "     |  async agenerate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  async agenerate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Asynchronously pass a sequence of prompts and return model generations.\n",
      "     |      \n",
      "     |      This method should make use of batched calls for models that expose a batched\n",
      "     |      API.\n",
      "     |      \n",
      "     |      Use this method when you want to:\n",
      "     |          1. take advantage of batched calls,\n",
      "     |          2. need more output from the model than just the top generated value,\n",
      "     |          3. are building chains that are agnostic to the underlying language model\n",
      "     |              type (e.g., pure text completion models vs chat models).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      "     |              converted to match the format of any language model (string for pure\n",
      "     |              text generation models and BaseMessages for chat models).\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          callbacks: Callbacks to pass through. Used for executing additional\n",
      "     |              functionality, such as logging or streaming, throughout generation.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          An LLMResult, which contains a list of candidate Generations for each input\n",
      "     |              prompt and additional model provider-specific output.\n",
      "     |  \n",
      "     |  async ainvoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Default implementation of ainvoke, which calls invoke in a thread pool.\n",
      "     |      Subclasses should override this method if they can run asynchronously.\n",
      "     |  \n",
      "     |  async apredict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Asynchronously pass a string to the model and return a string prediction.\n",
      "     |      \n",
      "     |      Use this method when calling pure text generation models and only the top\n",
      "     |          candidate generation is needed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: String input to pass to the model.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a string.\n",
      "     |  \n",
      "     |  async apredict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      "     |      Asynchronously pass messages to the model and return a message prediction.\n",
      "     |      \n",
      "     |      Use this method when calling chat models and only the top\n",
      "     |          candidate generation is needed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: A sequence of chat messages corresponding to a single model input.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a message.\n",
      "     |  \n",
      "     |  async astream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'AsyncIterator[str]'\n",
      "     |      Default implementation of astream, which calls ainvoke.\n",
      "     |      Subclasses should override this method if they support streaming output.\n",
      "     |  \n",
      "     |  batch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      "     |      Default implementation of batch, which calls invoke N times.\n",
      "     |      Subclasses should override this method if they can batch more efficiently.\n",
      "     |  \n",
      "     |  dict(self, **kwargs: 'Any') -> 'Dict'\n",
      "     |      Return a dictionary of the LLM.\n",
      "     |  \n",
      "     |  generate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  generate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Pass a sequence of prompts to the model and return model generations.\n",
      "     |      \n",
      "     |      This method should make use of batched calls for models that expose a batched\n",
      "     |      API.\n",
      "     |      \n",
      "     |      Use this method when you want to:\n",
      "     |          1. take advantage of batched calls,\n",
      "     |          2. need more output from the model than just the top generated value,\n",
      "     |          3. are building chains that are agnostic to the underlying language model\n",
      "     |              type (e.g., pure text completion models vs chat models).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      "     |              converted to match the format of any language model (string for pure\n",
      "     |              text generation models and BaseMessages for chat models).\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          callbacks: Callbacks to pass through. Used for executing additional\n",
      "     |              functionality, such as logging or streaming, throughout generation.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          An LLMResult, which contains a list of candidate Generations for each input\n",
      "     |              prompt and additional model provider-specific output.\n",
      "     |  \n",
      "     |  invoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |  \n",
      "     |  predict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Pass a single string input to the model and return a string prediction.\n",
      "     |      \n",
      "     |       Use this method when passing in raw text. If you want to pass in specific\n",
      "     |          types of chat messages, use predict_messages.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: String input to pass to the model.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a string.\n",
      "     |  \n",
      "     |  predict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      "     |      Pass a message sequence to the model and return a message prediction.\n",
      "     |      \n",
      "     |      Use this method when passing in chat messages. If you want to pass in raw text,\n",
      "     |          use predict.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: A sequence of chat messages corresponding to a single model input.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a message.\n",
      "     |  \n",
      "     |  save(self, file_path: 'Union[Path, str]') -> 'None'\n",
      "     |      Save the LLM.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          file_path: Path to file to save the LLM to.\n",
      "     |      \n",
      "     |      Example:\n",
      "     |      .. code-block:: python\n",
      "     |      \n",
      "     |          llm.save(file_path=\"path/llm.yaml\")\n",
      "     |  \n",
      "     |  stream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'Iterator[str]'\n",
      "     |      Default implementation of stream, which calls invoke.\n",
      "     |      Subclasses should override this method if they support streaming output.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  raise_deprecation(values: 'Dict') -> 'Dict' from pydantic.main.ModelMetaclass\n",
      "     |      Raise deprecation warning if callback_manager is used.\n",
      "     |  \n",
      "     |  set_verbose(verbose: 'Optional[bool]') -> 'bool' from pydantic.main.ModelMetaclass\n",
      "     |      If verbose is None, set it.\n",
      "     |      \n",
      "     |      This allows users to pass in None as verbose to access the global setting.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  Config = <class 'langchain.llms.base.BaseLLM.Config'>\n",
      "     |      Configuration for this pydantic object.\n",
      "     |  \n",
      "     |  \n",
      "     |  __orig_bases__ = (langchain.schema.language_model.BaseLanguageModel[st...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.schema.language_model.BaseLanguageModel:\n",
      "     |  \n",
      "     |  get_num_tokens(self, text: 'str') -> 'int'\n",
      "     |      Get the number of tokens present in the text.\n",
      "     |      \n",
      "     |      Useful for checking if an input will fit in a model's context window.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: The string input to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The integer number of tokens in the text.\n",
      "     |  \n",
      "     |  get_num_tokens_from_messages(self, messages: 'List[BaseMessage]') -> 'int'\n",
      "     |      Get the number of tokens in the messages.\n",
      "     |      \n",
      "     |      Useful for checking if an input will fit in a model's context window.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: The message inputs to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The sum of the number of tokens across the messages.\n",
      "     |  \n",
      "     |  get_token_ids(self, text: 'str') -> 'List[int]'\n",
      "     |      Return the ordered ids of the tokens in a text.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: The string input to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          A list of ids corresponding to the tokens in the text, in order they occur\n",
      "     |              in the text.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from langchain.schema.language_model.BaseLanguageModel:\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.load.serializable.Serializable:\n",
      "     |  \n",
      "     |  __init__(self, **kwargs: Any) -> None\n",
      "     |      Create a new model by parsing and validating input data from keyword arguments.\n",
      "     |      \n",
      "     |      Raises ValidationError if the input data cannot be parsed to form a valid model.\n",
      "     |  \n",
      "     |  to_json(self) -> Union[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\n",
      "     |  \n",
      "     |  to_json_not_implemented(self) -> langchain.load.serializable.SerializedNotImplemented\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from langchain.load.serializable.Serializable:\n",
      "     |  \n",
      "     |  lc_attributes\n",
      "     |      Return a list of attribute names that should be included in the\n",
      "     |      serialized kwargs. These attributes must be accepted by the\n",
      "     |      constructor.\n",
      "     |  \n",
      "     |  lc_namespace\n",
      "     |      Return the namespace of the langchain object.\n",
      "     |      eg. [\"langchain\", \"llms\", \"openai\"]\n",
      "     |  \n",
      "     |  lc_secrets\n",
      "     |      Return a map of constructor argument names to secret ids.\n",
      "     |      eg. {\"openai_api_key\": \"OPENAI_API_KEY\"}\n",
      "     |  \n",
      "     |  lc_serializable\n",
      "     |      Return whether or not the class is serializable.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __eq__(self, other: Any) -> bool\n",
      "     |  \n",
      "     |  __getstate__(self) -> 'DictAny'\n",
      "     |  \n",
      "     |  __iter__(self) -> 'TupleGenerator'\n",
      "     |      so `dict(model)` works\n",
      "     |  \n",
      "     |  __repr_args__(self) -> 'ReprArgs'\n",
      "     |  \n",
      "     |  __setattr__(self, name, value)\n",
      "     |  \n",
      "     |  __setstate__(self, state: 'DictAny') -> None\n",
      "     |  \n",
      "     |  copy(self: 'Model', *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, update: Optional[ForwardRef('DictStrAny')] = None, deep: bool = False) -> 'Model'\n",
      "     |      Duplicate a model, optionally choose which fields to include, exclude and change.\n",
      "     |      \n",
      "     |      :param include: fields to include in new model\n",
      "     |      :param exclude: fields to exclude from new model, as with values this takes precedence over include\n",
      "     |      :param update: values to change/add in the new model. Note: the data is not validated before creating\n",
      "     |          the new model: you should trust this data\n",
      "     |      :param deep: set to `True` to make a deep copy of the model\n",
      "     |      :return: new model instance\n",
      "     |  \n",
      "     |  json(self, *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) -> 'unicode'\n",
      "     |      Generate a JSON representation of the model, `include` and `exclude` arguments as per `dict()`.\n",
      "     |      \n",
      "     |      `encoder` is an optional function to supply as `default` to json.dumps(), other arguments as per `json.dumps()`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __get_validators__() -> 'CallableGenerator' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  __try_update_forward_refs__(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      "     |      Same as update_forward_refs but will not raise exception\n",
      "     |      when forward references are not defined.\n",
      "     |  \n",
      "     |  construct(_fields_set: Optional[ForwardRef('SetStr')] = None, **values: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |      Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data.\n",
      "     |      Default values are respected, but no other validation is performed.\n",
      "     |      Behaves as if `Config.extra = 'allow'` was set since it adds all passed values\n",
      "     |  \n",
      "     |  from_orm(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_file(path: Union[str, pathlib.Path], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_obj(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_raw(b: Union[str, bytes], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema(by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}') -> 'DictStrAny' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema_json(*, by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}', **dumps_kwargs: Any) -> 'unicode' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  update_forward_refs(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      "     |      Try to update ForwardRefs on fields based on this Model, globalns and localns.\n",
      "     |  \n",
      "     |  validate(value: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __fields_set__\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.utils.Representation:\n",
      "     |  \n",
      "     |  __pretty__(self, fmt: Callable[[Any], Any], **kwargs: Any) -> Generator[Any, NoneType, NoneType]\n",
      "     |      Used by devtools (https://python-devtools.helpmanual.io/) to provide a human readable representations of objects\n",
      "     |  \n",
      "     |  __repr__(self) -> 'unicode'\n",
      "     |  \n",
      "     |  __repr_name__(self) -> 'unicode'\n",
      "     |      Name of the instance's class, used in __repr__.\n",
      "     |  \n",
      "     |  __repr_str__(self, join_str: 'unicode') -> 'unicode'\n",
      "     |  \n",
      "     |  __rich_repr__(self) -> 'RichReprResult'\n",
      "     |      Get fields for Rich library\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.schema.runnable.base.Runnable:\n",
      "     |  \n",
      "     |  __or__(self, other: 'Union[Runnable[Any, Other], Callable[[Any], Other], Mapping[str, Union[Runnable[Any, Other], Callable[[Any], Other], Any]]]') -> 'RunnableSequence[Input, Other]'\n",
      "     |      Return self|value.\n",
      "     |  \n",
      "     |  __ror__(self, other: 'Union[Runnable[Other, Any], Callable[[Any], Other], Mapping[str, Union[Runnable[Other, Any], Callable[[Other], Any], Any]]]') -> 'RunnableSequence[Other, Output]'\n",
      "     |      Return value|self.\n",
      "     |  \n",
      "     |  async atransform(self, input: 'AsyncIterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'AsyncIterator[Output]'\n",
      "     |      Default implementation of atransform, which buffers input and calls astream.\n",
      "     |      Subclasses should override this method if they can start producing output while\n",
      "     |      input is still being generated.\n",
      "     |  \n",
      "     |  bind(self, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      "     |      Bind arguments to a Runnable, returning a new Runnable.\n",
      "     |  \n",
      "     |  map(self) -> 'Runnable[List[Input], List[Output]]'\n",
      "     |      Return a new Runnable that maps a list of inputs to a list of outputs,\n",
      "     |      by calling invoke() with each input.\n",
      "     |  \n",
      "     |  transform(self, input: 'Iterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'Iterator[Output]'\n",
      "     |      Default implementation of transform, which buffers input and then calls stream.\n",
      "     |      Subclasses should override this method if they can start producing output while\n",
      "     |      input is still being generated.\n",
      "     |  \n",
      "     |  with_config(self, config: 'Optional[RunnableConfig]' = None, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      "     |      Bind config to a Runnable, returning a new Runnable.\n",
      "     |  \n",
      "     |  with_fallbacks(self, fallbacks: 'Sequence[Runnable[Input, Output]]', *, exceptions_to_handle: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,)) -> 'RunnableWithFallbacks[Input, Output]'\n",
      "     |  \n",
      "     |  with_retry(self, *, retry_if_exception_type: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,), wait_exponential_jitter: 'bool' = True, stop_after_attempt: 'int' = 3) -> 'Runnable[Input, Output]'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __class_getitem__(params) from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  __init_subclass__(*args, **kwargs) from pydantic.main.ModelMetaclass\n",
      "     |      This method is called when a class is subclassed.\n",
      "     |      \n",
      "     |      The default implementation does nothing. It may be\n",
      "     |      overridden to extend subclasses.\n",
      "    \n",
      "    class PredictionGuard(langchain.llms.base.LLM)\n",
      "     |  PredictionGuard(*, cache: Optional[bool] = None, verbose: bool = None, callbacks: Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager, NoneType] = None, callback_manager: Optional[langchain.callbacks.base.BaseCallbackManager] = None, tags: Optional[List[str]] = None, metadata: Optional[Dict[str, Any]] = None, client: Any = None, model: Optional[str] = 'MPT-7B-Instruct', output: Optional[Dict[str, Any]] = None, max_tokens: int = 256, temperature: float = 0.75, token: Optional[str] = None, stop: Optional[List[str]] = None) -> None\n",
      "     |  \n",
      "     |  Prediction Guard large language models.\n",
      "     |  \n",
      "     |  To use, you should have the ``predictionguard`` python package installed, and the\n",
      "     |  environment variable ``PREDICTIONGUARD_TOKEN`` set with your access token, or pass\n",
      "     |  it as a named parameter to the constructor. To use Prediction Guard's API along\n",
      "     |  with OpenAI models, set the environment variable ``OPENAI_API_KEY`` with your\n",
      "     |  OpenAI API key as well.\n",
      "     |  \n",
      "     |  Example:\n",
      "     |      .. code-block:: python\n",
      "     |  \n",
      "     |          pgllm = PredictionGuard(model=\"MPT-7B-Instruct\",\n",
      "     |                                  token=\"my-access-token\",\n",
      "     |                                  output={\n",
      "     |                                      \"type\": \"boolean\"\n",
      "     |                                  })\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      PredictionGuard\n",
      "     |      langchain.llms.base.LLM\n",
      "     |      langchain.llms.base.BaseLLM\n",
      "     |      langchain.schema.language_model.BaseLanguageModel\n",
      "     |      langchain.load.serializable.Serializable\n",
      "     |      pydantic.main.BaseModel\n",
      "     |      pydantic.utils.Representation\n",
      "     |      langchain.schema.runnable.base.Runnable\n",
      "     |      typing.Generic\n",
      "     |      abc.ABC\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  validate_environment(values: Dict) -> Dict from pydantic.main.ModelMetaclass\n",
      "     |      Validate that the access token and python package exists in environment.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __json_encoder__ = pydantic_encoder(obj: Any) -> Any\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  Config = <class 'langchain.llms.predictionguard.PredictionGuard.Config...\n",
      "     |      Configuration for this pydantic object.\n",
      "     |  \n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {'client': typing.Any, 'max_tokens': <class 'int'>, ...\n",
      "     |  \n",
      "     |  __class_vars__ = set()\n",
      "     |  \n",
      "     |  __config__ = <class 'langchain.llms.predictionguard.Config'>\n",
      "     |  \n",
      "     |  __custom_root_type__ = False\n",
      "     |  \n",
      "     |  __exclude_fields__ = {'callback_manager': True, 'callbacks': True, 'me...\n",
      "     |  \n",
      "     |  __fields__ = {'cache': ModelField(name='cache', type=Optional[bool], r...\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "     |  \n",
      "     |  __include_fields__ = None\n",
      "     |  \n",
      "     |  __parameters__ = ()\n",
      "     |  \n",
      "     |  __post_root_validators__ = [(False, <function BaseLLM.raise_deprecatio...\n",
      "     |  \n",
      "     |  __pre_root_validators__ = []\n",
      "     |  \n",
      "     |  __private_attributes__ = {'_lc_kwargs': ModelPrivateAttr(default=Pydan...\n",
      "     |  \n",
      "     |  __schema_cache__ = {}\n",
      "     |  \n",
      "     |  __signature__ = <Signature (*, cache: Optional[bool] = None, ver... No...\n",
      "     |  \n",
      "     |  __validators__ = {'verbose': [<pydantic.class_validators.Validator obj...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  __call__(self, prompt: 'str', stop: 'Optional[List[str]]' = None, callbacks: 'Callbacks' = None, *, tags: 'Optional[List[str]]' = None, metadata: 'Optional[Dict[str, Any]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Check Cache and run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  __str__(self) -> 'str'\n",
      "     |      Get a string representation of the object for printing.\n",
      "     |  \n",
      "     |  async abatch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      "     |      Default implementation of abatch, which calls ainvoke N times.\n",
      "     |      Subclasses should override this method if they can batch more efficiently.\n",
      "     |  \n",
      "     |  async agenerate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  async agenerate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Asynchronously pass a sequence of prompts and return model generations.\n",
      "     |      \n",
      "     |      This method should make use of batched calls for models that expose a batched\n",
      "     |      API.\n",
      "     |      \n",
      "     |      Use this method when you want to:\n",
      "     |          1. take advantage of batched calls,\n",
      "     |          2. need more output from the model than just the top generated value,\n",
      "     |          3. are building chains that are agnostic to the underlying language model\n",
      "     |              type (e.g., pure text completion models vs chat models).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      "     |              converted to match the format of any language model (string for pure\n",
      "     |              text generation models and BaseMessages for chat models).\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          callbacks: Callbacks to pass through. Used for executing additional\n",
      "     |              functionality, such as logging or streaming, throughout generation.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          An LLMResult, which contains a list of candidate Generations for each input\n",
      "     |              prompt and additional model provider-specific output.\n",
      "     |  \n",
      "     |  async ainvoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Default implementation of ainvoke, which calls invoke in a thread pool.\n",
      "     |      Subclasses should override this method if they can run asynchronously.\n",
      "     |  \n",
      "     |  async apredict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Asynchronously pass a string to the model and return a string prediction.\n",
      "     |      \n",
      "     |      Use this method when calling pure text generation models and only the top\n",
      "     |          candidate generation is needed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: String input to pass to the model.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a string.\n",
      "     |  \n",
      "     |  async apredict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      "     |      Asynchronously pass messages to the model and return a message prediction.\n",
      "     |      \n",
      "     |      Use this method when calling chat models and only the top\n",
      "     |          candidate generation is needed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: A sequence of chat messages corresponding to a single model input.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a message.\n",
      "     |  \n",
      "     |  async astream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'AsyncIterator[str]'\n",
      "     |      Default implementation of astream, which calls ainvoke.\n",
      "     |      Subclasses should override this method if they support streaming output.\n",
      "     |  \n",
      "     |  batch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      "     |      Default implementation of batch, which calls invoke N times.\n",
      "     |      Subclasses should override this method if they can batch more efficiently.\n",
      "     |  \n",
      "     |  dict(self, **kwargs: 'Any') -> 'Dict'\n",
      "     |      Return a dictionary of the LLM.\n",
      "     |  \n",
      "     |  generate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  generate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Pass a sequence of prompts to the model and return model generations.\n",
      "     |      \n",
      "     |      This method should make use of batched calls for models that expose a batched\n",
      "     |      API.\n",
      "     |      \n",
      "     |      Use this method when you want to:\n",
      "     |          1. take advantage of batched calls,\n",
      "     |          2. need more output from the model than just the top generated value,\n",
      "     |          3. are building chains that are agnostic to the underlying language model\n",
      "     |              type (e.g., pure text completion models vs chat models).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      "     |              converted to match the format of any language model (string for pure\n",
      "     |              text generation models and BaseMessages for chat models).\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          callbacks: Callbacks to pass through. Used for executing additional\n",
      "     |              functionality, such as logging or streaming, throughout generation.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          An LLMResult, which contains a list of candidate Generations for each input\n",
      "     |              prompt and additional model provider-specific output.\n",
      "     |  \n",
      "     |  invoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |  \n",
      "     |  predict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Pass a single string input to the model and return a string prediction.\n",
      "     |      \n",
      "     |       Use this method when passing in raw text. If you want to pass in specific\n",
      "     |          types of chat messages, use predict_messages.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: String input to pass to the model.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a string.\n",
      "     |  \n",
      "     |  predict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      "     |      Pass a message sequence to the model and return a message prediction.\n",
      "     |      \n",
      "     |      Use this method when passing in chat messages. If you want to pass in raw text,\n",
      "     |          use predict.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: A sequence of chat messages corresponding to a single model input.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a message.\n",
      "     |  \n",
      "     |  save(self, file_path: 'Union[Path, str]') -> 'None'\n",
      "     |      Save the LLM.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          file_path: Path to file to save the LLM to.\n",
      "     |      \n",
      "     |      Example:\n",
      "     |      .. code-block:: python\n",
      "     |      \n",
      "     |          llm.save(file_path=\"path/llm.yaml\")\n",
      "     |  \n",
      "     |  stream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'Iterator[str]'\n",
      "     |      Default implementation of stream, which calls invoke.\n",
      "     |      Subclasses should override this method if they support streaming output.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  raise_deprecation(values: 'Dict') -> 'Dict' from pydantic.main.ModelMetaclass\n",
      "     |      Raise deprecation warning if callback_manager is used.\n",
      "     |  \n",
      "     |  set_verbose(verbose: 'Optional[bool]') -> 'bool' from pydantic.main.ModelMetaclass\n",
      "     |      If verbose is None, set it.\n",
      "     |      \n",
      "     |      This allows users to pass in None as verbose to access the global setting.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  __orig_bases__ = (langchain.schema.language_model.BaseLanguageModel[st...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.schema.language_model.BaseLanguageModel:\n",
      "     |  \n",
      "     |  get_num_tokens(self, text: 'str') -> 'int'\n",
      "     |      Get the number of tokens present in the text.\n",
      "     |      \n",
      "     |      Useful for checking if an input will fit in a model's context window.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: The string input to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The integer number of tokens in the text.\n",
      "     |  \n",
      "     |  get_num_tokens_from_messages(self, messages: 'List[BaseMessage]') -> 'int'\n",
      "     |      Get the number of tokens in the messages.\n",
      "     |      \n",
      "     |      Useful for checking if an input will fit in a model's context window.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: The message inputs to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The sum of the number of tokens across the messages.\n",
      "     |  \n",
      "     |  get_token_ids(self, text: 'str') -> 'List[int]'\n",
      "     |      Return the ordered ids of the tokens in a text.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: The string input to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          A list of ids corresponding to the tokens in the text, in order they occur\n",
      "     |              in the text.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from langchain.schema.language_model.BaseLanguageModel:\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.load.serializable.Serializable:\n",
      "     |  \n",
      "     |  __init__(self, **kwargs: Any) -> None\n",
      "     |      Create a new model by parsing and validating input data from keyword arguments.\n",
      "     |      \n",
      "     |      Raises ValidationError if the input data cannot be parsed to form a valid model.\n",
      "     |  \n",
      "     |  to_json(self) -> Union[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\n",
      "     |  \n",
      "     |  to_json_not_implemented(self) -> langchain.load.serializable.SerializedNotImplemented\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from langchain.load.serializable.Serializable:\n",
      "     |  \n",
      "     |  lc_attributes\n",
      "     |      Return a list of attribute names that should be included in the\n",
      "     |      serialized kwargs. These attributes must be accepted by the\n",
      "     |      constructor.\n",
      "     |  \n",
      "     |  lc_namespace\n",
      "     |      Return the namespace of the langchain object.\n",
      "     |      eg. [\"langchain\", \"llms\", \"openai\"]\n",
      "     |  \n",
      "     |  lc_secrets\n",
      "     |      Return a map of constructor argument names to secret ids.\n",
      "     |      eg. {\"openai_api_key\": \"OPENAI_API_KEY\"}\n",
      "     |  \n",
      "     |  lc_serializable\n",
      "     |      Return whether or not the class is serializable.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __eq__(self, other: Any) -> bool\n",
      "     |  \n",
      "     |  __getstate__(self) -> 'DictAny'\n",
      "     |  \n",
      "     |  __iter__(self) -> 'TupleGenerator'\n",
      "     |      so `dict(model)` works\n",
      "     |  \n",
      "     |  __repr_args__(self) -> 'ReprArgs'\n",
      "     |  \n",
      "     |  __setattr__(self, name, value)\n",
      "     |  \n",
      "     |  __setstate__(self, state: 'DictAny') -> None\n",
      "     |  \n",
      "     |  copy(self: 'Model', *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, update: Optional[ForwardRef('DictStrAny')] = None, deep: bool = False) -> 'Model'\n",
      "     |      Duplicate a model, optionally choose which fields to include, exclude and change.\n",
      "     |      \n",
      "     |      :param include: fields to include in new model\n",
      "     |      :param exclude: fields to exclude from new model, as with values this takes precedence over include\n",
      "     |      :param update: values to change/add in the new model. Note: the data is not validated before creating\n",
      "     |          the new model: you should trust this data\n",
      "     |      :param deep: set to `True` to make a deep copy of the model\n",
      "     |      :return: new model instance\n",
      "     |  \n",
      "     |  json(self, *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) -> 'unicode'\n",
      "     |      Generate a JSON representation of the model, `include` and `exclude` arguments as per `dict()`.\n",
      "     |      \n",
      "     |      `encoder` is an optional function to supply as `default` to json.dumps(), other arguments as per `json.dumps()`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __get_validators__() -> 'CallableGenerator' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  __try_update_forward_refs__(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      "     |      Same as update_forward_refs but will not raise exception\n",
      "     |      when forward references are not defined.\n",
      "     |  \n",
      "     |  construct(_fields_set: Optional[ForwardRef('SetStr')] = None, **values: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |      Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data.\n",
      "     |      Default values are respected, but no other validation is performed.\n",
      "     |      Behaves as if `Config.extra = 'allow'` was set since it adds all passed values\n",
      "     |  \n",
      "     |  from_orm(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_file(path: Union[str, pathlib.Path], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_obj(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_raw(b: Union[str, bytes], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema(by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}') -> 'DictStrAny' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema_json(*, by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}', **dumps_kwargs: Any) -> 'unicode' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  update_forward_refs(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      "     |      Try to update ForwardRefs on fields based on this Model, globalns and localns.\n",
      "     |  \n",
      "     |  validate(value: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __fields_set__\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.utils.Representation:\n",
      "     |  \n",
      "     |  __pretty__(self, fmt: Callable[[Any], Any], **kwargs: Any) -> Generator[Any, NoneType, NoneType]\n",
      "     |      Used by devtools (https://python-devtools.helpmanual.io/) to provide a human readable representations of objects\n",
      "     |  \n",
      "     |  __repr__(self) -> 'unicode'\n",
      "     |  \n",
      "     |  __repr_name__(self) -> 'unicode'\n",
      "     |      Name of the instance's class, used in __repr__.\n",
      "     |  \n",
      "     |  __repr_str__(self, join_str: 'unicode') -> 'unicode'\n",
      "     |  \n",
      "     |  __rich_repr__(self) -> 'RichReprResult'\n",
      "     |      Get fields for Rich library\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.schema.runnable.base.Runnable:\n",
      "     |  \n",
      "     |  __or__(self, other: 'Union[Runnable[Any, Other], Callable[[Any], Other], Mapping[str, Union[Runnable[Any, Other], Callable[[Any], Other], Any]]]') -> 'RunnableSequence[Input, Other]'\n",
      "     |      Return self|value.\n",
      "     |  \n",
      "     |  __ror__(self, other: 'Union[Runnable[Other, Any], Callable[[Any], Other], Mapping[str, Union[Runnable[Other, Any], Callable[[Other], Any], Any]]]') -> 'RunnableSequence[Other, Output]'\n",
      "     |      Return value|self.\n",
      "     |  \n",
      "     |  async atransform(self, input: 'AsyncIterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'AsyncIterator[Output]'\n",
      "     |      Default implementation of atransform, which buffers input and calls astream.\n",
      "     |      Subclasses should override this method if they can start producing output while\n",
      "     |      input is still being generated.\n",
      "     |  \n",
      "     |  bind(self, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      "     |      Bind arguments to a Runnable, returning a new Runnable.\n",
      "     |  \n",
      "     |  map(self) -> 'Runnable[List[Input], List[Output]]'\n",
      "     |      Return a new Runnable that maps a list of inputs to a list of outputs,\n",
      "     |      by calling invoke() with each input.\n",
      "     |  \n",
      "     |  transform(self, input: 'Iterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'Iterator[Output]'\n",
      "     |      Default implementation of transform, which buffers input and then calls stream.\n",
      "     |      Subclasses should override this method if they can start producing output while\n",
      "     |      input is still being generated.\n",
      "     |  \n",
      "     |  with_config(self, config: 'Optional[RunnableConfig]' = None, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      "     |      Bind config to a Runnable, returning a new Runnable.\n",
      "     |  \n",
      "     |  with_fallbacks(self, fallbacks: 'Sequence[Runnable[Input, Output]]', *, exceptions_to_handle: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,)) -> 'RunnableWithFallbacks[Input, Output]'\n",
      "     |  \n",
      "     |  with_retry(self, *, retry_if_exception_type: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,), wait_exponential_jitter: 'bool' = True, stop_after_attempt: 'int' = 3) -> 'Runnable[Input, Output]'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __class_getitem__(params) from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  __init_subclass__(*args, **kwargs) from pydantic.main.ModelMetaclass\n",
      "     |      This method is called when a class is subclassed.\n",
      "     |      \n",
      "     |      The default implementation does nothing. It may be\n",
      "     |      overridden to extend subclasses.\n",
      "    \n",
      "    class PromptLayerOpenAI(langchain.llms.openai.OpenAI)\n",
      "     |  PromptLayerOpenAI(*, cache: Optional[bool] = None, verbose: bool = None, callbacks: Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager, NoneType] = None, callback_manager: Optional[langchain.callbacks.base.BaseCallbackManager] = None, tags: Optional[List[str]] = None, metadata: Optional[Dict[str, Any]] = None, client: Any = None, model: str = 'text-davinci-003', temperature: float = 0.7, max_tokens: int = 256, top_p: float = 1, frequency_penalty: float = 0, presence_penalty: float = 0, n: int = 1, best_of: int = 1, model_kwargs: Dict[str, Any] = None, openai_api_key: Optional[str] = None, openai_api_base: Optional[str] = None, openai_organization: Optional[str] = None, openai_proxy: Optional[str] = None, batch_size: int = 20, request_timeout: Union[float, Tuple[float, float], NoneType] = None, logit_bias: Optional[Dict[str, float]] = None, max_retries: int = 6, streaming: bool = False, allowed_special: Union[Literal['all'], AbstractSet[str]] = set(), disallowed_special: Union[Literal['all'], Collection[str]] = 'all', tiktoken_model_name: Optional[str] = None, pl_tags: Optional[List[str]] = None, return_pl_id: Optional[bool] = False) -> None\n",
      "     |  \n",
      "     |  PromptLayer OpenAI large language models.\n",
      "     |  \n",
      "     |  To use, you should have the ``openai`` and ``promptlayer`` python\n",
      "     |  package installed, and the environment variable ``OPENAI_API_KEY``\n",
      "     |  and ``PROMPTLAYER_API_KEY`` set with your openAI API key and\n",
      "     |  promptlayer key respectively.\n",
      "     |  \n",
      "     |  All parameters that can be passed to the OpenAI LLM can also\n",
      "     |  be passed here. The PromptLayerOpenAI LLM adds two optional\n",
      "     |  \n",
      "     |  parameters:\n",
      "     |      ``pl_tags``: List of strings to tag the request with.\n",
      "     |      ``return_pl_id``: If True, the PromptLayer request ID will be\n",
      "     |          returned in the ``generation_info`` field of the\n",
      "     |          ``Generation`` object.\n",
      "     |  \n",
      "     |  Example:\n",
      "     |      .. code-block:: python\n",
      "     |  \n",
      "     |          from langchain.llms import PromptLayerOpenAI\n",
      "     |          openai = PromptLayerOpenAI(model_name=\"text-davinci-003\")\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      PromptLayerOpenAI\n",
      "     |      langchain.llms.openai.OpenAI\n",
      "     |      langchain.llms.openai.BaseOpenAI\n",
      "     |      langchain.llms.base.BaseLLM\n",
      "     |      langchain.schema.language_model.BaseLanguageModel\n",
      "     |      langchain.load.serializable.Serializable\n",
      "     |      pydantic.main.BaseModel\n",
      "     |      pydantic.utils.Representation\n",
      "     |      langchain.schema.runnable.base.Runnable\n",
      "     |      typing.Generic\n",
      "     |      abc.ABC\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __json_encoder__ = pydantic_encoder(obj: Any) -> Any\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {'pl_tags': typing.Optional[typing.List[str]], 'retu...\n",
      "     |  \n",
      "     |  __class_vars__ = set()\n",
      "     |  \n",
      "     |  __config__ = <class 'langchain.llms.promptlayer_openai.Config'>\n",
      "     |  \n",
      "     |  __custom_root_type__ = False\n",
      "     |  \n",
      "     |  __exclude_fields__ = {'callback_manager': True, 'callbacks': True, 'me...\n",
      "     |  \n",
      "     |  __fields__ = {'allowed_special': ModelField(name='allowed_special', ty...\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "     |  \n",
      "     |  __include_fields__ = None\n",
      "     |  \n",
      "     |  __parameters__ = ()\n",
      "     |  \n",
      "     |  __post_root_validators__ = [(False, <function BaseLLM.raise_deprecatio...\n",
      "     |  \n",
      "     |  __pre_root_validators__ = [<function BaseOpenAI.build_extra>]\n",
      "     |  \n",
      "     |  __private_attributes__ = {'_lc_kwargs': ModelPrivateAttr(default=Pydan...\n",
      "     |  \n",
      "     |  __schema_cache__ = {}\n",
      "     |  \n",
      "     |  __signature__ = <Signature (*, cache: Optional[bool] = None, ver...e, ...\n",
      "     |  \n",
      "     |  __validators__ = {'verbose': [<pydantic.class_validators.Validator obj...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.llms.openai.BaseOpenAI:\n",
      "     |  \n",
      "     |  create_llm_result(self, choices: 'Any', prompts: 'List[str]', token_usage: 'Dict[str, int]') -> 'LLMResult'\n",
      "     |      Create the LLMResult from the choices and prompts.\n",
      "     |  \n",
      "     |  get_sub_prompts(self, params: 'Dict[str, Any]', prompts: 'List[str]', stop: 'Optional[List[str]]' = None) -> 'List[List[str]]'\n",
      "     |      Get the sub prompts for llm call.\n",
      "     |  \n",
      "     |  get_token_ids(self, text: 'str') -> 'List[int]'\n",
      "     |      Get the token IDs using the tiktoken package.\n",
      "     |  \n",
      "     |  max_tokens_for_prompt(self, prompt: 'str') -> 'int'\n",
      "     |      Calculate the maximum number of tokens possible to generate for a prompt.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompt: The prompt to pass into the model.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The maximum number of tokens to generate for a prompt.\n",
      "     |      \n",
      "     |      Example:\n",
      "     |          .. code-block:: python\n",
      "     |      \n",
      "     |              max_tokens = openai.max_token_for_prompt(\"Tell me a joke.\")\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from langchain.llms.openai.BaseOpenAI:\n",
      "     |  \n",
      "     |  build_extra(values: 'Dict[str, Any]') -> 'Dict[str, Any]' from pydantic.main.ModelMetaclass\n",
      "     |      Build extra kwargs from additional params that were passed in.\n",
      "     |  \n",
      "     |  validate_environment(values: 'Dict') -> 'Dict' from pydantic.main.ModelMetaclass\n",
      "     |      Validate that api key and python package exists in environment.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from langchain.llms.openai.BaseOpenAI:\n",
      "     |  \n",
      "     |  __new__(cls, **data: 'Any') -> 'Union[OpenAIChat, BaseOpenAI]'\n",
      "     |      Initialize the OpenAI object.\n",
      "     |  \n",
      "     |  modelname_to_contextsize(modelname: 'str') -> 'int'\n",
      "     |      Calculate the maximum number of tokens possible to generate for a model.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          modelname: The modelname we want to know the context size for.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The maximum context size\n",
      "     |      \n",
      "     |      Example:\n",
      "     |          .. code-block:: python\n",
      "     |      \n",
      "     |              max_tokens = openai.modelname_to_contextsize(\"text-davinci-003\")\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from langchain.llms.openai.BaseOpenAI:\n",
      "     |  \n",
      "     |  lc_secrets\n",
      "     |      Return a map of constructor argument names to secret ids.\n",
      "     |      eg. {\"openai_api_key\": \"OPENAI_API_KEY\"}\n",
      "     |  \n",
      "     |  lc_serializable\n",
      "     |      Return whether or not the class is serializable.\n",
      "     |  \n",
      "     |  max_context_size\n",
      "     |      Get max context size for this model.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from langchain.llms.openai.BaseOpenAI:\n",
      "     |  \n",
      "     |  Config = <class 'langchain.llms.openai.BaseOpenAI.Config'>\n",
      "     |      Configuration for this pydantic object.\n",
      "     |  \n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  __call__(self, prompt: 'str', stop: 'Optional[List[str]]' = None, callbacks: 'Callbacks' = None, *, tags: 'Optional[List[str]]' = None, metadata: 'Optional[Dict[str, Any]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Check Cache and run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  __str__(self) -> 'str'\n",
      "     |      Get a string representation of the object for printing.\n",
      "     |  \n",
      "     |  async abatch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      "     |      Default implementation of abatch, which calls ainvoke N times.\n",
      "     |      Subclasses should override this method if they can batch more efficiently.\n",
      "     |  \n",
      "     |  async agenerate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  async agenerate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Asynchronously pass a sequence of prompts and return model generations.\n",
      "     |      \n",
      "     |      This method should make use of batched calls for models that expose a batched\n",
      "     |      API.\n",
      "     |      \n",
      "     |      Use this method when you want to:\n",
      "     |          1. take advantage of batched calls,\n",
      "     |          2. need more output from the model than just the top generated value,\n",
      "     |          3. are building chains that are agnostic to the underlying language model\n",
      "     |              type (e.g., pure text completion models vs chat models).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      "     |              converted to match the format of any language model (string for pure\n",
      "     |              text generation models and BaseMessages for chat models).\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          callbacks: Callbacks to pass through. Used for executing additional\n",
      "     |              functionality, such as logging or streaming, throughout generation.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          An LLMResult, which contains a list of candidate Generations for each input\n",
      "     |              prompt and additional model provider-specific output.\n",
      "     |  \n",
      "     |  async ainvoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Default implementation of ainvoke, which calls invoke in a thread pool.\n",
      "     |      Subclasses should override this method if they can run asynchronously.\n",
      "     |  \n",
      "     |  async apredict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Asynchronously pass a string to the model and return a string prediction.\n",
      "     |      \n",
      "     |      Use this method when calling pure text generation models and only the top\n",
      "     |          candidate generation is needed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: String input to pass to the model.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a string.\n",
      "     |  \n",
      "     |  async apredict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      "     |      Asynchronously pass messages to the model and return a message prediction.\n",
      "     |      \n",
      "     |      Use this method when calling chat models and only the top\n",
      "     |          candidate generation is needed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: A sequence of chat messages corresponding to a single model input.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a message.\n",
      "     |  \n",
      "     |  async astream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'AsyncIterator[str]'\n",
      "     |      Default implementation of astream, which calls ainvoke.\n",
      "     |      Subclasses should override this method if they support streaming output.\n",
      "     |  \n",
      "     |  batch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      "     |      Default implementation of batch, which calls invoke N times.\n",
      "     |      Subclasses should override this method if they can batch more efficiently.\n",
      "     |  \n",
      "     |  dict(self, **kwargs: 'Any') -> 'Dict'\n",
      "     |      Return a dictionary of the LLM.\n",
      "     |  \n",
      "     |  generate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  generate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Pass a sequence of prompts to the model and return model generations.\n",
      "     |      \n",
      "     |      This method should make use of batched calls for models that expose a batched\n",
      "     |      API.\n",
      "     |      \n",
      "     |      Use this method when you want to:\n",
      "     |          1. take advantage of batched calls,\n",
      "     |          2. need more output from the model than just the top generated value,\n",
      "     |          3. are building chains that are agnostic to the underlying language model\n",
      "     |              type (e.g., pure text completion models vs chat models).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      "     |              converted to match the format of any language model (string for pure\n",
      "     |              text generation models and BaseMessages for chat models).\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          callbacks: Callbacks to pass through. Used for executing additional\n",
      "     |              functionality, such as logging or streaming, throughout generation.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          An LLMResult, which contains a list of candidate Generations for each input\n",
      "     |              prompt and additional model provider-specific output.\n",
      "     |  \n",
      "     |  invoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |  \n",
      "     |  predict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Pass a single string input to the model and return a string prediction.\n",
      "     |      \n",
      "     |       Use this method when passing in raw text. If you want to pass in specific\n",
      "     |          types of chat messages, use predict_messages.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: String input to pass to the model.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a string.\n",
      "     |  \n",
      "     |  predict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      "     |      Pass a message sequence to the model and return a message prediction.\n",
      "     |      \n",
      "     |      Use this method when passing in chat messages. If you want to pass in raw text,\n",
      "     |          use predict.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: A sequence of chat messages corresponding to a single model input.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a message.\n",
      "     |  \n",
      "     |  save(self, file_path: 'Union[Path, str]') -> 'None'\n",
      "     |      Save the LLM.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          file_path: Path to file to save the LLM to.\n",
      "     |      \n",
      "     |      Example:\n",
      "     |      .. code-block:: python\n",
      "     |      \n",
      "     |          llm.save(file_path=\"path/llm.yaml\")\n",
      "     |  \n",
      "     |  stream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'Iterator[str]'\n",
      "     |      Default implementation of stream, which calls invoke.\n",
      "     |      Subclasses should override this method if they support streaming output.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  raise_deprecation(values: 'Dict') -> 'Dict' from pydantic.main.ModelMetaclass\n",
      "     |      Raise deprecation warning if callback_manager is used.\n",
      "     |  \n",
      "     |  set_verbose(verbose: 'Optional[bool]') -> 'bool' from pydantic.main.ModelMetaclass\n",
      "     |      If verbose is None, set it.\n",
      "     |      \n",
      "     |      This allows users to pass in None as verbose to access the global setting.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  __orig_bases__ = (langchain.schema.language_model.BaseLanguageModel[st...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.schema.language_model.BaseLanguageModel:\n",
      "     |  \n",
      "     |  get_num_tokens(self, text: 'str') -> 'int'\n",
      "     |      Get the number of tokens present in the text.\n",
      "     |      \n",
      "     |      Useful for checking if an input will fit in a model's context window.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: The string input to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The integer number of tokens in the text.\n",
      "     |  \n",
      "     |  get_num_tokens_from_messages(self, messages: 'List[BaseMessage]') -> 'int'\n",
      "     |      Get the number of tokens in the messages.\n",
      "     |      \n",
      "     |      Useful for checking if an input will fit in a model's context window.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: The message inputs to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The sum of the number of tokens across the messages.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from langchain.schema.language_model.BaseLanguageModel:\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.load.serializable.Serializable:\n",
      "     |  \n",
      "     |  __init__(self, **kwargs: Any) -> None\n",
      "     |      Create a new model by parsing and validating input data from keyword arguments.\n",
      "     |      \n",
      "     |      Raises ValidationError if the input data cannot be parsed to form a valid model.\n",
      "     |  \n",
      "     |  to_json(self) -> Union[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\n",
      "     |  \n",
      "     |  to_json_not_implemented(self) -> langchain.load.serializable.SerializedNotImplemented\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from langchain.load.serializable.Serializable:\n",
      "     |  \n",
      "     |  lc_attributes\n",
      "     |      Return a list of attribute names that should be included in the\n",
      "     |      serialized kwargs. These attributes must be accepted by the\n",
      "     |      constructor.\n",
      "     |  \n",
      "     |  lc_namespace\n",
      "     |      Return the namespace of the langchain object.\n",
      "     |      eg. [\"langchain\", \"llms\", \"openai\"]\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __eq__(self, other: Any) -> bool\n",
      "     |  \n",
      "     |  __getstate__(self) -> 'DictAny'\n",
      "     |  \n",
      "     |  __iter__(self) -> 'TupleGenerator'\n",
      "     |      so `dict(model)` works\n",
      "     |  \n",
      "     |  __repr_args__(self) -> 'ReprArgs'\n",
      "     |  \n",
      "     |  __setattr__(self, name, value)\n",
      "     |  \n",
      "     |  __setstate__(self, state: 'DictAny') -> None\n",
      "     |  \n",
      "     |  copy(self: 'Model', *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, update: Optional[ForwardRef('DictStrAny')] = None, deep: bool = False) -> 'Model'\n",
      "     |      Duplicate a model, optionally choose which fields to include, exclude and change.\n",
      "     |      \n",
      "     |      :param include: fields to include in new model\n",
      "     |      :param exclude: fields to exclude from new model, as with values this takes precedence over include\n",
      "     |      :param update: values to change/add in the new model. Note: the data is not validated before creating\n",
      "     |          the new model: you should trust this data\n",
      "     |      :param deep: set to `True` to make a deep copy of the model\n",
      "     |      :return: new model instance\n",
      "     |  \n",
      "     |  json(self, *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) -> 'unicode'\n",
      "     |      Generate a JSON representation of the model, `include` and `exclude` arguments as per `dict()`.\n",
      "     |      \n",
      "     |      `encoder` is an optional function to supply as `default` to json.dumps(), other arguments as per `json.dumps()`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __get_validators__() -> 'CallableGenerator' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  __try_update_forward_refs__(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      "     |      Same as update_forward_refs but will not raise exception\n",
      "     |      when forward references are not defined.\n",
      "     |  \n",
      "     |  construct(_fields_set: Optional[ForwardRef('SetStr')] = None, **values: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |      Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data.\n",
      "     |      Default values are respected, but no other validation is performed.\n",
      "     |      Behaves as if `Config.extra = 'allow'` was set since it adds all passed values\n",
      "     |  \n",
      "     |  from_orm(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_file(path: Union[str, pathlib.Path], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_obj(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_raw(b: Union[str, bytes], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema(by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}') -> 'DictStrAny' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema_json(*, by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}', **dumps_kwargs: Any) -> 'unicode' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  update_forward_refs(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      "     |      Try to update ForwardRefs on fields based on this Model, globalns and localns.\n",
      "     |  \n",
      "     |  validate(value: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __fields_set__\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.utils.Representation:\n",
      "     |  \n",
      "     |  __pretty__(self, fmt: Callable[[Any], Any], **kwargs: Any) -> Generator[Any, NoneType, NoneType]\n",
      "     |      Used by devtools (https://python-devtools.helpmanual.io/) to provide a human readable representations of objects\n",
      "     |  \n",
      "     |  __repr__(self) -> 'unicode'\n",
      "     |  \n",
      "     |  __repr_name__(self) -> 'unicode'\n",
      "     |      Name of the instance's class, used in __repr__.\n",
      "     |  \n",
      "     |  __repr_str__(self, join_str: 'unicode') -> 'unicode'\n",
      "     |  \n",
      "     |  __rich_repr__(self) -> 'RichReprResult'\n",
      "     |      Get fields for Rich library\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.schema.runnable.base.Runnable:\n",
      "     |  \n",
      "     |  __or__(self, other: 'Union[Runnable[Any, Other], Callable[[Any], Other], Mapping[str, Union[Runnable[Any, Other], Callable[[Any], Other], Any]]]') -> 'RunnableSequence[Input, Other]'\n",
      "     |      Return self|value.\n",
      "     |  \n",
      "     |  __ror__(self, other: 'Union[Runnable[Other, Any], Callable[[Any], Other], Mapping[str, Union[Runnable[Other, Any], Callable[[Other], Any], Any]]]') -> 'RunnableSequence[Other, Output]'\n",
      "     |      Return value|self.\n",
      "     |  \n",
      "     |  async atransform(self, input: 'AsyncIterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'AsyncIterator[Output]'\n",
      "     |      Default implementation of atransform, which buffers input and calls astream.\n",
      "     |      Subclasses should override this method if they can start producing output while\n",
      "     |      input is still being generated.\n",
      "     |  \n",
      "     |  bind(self, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      "     |      Bind arguments to a Runnable, returning a new Runnable.\n",
      "     |  \n",
      "     |  map(self) -> 'Runnable[List[Input], List[Output]]'\n",
      "     |      Return a new Runnable that maps a list of inputs to a list of outputs,\n",
      "     |      by calling invoke() with each input.\n",
      "     |  \n",
      "     |  transform(self, input: 'Iterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'Iterator[Output]'\n",
      "     |      Default implementation of transform, which buffers input and then calls stream.\n",
      "     |      Subclasses should override this method if they can start producing output while\n",
      "     |      input is still being generated.\n",
      "     |  \n",
      "     |  with_config(self, config: 'Optional[RunnableConfig]' = None, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      "     |      Bind config to a Runnable, returning a new Runnable.\n",
      "     |  \n",
      "     |  with_fallbacks(self, fallbacks: 'Sequence[Runnable[Input, Output]]', *, exceptions_to_handle: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,)) -> 'RunnableWithFallbacks[Input, Output]'\n",
      "     |  \n",
      "     |  with_retry(self, *, retry_if_exception_type: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,), wait_exponential_jitter: 'bool' = True, stop_after_attempt: 'int' = 3) -> 'Runnable[Input, Output]'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __class_getitem__(params) from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  __init_subclass__(*args, **kwargs) from pydantic.main.ModelMetaclass\n",
      "     |      This method is called when a class is subclassed.\n",
      "     |      \n",
      "     |      The default implementation does nothing. It may be\n",
      "     |      overridden to extend subclasses.\n",
      "    \n",
      "    class PromptLayerOpenAIChat(langchain.llms.openai.OpenAIChat)\n",
      "     |  PromptLayerOpenAIChat(*, cache: Optional[bool] = None, verbose: bool = None, callbacks: Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager, NoneType] = None, callback_manager: Optional[langchain.callbacks.base.BaseCallbackManager] = None, tags: Optional[List[str]] = None, metadata: Optional[Dict[str, Any]] = None, client: Any = None, model_name: str = 'gpt-3.5-turbo', model_kwargs: Dict[str, Any] = None, openai_api_key: Optional[str] = None, openai_api_base: Optional[str] = None, openai_proxy: Optional[str] = None, max_retries: int = 6, prefix_messages: List = None, streaming: bool = False, allowed_special: Union[Literal['all'], AbstractSet[str]] = set(), disallowed_special: Union[Literal['all'], Collection[str]] = 'all', pl_tags: Optional[List[str]] = None, return_pl_id: Optional[bool] = False) -> None\n",
      "     |  \n",
      "     |  Wrapper around OpenAI large language models.\n",
      "     |  \n",
      "     |  To use, you should have the ``openai`` and ``promptlayer`` python\n",
      "     |  package installed, and the environment variable ``OPENAI_API_KEY``\n",
      "     |  and ``PROMPTLAYER_API_KEY`` set with your openAI API key and\n",
      "     |  promptlayer key respectively.\n",
      "     |  \n",
      "     |  All parameters that can be passed to the OpenAIChat LLM can also\n",
      "     |  be passed here. The PromptLayerOpenAIChat adds two optional\n",
      "     |  \n",
      "     |  parameters:\n",
      "     |      ``pl_tags``: List of strings to tag the request with.\n",
      "     |      ``return_pl_id``: If True, the PromptLayer request ID will be\n",
      "     |          returned in the ``generation_info`` field of the\n",
      "     |          ``Generation`` object.\n",
      "     |  \n",
      "     |  Example:\n",
      "     |      .. code-block:: python\n",
      "     |  \n",
      "     |          from langchain.llms import PromptLayerOpenAIChat\n",
      "     |          openaichat = PromptLayerOpenAIChat(model_name=\"gpt-3.5-turbo\")\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      PromptLayerOpenAIChat\n",
      "     |      langchain.llms.openai.OpenAIChat\n",
      "     |      langchain.llms.base.BaseLLM\n",
      "     |      langchain.schema.language_model.BaseLanguageModel\n",
      "     |      langchain.load.serializable.Serializable\n",
      "     |      pydantic.main.BaseModel\n",
      "     |      pydantic.utils.Representation\n",
      "     |      langchain.schema.runnable.base.Runnable\n",
      "     |      typing.Generic\n",
      "     |      abc.ABC\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __json_encoder__ = pydantic_encoder(obj: Any) -> Any\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {'pl_tags': typing.Optional[typing.List[str]], 'retu...\n",
      "     |  \n",
      "     |  __class_vars__ = set()\n",
      "     |  \n",
      "     |  __config__ = <class 'langchain.llms.promptlayer_openai.Config'>\n",
      "     |  \n",
      "     |  __custom_root_type__ = False\n",
      "     |  \n",
      "     |  __exclude_fields__ = {'callback_manager': True, 'callbacks': True, 'me...\n",
      "     |  \n",
      "     |  __fields__ = {'allowed_special': ModelField(name='allowed_special', ty...\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "     |  \n",
      "     |  __include_fields__ = None\n",
      "     |  \n",
      "     |  __parameters__ = ()\n",
      "     |  \n",
      "     |  __post_root_validators__ = [(False, <function BaseLLM.raise_deprecatio...\n",
      "     |  \n",
      "     |  __pre_root_validators__ = [<function OpenAIChat.build_extra>]\n",
      "     |  \n",
      "     |  __private_attributes__ = {'_lc_kwargs': ModelPrivateAttr(default=Pydan...\n",
      "     |  \n",
      "     |  __schema_cache__ = {}\n",
      "     |  \n",
      "     |  __signature__ = <Signature (*, cache: Optional[bool] = None, ver...e, ...\n",
      "     |  \n",
      "     |  __validators__ = {'verbose': [<pydantic.class_validators.Validator obj...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.llms.openai.OpenAIChat:\n",
      "     |  \n",
      "     |  get_token_ids(self, text: 'str') -> 'List[int]'\n",
      "     |      Get the token IDs using the tiktoken package.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from langchain.llms.openai.OpenAIChat:\n",
      "     |  \n",
      "     |  build_extra(values: 'Dict[str, Any]') -> 'Dict[str, Any]' from pydantic.main.ModelMetaclass\n",
      "     |      Build extra kwargs from additional params that were passed in.\n",
      "     |  \n",
      "     |  validate_environment(values: 'Dict') -> 'Dict' from pydantic.main.ModelMetaclass\n",
      "     |      Validate that api key and python package exists in environment.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  __call__(self, prompt: 'str', stop: 'Optional[List[str]]' = None, callbacks: 'Callbacks' = None, *, tags: 'Optional[List[str]]' = None, metadata: 'Optional[Dict[str, Any]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Check Cache and run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  __str__(self) -> 'str'\n",
      "     |      Get a string representation of the object for printing.\n",
      "     |  \n",
      "     |  async abatch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      "     |      Default implementation of abatch, which calls ainvoke N times.\n",
      "     |      Subclasses should override this method if they can batch more efficiently.\n",
      "     |  \n",
      "     |  async agenerate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  async agenerate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Asynchronously pass a sequence of prompts and return model generations.\n",
      "     |      \n",
      "     |      This method should make use of batched calls for models that expose a batched\n",
      "     |      API.\n",
      "     |      \n",
      "     |      Use this method when you want to:\n",
      "     |          1. take advantage of batched calls,\n",
      "     |          2. need more output from the model than just the top generated value,\n",
      "     |          3. are building chains that are agnostic to the underlying language model\n",
      "     |              type (e.g., pure text completion models vs chat models).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      "     |              converted to match the format of any language model (string for pure\n",
      "     |              text generation models and BaseMessages for chat models).\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          callbacks: Callbacks to pass through. Used for executing additional\n",
      "     |              functionality, such as logging or streaming, throughout generation.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          An LLMResult, which contains a list of candidate Generations for each input\n",
      "     |              prompt and additional model provider-specific output.\n",
      "     |  \n",
      "     |  async ainvoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Default implementation of ainvoke, which calls invoke in a thread pool.\n",
      "     |      Subclasses should override this method if they can run asynchronously.\n",
      "     |  \n",
      "     |  async apredict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Asynchronously pass a string to the model and return a string prediction.\n",
      "     |      \n",
      "     |      Use this method when calling pure text generation models and only the top\n",
      "     |          candidate generation is needed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: String input to pass to the model.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a string.\n",
      "     |  \n",
      "     |  async apredict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      "     |      Asynchronously pass messages to the model and return a message prediction.\n",
      "     |      \n",
      "     |      Use this method when calling chat models and only the top\n",
      "     |          candidate generation is needed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: A sequence of chat messages corresponding to a single model input.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a message.\n",
      "     |  \n",
      "     |  async astream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'AsyncIterator[str]'\n",
      "     |      Default implementation of astream, which calls ainvoke.\n",
      "     |      Subclasses should override this method if they support streaming output.\n",
      "     |  \n",
      "     |  batch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      "     |      Default implementation of batch, which calls invoke N times.\n",
      "     |      Subclasses should override this method if they can batch more efficiently.\n",
      "     |  \n",
      "     |  dict(self, **kwargs: 'Any') -> 'Dict'\n",
      "     |      Return a dictionary of the LLM.\n",
      "     |  \n",
      "     |  generate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  generate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Pass a sequence of prompts to the model and return model generations.\n",
      "     |      \n",
      "     |      This method should make use of batched calls for models that expose a batched\n",
      "     |      API.\n",
      "     |      \n",
      "     |      Use this method when you want to:\n",
      "     |          1. take advantage of batched calls,\n",
      "     |          2. need more output from the model than just the top generated value,\n",
      "     |          3. are building chains that are agnostic to the underlying language model\n",
      "     |              type (e.g., pure text completion models vs chat models).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      "     |              converted to match the format of any language model (string for pure\n",
      "     |              text generation models and BaseMessages for chat models).\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          callbacks: Callbacks to pass through. Used for executing additional\n",
      "     |              functionality, such as logging or streaming, throughout generation.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          An LLMResult, which contains a list of candidate Generations for each input\n",
      "     |              prompt and additional model provider-specific output.\n",
      "     |  \n",
      "     |  invoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |  \n",
      "     |  predict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Pass a single string input to the model and return a string prediction.\n",
      "     |      \n",
      "     |       Use this method when passing in raw text. If you want to pass in specific\n",
      "     |          types of chat messages, use predict_messages.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: String input to pass to the model.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a string.\n",
      "     |  \n",
      "     |  predict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      "     |      Pass a message sequence to the model and return a message prediction.\n",
      "     |      \n",
      "     |      Use this method when passing in chat messages. If you want to pass in raw text,\n",
      "     |          use predict.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: A sequence of chat messages corresponding to a single model input.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a message.\n",
      "     |  \n",
      "     |  save(self, file_path: 'Union[Path, str]') -> 'None'\n",
      "     |      Save the LLM.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          file_path: Path to file to save the LLM to.\n",
      "     |      \n",
      "     |      Example:\n",
      "     |      .. code-block:: python\n",
      "     |      \n",
      "     |          llm.save(file_path=\"path/llm.yaml\")\n",
      "     |  \n",
      "     |  stream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'Iterator[str]'\n",
      "     |      Default implementation of stream, which calls invoke.\n",
      "     |      Subclasses should override this method if they support streaming output.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  raise_deprecation(values: 'Dict') -> 'Dict' from pydantic.main.ModelMetaclass\n",
      "     |      Raise deprecation warning if callback_manager is used.\n",
      "     |  \n",
      "     |  set_verbose(verbose: 'Optional[bool]') -> 'bool' from pydantic.main.ModelMetaclass\n",
      "     |      If verbose is None, set it.\n",
      "     |      \n",
      "     |      This allows users to pass in None as verbose to access the global setting.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  Config = <class 'langchain.llms.base.BaseLLM.Config'>\n",
      "     |      Configuration for this pydantic object.\n",
      "     |  \n",
      "     |  \n",
      "     |  __orig_bases__ = (langchain.schema.language_model.BaseLanguageModel[st...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.schema.language_model.BaseLanguageModel:\n",
      "     |  \n",
      "     |  get_num_tokens(self, text: 'str') -> 'int'\n",
      "     |      Get the number of tokens present in the text.\n",
      "     |      \n",
      "     |      Useful for checking if an input will fit in a model's context window.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: The string input to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The integer number of tokens in the text.\n",
      "     |  \n",
      "     |  get_num_tokens_from_messages(self, messages: 'List[BaseMessage]') -> 'int'\n",
      "     |      Get the number of tokens in the messages.\n",
      "     |      \n",
      "     |      Useful for checking if an input will fit in a model's context window.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: The message inputs to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The sum of the number of tokens across the messages.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from langchain.schema.language_model.BaseLanguageModel:\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.load.serializable.Serializable:\n",
      "     |  \n",
      "     |  __init__(self, **kwargs: Any) -> None\n",
      "     |      Create a new model by parsing and validating input data from keyword arguments.\n",
      "     |      \n",
      "     |      Raises ValidationError if the input data cannot be parsed to form a valid model.\n",
      "     |  \n",
      "     |  to_json(self) -> Union[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\n",
      "     |  \n",
      "     |  to_json_not_implemented(self) -> langchain.load.serializable.SerializedNotImplemented\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from langchain.load.serializable.Serializable:\n",
      "     |  \n",
      "     |  lc_attributes\n",
      "     |      Return a list of attribute names that should be included in the\n",
      "     |      serialized kwargs. These attributes must be accepted by the\n",
      "     |      constructor.\n",
      "     |  \n",
      "     |  lc_namespace\n",
      "     |      Return the namespace of the langchain object.\n",
      "     |      eg. [\"langchain\", \"llms\", \"openai\"]\n",
      "     |  \n",
      "     |  lc_secrets\n",
      "     |      Return a map of constructor argument names to secret ids.\n",
      "     |      eg. {\"openai_api_key\": \"OPENAI_API_KEY\"}\n",
      "     |  \n",
      "     |  lc_serializable\n",
      "     |      Return whether or not the class is serializable.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __eq__(self, other: Any) -> bool\n",
      "     |  \n",
      "     |  __getstate__(self) -> 'DictAny'\n",
      "     |  \n",
      "     |  __iter__(self) -> 'TupleGenerator'\n",
      "     |      so `dict(model)` works\n",
      "     |  \n",
      "     |  __repr_args__(self) -> 'ReprArgs'\n",
      "     |  \n",
      "     |  __setattr__(self, name, value)\n",
      "     |  \n",
      "     |  __setstate__(self, state: 'DictAny') -> None\n",
      "     |  \n",
      "     |  copy(self: 'Model', *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, update: Optional[ForwardRef('DictStrAny')] = None, deep: bool = False) -> 'Model'\n",
      "     |      Duplicate a model, optionally choose which fields to include, exclude and change.\n",
      "     |      \n",
      "     |      :param include: fields to include in new model\n",
      "     |      :param exclude: fields to exclude from new model, as with values this takes precedence over include\n",
      "     |      :param update: values to change/add in the new model. Note: the data is not validated before creating\n",
      "     |          the new model: you should trust this data\n",
      "     |      :param deep: set to `True` to make a deep copy of the model\n",
      "     |      :return: new model instance\n",
      "     |  \n",
      "     |  json(self, *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) -> 'unicode'\n",
      "     |      Generate a JSON representation of the model, `include` and `exclude` arguments as per `dict()`.\n",
      "     |      \n",
      "     |      `encoder` is an optional function to supply as `default` to json.dumps(), other arguments as per `json.dumps()`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __get_validators__() -> 'CallableGenerator' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  __try_update_forward_refs__(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      "     |      Same as update_forward_refs but will not raise exception\n",
      "     |      when forward references are not defined.\n",
      "     |  \n",
      "     |  construct(_fields_set: Optional[ForwardRef('SetStr')] = None, **values: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |      Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data.\n",
      "     |      Default values are respected, but no other validation is performed.\n",
      "     |      Behaves as if `Config.extra = 'allow'` was set since it adds all passed values\n",
      "     |  \n",
      "     |  from_orm(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_file(path: Union[str, pathlib.Path], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_obj(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_raw(b: Union[str, bytes], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema(by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}') -> 'DictStrAny' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema_json(*, by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}', **dumps_kwargs: Any) -> 'unicode' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  update_forward_refs(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      "     |      Try to update ForwardRefs on fields based on this Model, globalns and localns.\n",
      "     |  \n",
      "     |  validate(value: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __fields_set__\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.utils.Representation:\n",
      "     |  \n",
      "     |  __pretty__(self, fmt: Callable[[Any], Any], **kwargs: Any) -> Generator[Any, NoneType, NoneType]\n",
      "     |      Used by devtools (https://python-devtools.helpmanual.io/) to provide a human readable representations of objects\n",
      "     |  \n",
      "     |  __repr__(self) -> 'unicode'\n",
      "     |  \n",
      "     |  __repr_name__(self) -> 'unicode'\n",
      "     |      Name of the instance's class, used in __repr__.\n",
      "     |  \n",
      "     |  __repr_str__(self, join_str: 'unicode') -> 'unicode'\n",
      "     |  \n",
      "     |  __rich_repr__(self) -> 'RichReprResult'\n",
      "     |      Get fields for Rich library\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.schema.runnable.base.Runnable:\n",
      "     |  \n",
      "     |  __or__(self, other: 'Union[Runnable[Any, Other], Callable[[Any], Other], Mapping[str, Union[Runnable[Any, Other], Callable[[Any], Other], Any]]]') -> 'RunnableSequence[Input, Other]'\n",
      "     |      Return self|value.\n",
      "     |  \n",
      "     |  __ror__(self, other: 'Union[Runnable[Other, Any], Callable[[Any], Other], Mapping[str, Union[Runnable[Other, Any], Callable[[Other], Any], Any]]]') -> 'RunnableSequence[Other, Output]'\n",
      "     |      Return value|self.\n",
      "     |  \n",
      "     |  async atransform(self, input: 'AsyncIterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'AsyncIterator[Output]'\n",
      "     |      Default implementation of atransform, which buffers input and calls astream.\n",
      "     |      Subclasses should override this method if they can start producing output while\n",
      "     |      input is still being generated.\n",
      "     |  \n",
      "     |  bind(self, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      "     |      Bind arguments to a Runnable, returning a new Runnable.\n",
      "     |  \n",
      "     |  map(self) -> 'Runnable[List[Input], List[Output]]'\n",
      "     |      Return a new Runnable that maps a list of inputs to a list of outputs,\n",
      "     |      by calling invoke() with each input.\n",
      "     |  \n",
      "     |  transform(self, input: 'Iterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'Iterator[Output]'\n",
      "     |      Default implementation of transform, which buffers input and then calls stream.\n",
      "     |      Subclasses should override this method if they can start producing output while\n",
      "     |      input is still being generated.\n",
      "     |  \n",
      "     |  with_config(self, config: 'Optional[RunnableConfig]' = None, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      "     |      Bind config to a Runnable, returning a new Runnable.\n",
      "     |  \n",
      "     |  with_fallbacks(self, fallbacks: 'Sequence[Runnable[Input, Output]]', *, exceptions_to_handle: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,)) -> 'RunnableWithFallbacks[Input, Output]'\n",
      "     |  \n",
      "     |  with_retry(self, *, retry_if_exception_type: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,), wait_exponential_jitter: 'bool' = True, stop_after_attempt: 'int' = 3) -> 'Runnable[Input, Output]'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __class_getitem__(params) from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  __init_subclass__(*args, **kwargs) from pydantic.main.ModelMetaclass\n",
      "     |      This method is called when a class is subclassed.\n",
      "     |      \n",
      "     |      The default implementation does nothing. It may be\n",
      "     |      overridden to extend subclasses.\n",
      "    \n",
      "    class RWKV(langchain.llms.base.LLM, pydantic.main.BaseModel)\n",
      "     |  RWKV(*, cache: Optional[bool] = None, verbose: bool = None, callbacks: Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager, NoneType] = None, callback_manager: Optional[langchain.callbacks.base.BaseCallbackManager] = None, tags: Optional[List[str]] = None, metadata: Optional[Dict[str, Any]] = None, model: str, tokens_path: str, strategy: str = 'cpu fp32', rwkv_verbose: bool = True, temperature: float = 1.0, top_p: float = 0.5, penalty_alpha_frequency: float = 0.4, penalty_alpha_presence: float = 0.4, CHUNK_LEN: int = 256, max_tokens_per_generation: int = 256, client: Any = None, tokenizer: Any = None, pipeline: Any = None, model_tokens: Any = None, model_state: Any = None) -> None\n",
      "     |  \n",
      "     |  RWKV language models.\n",
      "     |  \n",
      "     |  To use, you should have the ``rwkv`` python package installed, the\n",
      "     |  pre-trained model file, and the model's config information.\n",
      "     |  \n",
      "     |  Example:\n",
      "     |      .. code-block:: python\n",
      "     |  \n",
      "     |          from langchain.llms import RWKV\n",
      "     |          model = RWKV(model=\"./models/rwkv-3b-fp16.bin\", strategy=\"cpu fp32\")\n",
      "     |  \n",
      "     |          # Simplest invocation\n",
      "     |          response = model(\"Once upon a time, \")\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      RWKV\n",
      "     |      langchain.llms.base.LLM\n",
      "     |      langchain.llms.base.BaseLLM\n",
      "     |      langchain.schema.language_model.BaseLanguageModel\n",
      "     |      langchain.load.serializable.Serializable\n",
      "     |      pydantic.main.BaseModel\n",
      "     |      pydantic.utils.Representation\n",
      "     |      langchain.schema.runnable.base.Runnable\n",
      "     |      typing.Generic\n",
      "     |      abc.ABC\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  run_rnn(self, _tokens: List[str], newline_adj: int = 0) -> Any\n",
      "     |  \n",
      "     |  rwkv_generate(self, prompt: str) -> str\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  validate_environment(values: Dict) -> Dict from pydantic.main.ModelMetaclass\n",
      "     |      Validate that the python package exists in the environment.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __json_encoder__ = pydantic_encoder(obj: Any) -> Any\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  Config = <class 'langchain.llms.rwkv.RWKV.Config'>\n",
      "     |      Configuration for this pydantic object.\n",
      "     |  \n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {'CHUNK_LEN': <class 'int'>, 'client': typing.Any, '...\n",
      "     |  \n",
      "     |  __class_vars__ = set()\n",
      "     |  \n",
      "     |  __config__ = <class 'langchain.llms.rwkv.Config'>\n",
      "     |  \n",
      "     |  __custom_root_type__ = False\n",
      "     |  \n",
      "     |  __exclude_fields__ = {'callback_manager': True, 'callbacks': True, 'me...\n",
      "     |  \n",
      "     |  __fields__ = {'CHUNK_LEN': ModelField(name='CHUNK_LEN', type=int, requ...\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "     |  \n",
      "     |  __include_fields__ = None\n",
      "     |  \n",
      "     |  __parameters__ = ()\n",
      "     |  \n",
      "     |  __post_root_validators__ = [(False, <function BaseLLM.raise_deprecatio...\n",
      "     |  \n",
      "     |  __pre_root_validators__ = []\n",
      "     |  \n",
      "     |  __private_attributes__ = {'_lc_kwargs': ModelPrivateAttr(default=Pydan...\n",
      "     |  \n",
      "     |  __schema_cache__ = {}\n",
      "     |  \n",
      "     |  __signature__ = <Signature (*, cache: Optional[bool] = None, ver...ns:...\n",
      "     |  \n",
      "     |  __validators__ = {'verbose': [<pydantic.class_validators.Validator obj...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  __call__(self, prompt: 'str', stop: 'Optional[List[str]]' = None, callbacks: 'Callbacks' = None, *, tags: 'Optional[List[str]]' = None, metadata: 'Optional[Dict[str, Any]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Check Cache and run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  __str__(self) -> 'str'\n",
      "     |      Get a string representation of the object for printing.\n",
      "     |  \n",
      "     |  async abatch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      "     |      Default implementation of abatch, which calls ainvoke N times.\n",
      "     |      Subclasses should override this method if they can batch more efficiently.\n",
      "     |  \n",
      "     |  async agenerate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  async agenerate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Asynchronously pass a sequence of prompts and return model generations.\n",
      "     |      \n",
      "     |      This method should make use of batched calls for models that expose a batched\n",
      "     |      API.\n",
      "     |      \n",
      "     |      Use this method when you want to:\n",
      "     |          1. take advantage of batched calls,\n",
      "     |          2. need more output from the model than just the top generated value,\n",
      "     |          3. are building chains that are agnostic to the underlying language model\n",
      "     |              type (e.g., pure text completion models vs chat models).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      "     |              converted to match the format of any language model (string for pure\n",
      "     |              text generation models and BaseMessages for chat models).\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          callbacks: Callbacks to pass through. Used for executing additional\n",
      "     |              functionality, such as logging or streaming, throughout generation.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          An LLMResult, which contains a list of candidate Generations for each input\n",
      "     |              prompt and additional model provider-specific output.\n",
      "     |  \n",
      "     |  async ainvoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Default implementation of ainvoke, which calls invoke in a thread pool.\n",
      "     |      Subclasses should override this method if they can run asynchronously.\n",
      "     |  \n",
      "     |  async apredict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Asynchronously pass a string to the model and return a string prediction.\n",
      "     |      \n",
      "     |      Use this method when calling pure text generation models and only the top\n",
      "     |          candidate generation is needed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: String input to pass to the model.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a string.\n",
      "     |  \n",
      "     |  async apredict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      "     |      Asynchronously pass messages to the model and return a message prediction.\n",
      "     |      \n",
      "     |      Use this method when calling chat models and only the top\n",
      "     |          candidate generation is needed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: A sequence of chat messages corresponding to a single model input.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a message.\n",
      "     |  \n",
      "     |  async astream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'AsyncIterator[str]'\n",
      "     |      Default implementation of astream, which calls ainvoke.\n",
      "     |      Subclasses should override this method if they support streaming output.\n",
      "     |  \n",
      "     |  batch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      "     |      Default implementation of batch, which calls invoke N times.\n",
      "     |      Subclasses should override this method if they can batch more efficiently.\n",
      "     |  \n",
      "     |  dict(self, **kwargs: 'Any') -> 'Dict'\n",
      "     |      Return a dictionary of the LLM.\n",
      "     |  \n",
      "     |  generate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  generate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Pass a sequence of prompts to the model and return model generations.\n",
      "     |      \n",
      "     |      This method should make use of batched calls for models that expose a batched\n",
      "     |      API.\n",
      "     |      \n",
      "     |      Use this method when you want to:\n",
      "     |          1. take advantage of batched calls,\n",
      "     |          2. need more output from the model than just the top generated value,\n",
      "     |          3. are building chains that are agnostic to the underlying language model\n",
      "     |              type (e.g., pure text completion models vs chat models).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      "     |              converted to match the format of any language model (string for pure\n",
      "     |              text generation models and BaseMessages for chat models).\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          callbacks: Callbacks to pass through. Used for executing additional\n",
      "     |              functionality, such as logging or streaming, throughout generation.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          An LLMResult, which contains a list of candidate Generations for each input\n",
      "     |              prompt and additional model provider-specific output.\n",
      "     |  \n",
      "     |  invoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |  \n",
      "     |  predict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Pass a single string input to the model and return a string prediction.\n",
      "     |      \n",
      "     |       Use this method when passing in raw text. If you want to pass in specific\n",
      "     |          types of chat messages, use predict_messages.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: String input to pass to the model.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a string.\n",
      "     |  \n",
      "     |  predict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      "     |      Pass a message sequence to the model and return a message prediction.\n",
      "     |      \n",
      "     |      Use this method when passing in chat messages. If you want to pass in raw text,\n",
      "     |          use predict.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: A sequence of chat messages corresponding to a single model input.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a message.\n",
      "     |  \n",
      "     |  save(self, file_path: 'Union[Path, str]') -> 'None'\n",
      "     |      Save the LLM.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          file_path: Path to file to save the LLM to.\n",
      "     |      \n",
      "     |      Example:\n",
      "     |      .. code-block:: python\n",
      "     |      \n",
      "     |          llm.save(file_path=\"path/llm.yaml\")\n",
      "     |  \n",
      "     |  stream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'Iterator[str]'\n",
      "     |      Default implementation of stream, which calls invoke.\n",
      "     |      Subclasses should override this method if they support streaming output.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  raise_deprecation(values: 'Dict') -> 'Dict' from pydantic.main.ModelMetaclass\n",
      "     |      Raise deprecation warning if callback_manager is used.\n",
      "     |  \n",
      "     |  set_verbose(verbose: 'Optional[bool]') -> 'bool' from pydantic.main.ModelMetaclass\n",
      "     |      If verbose is None, set it.\n",
      "     |      \n",
      "     |      This allows users to pass in None as verbose to access the global setting.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  __orig_bases__ = (langchain.schema.language_model.BaseLanguageModel[st...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.schema.language_model.BaseLanguageModel:\n",
      "     |  \n",
      "     |  get_num_tokens(self, text: 'str') -> 'int'\n",
      "     |      Get the number of tokens present in the text.\n",
      "     |      \n",
      "     |      Useful for checking if an input will fit in a model's context window.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: The string input to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The integer number of tokens in the text.\n",
      "     |  \n",
      "     |  get_num_tokens_from_messages(self, messages: 'List[BaseMessage]') -> 'int'\n",
      "     |      Get the number of tokens in the messages.\n",
      "     |      \n",
      "     |      Useful for checking if an input will fit in a model's context window.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: The message inputs to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The sum of the number of tokens across the messages.\n",
      "     |  \n",
      "     |  get_token_ids(self, text: 'str') -> 'List[int]'\n",
      "     |      Return the ordered ids of the tokens in a text.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: The string input to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          A list of ids corresponding to the tokens in the text, in order they occur\n",
      "     |              in the text.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from langchain.schema.language_model.BaseLanguageModel:\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.load.serializable.Serializable:\n",
      "     |  \n",
      "     |  __init__(self, **kwargs: Any) -> None\n",
      "     |      Create a new model by parsing and validating input data from keyword arguments.\n",
      "     |      \n",
      "     |      Raises ValidationError if the input data cannot be parsed to form a valid model.\n",
      "     |  \n",
      "     |  to_json(self) -> Union[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\n",
      "     |  \n",
      "     |  to_json_not_implemented(self) -> langchain.load.serializable.SerializedNotImplemented\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from langchain.load.serializable.Serializable:\n",
      "     |  \n",
      "     |  lc_attributes\n",
      "     |      Return a list of attribute names that should be included in the\n",
      "     |      serialized kwargs. These attributes must be accepted by the\n",
      "     |      constructor.\n",
      "     |  \n",
      "     |  lc_namespace\n",
      "     |      Return the namespace of the langchain object.\n",
      "     |      eg. [\"langchain\", \"llms\", \"openai\"]\n",
      "     |  \n",
      "     |  lc_secrets\n",
      "     |      Return a map of constructor argument names to secret ids.\n",
      "     |      eg. {\"openai_api_key\": \"OPENAI_API_KEY\"}\n",
      "     |  \n",
      "     |  lc_serializable\n",
      "     |      Return whether or not the class is serializable.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __eq__(self, other: Any) -> bool\n",
      "     |  \n",
      "     |  __getstate__(self) -> 'DictAny'\n",
      "     |  \n",
      "     |  __iter__(self) -> 'TupleGenerator'\n",
      "     |      so `dict(model)` works\n",
      "     |  \n",
      "     |  __repr_args__(self) -> 'ReprArgs'\n",
      "     |  \n",
      "     |  __setattr__(self, name, value)\n",
      "     |  \n",
      "     |  __setstate__(self, state: 'DictAny') -> None\n",
      "     |  \n",
      "     |  copy(self: 'Model', *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, update: Optional[ForwardRef('DictStrAny')] = None, deep: bool = False) -> 'Model'\n",
      "     |      Duplicate a model, optionally choose which fields to include, exclude and change.\n",
      "     |      \n",
      "     |      :param include: fields to include in new model\n",
      "     |      :param exclude: fields to exclude from new model, as with values this takes precedence over include\n",
      "     |      :param update: values to change/add in the new model. Note: the data is not validated before creating\n",
      "     |          the new model: you should trust this data\n",
      "     |      :param deep: set to `True` to make a deep copy of the model\n",
      "     |      :return: new model instance\n",
      "     |  \n",
      "     |  json(self, *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) -> 'unicode'\n",
      "     |      Generate a JSON representation of the model, `include` and `exclude` arguments as per `dict()`.\n",
      "     |      \n",
      "     |      `encoder` is an optional function to supply as `default` to json.dumps(), other arguments as per `json.dumps()`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __get_validators__() -> 'CallableGenerator' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  __try_update_forward_refs__(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      "     |      Same as update_forward_refs but will not raise exception\n",
      "     |      when forward references are not defined.\n",
      "     |  \n",
      "     |  construct(_fields_set: Optional[ForwardRef('SetStr')] = None, **values: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |      Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data.\n",
      "     |      Default values are respected, but no other validation is performed.\n",
      "     |      Behaves as if `Config.extra = 'allow'` was set since it adds all passed values\n",
      "     |  \n",
      "     |  from_orm(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_file(path: Union[str, pathlib.Path], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_obj(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_raw(b: Union[str, bytes], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema(by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}') -> 'DictStrAny' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema_json(*, by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}', **dumps_kwargs: Any) -> 'unicode' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  update_forward_refs(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      "     |      Try to update ForwardRefs on fields based on this Model, globalns and localns.\n",
      "     |  \n",
      "     |  validate(value: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __fields_set__\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.utils.Representation:\n",
      "     |  \n",
      "     |  __pretty__(self, fmt: Callable[[Any], Any], **kwargs: Any) -> Generator[Any, NoneType, NoneType]\n",
      "     |      Used by devtools (https://python-devtools.helpmanual.io/) to provide a human readable representations of objects\n",
      "     |  \n",
      "     |  __repr__(self) -> 'unicode'\n",
      "     |  \n",
      "     |  __repr_name__(self) -> 'unicode'\n",
      "     |      Name of the instance's class, used in __repr__.\n",
      "     |  \n",
      "     |  __repr_str__(self, join_str: 'unicode') -> 'unicode'\n",
      "     |  \n",
      "     |  __rich_repr__(self) -> 'RichReprResult'\n",
      "     |      Get fields for Rich library\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.schema.runnable.base.Runnable:\n",
      "     |  \n",
      "     |  __or__(self, other: 'Union[Runnable[Any, Other], Callable[[Any], Other], Mapping[str, Union[Runnable[Any, Other], Callable[[Any], Other], Any]]]') -> 'RunnableSequence[Input, Other]'\n",
      "     |      Return self|value.\n",
      "     |  \n",
      "     |  __ror__(self, other: 'Union[Runnable[Other, Any], Callable[[Any], Other], Mapping[str, Union[Runnable[Other, Any], Callable[[Other], Any], Any]]]') -> 'RunnableSequence[Other, Output]'\n",
      "     |      Return value|self.\n",
      "     |  \n",
      "     |  async atransform(self, input: 'AsyncIterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'AsyncIterator[Output]'\n",
      "     |      Default implementation of atransform, which buffers input and calls astream.\n",
      "     |      Subclasses should override this method if they can start producing output while\n",
      "     |      input is still being generated.\n",
      "     |  \n",
      "     |  bind(self, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      "     |      Bind arguments to a Runnable, returning a new Runnable.\n",
      "     |  \n",
      "     |  map(self) -> 'Runnable[List[Input], List[Output]]'\n",
      "     |      Return a new Runnable that maps a list of inputs to a list of outputs,\n",
      "     |      by calling invoke() with each input.\n",
      "     |  \n",
      "     |  transform(self, input: 'Iterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'Iterator[Output]'\n",
      "     |      Default implementation of transform, which buffers input and then calls stream.\n",
      "     |      Subclasses should override this method if they can start producing output while\n",
      "     |      input is still being generated.\n",
      "     |  \n",
      "     |  with_config(self, config: 'Optional[RunnableConfig]' = None, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      "     |      Bind config to a Runnable, returning a new Runnable.\n",
      "     |  \n",
      "     |  with_fallbacks(self, fallbacks: 'Sequence[Runnable[Input, Output]]', *, exceptions_to_handle: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,)) -> 'RunnableWithFallbacks[Input, Output]'\n",
      "     |  \n",
      "     |  with_retry(self, *, retry_if_exception_type: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,), wait_exponential_jitter: 'bool' = True, stop_after_attempt: 'int' = 3) -> 'Runnable[Input, Output]'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __class_getitem__(params) from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  __init_subclass__(*args, **kwargs) from pydantic.main.ModelMetaclass\n",
      "     |      This method is called when a class is subclassed.\n",
      "     |      \n",
      "     |      The default implementation does nothing. It may be\n",
      "     |      overridden to extend subclasses.\n",
      "    \n",
      "    class Replicate(langchain.llms.base.LLM)\n",
      "     |  Replicate(*, cache: Optional[bool] = None, verbose: bool = None, callbacks: Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager, NoneType] = None, callback_manager: Optional[langchain.callbacks.base.BaseCallbackManager] = None, tags: Optional[List[str]] = None, metadata: Optional[Dict[str, Any]] = None, model: str, input: Dict[str, Any] = None, model_kwargs: Dict[str, Any] = None, replicate_api_token: Optional[str] = None, prompt_key: Optional[str] = None, streaming: bool = False, stop: Optional[List[str]] = []) -> None\n",
      "     |  \n",
      "     |  Replicate models.\n",
      "     |  \n",
      "     |  To use, you should have the ``replicate`` python package installed,\n",
      "     |  and the environment variable ``REPLICATE_API_TOKEN`` set with your API token.\n",
      "     |  You can find your token here: https://replicate.com/account\n",
      "     |  \n",
      "     |  The model param is required, but any other model parameters can also\n",
      "     |  be passed in with the format input={model_param: value, ...}\n",
      "     |  \n",
      "     |  Example:\n",
      "     |      .. code-block:: python\n",
      "     |  \n",
      "     |          from langchain.llms import Replicate\n",
      "     |          replicate = Replicate(model=\"stability-ai/stable-diffusion:                                          27b93a2413e7f36cd83da926f365628                                         0b2931564ff050bf9575f1fdf9bcd7478\",\n",
      "     |                                input={\"image_dimensions\": \"512x512\"})\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Replicate\n",
      "     |      langchain.llms.base.LLM\n",
      "     |      langchain.llms.base.BaseLLM\n",
      "     |      langchain.schema.language_model.BaseLanguageModel\n",
      "     |      langchain.load.serializable.Serializable\n",
      "     |      pydantic.main.BaseModel\n",
      "     |      pydantic.utils.Representation\n",
      "     |      langchain.schema.runnable.base.Runnable\n",
      "     |      typing.Generic\n",
      "     |      abc.ABC\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  build_extra(values: Dict[str, Any]) -> Dict[str, Any] from pydantic.main.ModelMetaclass\n",
      "     |      Build extra kwargs from additional params that were passed in.\n",
      "     |  \n",
      "     |  validate_environment(values: Dict) -> Dict from pydantic.main.ModelMetaclass\n",
      "     |      Validate that api key and python package exists in environment.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __json_encoder__ = pydantic_encoder(obj: Any) -> Any\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  lc_secrets\n",
      "     |      Return a map of constructor argument names to secret ids.\n",
      "     |      eg. {\"openai_api_key\": \"OPENAI_API_KEY\"}\n",
      "     |  \n",
      "     |  lc_serializable\n",
      "     |      Return whether or not the class is serializable.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  Config = <class 'langchain.llms.replicate.Replicate.Config'>\n",
      "     |      Configuration for this pydantic config.\n",
      "     |  \n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {'input': typing.Dict[str, typing.Any], 'model': <cl...\n",
      "     |  \n",
      "     |  __class_vars__ = set()\n",
      "     |  \n",
      "     |  __config__ = <class 'langchain.llms.replicate.Config'>\n",
      "     |  \n",
      "     |  __custom_root_type__ = False\n",
      "     |  \n",
      "     |  __exclude_fields__ = {'callback_manager': True, 'callbacks': True, 'me...\n",
      "     |  \n",
      "     |  __fields__ = {'cache': ModelField(name='cache', type=Optional[bool], r...\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "     |  \n",
      "     |  __include_fields__ = None\n",
      "     |  \n",
      "     |  __parameters__ = ()\n",
      "     |  \n",
      "     |  __post_root_validators__ = [(False, <function BaseLLM.raise_deprecatio...\n",
      "     |  \n",
      "     |  __pre_root_validators__ = [<function Replicate.build_extra>]\n",
      "     |  \n",
      "     |  __private_attributes__ = {'_lc_kwargs': ModelPrivateAttr(default=Pydan...\n",
      "     |  \n",
      "     |  __schema_cache__ = {}\n",
      "     |  \n",
      "     |  __signature__ = <Signature (*, cache: Optional[bool] = None, ver...= F...\n",
      "     |  \n",
      "     |  __validators__ = {'verbose': [<pydantic.class_validators.Validator obj...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  __call__(self, prompt: 'str', stop: 'Optional[List[str]]' = None, callbacks: 'Callbacks' = None, *, tags: 'Optional[List[str]]' = None, metadata: 'Optional[Dict[str, Any]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Check Cache and run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  __str__(self) -> 'str'\n",
      "     |      Get a string representation of the object for printing.\n",
      "     |  \n",
      "     |  async abatch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      "     |      Default implementation of abatch, which calls ainvoke N times.\n",
      "     |      Subclasses should override this method if they can batch more efficiently.\n",
      "     |  \n",
      "     |  async agenerate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  async agenerate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Asynchronously pass a sequence of prompts and return model generations.\n",
      "     |      \n",
      "     |      This method should make use of batched calls for models that expose a batched\n",
      "     |      API.\n",
      "     |      \n",
      "     |      Use this method when you want to:\n",
      "     |          1. take advantage of batched calls,\n",
      "     |          2. need more output from the model than just the top generated value,\n",
      "     |          3. are building chains that are agnostic to the underlying language model\n",
      "     |              type (e.g., pure text completion models vs chat models).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      "     |              converted to match the format of any language model (string for pure\n",
      "     |              text generation models and BaseMessages for chat models).\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          callbacks: Callbacks to pass through. Used for executing additional\n",
      "     |              functionality, such as logging or streaming, throughout generation.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          An LLMResult, which contains a list of candidate Generations for each input\n",
      "     |              prompt and additional model provider-specific output.\n",
      "     |  \n",
      "     |  async ainvoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Default implementation of ainvoke, which calls invoke in a thread pool.\n",
      "     |      Subclasses should override this method if they can run asynchronously.\n",
      "     |  \n",
      "     |  async apredict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Asynchronously pass a string to the model and return a string prediction.\n",
      "     |      \n",
      "     |      Use this method when calling pure text generation models and only the top\n",
      "     |          candidate generation is needed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: String input to pass to the model.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a string.\n",
      "     |  \n",
      "     |  async apredict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      "     |      Asynchronously pass messages to the model and return a message prediction.\n",
      "     |      \n",
      "     |      Use this method when calling chat models and only the top\n",
      "     |          candidate generation is needed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: A sequence of chat messages corresponding to a single model input.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a message.\n",
      "     |  \n",
      "     |  async astream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'AsyncIterator[str]'\n",
      "     |      Default implementation of astream, which calls ainvoke.\n",
      "     |      Subclasses should override this method if they support streaming output.\n",
      "     |  \n",
      "     |  batch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      "     |      Default implementation of batch, which calls invoke N times.\n",
      "     |      Subclasses should override this method if they can batch more efficiently.\n",
      "     |  \n",
      "     |  dict(self, **kwargs: 'Any') -> 'Dict'\n",
      "     |      Return a dictionary of the LLM.\n",
      "     |  \n",
      "     |  generate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  generate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Pass a sequence of prompts to the model and return model generations.\n",
      "     |      \n",
      "     |      This method should make use of batched calls for models that expose a batched\n",
      "     |      API.\n",
      "     |      \n",
      "     |      Use this method when you want to:\n",
      "     |          1. take advantage of batched calls,\n",
      "     |          2. need more output from the model than just the top generated value,\n",
      "     |          3. are building chains that are agnostic to the underlying language model\n",
      "     |              type (e.g., pure text completion models vs chat models).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      "     |              converted to match the format of any language model (string for pure\n",
      "     |              text generation models and BaseMessages for chat models).\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          callbacks: Callbacks to pass through. Used for executing additional\n",
      "     |              functionality, such as logging or streaming, throughout generation.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          An LLMResult, which contains a list of candidate Generations for each input\n",
      "     |              prompt and additional model provider-specific output.\n",
      "     |  \n",
      "     |  invoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |  \n",
      "     |  predict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Pass a single string input to the model and return a string prediction.\n",
      "     |      \n",
      "     |       Use this method when passing in raw text. If you want to pass in specific\n",
      "     |          types of chat messages, use predict_messages.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: String input to pass to the model.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a string.\n",
      "     |  \n",
      "     |  predict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      "     |      Pass a message sequence to the model and return a message prediction.\n",
      "     |      \n",
      "     |      Use this method when passing in chat messages. If you want to pass in raw text,\n",
      "     |          use predict.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: A sequence of chat messages corresponding to a single model input.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a message.\n",
      "     |  \n",
      "     |  save(self, file_path: 'Union[Path, str]') -> 'None'\n",
      "     |      Save the LLM.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          file_path: Path to file to save the LLM to.\n",
      "     |      \n",
      "     |      Example:\n",
      "     |      .. code-block:: python\n",
      "     |      \n",
      "     |          llm.save(file_path=\"path/llm.yaml\")\n",
      "     |  \n",
      "     |  stream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'Iterator[str]'\n",
      "     |      Default implementation of stream, which calls invoke.\n",
      "     |      Subclasses should override this method if they support streaming output.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  raise_deprecation(values: 'Dict') -> 'Dict' from pydantic.main.ModelMetaclass\n",
      "     |      Raise deprecation warning if callback_manager is used.\n",
      "     |  \n",
      "     |  set_verbose(verbose: 'Optional[bool]') -> 'bool' from pydantic.main.ModelMetaclass\n",
      "     |      If verbose is None, set it.\n",
      "     |      \n",
      "     |      This allows users to pass in None as verbose to access the global setting.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  __orig_bases__ = (langchain.schema.language_model.BaseLanguageModel[st...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.schema.language_model.BaseLanguageModel:\n",
      "     |  \n",
      "     |  get_num_tokens(self, text: 'str') -> 'int'\n",
      "     |      Get the number of tokens present in the text.\n",
      "     |      \n",
      "     |      Useful for checking if an input will fit in a model's context window.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: The string input to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The integer number of tokens in the text.\n",
      "     |  \n",
      "     |  get_num_tokens_from_messages(self, messages: 'List[BaseMessage]') -> 'int'\n",
      "     |      Get the number of tokens in the messages.\n",
      "     |      \n",
      "     |      Useful for checking if an input will fit in a model's context window.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: The message inputs to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The sum of the number of tokens across the messages.\n",
      "     |  \n",
      "     |  get_token_ids(self, text: 'str') -> 'List[int]'\n",
      "     |      Return the ordered ids of the tokens in a text.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: The string input to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          A list of ids corresponding to the tokens in the text, in order they occur\n",
      "     |              in the text.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from langchain.schema.language_model.BaseLanguageModel:\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.load.serializable.Serializable:\n",
      "     |  \n",
      "     |  __init__(self, **kwargs: Any) -> None\n",
      "     |      Create a new model by parsing and validating input data from keyword arguments.\n",
      "     |      \n",
      "     |      Raises ValidationError if the input data cannot be parsed to form a valid model.\n",
      "     |  \n",
      "     |  to_json(self) -> Union[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\n",
      "     |  \n",
      "     |  to_json_not_implemented(self) -> langchain.load.serializable.SerializedNotImplemented\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from langchain.load.serializable.Serializable:\n",
      "     |  \n",
      "     |  lc_attributes\n",
      "     |      Return a list of attribute names that should be included in the\n",
      "     |      serialized kwargs. These attributes must be accepted by the\n",
      "     |      constructor.\n",
      "     |  \n",
      "     |  lc_namespace\n",
      "     |      Return the namespace of the langchain object.\n",
      "     |      eg. [\"langchain\", \"llms\", \"openai\"]\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __eq__(self, other: Any) -> bool\n",
      "     |  \n",
      "     |  __getstate__(self) -> 'DictAny'\n",
      "     |  \n",
      "     |  __iter__(self) -> 'TupleGenerator'\n",
      "     |      so `dict(model)` works\n",
      "     |  \n",
      "     |  __repr_args__(self) -> 'ReprArgs'\n",
      "     |  \n",
      "     |  __setattr__(self, name, value)\n",
      "     |  \n",
      "     |  __setstate__(self, state: 'DictAny') -> None\n",
      "     |  \n",
      "     |  copy(self: 'Model', *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, update: Optional[ForwardRef('DictStrAny')] = None, deep: bool = False) -> 'Model'\n",
      "     |      Duplicate a model, optionally choose which fields to include, exclude and change.\n",
      "     |      \n",
      "     |      :param include: fields to include in new model\n",
      "     |      :param exclude: fields to exclude from new model, as with values this takes precedence over include\n",
      "     |      :param update: values to change/add in the new model. Note: the data is not validated before creating\n",
      "     |          the new model: you should trust this data\n",
      "     |      :param deep: set to `True` to make a deep copy of the model\n",
      "     |      :return: new model instance\n",
      "     |  \n",
      "     |  json(self, *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) -> 'unicode'\n",
      "     |      Generate a JSON representation of the model, `include` and `exclude` arguments as per `dict()`.\n",
      "     |      \n",
      "     |      `encoder` is an optional function to supply as `default` to json.dumps(), other arguments as per `json.dumps()`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __get_validators__() -> 'CallableGenerator' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  __try_update_forward_refs__(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      "     |      Same as update_forward_refs but will not raise exception\n",
      "     |      when forward references are not defined.\n",
      "     |  \n",
      "     |  construct(_fields_set: Optional[ForwardRef('SetStr')] = None, **values: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |      Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data.\n",
      "     |      Default values are respected, but no other validation is performed.\n",
      "     |      Behaves as if `Config.extra = 'allow'` was set since it adds all passed values\n",
      "     |  \n",
      "     |  from_orm(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_file(path: Union[str, pathlib.Path], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_obj(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_raw(b: Union[str, bytes], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema(by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}') -> 'DictStrAny' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema_json(*, by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}', **dumps_kwargs: Any) -> 'unicode' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  update_forward_refs(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      "     |      Try to update ForwardRefs on fields based on this Model, globalns and localns.\n",
      "     |  \n",
      "     |  validate(value: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __fields_set__\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.utils.Representation:\n",
      "     |  \n",
      "     |  __pretty__(self, fmt: Callable[[Any], Any], **kwargs: Any) -> Generator[Any, NoneType, NoneType]\n",
      "     |      Used by devtools (https://python-devtools.helpmanual.io/) to provide a human readable representations of objects\n",
      "     |  \n",
      "     |  __repr__(self) -> 'unicode'\n",
      "     |  \n",
      "     |  __repr_name__(self) -> 'unicode'\n",
      "     |      Name of the instance's class, used in __repr__.\n",
      "     |  \n",
      "     |  __repr_str__(self, join_str: 'unicode') -> 'unicode'\n",
      "     |  \n",
      "     |  __rich_repr__(self) -> 'RichReprResult'\n",
      "     |      Get fields for Rich library\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.schema.runnable.base.Runnable:\n",
      "     |  \n",
      "     |  __or__(self, other: 'Union[Runnable[Any, Other], Callable[[Any], Other], Mapping[str, Union[Runnable[Any, Other], Callable[[Any], Other], Any]]]') -> 'RunnableSequence[Input, Other]'\n",
      "     |      Return self|value.\n",
      "     |  \n",
      "     |  __ror__(self, other: 'Union[Runnable[Other, Any], Callable[[Any], Other], Mapping[str, Union[Runnable[Other, Any], Callable[[Other], Any], Any]]]') -> 'RunnableSequence[Other, Output]'\n",
      "     |      Return value|self.\n",
      "     |  \n",
      "     |  async atransform(self, input: 'AsyncIterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'AsyncIterator[Output]'\n",
      "     |      Default implementation of atransform, which buffers input and calls astream.\n",
      "     |      Subclasses should override this method if they can start producing output while\n",
      "     |      input is still being generated.\n",
      "     |  \n",
      "     |  bind(self, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      "     |      Bind arguments to a Runnable, returning a new Runnable.\n",
      "     |  \n",
      "     |  map(self) -> 'Runnable[List[Input], List[Output]]'\n",
      "     |      Return a new Runnable that maps a list of inputs to a list of outputs,\n",
      "     |      by calling invoke() with each input.\n",
      "     |  \n",
      "     |  transform(self, input: 'Iterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'Iterator[Output]'\n",
      "     |      Default implementation of transform, which buffers input and then calls stream.\n",
      "     |      Subclasses should override this method if they can start producing output while\n",
      "     |      input is still being generated.\n",
      "     |  \n",
      "     |  with_config(self, config: 'Optional[RunnableConfig]' = None, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      "     |      Bind config to a Runnable, returning a new Runnable.\n",
      "     |  \n",
      "     |  with_fallbacks(self, fallbacks: 'Sequence[Runnable[Input, Output]]', *, exceptions_to_handle: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,)) -> 'RunnableWithFallbacks[Input, Output]'\n",
      "     |  \n",
      "     |  with_retry(self, *, retry_if_exception_type: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,), wait_exponential_jitter: 'bool' = True, stop_after_attempt: 'int' = 3) -> 'Runnable[Input, Output]'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __class_getitem__(params) from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  __init_subclass__(*args, **kwargs) from pydantic.main.ModelMetaclass\n",
      "     |      This method is called when a class is subclassed.\n",
      "     |      \n",
      "     |      The default implementation does nothing. It may be\n",
      "     |      overridden to extend subclasses.\n",
      "    \n",
      "    class SagemakerEndpoint(langchain.llms.base.LLM)\n",
      "     |  SagemakerEndpoint(*, cache: Optional[bool] = None, verbose: bool = None, callbacks: Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager, NoneType] = None, callback_manager: Optional[langchain.callbacks.base.BaseCallbackManager] = None, tags: Optional[List[str]] = None, metadata: Optional[Dict[str, Any]] = None, client: Any = None, endpoint_name: str = '', region_name: str = '', credentials_profile_name: Optional[str] = None, content_handler: langchain.llms.sagemaker_endpoint.LLMContentHandler, model_kwargs: Optional[Dict] = None, endpoint_kwargs: Optional[Dict] = None) -> None\n",
      "     |  \n",
      "     |  Sagemaker Inference Endpoint models.\n",
      "     |  \n",
      "     |  To use, you must supply the endpoint name from your deployed\n",
      "     |  Sagemaker model & the region where it is deployed.\n",
      "     |  \n",
      "     |  To authenticate, the AWS client uses the following methods to\n",
      "     |  automatically load credentials:\n",
      "     |  https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html\n",
      "     |  \n",
      "     |  If a specific credential profile should be used, you must pass\n",
      "     |  the name of the profile from the ~/.aws/credentials file that is to be used.\n",
      "     |  \n",
      "     |  Make sure the credentials / roles used have the required policies to\n",
      "     |  access the Sagemaker endpoint.\n",
      "     |  See: https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      SagemakerEndpoint\n",
      "     |      langchain.llms.base.LLM\n",
      "     |      langchain.llms.base.BaseLLM\n",
      "     |      langchain.schema.language_model.BaseLanguageModel\n",
      "     |      langchain.load.serializable.Serializable\n",
      "     |      pydantic.main.BaseModel\n",
      "     |      pydantic.utils.Representation\n",
      "     |      langchain.schema.runnable.base.Runnable\n",
      "     |      typing.Generic\n",
      "     |      abc.ABC\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  validate_environment(values: Dict) -> Dict from pydantic.main.ModelMetaclass\n",
      "     |      Validate that AWS credentials to and python package exists in environment.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __json_encoder__ = pydantic_encoder(obj: Any) -> Any\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  Config = <class 'langchain.llms.sagemaker_endpoint.SagemakerEndpoint.C...\n",
      "     |      Configuration for this pydantic object.\n",
      "     |  \n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {'client': typing.Any, 'content_handler': <class 'la...\n",
      "     |  \n",
      "     |  __class_vars__ = set()\n",
      "     |  \n",
      "     |  __config__ = <class 'langchain.llms.sagemaker_endpoint.Config'>\n",
      "     |  \n",
      "     |  __custom_root_type__ = False\n",
      "     |  \n",
      "     |  __exclude_fields__ = {'callback_manager': True, 'callbacks': True, 'me...\n",
      "     |  \n",
      "     |  __fields__ = {'cache': ModelField(name='cache', type=Optional[bool], r...\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "     |  \n",
      "     |  __include_fields__ = None\n",
      "     |  \n",
      "     |  __parameters__ = ()\n",
      "     |  \n",
      "     |  __post_root_validators__ = [(False, <function BaseLLM.raise_deprecatio...\n",
      "     |  \n",
      "     |  __pre_root_validators__ = []\n",
      "     |  \n",
      "     |  __private_attributes__ = {'_lc_kwargs': ModelPrivateAttr(default=Pydan...\n",
      "     |  \n",
      "     |  __schema_cache__ = {}\n",
      "     |  \n",
      "     |  __signature__ = <Signature (*, cache: Optional[bool] = None, ver... en...\n",
      "     |  \n",
      "     |  __validators__ = {'verbose': [<pydantic.class_validators.Validator obj...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  __call__(self, prompt: 'str', stop: 'Optional[List[str]]' = None, callbacks: 'Callbacks' = None, *, tags: 'Optional[List[str]]' = None, metadata: 'Optional[Dict[str, Any]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Check Cache and run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  __str__(self) -> 'str'\n",
      "     |      Get a string representation of the object for printing.\n",
      "     |  \n",
      "     |  async abatch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      "     |      Default implementation of abatch, which calls ainvoke N times.\n",
      "     |      Subclasses should override this method if they can batch more efficiently.\n",
      "     |  \n",
      "     |  async agenerate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  async agenerate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Asynchronously pass a sequence of prompts and return model generations.\n",
      "     |      \n",
      "     |      This method should make use of batched calls for models that expose a batched\n",
      "     |      API.\n",
      "     |      \n",
      "     |      Use this method when you want to:\n",
      "     |          1. take advantage of batched calls,\n",
      "     |          2. need more output from the model than just the top generated value,\n",
      "     |          3. are building chains that are agnostic to the underlying language model\n",
      "     |              type (e.g., pure text completion models vs chat models).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      "     |              converted to match the format of any language model (string for pure\n",
      "     |              text generation models and BaseMessages for chat models).\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          callbacks: Callbacks to pass through. Used for executing additional\n",
      "     |              functionality, such as logging or streaming, throughout generation.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          An LLMResult, which contains a list of candidate Generations for each input\n",
      "     |              prompt and additional model provider-specific output.\n",
      "     |  \n",
      "     |  async ainvoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Default implementation of ainvoke, which calls invoke in a thread pool.\n",
      "     |      Subclasses should override this method if they can run asynchronously.\n",
      "     |  \n",
      "     |  async apredict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Asynchronously pass a string to the model and return a string prediction.\n",
      "     |      \n",
      "     |      Use this method when calling pure text generation models and only the top\n",
      "     |          candidate generation is needed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: String input to pass to the model.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a string.\n",
      "     |  \n",
      "     |  async apredict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      "     |      Asynchronously pass messages to the model and return a message prediction.\n",
      "     |      \n",
      "     |      Use this method when calling chat models and only the top\n",
      "     |          candidate generation is needed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: A sequence of chat messages corresponding to a single model input.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a message.\n",
      "     |  \n",
      "     |  async astream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'AsyncIterator[str]'\n",
      "     |      Default implementation of astream, which calls ainvoke.\n",
      "     |      Subclasses should override this method if they support streaming output.\n",
      "     |  \n",
      "     |  batch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      "     |      Default implementation of batch, which calls invoke N times.\n",
      "     |      Subclasses should override this method if they can batch more efficiently.\n",
      "     |  \n",
      "     |  dict(self, **kwargs: 'Any') -> 'Dict'\n",
      "     |      Return a dictionary of the LLM.\n",
      "     |  \n",
      "     |  generate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  generate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Pass a sequence of prompts to the model and return model generations.\n",
      "     |      \n",
      "     |      This method should make use of batched calls for models that expose a batched\n",
      "     |      API.\n",
      "     |      \n",
      "     |      Use this method when you want to:\n",
      "     |          1. take advantage of batched calls,\n",
      "     |          2. need more output from the model than just the top generated value,\n",
      "     |          3. are building chains that are agnostic to the underlying language model\n",
      "     |              type (e.g., pure text completion models vs chat models).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      "     |              converted to match the format of any language model (string for pure\n",
      "     |              text generation models and BaseMessages for chat models).\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          callbacks: Callbacks to pass through. Used for executing additional\n",
      "     |              functionality, such as logging or streaming, throughout generation.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          An LLMResult, which contains a list of candidate Generations for each input\n",
      "     |              prompt and additional model provider-specific output.\n",
      "     |  \n",
      "     |  invoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |  \n",
      "     |  predict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Pass a single string input to the model and return a string prediction.\n",
      "     |      \n",
      "     |       Use this method when passing in raw text. If you want to pass in specific\n",
      "     |          types of chat messages, use predict_messages.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: String input to pass to the model.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a string.\n",
      "     |  \n",
      "     |  predict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      "     |      Pass a message sequence to the model and return a message prediction.\n",
      "     |      \n",
      "     |      Use this method when passing in chat messages. If you want to pass in raw text,\n",
      "     |          use predict.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: A sequence of chat messages corresponding to a single model input.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a message.\n",
      "     |  \n",
      "     |  save(self, file_path: 'Union[Path, str]') -> 'None'\n",
      "     |      Save the LLM.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          file_path: Path to file to save the LLM to.\n",
      "     |      \n",
      "     |      Example:\n",
      "     |      .. code-block:: python\n",
      "     |      \n",
      "     |          llm.save(file_path=\"path/llm.yaml\")\n",
      "     |  \n",
      "     |  stream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'Iterator[str]'\n",
      "     |      Default implementation of stream, which calls invoke.\n",
      "     |      Subclasses should override this method if they support streaming output.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  raise_deprecation(values: 'Dict') -> 'Dict' from pydantic.main.ModelMetaclass\n",
      "     |      Raise deprecation warning if callback_manager is used.\n",
      "     |  \n",
      "     |  set_verbose(verbose: 'Optional[bool]') -> 'bool' from pydantic.main.ModelMetaclass\n",
      "     |      If verbose is None, set it.\n",
      "     |      \n",
      "     |      This allows users to pass in None as verbose to access the global setting.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  __orig_bases__ = (langchain.schema.language_model.BaseLanguageModel[st...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.schema.language_model.BaseLanguageModel:\n",
      "     |  \n",
      "     |  get_num_tokens(self, text: 'str') -> 'int'\n",
      "     |      Get the number of tokens present in the text.\n",
      "     |      \n",
      "     |      Useful for checking if an input will fit in a model's context window.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: The string input to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The integer number of tokens in the text.\n",
      "     |  \n",
      "     |  get_num_tokens_from_messages(self, messages: 'List[BaseMessage]') -> 'int'\n",
      "     |      Get the number of tokens in the messages.\n",
      "     |      \n",
      "     |      Useful for checking if an input will fit in a model's context window.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: The message inputs to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The sum of the number of tokens across the messages.\n",
      "     |  \n",
      "     |  get_token_ids(self, text: 'str') -> 'List[int]'\n",
      "     |      Return the ordered ids of the tokens in a text.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: The string input to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          A list of ids corresponding to the tokens in the text, in order they occur\n",
      "     |              in the text.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from langchain.schema.language_model.BaseLanguageModel:\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.load.serializable.Serializable:\n",
      "     |  \n",
      "     |  __init__(self, **kwargs: Any) -> None\n",
      "     |      Create a new model by parsing and validating input data from keyword arguments.\n",
      "     |      \n",
      "     |      Raises ValidationError if the input data cannot be parsed to form a valid model.\n",
      "     |  \n",
      "     |  to_json(self) -> Union[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\n",
      "     |  \n",
      "     |  to_json_not_implemented(self) -> langchain.load.serializable.SerializedNotImplemented\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from langchain.load.serializable.Serializable:\n",
      "     |  \n",
      "     |  lc_attributes\n",
      "     |      Return a list of attribute names that should be included in the\n",
      "     |      serialized kwargs. These attributes must be accepted by the\n",
      "     |      constructor.\n",
      "     |  \n",
      "     |  lc_namespace\n",
      "     |      Return the namespace of the langchain object.\n",
      "     |      eg. [\"langchain\", \"llms\", \"openai\"]\n",
      "     |  \n",
      "     |  lc_secrets\n",
      "     |      Return a map of constructor argument names to secret ids.\n",
      "     |      eg. {\"openai_api_key\": \"OPENAI_API_KEY\"}\n",
      "     |  \n",
      "     |  lc_serializable\n",
      "     |      Return whether or not the class is serializable.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __eq__(self, other: Any) -> bool\n",
      "     |  \n",
      "     |  __getstate__(self) -> 'DictAny'\n",
      "     |  \n",
      "     |  __iter__(self) -> 'TupleGenerator'\n",
      "     |      so `dict(model)` works\n",
      "     |  \n",
      "     |  __repr_args__(self) -> 'ReprArgs'\n",
      "     |  \n",
      "     |  __setattr__(self, name, value)\n",
      "     |  \n",
      "     |  __setstate__(self, state: 'DictAny') -> None\n",
      "     |  \n",
      "     |  copy(self: 'Model', *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, update: Optional[ForwardRef('DictStrAny')] = None, deep: bool = False) -> 'Model'\n",
      "     |      Duplicate a model, optionally choose which fields to include, exclude and change.\n",
      "     |      \n",
      "     |      :param include: fields to include in new model\n",
      "     |      :param exclude: fields to exclude from new model, as with values this takes precedence over include\n",
      "     |      :param update: values to change/add in the new model. Note: the data is not validated before creating\n",
      "     |          the new model: you should trust this data\n",
      "     |      :param deep: set to `True` to make a deep copy of the model\n",
      "     |      :return: new model instance\n",
      "     |  \n",
      "     |  json(self, *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) -> 'unicode'\n",
      "     |      Generate a JSON representation of the model, `include` and `exclude` arguments as per `dict()`.\n",
      "     |      \n",
      "     |      `encoder` is an optional function to supply as `default` to json.dumps(), other arguments as per `json.dumps()`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __get_validators__() -> 'CallableGenerator' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  __try_update_forward_refs__(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      "     |      Same as update_forward_refs but will not raise exception\n",
      "     |      when forward references are not defined.\n",
      "     |  \n",
      "     |  construct(_fields_set: Optional[ForwardRef('SetStr')] = None, **values: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |      Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data.\n",
      "     |      Default values are respected, but no other validation is performed.\n",
      "     |      Behaves as if `Config.extra = 'allow'` was set since it adds all passed values\n",
      "     |  \n",
      "     |  from_orm(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_file(path: Union[str, pathlib.Path], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_obj(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_raw(b: Union[str, bytes], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema(by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}') -> 'DictStrAny' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema_json(*, by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}', **dumps_kwargs: Any) -> 'unicode' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  update_forward_refs(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      "     |      Try to update ForwardRefs on fields based on this Model, globalns and localns.\n",
      "     |  \n",
      "     |  validate(value: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __fields_set__\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.utils.Representation:\n",
      "     |  \n",
      "     |  __pretty__(self, fmt: Callable[[Any], Any], **kwargs: Any) -> Generator[Any, NoneType, NoneType]\n",
      "     |      Used by devtools (https://python-devtools.helpmanual.io/) to provide a human readable representations of objects\n",
      "     |  \n",
      "     |  __repr__(self) -> 'unicode'\n",
      "     |  \n",
      "     |  __repr_name__(self) -> 'unicode'\n",
      "     |      Name of the instance's class, used in __repr__.\n",
      "     |  \n",
      "     |  __repr_str__(self, join_str: 'unicode') -> 'unicode'\n",
      "     |  \n",
      "     |  __rich_repr__(self) -> 'RichReprResult'\n",
      "     |      Get fields for Rich library\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.schema.runnable.base.Runnable:\n",
      "     |  \n",
      "     |  __or__(self, other: 'Union[Runnable[Any, Other], Callable[[Any], Other], Mapping[str, Union[Runnable[Any, Other], Callable[[Any], Other], Any]]]') -> 'RunnableSequence[Input, Other]'\n",
      "     |      Return self|value.\n",
      "     |  \n",
      "     |  __ror__(self, other: 'Union[Runnable[Other, Any], Callable[[Any], Other], Mapping[str, Union[Runnable[Other, Any], Callable[[Other], Any], Any]]]') -> 'RunnableSequence[Other, Output]'\n",
      "     |      Return value|self.\n",
      "     |  \n",
      "     |  async atransform(self, input: 'AsyncIterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'AsyncIterator[Output]'\n",
      "     |      Default implementation of atransform, which buffers input and calls astream.\n",
      "     |      Subclasses should override this method if they can start producing output while\n",
      "     |      input is still being generated.\n",
      "     |  \n",
      "     |  bind(self, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      "     |      Bind arguments to a Runnable, returning a new Runnable.\n",
      "     |  \n",
      "     |  map(self) -> 'Runnable[List[Input], List[Output]]'\n",
      "     |      Return a new Runnable that maps a list of inputs to a list of outputs,\n",
      "     |      by calling invoke() with each input.\n",
      "     |  \n",
      "     |  transform(self, input: 'Iterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'Iterator[Output]'\n",
      "     |      Default implementation of transform, which buffers input and then calls stream.\n",
      "     |      Subclasses should override this method if they can start producing output while\n",
      "     |      input is still being generated.\n",
      "     |  \n",
      "     |  with_config(self, config: 'Optional[RunnableConfig]' = None, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      "     |      Bind config to a Runnable, returning a new Runnable.\n",
      "     |  \n",
      "     |  with_fallbacks(self, fallbacks: 'Sequence[Runnable[Input, Output]]', *, exceptions_to_handle: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,)) -> 'RunnableWithFallbacks[Input, Output]'\n",
      "     |  \n",
      "     |  with_retry(self, *, retry_if_exception_type: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,), wait_exponential_jitter: 'bool' = True, stop_after_attempt: 'int' = 3) -> 'Runnable[Input, Output]'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __class_getitem__(params) from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  __init_subclass__(*args, **kwargs) from pydantic.main.ModelMetaclass\n",
      "     |      This method is called when a class is subclassed.\n",
      "     |      \n",
      "     |      The default implementation does nothing. It may be\n",
      "     |      overridden to extend subclasses.\n",
      "    \n",
      "    class SelfHostedHuggingFaceLLM(langchain.llms.self_hosted.SelfHostedPipeline)\n",
      "     |  SelfHostedHuggingFaceLLM(*, cache: Optional[bool] = None, verbose: bool = None, callbacks: Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager, NoneType] = None, callback_manager: Optional[langchain.callbacks.base.BaseCallbackManager] = None, tags: Optional[List[str]] = None, metadata: Optional[Dict[str, Any]] = None, pipeline_ref: Any = None, client: Any = None, inference_fn: Callable = <function _generate_text at 0x00000227B0748310>, hardware: Any = None, model_load_fn: Callable = <function _load_transformer at 0x00000227B07483A0>, load_fn_kwargs: Optional[dict] = None, model_reqs: List[str] = ['./', 'transformers', 'torch'], model_id: str = 'gpt2', task: str = 'text-generation', device: int = 0, model_kwargs: Optional[dict] = None) -> None\n",
      "     |  \n",
      "     |  HuggingFace Pipeline API to run on self-hosted remote hardware.\n",
      "     |  \n",
      "     |  Supported hardware includes auto-launched instances on AWS, GCP, Azure,\n",
      "     |  and Lambda, as well as servers specified\n",
      "     |  by IP address and SSH credentials (such as on-prem, or another cloud\n",
      "     |  like Paperspace, Coreweave, etc.).\n",
      "     |  \n",
      "     |  To use, you should have the ``runhouse`` python package installed.\n",
      "     |  \n",
      "     |  Only supports `text-generation`, `text2text-generation` and `summarization` for now.\n",
      "     |  \n",
      "     |  Example using from_model_id:\n",
      "     |      .. code-block:: python\n",
      "     |  \n",
      "     |          from langchain.llms import SelfHostedHuggingFaceLLM\n",
      "     |          import runhouse as rh\n",
      "     |          gpu = rh.cluster(name=\"rh-a10x\", instance_type=\"A100:1\")\n",
      "     |          hf = SelfHostedHuggingFaceLLM(\n",
      "     |              model_id=\"google/flan-t5-large\", task=\"text2text-generation\",\n",
      "     |              hardware=gpu\n",
      "     |          )\n",
      "     |  Example passing fn that generates a pipeline (bc the pipeline is not serializable):\n",
      "     |      .. code-block:: python\n",
      "     |  \n",
      "     |          from langchain.llms import SelfHostedHuggingFaceLLM\n",
      "     |          from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
      "     |          import runhouse as rh\n",
      "     |  \n",
      "     |          def get_pipeline():\n",
      "     |              model_id = \"gpt2\"\n",
      "     |              tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
      "     |              model = AutoModelForCausalLM.from_pretrained(model_id)\n",
      "     |              pipe = pipeline(\n",
      "     |                  \"text-generation\", model=model, tokenizer=tokenizer\n",
      "     |              )\n",
      "     |              return pipe\n",
      "     |          hf = SelfHostedHuggingFaceLLM(\n",
      "     |              model_load_fn=get_pipeline, model_id=\"gpt2\", hardware=gpu)\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      SelfHostedHuggingFaceLLM\n",
      "     |      langchain.llms.self_hosted.SelfHostedPipeline\n",
      "     |      langchain.llms.base.LLM\n",
      "     |      langchain.llms.base.BaseLLM\n",
      "     |      langchain.schema.language_model.BaseLanguageModel\n",
      "     |      langchain.load.serializable.Serializable\n",
      "     |      pydantic.main.BaseModel\n",
      "     |      pydantic.utils.Representation\n",
      "     |      langchain.schema.runnable.base.Runnable\n",
      "     |      typing.Generic\n",
      "     |      abc.ABC\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, **kwargs: Any)\n",
      "     |      Construct the pipeline remotely using an auxiliary function.\n",
      "     |      \n",
      "     |      The load function needs to be importable to be imported\n",
      "     |      and run on the server, i.e. in a module and not a REPL or closure.\n",
      "     |      Then, initialize the remote inference function.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __json_encoder__ = pydantic_encoder(obj: Any) -> Any\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  Config = <class 'langchain.llms.self_hosted_hugging_face.SelfHostedHug...\n",
      "     |      Configuration for this pydantic object.\n",
      "     |  \n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {'device': <class 'int'>, 'hardware': typing.Any, 'i...\n",
      "     |  \n",
      "     |  __class_vars__ = set()\n",
      "     |  \n",
      "     |  __config__ = <class 'langchain.llms.self_hosted_hugging_face.Config'>\n",
      "     |  \n",
      "     |  __custom_root_type__ = False\n",
      "     |  \n",
      "     |  __exclude_fields__ = {'callback_manager': True, 'callbacks': True, 'me...\n",
      "     |  \n",
      "     |  __fields__ = {'cache': ModelField(name='cache', type=Optional[bool], r...\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "     |  \n",
      "     |  __include_fields__ = None\n",
      "     |  \n",
      "     |  __parameters__ = ()\n",
      "     |  \n",
      "     |  __post_root_validators__ = [(False, <function BaseLLM.raise_deprecatio...\n",
      "     |  \n",
      "     |  __pre_root_validators__ = []\n",
      "     |  \n",
      "     |  __private_attributes__ = {'_lc_kwargs': ModelPrivateAttr(default=Pydan...\n",
      "     |  \n",
      "     |  __schema_cache__ = {}\n",
      "     |  \n",
      "     |  __signature__ = <Signature (*, cache: Optional[bool] = None, ver... 0,...\n",
      "     |  \n",
      "     |  __validators__ = {'verbose': [<pydantic.class_validators.Validator obj...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from langchain.llms.self_hosted.SelfHostedPipeline:\n",
      "     |  \n",
      "     |  from_pipeline(pipeline: Any, hardware: Any, model_reqs: Optional[List[str]] = None, device: int = 0, **kwargs: Any) -> langchain.llms.base.LLM from pydantic.main.ModelMetaclass\n",
      "     |      Init the SelfHostedPipeline from a pipeline object or string.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  __call__(self, prompt: 'str', stop: 'Optional[List[str]]' = None, callbacks: 'Callbacks' = None, *, tags: 'Optional[List[str]]' = None, metadata: 'Optional[Dict[str, Any]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Check Cache and run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  __str__(self) -> 'str'\n",
      "     |      Get a string representation of the object for printing.\n",
      "     |  \n",
      "     |  async abatch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      "     |      Default implementation of abatch, which calls ainvoke N times.\n",
      "     |      Subclasses should override this method if they can batch more efficiently.\n",
      "     |  \n",
      "     |  async agenerate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  async agenerate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Asynchronously pass a sequence of prompts and return model generations.\n",
      "     |      \n",
      "     |      This method should make use of batched calls for models that expose a batched\n",
      "     |      API.\n",
      "     |      \n",
      "     |      Use this method when you want to:\n",
      "     |          1. take advantage of batched calls,\n",
      "     |          2. need more output from the model than just the top generated value,\n",
      "     |          3. are building chains that are agnostic to the underlying language model\n",
      "     |              type (e.g., pure text completion models vs chat models).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      "     |              converted to match the format of any language model (string for pure\n",
      "     |              text generation models and BaseMessages for chat models).\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          callbacks: Callbacks to pass through. Used for executing additional\n",
      "     |              functionality, such as logging or streaming, throughout generation.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          An LLMResult, which contains a list of candidate Generations for each input\n",
      "     |              prompt and additional model provider-specific output.\n",
      "     |  \n",
      "     |  async ainvoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Default implementation of ainvoke, which calls invoke in a thread pool.\n",
      "     |      Subclasses should override this method if they can run asynchronously.\n",
      "     |  \n",
      "     |  async apredict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Asynchronously pass a string to the model and return a string prediction.\n",
      "     |      \n",
      "     |      Use this method when calling pure text generation models and only the top\n",
      "     |          candidate generation is needed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: String input to pass to the model.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a string.\n",
      "     |  \n",
      "     |  async apredict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      "     |      Asynchronously pass messages to the model and return a message prediction.\n",
      "     |      \n",
      "     |      Use this method when calling chat models and only the top\n",
      "     |          candidate generation is needed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: A sequence of chat messages corresponding to a single model input.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a message.\n",
      "     |  \n",
      "     |  async astream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'AsyncIterator[str]'\n",
      "     |      Default implementation of astream, which calls ainvoke.\n",
      "     |      Subclasses should override this method if they support streaming output.\n",
      "     |  \n",
      "     |  batch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      "     |      Default implementation of batch, which calls invoke N times.\n",
      "     |      Subclasses should override this method if they can batch more efficiently.\n",
      "     |  \n",
      "     |  dict(self, **kwargs: 'Any') -> 'Dict'\n",
      "     |      Return a dictionary of the LLM.\n",
      "     |  \n",
      "     |  generate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  generate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Pass a sequence of prompts to the model and return model generations.\n",
      "     |      \n",
      "     |      This method should make use of batched calls for models that expose a batched\n",
      "     |      API.\n",
      "     |      \n",
      "     |      Use this method when you want to:\n",
      "     |          1. take advantage of batched calls,\n",
      "     |          2. need more output from the model than just the top generated value,\n",
      "     |          3. are building chains that are agnostic to the underlying language model\n",
      "     |              type (e.g., pure text completion models vs chat models).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      "     |              converted to match the format of any language model (string for pure\n",
      "     |              text generation models and BaseMessages for chat models).\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          callbacks: Callbacks to pass through. Used for executing additional\n",
      "     |              functionality, such as logging or streaming, throughout generation.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          An LLMResult, which contains a list of candidate Generations for each input\n",
      "     |              prompt and additional model provider-specific output.\n",
      "     |  \n",
      "     |  invoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |  \n",
      "     |  predict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Pass a single string input to the model and return a string prediction.\n",
      "     |      \n",
      "     |       Use this method when passing in raw text. If you want to pass in specific\n",
      "     |          types of chat messages, use predict_messages.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: String input to pass to the model.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a string.\n",
      "     |  \n",
      "     |  predict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      "     |      Pass a message sequence to the model and return a message prediction.\n",
      "     |      \n",
      "     |      Use this method when passing in chat messages. If you want to pass in raw text,\n",
      "     |          use predict.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: A sequence of chat messages corresponding to a single model input.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a message.\n",
      "     |  \n",
      "     |  save(self, file_path: 'Union[Path, str]') -> 'None'\n",
      "     |      Save the LLM.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          file_path: Path to file to save the LLM to.\n",
      "     |      \n",
      "     |      Example:\n",
      "     |      .. code-block:: python\n",
      "     |      \n",
      "     |          llm.save(file_path=\"path/llm.yaml\")\n",
      "     |  \n",
      "     |  stream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'Iterator[str]'\n",
      "     |      Default implementation of stream, which calls invoke.\n",
      "     |      Subclasses should override this method if they support streaming output.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  raise_deprecation(values: 'Dict') -> 'Dict' from pydantic.main.ModelMetaclass\n",
      "     |      Raise deprecation warning if callback_manager is used.\n",
      "     |  \n",
      "     |  set_verbose(verbose: 'Optional[bool]') -> 'bool' from pydantic.main.ModelMetaclass\n",
      "     |      If verbose is None, set it.\n",
      "     |      \n",
      "     |      This allows users to pass in None as verbose to access the global setting.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  __orig_bases__ = (langchain.schema.language_model.BaseLanguageModel[st...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.schema.language_model.BaseLanguageModel:\n",
      "     |  \n",
      "     |  get_num_tokens(self, text: 'str') -> 'int'\n",
      "     |      Get the number of tokens present in the text.\n",
      "     |      \n",
      "     |      Useful for checking if an input will fit in a model's context window.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: The string input to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The integer number of tokens in the text.\n",
      "     |  \n",
      "     |  get_num_tokens_from_messages(self, messages: 'List[BaseMessage]') -> 'int'\n",
      "     |      Get the number of tokens in the messages.\n",
      "     |      \n",
      "     |      Useful for checking if an input will fit in a model's context window.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: The message inputs to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The sum of the number of tokens across the messages.\n",
      "     |  \n",
      "     |  get_token_ids(self, text: 'str') -> 'List[int]'\n",
      "     |      Return the ordered ids of the tokens in a text.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: The string input to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          A list of ids corresponding to the tokens in the text, in order they occur\n",
      "     |              in the text.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from langchain.schema.language_model.BaseLanguageModel:\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.load.serializable.Serializable:\n",
      "     |  \n",
      "     |  to_json(self) -> Union[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\n",
      "     |  \n",
      "     |  to_json_not_implemented(self) -> langchain.load.serializable.SerializedNotImplemented\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from langchain.load.serializable.Serializable:\n",
      "     |  \n",
      "     |  lc_attributes\n",
      "     |      Return a list of attribute names that should be included in the\n",
      "     |      serialized kwargs. These attributes must be accepted by the\n",
      "     |      constructor.\n",
      "     |  \n",
      "     |  lc_namespace\n",
      "     |      Return the namespace of the langchain object.\n",
      "     |      eg. [\"langchain\", \"llms\", \"openai\"]\n",
      "     |  \n",
      "     |  lc_secrets\n",
      "     |      Return a map of constructor argument names to secret ids.\n",
      "     |      eg. {\"openai_api_key\": \"OPENAI_API_KEY\"}\n",
      "     |  \n",
      "     |  lc_serializable\n",
      "     |      Return whether or not the class is serializable.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __eq__(self, other: Any) -> bool\n",
      "     |  \n",
      "     |  __getstate__(self) -> 'DictAny'\n",
      "     |  \n",
      "     |  __iter__(self) -> 'TupleGenerator'\n",
      "     |      so `dict(model)` works\n",
      "     |  \n",
      "     |  __repr_args__(self) -> 'ReprArgs'\n",
      "     |  \n",
      "     |  __setattr__(self, name, value)\n",
      "     |  \n",
      "     |  __setstate__(self, state: 'DictAny') -> None\n",
      "     |  \n",
      "     |  copy(self: 'Model', *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, update: Optional[ForwardRef('DictStrAny')] = None, deep: bool = False) -> 'Model'\n",
      "     |      Duplicate a model, optionally choose which fields to include, exclude and change.\n",
      "     |      \n",
      "     |      :param include: fields to include in new model\n",
      "     |      :param exclude: fields to exclude from new model, as with values this takes precedence over include\n",
      "     |      :param update: values to change/add in the new model. Note: the data is not validated before creating\n",
      "     |          the new model: you should trust this data\n",
      "     |      :param deep: set to `True` to make a deep copy of the model\n",
      "     |      :return: new model instance\n",
      "     |  \n",
      "     |  json(self, *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) -> 'unicode'\n",
      "     |      Generate a JSON representation of the model, `include` and `exclude` arguments as per `dict()`.\n",
      "     |      \n",
      "     |      `encoder` is an optional function to supply as `default` to json.dumps(), other arguments as per `json.dumps()`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __get_validators__() -> 'CallableGenerator' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  __try_update_forward_refs__(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      "     |      Same as update_forward_refs but will not raise exception\n",
      "     |      when forward references are not defined.\n",
      "     |  \n",
      "     |  construct(_fields_set: Optional[ForwardRef('SetStr')] = None, **values: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |      Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data.\n",
      "     |      Default values are respected, but no other validation is performed.\n",
      "     |      Behaves as if `Config.extra = 'allow'` was set since it adds all passed values\n",
      "     |  \n",
      "     |  from_orm(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_file(path: Union[str, pathlib.Path], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_obj(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_raw(b: Union[str, bytes], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema(by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}') -> 'DictStrAny' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema_json(*, by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}', **dumps_kwargs: Any) -> 'unicode' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  update_forward_refs(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      "     |      Try to update ForwardRefs on fields based on this Model, globalns and localns.\n",
      "     |  \n",
      "     |  validate(value: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __fields_set__\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.utils.Representation:\n",
      "     |  \n",
      "     |  __pretty__(self, fmt: Callable[[Any], Any], **kwargs: Any) -> Generator[Any, NoneType, NoneType]\n",
      "     |      Used by devtools (https://python-devtools.helpmanual.io/) to provide a human readable representations of objects\n",
      "     |  \n",
      "     |  __repr__(self) -> 'unicode'\n",
      "     |  \n",
      "     |  __repr_name__(self) -> 'unicode'\n",
      "     |      Name of the instance's class, used in __repr__.\n",
      "     |  \n",
      "     |  __repr_str__(self, join_str: 'unicode') -> 'unicode'\n",
      "     |  \n",
      "     |  __rich_repr__(self) -> 'RichReprResult'\n",
      "     |      Get fields for Rich library\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.schema.runnable.base.Runnable:\n",
      "     |  \n",
      "     |  __or__(self, other: 'Union[Runnable[Any, Other], Callable[[Any], Other], Mapping[str, Union[Runnable[Any, Other], Callable[[Any], Other], Any]]]') -> 'RunnableSequence[Input, Other]'\n",
      "     |      Return self|value.\n",
      "     |  \n",
      "     |  __ror__(self, other: 'Union[Runnable[Other, Any], Callable[[Any], Other], Mapping[str, Union[Runnable[Other, Any], Callable[[Other], Any], Any]]]') -> 'RunnableSequence[Other, Output]'\n",
      "     |      Return value|self.\n",
      "     |  \n",
      "     |  async atransform(self, input: 'AsyncIterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'AsyncIterator[Output]'\n",
      "     |      Default implementation of atransform, which buffers input and calls astream.\n",
      "     |      Subclasses should override this method if they can start producing output while\n",
      "     |      input is still being generated.\n",
      "     |  \n",
      "     |  bind(self, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      "     |      Bind arguments to a Runnable, returning a new Runnable.\n",
      "     |  \n",
      "     |  map(self) -> 'Runnable[List[Input], List[Output]]'\n",
      "     |      Return a new Runnable that maps a list of inputs to a list of outputs,\n",
      "     |      by calling invoke() with each input.\n",
      "     |  \n",
      "     |  transform(self, input: 'Iterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'Iterator[Output]'\n",
      "     |      Default implementation of transform, which buffers input and then calls stream.\n",
      "     |      Subclasses should override this method if they can start producing output while\n",
      "     |      input is still being generated.\n",
      "     |  \n",
      "     |  with_config(self, config: 'Optional[RunnableConfig]' = None, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      "     |      Bind config to a Runnable, returning a new Runnable.\n",
      "     |  \n",
      "     |  with_fallbacks(self, fallbacks: 'Sequence[Runnable[Input, Output]]', *, exceptions_to_handle: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,)) -> 'RunnableWithFallbacks[Input, Output]'\n",
      "     |  \n",
      "     |  with_retry(self, *, retry_if_exception_type: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,), wait_exponential_jitter: 'bool' = True, stop_after_attempt: 'int' = 3) -> 'Runnable[Input, Output]'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __class_getitem__(params) from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  __init_subclass__(*args, **kwargs) from pydantic.main.ModelMetaclass\n",
      "     |      This method is called when a class is subclassed.\n",
      "     |      \n",
      "     |      The default implementation does nothing. It may be\n",
      "     |      overridden to extend subclasses.\n",
      "    \n",
      "    class SelfHostedPipeline(langchain.llms.base.LLM)\n",
      "     |  SelfHostedPipeline(*, cache: Optional[bool] = None, verbose: bool = None, callbacks: Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager, NoneType] = None, callback_manager: Optional[langchain.callbacks.base.BaseCallbackManager] = None, tags: Optional[List[str]] = None, metadata: Optional[Dict[str, Any]] = None, pipeline_ref: Any = None, client: Any = None, inference_fn: Callable = <function _generate_text at 0x00000227B06BBD90>, hardware: Any = None, model_load_fn: Callable, load_fn_kwargs: Optional[dict] = None, model_reqs: List[str] = ['./', 'torch']) -> None\n",
      "     |  \n",
      "     |  Model inference on self-hosted remote hardware.\n",
      "     |  \n",
      "     |  Supported hardware includes auto-launched instances on AWS, GCP, Azure,\n",
      "     |  and Lambda, as well as servers specified\n",
      "     |  by IP address and SSH credentials (such as on-prem, or another\n",
      "     |  cloud like Paperspace, Coreweave, etc.).\n",
      "     |  \n",
      "     |  To use, you should have the ``runhouse`` python package installed.\n",
      "     |  \n",
      "     |  Example for custom pipeline and inference functions:\n",
      "     |      .. code-block:: python\n",
      "     |  \n",
      "     |          from langchain.llms import SelfHostedPipeline\n",
      "     |          from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
      "     |          import runhouse as rh\n",
      "     |  \n",
      "     |          def load_pipeline():\n",
      "     |              tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
      "     |              model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
      "     |              return pipeline(\n",
      "     |                  \"text-generation\", model=model, tokenizer=tokenizer,\n",
      "     |                  max_new_tokens=10\n",
      "     |              )\n",
      "     |          def inference_fn(pipeline, prompt, stop = None):\n",
      "     |              return pipeline(prompt)[0][\"generated_text\"]\n",
      "     |  \n",
      "     |          gpu = rh.cluster(name=\"rh-a10x\", instance_type=\"A100:1\")\n",
      "     |          llm = SelfHostedPipeline(\n",
      "     |              model_load_fn=load_pipeline,\n",
      "     |              hardware=gpu,\n",
      "     |              model_reqs=model_reqs, inference_fn=inference_fn\n",
      "     |          )\n",
      "     |  Example for <2GB model (can be serialized and sent directly to the server):\n",
      "     |      .. code-block:: python\n",
      "     |  \n",
      "     |          from langchain.llms import SelfHostedPipeline\n",
      "     |          import runhouse as rh\n",
      "     |          gpu = rh.cluster(name=\"rh-a10x\", instance_type=\"A100:1\")\n",
      "     |          my_model = ...\n",
      "     |          llm = SelfHostedPipeline.from_pipeline(\n",
      "     |              pipeline=my_model,\n",
      "     |              hardware=gpu,\n",
      "     |              model_reqs=[\"./\", \"torch\", \"transformers\"],\n",
      "     |          )\n",
      "     |  Example passing model path for larger models:\n",
      "     |      .. code-block:: python\n",
      "     |  \n",
      "     |          from langchain.llms import SelfHostedPipeline\n",
      "     |          import runhouse as rh\n",
      "     |          import pickle\n",
      "     |          from transformers import pipeline\n",
      "     |  \n",
      "     |          generator = pipeline(model=\"gpt2\")\n",
      "     |          rh.blob(pickle.dumps(generator), path=\"models/pipeline.pkl\"\n",
      "     |              ).save().to(gpu, path=\"models\")\n",
      "     |          llm = SelfHostedPipeline.from_pipeline(\n",
      "     |              pipeline=\"models/pipeline.pkl\",\n",
      "     |              hardware=gpu,\n",
      "     |              model_reqs=[\"./\", \"torch\", \"transformers\"],\n",
      "     |          )\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      SelfHostedPipeline\n",
      "     |      langchain.llms.base.LLM\n",
      "     |      langchain.llms.base.BaseLLM\n",
      "     |      langchain.schema.language_model.BaseLanguageModel\n",
      "     |      langchain.load.serializable.Serializable\n",
      "     |      pydantic.main.BaseModel\n",
      "     |      pydantic.utils.Representation\n",
      "     |      langchain.schema.runnable.base.Runnable\n",
      "     |      typing.Generic\n",
      "     |      abc.ABC\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, **kwargs: Any)\n",
      "     |      Init the pipeline with an auxiliary function.\n",
      "     |      \n",
      "     |      The load function must be in global scope to be imported\n",
      "     |      and run on the server, i.e. in a module and not a REPL or closure.\n",
      "     |      Then, initialize the remote inference function.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  from_pipeline(pipeline: Any, hardware: Any, model_reqs: Optional[List[str]] = None, device: int = 0, **kwargs: Any) -> langchain.llms.base.LLM from pydantic.main.ModelMetaclass\n",
      "     |      Init the SelfHostedPipeline from a pipeline object or string.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __json_encoder__ = pydantic_encoder(obj: Any) -> Any\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  Config = <class 'langchain.llms.self_hosted.SelfHostedPipeline.Config'...\n",
      "     |      Configuration for this pydantic object.\n",
      "     |  \n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {'client': typing.Any, 'hardware': typing.Any, 'infe...\n",
      "     |  \n",
      "     |  __class_vars__ = set()\n",
      "     |  \n",
      "     |  __config__ = <class 'langchain.llms.self_hosted.Config'>\n",
      "     |  \n",
      "     |  __custom_root_type__ = False\n",
      "     |  \n",
      "     |  __exclude_fields__ = {'callback_manager': True, 'callbacks': True, 'me...\n",
      "     |  \n",
      "     |  __fields__ = {'cache': ModelField(name='cache', type=Optional[bool], r...\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "     |  \n",
      "     |  __include_fields__ = None\n",
      "     |  \n",
      "     |  __parameters__ = ()\n",
      "     |  \n",
      "     |  __post_root_validators__ = [(False, <function BaseLLM.raise_deprecatio...\n",
      "     |  \n",
      "     |  __pre_root_validators__ = []\n",
      "     |  \n",
      "     |  __private_attributes__ = {'_lc_kwargs': ModelPrivateAttr(default=Pydan...\n",
      "     |  \n",
      "     |  __schema_cache__ = {}\n",
      "     |  \n",
      "     |  __signature__ = <Signature (*, cache: Optional[bool] = None, ver...mod...\n",
      "     |  \n",
      "     |  __validators__ = {'verbose': [<pydantic.class_validators.Validator obj...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  __call__(self, prompt: 'str', stop: 'Optional[List[str]]' = None, callbacks: 'Callbacks' = None, *, tags: 'Optional[List[str]]' = None, metadata: 'Optional[Dict[str, Any]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Check Cache and run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  __str__(self) -> 'str'\n",
      "     |      Get a string representation of the object for printing.\n",
      "     |  \n",
      "     |  async abatch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      "     |      Default implementation of abatch, which calls ainvoke N times.\n",
      "     |      Subclasses should override this method if they can batch more efficiently.\n",
      "     |  \n",
      "     |  async agenerate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  async agenerate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Asynchronously pass a sequence of prompts and return model generations.\n",
      "     |      \n",
      "     |      This method should make use of batched calls for models that expose a batched\n",
      "     |      API.\n",
      "     |      \n",
      "     |      Use this method when you want to:\n",
      "     |          1. take advantage of batched calls,\n",
      "     |          2. need more output from the model than just the top generated value,\n",
      "     |          3. are building chains that are agnostic to the underlying language model\n",
      "     |              type (e.g., pure text completion models vs chat models).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      "     |              converted to match the format of any language model (string for pure\n",
      "     |              text generation models and BaseMessages for chat models).\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          callbacks: Callbacks to pass through. Used for executing additional\n",
      "     |              functionality, such as logging or streaming, throughout generation.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          An LLMResult, which contains a list of candidate Generations for each input\n",
      "     |              prompt and additional model provider-specific output.\n",
      "     |  \n",
      "     |  async ainvoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Default implementation of ainvoke, which calls invoke in a thread pool.\n",
      "     |      Subclasses should override this method if they can run asynchronously.\n",
      "     |  \n",
      "     |  async apredict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Asynchronously pass a string to the model and return a string prediction.\n",
      "     |      \n",
      "     |      Use this method when calling pure text generation models and only the top\n",
      "     |          candidate generation is needed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: String input to pass to the model.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a string.\n",
      "     |  \n",
      "     |  async apredict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      "     |      Asynchronously pass messages to the model and return a message prediction.\n",
      "     |      \n",
      "     |      Use this method when calling chat models and only the top\n",
      "     |          candidate generation is needed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: A sequence of chat messages corresponding to a single model input.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a message.\n",
      "     |  \n",
      "     |  async astream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'AsyncIterator[str]'\n",
      "     |      Default implementation of astream, which calls ainvoke.\n",
      "     |      Subclasses should override this method if they support streaming output.\n",
      "     |  \n",
      "     |  batch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      "     |      Default implementation of batch, which calls invoke N times.\n",
      "     |      Subclasses should override this method if they can batch more efficiently.\n",
      "     |  \n",
      "     |  dict(self, **kwargs: 'Any') -> 'Dict'\n",
      "     |      Return a dictionary of the LLM.\n",
      "     |  \n",
      "     |  generate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  generate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Pass a sequence of prompts to the model and return model generations.\n",
      "     |      \n",
      "     |      This method should make use of batched calls for models that expose a batched\n",
      "     |      API.\n",
      "     |      \n",
      "     |      Use this method when you want to:\n",
      "     |          1. take advantage of batched calls,\n",
      "     |          2. need more output from the model than just the top generated value,\n",
      "     |          3. are building chains that are agnostic to the underlying language model\n",
      "     |              type (e.g., pure text completion models vs chat models).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      "     |              converted to match the format of any language model (string for pure\n",
      "     |              text generation models and BaseMessages for chat models).\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          callbacks: Callbacks to pass through. Used for executing additional\n",
      "     |              functionality, such as logging or streaming, throughout generation.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          An LLMResult, which contains a list of candidate Generations for each input\n",
      "     |              prompt and additional model provider-specific output.\n",
      "     |  \n",
      "     |  invoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |  \n",
      "     |  predict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Pass a single string input to the model and return a string prediction.\n",
      "     |      \n",
      "     |       Use this method when passing in raw text. If you want to pass in specific\n",
      "     |          types of chat messages, use predict_messages.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: String input to pass to the model.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a string.\n",
      "     |  \n",
      "     |  predict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      "     |      Pass a message sequence to the model and return a message prediction.\n",
      "     |      \n",
      "     |      Use this method when passing in chat messages. If you want to pass in raw text,\n",
      "     |          use predict.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: A sequence of chat messages corresponding to a single model input.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a message.\n",
      "     |  \n",
      "     |  save(self, file_path: 'Union[Path, str]') -> 'None'\n",
      "     |      Save the LLM.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          file_path: Path to file to save the LLM to.\n",
      "     |      \n",
      "     |      Example:\n",
      "     |      .. code-block:: python\n",
      "     |      \n",
      "     |          llm.save(file_path=\"path/llm.yaml\")\n",
      "     |  \n",
      "     |  stream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'Iterator[str]'\n",
      "     |      Default implementation of stream, which calls invoke.\n",
      "     |      Subclasses should override this method if they support streaming output.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  raise_deprecation(values: 'Dict') -> 'Dict' from pydantic.main.ModelMetaclass\n",
      "     |      Raise deprecation warning if callback_manager is used.\n",
      "     |  \n",
      "     |  set_verbose(verbose: 'Optional[bool]') -> 'bool' from pydantic.main.ModelMetaclass\n",
      "     |      If verbose is None, set it.\n",
      "     |      \n",
      "     |      This allows users to pass in None as verbose to access the global setting.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  __orig_bases__ = (langchain.schema.language_model.BaseLanguageModel[st...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.schema.language_model.BaseLanguageModel:\n",
      "     |  \n",
      "     |  get_num_tokens(self, text: 'str') -> 'int'\n",
      "     |      Get the number of tokens present in the text.\n",
      "     |      \n",
      "     |      Useful for checking if an input will fit in a model's context window.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: The string input to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The integer number of tokens in the text.\n",
      "     |  \n",
      "     |  get_num_tokens_from_messages(self, messages: 'List[BaseMessage]') -> 'int'\n",
      "     |      Get the number of tokens in the messages.\n",
      "     |      \n",
      "     |      Useful for checking if an input will fit in a model's context window.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: The message inputs to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The sum of the number of tokens across the messages.\n",
      "     |  \n",
      "     |  get_token_ids(self, text: 'str') -> 'List[int]'\n",
      "     |      Return the ordered ids of the tokens in a text.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: The string input to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          A list of ids corresponding to the tokens in the text, in order they occur\n",
      "     |              in the text.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from langchain.schema.language_model.BaseLanguageModel:\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.load.serializable.Serializable:\n",
      "     |  \n",
      "     |  to_json(self) -> Union[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\n",
      "     |  \n",
      "     |  to_json_not_implemented(self) -> langchain.load.serializable.SerializedNotImplemented\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from langchain.load.serializable.Serializable:\n",
      "     |  \n",
      "     |  lc_attributes\n",
      "     |      Return a list of attribute names that should be included in the\n",
      "     |      serialized kwargs. These attributes must be accepted by the\n",
      "     |      constructor.\n",
      "     |  \n",
      "     |  lc_namespace\n",
      "     |      Return the namespace of the langchain object.\n",
      "     |      eg. [\"langchain\", \"llms\", \"openai\"]\n",
      "     |  \n",
      "     |  lc_secrets\n",
      "     |      Return a map of constructor argument names to secret ids.\n",
      "     |      eg. {\"openai_api_key\": \"OPENAI_API_KEY\"}\n",
      "     |  \n",
      "     |  lc_serializable\n",
      "     |      Return whether or not the class is serializable.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __eq__(self, other: Any) -> bool\n",
      "     |  \n",
      "     |  __getstate__(self) -> 'DictAny'\n",
      "     |  \n",
      "     |  __iter__(self) -> 'TupleGenerator'\n",
      "     |      so `dict(model)` works\n",
      "     |  \n",
      "     |  __repr_args__(self) -> 'ReprArgs'\n",
      "     |  \n",
      "     |  __setattr__(self, name, value)\n",
      "     |  \n",
      "     |  __setstate__(self, state: 'DictAny') -> None\n",
      "     |  \n",
      "     |  copy(self: 'Model', *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, update: Optional[ForwardRef('DictStrAny')] = None, deep: bool = False) -> 'Model'\n",
      "     |      Duplicate a model, optionally choose which fields to include, exclude and change.\n",
      "     |      \n",
      "     |      :param include: fields to include in new model\n",
      "     |      :param exclude: fields to exclude from new model, as with values this takes precedence over include\n",
      "     |      :param update: values to change/add in the new model. Note: the data is not validated before creating\n",
      "     |          the new model: you should trust this data\n",
      "     |      :param deep: set to `True` to make a deep copy of the model\n",
      "     |      :return: new model instance\n",
      "     |  \n",
      "     |  json(self, *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) -> 'unicode'\n",
      "     |      Generate a JSON representation of the model, `include` and `exclude` arguments as per `dict()`.\n",
      "     |      \n",
      "     |      `encoder` is an optional function to supply as `default` to json.dumps(), other arguments as per `json.dumps()`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __get_validators__() -> 'CallableGenerator' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  __try_update_forward_refs__(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      "     |      Same as update_forward_refs but will not raise exception\n",
      "     |      when forward references are not defined.\n",
      "     |  \n",
      "     |  construct(_fields_set: Optional[ForwardRef('SetStr')] = None, **values: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |      Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data.\n",
      "     |      Default values are respected, but no other validation is performed.\n",
      "     |      Behaves as if `Config.extra = 'allow'` was set since it adds all passed values\n",
      "     |  \n",
      "     |  from_orm(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_file(path: Union[str, pathlib.Path], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_obj(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_raw(b: Union[str, bytes], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema(by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}') -> 'DictStrAny' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema_json(*, by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}', **dumps_kwargs: Any) -> 'unicode' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  update_forward_refs(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      "     |      Try to update ForwardRefs on fields based on this Model, globalns and localns.\n",
      "     |  \n",
      "     |  validate(value: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __fields_set__\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.utils.Representation:\n",
      "     |  \n",
      "     |  __pretty__(self, fmt: Callable[[Any], Any], **kwargs: Any) -> Generator[Any, NoneType, NoneType]\n",
      "     |      Used by devtools (https://python-devtools.helpmanual.io/) to provide a human readable representations of objects\n",
      "     |  \n",
      "     |  __repr__(self) -> 'unicode'\n",
      "     |  \n",
      "     |  __repr_name__(self) -> 'unicode'\n",
      "     |      Name of the instance's class, used in __repr__.\n",
      "     |  \n",
      "     |  __repr_str__(self, join_str: 'unicode') -> 'unicode'\n",
      "     |  \n",
      "     |  __rich_repr__(self) -> 'RichReprResult'\n",
      "     |      Get fields for Rich library\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.schema.runnable.base.Runnable:\n",
      "     |  \n",
      "     |  __or__(self, other: 'Union[Runnable[Any, Other], Callable[[Any], Other], Mapping[str, Union[Runnable[Any, Other], Callable[[Any], Other], Any]]]') -> 'RunnableSequence[Input, Other]'\n",
      "     |      Return self|value.\n",
      "     |  \n",
      "     |  __ror__(self, other: 'Union[Runnable[Other, Any], Callable[[Any], Other], Mapping[str, Union[Runnable[Other, Any], Callable[[Other], Any], Any]]]') -> 'RunnableSequence[Other, Output]'\n",
      "     |      Return value|self.\n",
      "     |  \n",
      "     |  async atransform(self, input: 'AsyncIterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'AsyncIterator[Output]'\n",
      "     |      Default implementation of atransform, which buffers input and calls astream.\n",
      "     |      Subclasses should override this method if they can start producing output while\n",
      "     |      input is still being generated.\n",
      "     |  \n",
      "     |  bind(self, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      "     |      Bind arguments to a Runnable, returning a new Runnable.\n",
      "     |  \n",
      "     |  map(self) -> 'Runnable[List[Input], List[Output]]'\n",
      "     |      Return a new Runnable that maps a list of inputs to a list of outputs,\n",
      "     |      by calling invoke() with each input.\n",
      "     |  \n",
      "     |  transform(self, input: 'Iterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'Iterator[Output]'\n",
      "     |      Default implementation of transform, which buffers input and then calls stream.\n",
      "     |      Subclasses should override this method if they can start producing output while\n",
      "     |      input is still being generated.\n",
      "     |  \n",
      "     |  with_config(self, config: 'Optional[RunnableConfig]' = None, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      "     |      Bind config to a Runnable, returning a new Runnable.\n",
      "     |  \n",
      "     |  with_fallbacks(self, fallbacks: 'Sequence[Runnable[Input, Output]]', *, exceptions_to_handle: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,)) -> 'RunnableWithFallbacks[Input, Output]'\n",
      "     |  \n",
      "     |  with_retry(self, *, retry_if_exception_type: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,), wait_exponential_jitter: 'bool' = True, stop_after_attempt: 'int' = 3) -> 'Runnable[Input, Output]'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __class_getitem__(params) from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  __init_subclass__(*args, **kwargs) from pydantic.main.ModelMetaclass\n",
      "     |      This method is called when a class is subclassed.\n",
      "     |      \n",
      "     |      The default implementation does nothing. It may be\n",
      "     |      overridden to extend subclasses.\n",
      "    \n",
      "    class StochasticAI(langchain.llms.base.LLM)\n",
      "     |  StochasticAI(*, cache: Optional[bool] = None, verbose: bool = None, callbacks: Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager, NoneType] = None, callback_manager: Optional[langchain.callbacks.base.BaseCallbackManager] = None, tags: Optional[List[str]] = None, metadata: Optional[Dict[str, Any]] = None, api_url: str = '', model_kwargs: Dict[str, Any] = None, stochasticai_api_key: Optional[str] = None) -> None\n",
      "     |  \n",
      "     |  StochasticAI large language models.\n",
      "     |  \n",
      "     |  To use, you should have the environment variable ``STOCHASTICAI_API_KEY``\n",
      "     |  set with your API key.\n",
      "     |  \n",
      "     |  Example:\n",
      "     |      .. code-block:: python\n",
      "     |  \n",
      "     |          from langchain.llms import StochasticAI\n",
      "     |          stochasticai = StochasticAI(api_url=\"\")\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      StochasticAI\n",
      "     |      langchain.llms.base.LLM\n",
      "     |      langchain.llms.base.BaseLLM\n",
      "     |      langchain.schema.language_model.BaseLanguageModel\n",
      "     |      langchain.load.serializable.Serializable\n",
      "     |      pydantic.main.BaseModel\n",
      "     |      pydantic.utils.Representation\n",
      "     |      langchain.schema.runnable.base.Runnable\n",
      "     |      typing.Generic\n",
      "     |      abc.ABC\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  build_extra(values: Dict[str, Any]) -> Dict[str, Any] from pydantic.main.ModelMetaclass\n",
      "     |      Build extra kwargs from additional params that were passed in.\n",
      "     |  \n",
      "     |  validate_environment(values: Dict) -> Dict from pydantic.main.ModelMetaclass\n",
      "     |      Validate that api key exists in environment.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __json_encoder__ = pydantic_encoder(obj: Any) -> Any\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  Config = <class 'langchain.llms.stochasticai.StochasticAI.Config'>\n",
      "     |      Configuration for this pydantic object.\n",
      "     |  \n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {'api_url': <class 'str'>, 'model_kwargs': typing.Di...\n",
      "     |  \n",
      "     |  __class_vars__ = set()\n",
      "     |  \n",
      "     |  __config__ = <class 'langchain.llms.stochasticai.Config'>\n",
      "     |  \n",
      "     |  __custom_root_type__ = False\n",
      "     |  \n",
      "     |  __exclude_fields__ = {'callback_manager': True, 'callbacks': True, 'me...\n",
      "     |  \n",
      "     |  __fields__ = {'api_url': ModelField(name='api_url', type=str, required...\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "     |  \n",
      "     |  __include_fields__ = None\n",
      "     |  \n",
      "     |  __parameters__ = ()\n",
      "     |  \n",
      "     |  __post_root_validators__ = [(False, <function BaseLLM.raise_deprecatio...\n",
      "     |  \n",
      "     |  __pre_root_validators__ = [<function StochasticAI.build_extra>]\n",
      "     |  \n",
      "     |  __private_attributes__ = {'_lc_kwargs': ModelPrivateAttr(default=Pydan...\n",
      "     |  \n",
      "     |  __schema_cache__ = {}\n",
      "     |  \n",
      "     |  __signature__ = <Signature (*, cache: Optional[bool] = None, ver...cha...\n",
      "     |  \n",
      "     |  __validators__ = {'verbose': [<pydantic.class_validators.Validator obj...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  __call__(self, prompt: 'str', stop: 'Optional[List[str]]' = None, callbacks: 'Callbacks' = None, *, tags: 'Optional[List[str]]' = None, metadata: 'Optional[Dict[str, Any]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Check Cache and run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  __str__(self) -> 'str'\n",
      "     |      Get a string representation of the object for printing.\n",
      "     |  \n",
      "     |  async abatch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      "     |      Default implementation of abatch, which calls ainvoke N times.\n",
      "     |      Subclasses should override this method if they can batch more efficiently.\n",
      "     |  \n",
      "     |  async agenerate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  async agenerate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Asynchronously pass a sequence of prompts and return model generations.\n",
      "     |      \n",
      "     |      This method should make use of batched calls for models that expose a batched\n",
      "     |      API.\n",
      "     |      \n",
      "     |      Use this method when you want to:\n",
      "     |          1. take advantage of batched calls,\n",
      "     |          2. need more output from the model than just the top generated value,\n",
      "     |          3. are building chains that are agnostic to the underlying language model\n",
      "     |              type (e.g., pure text completion models vs chat models).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      "     |              converted to match the format of any language model (string for pure\n",
      "     |              text generation models and BaseMessages for chat models).\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          callbacks: Callbacks to pass through. Used for executing additional\n",
      "     |              functionality, such as logging or streaming, throughout generation.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          An LLMResult, which contains a list of candidate Generations for each input\n",
      "     |              prompt and additional model provider-specific output.\n",
      "     |  \n",
      "     |  async ainvoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Default implementation of ainvoke, which calls invoke in a thread pool.\n",
      "     |      Subclasses should override this method if they can run asynchronously.\n",
      "     |  \n",
      "     |  async apredict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Asynchronously pass a string to the model and return a string prediction.\n",
      "     |      \n",
      "     |      Use this method when calling pure text generation models and only the top\n",
      "     |          candidate generation is needed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: String input to pass to the model.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a string.\n",
      "     |  \n",
      "     |  async apredict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      "     |      Asynchronously pass messages to the model and return a message prediction.\n",
      "     |      \n",
      "     |      Use this method when calling chat models and only the top\n",
      "     |          candidate generation is needed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: A sequence of chat messages corresponding to a single model input.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a message.\n",
      "     |  \n",
      "     |  async astream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'AsyncIterator[str]'\n",
      "     |      Default implementation of astream, which calls ainvoke.\n",
      "     |      Subclasses should override this method if they support streaming output.\n",
      "     |  \n",
      "     |  batch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      "     |      Default implementation of batch, which calls invoke N times.\n",
      "     |      Subclasses should override this method if they can batch more efficiently.\n",
      "     |  \n",
      "     |  dict(self, **kwargs: 'Any') -> 'Dict'\n",
      "     |      Return a dictionary of the LLM.\n",
      "     |  \n",
      "     |  generate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  generate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Pass a sequence of prompts to the model and return model generations.\n",
      "     |      \n",
      "     |      This method should make use of batched calls for models that expose a batched\n",
      "     |      API.\n",
      "     |      \n",
      "     |      Use this method when you want to:\n",
      "     |          1. take advantage of batched calls,\n",
      "     |          2. need more output from the model than just the top generated value,\n",
      "     |          3. are building chains that are agnostic to the underlying language model\n",
      "     |              type (e.g., pure text completion models vs chat models).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      "     |              converted to match the format of any language model (string for pure\n",
      "     |              text generation models and BaseMessages for chat models).\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          callbacks: Callbacks to pass through. Used for executing additional\n",
      "     |              functionality, such as logging or streaming, throughout generation.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          An LLMResult, which contains a list of candidate Generations for each input\n",
      "     |              prompt and additional model provider-specific output.\n",
      "     |  \n",
      "     |  invoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |  \n",
      "     |  predict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Pass a single string input to the model and return a string prediction.\n",
      "     |      \n",
      "     |       Use this method when passing in raw text. If you want to pass in specific\n",
      "     |          types of chat messages, use predict_messages.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: String input to pass to the model.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a string.\n",
      "     |  \n",
      "     |  predict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      "     |      Pass a message sequence to the model and return a message prediction.\n",
      "     |      \n",
      "     |      Use this method when passing in chat messages. If you want to pass in raw text,\n",
      "     |          use predict.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: A sequence of chat messages corresponding to a single model input.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a message.\n",
      "     |  \n",
      "     |  save(self, file_path: 'Union[Path, str]') -> 'None'\n",
      "     |      Save the LLM.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          file_path: Path to file to save the LLM to.\n",
      "     |      \n",
      "     |      Example:\n",
      "     |      .. code-block:: python\n",
      "     |      \n",
      "     |          llm.save(file_path=\"path/llm.yaml\")\n",
      "     |  \n",
      "     |  stream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'Iterator[str]'\n",
      "     |      Default implementation of stream, which calls invoke.\n",
      "     |      Subclasses should override this method if they support streaming output.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  raise_deprecation(values: 'Dict') -> 'Dict' from pydantic.main.ModelMetaclass\n",
      "     |      Raise deprecation warning if callback_manager is used.\n",
      "     |  \n",
      "     |  set_verbose(verbose: 'Optional[bool]') -> 'bool' from pydantic.main.ModelMetaclass\n",
      "     |      If verbose is None, set it.\n",
      "     |      \n",
      "     |      This allows users to pass in None as verbose to access the global setting.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  __orig_bases__ = (langchain.schema.language_model.BaseLanguageModel[st...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.schema.language_model.BaseLanguageModel:\n",
      "     |  \n",
      "     |  get_num_tokens(self, text: 'str') -> 'int'\n",
      "     |      Get the number of tokens present in the text.\n",
      "     |      \n",
      "     |      Useful for checking if an input will fit in a model's context window.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: The string input to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The integer number of tokens in the text.\n",
      "     |  \n",
      "     |  get_num_tokens_from_messages(self, messages: 'List[BaseMessage]') -> 'int'\n",
      "     |      Get the number of tokens in the messages.\n",
      "     |      \n",
      "     |      Useful for checking if an input will fit in a model's context window.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: The message inputs to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The sum of the number of tokens across the messages.\n",
      "     |  \n",
      "     |  get_token_ids(self, text: 'str') -> 'List[int]'\n",
      "     |      Return the ordered ids of the tokens in a text.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: The string input to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          A list of ids corresponding to the tokens in the text, in order they occur\n",
      "     |              in the text.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from langchain.schema.language_model.BaseLanguageModel:\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.load.serializable.Serializable:\n",
      "     |  \n",
      "     |  __init__(self, **kwargs: Any) -> None\n",
      "     |      Create a new model by parsing and validating input data from keyword arguments.\n",
      "     |      \n",
      "     |      Raises ValidationError if the input data cannot be parsed to form a valid model.\n",
      "     |  \n",
      "     |  to_json(self) -> Union[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\n",
      "     |  \n",
      "     |  to_json_not_implemented(self) -> langchain.load.serializable.SerializedNotImplemented\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from langchain.load.serializable.Serializable:\n",
      "     |  \n",
      "     |  lc_attributes\n",
      "     |      Return a list of attribute names that should be included in the\n",
      "     |      serialized kwargs. These attributes must be accepted by the\n",
      "     |      constructor.\n",
      "     |  \n",
      "     |  lc_namespace\n",
      "     |      Return the namespace of the langchain object.\n",
      "     |      eg. [\"langchain\", \"llms\", \"openai\"]\n",
      "     |  \n",
      "     |  lc_secrets\n",
      "     |      Return a map of constructor argument names to secret ids.\n",
      "     |      eg. {\"openai_api_key\": \"OPENAI_API_KEY\"}\n",
      "     |  \n",
      "     |  lc_serializable\n",
      "     |      Return whether or not the class is serializable.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __eq__(self, other: Any) -> bool\n",
      "     |  \n",
      "     |  __getstate__(self) -> 'DictAny'\n",
      "     |  \n",
      "     |  __iter__(self) -> 'TupleGenerator'\n",
      "     |      so `dict(model)` works\n",
      "     |  \n",
      "     |  __repr_args__(self) -> 'ReprArgs'\n",
      "     |  \n",
      "     |  __setattr__(self, name, value)\n",
      "     |  \n",
      "     |  __setstate__(self, state: 'DictAny') -> None\n",
      "     |  \n",
      "     |  copy(self: 'Model', *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, update: Optional[ForwardRef('DictStrAny')] = None, deep: bool = False) -> 'Model'\n",
      "     |      Duplicate a model, optionally choose which fields to include, exclude and change.\n",
      "     |      \n",
      "     |      :param include: fields to include in new model\n",
      "     |      :param exclude: fields to exclude from new model, as with values this takes precedence over include\n",
      "     |      :param update: values to change/add in the new model. Note: the data is not validated before creating\n",
      "     |          the new model: you should trust this data\n",
      "     |      :param deep: set to `True` to make a deep copy of the model\n",
      "     |      :return: new model instance\n",
      "     |  \n",
      "     |  json(self, *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) -> 'unicode'\n",
      "     |      Generate a JSON representation of the model, `include` and `exclude` arguments as per `dict()`.\n",
      "     |      \n",
      "     |      `encoder` is an optional function to supply as `default` to json.dumps(), other arguments as per `json.dumps()`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __get_validators__() -> 'CallableGenerator' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  __try_update_forward_refs__(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      "     |      Same as update_forward_refs but will not raise exception\n",
      "     |      when forward references are not defined.\n",
      "     |  \n",
      "     |  construct(_fields_set: Optional[ForwardRef('SetStr')] = None, **values: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |      Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data.\n",
      "     |      Default values are respected, but no other validation is performed.\n",
      "     |      Behaves as if `Config.extra = 'allow'` was set since it adds all passed values\n",
      "     |  \n",
      "     |  from_orm(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_file(path: Union[str, pathlib.Path], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_obj(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_raw(b: Union[str, bytes], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema(by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}') -> 'DictStrAny' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema_json(*, by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}', **dumps_kwargs: Any) -> 'unicode' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  update_forward_refs(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      "     |      Try to update ForwardRefs on fields based on this Model, globalns and localns.\n",
      "     |  \n",
      "     |  validate(value: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __fields_set__\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.utils.Representation:\n",
      "     |  \n",
      "     |  __pretty__(self, fmt: Callable[[Any], Any], **kwargs: Any) -> Generator[Any, NoneType, NoneType]\n",
      "     |      Used by devtools (https://python-devtools.helpmanual.io/) to provide a human readable representations of objects\n",
      "     |  \n",
      "     |  __repr__(self) -> 'unicode'\n",
      "     |  \n",
      "     |  __repr_name__(self) -> 'unicode'\n",
      "     |      Name of the instance's class, used in __repr__.\n",
      "     |  \n",
      "     |  __repr_str__(self, join_str: 'unicode') -> 'unicode'\n",
      "     |  \n",
      "     |  __rich_repr__(self) -> 'RichReprResult'\n",
      "     |      Get fields for Rich library\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.schema.runnable.base.Runnable:\n",
      "     |  \n",
      "     |  __or__(self, other: 'Union[Runnable[Any, Other], Callable[[Any], Other], Mapping[str, Union[Runnable[Any, Other], Callable[[Any], Other], Any]]]') -> 'RunnableSequence[Input, Other]'\n",
      "     |      Return self|value.\n",
      "     |  \n",
      "     |  __ror__(self, other: 'Union[Runnable[Other, Any], Callable[[Any], Other], Mapping[str, Union[Runnable[Other, Any], Callable[[Other], Any], Any]]]') -> 'RunnableSequence[Other, Output]'\n",
      "     |      Return value|self.\n",
      "     |  \n",
      "     |  async atransform(self, input: 'AsyncIterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'AsyncIterator[Output]'\n",
      "     |      Default implementation of atransform, which buffers input and calls astream.\n",
      "     |      Subclasses should override this method if they can start producing output while\n",
      "     |      input is still being generated.\n",
      "     |  \n",
      "     |  bind(self, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      "     |      Bind arguments to a Runnable, returning a new Runnable.\n",
      "     |  \n",
      "     |  map(self) -> 'Runnable[List[Input], List[Output]]'\n",
      "     |      Return a new Runnable that maps a list of inputs to a list of outputs,\n",
      "     |      by calling invoke() with each input.\n",
      "     |  \n",
      "     |  transform(self, input: 'Iterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'Iterator[Output]'\n",
      "     |      Default implementation of transform, which buffers input and then calls stream.\n",
      "     |      Subclasses should override this method if they can start producing output while\n",
      "     |      input is still being generated.\n",
      "     |  \n",
      "     |  with_config(self, config: 'Optional[RunnableConfig]' = None, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      "     |      Bind config to a Runnable, returning a new Runnable.\n",
      "     |  \n",
      "     |  with_fallbacks(self, fallbacks: 'Sequence[Runnable[Input, Output]]', *, exceptions_to_handle: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,)) -> 'RunnableWithFallbacks[Input, Output]'\n",
      "     |  \n",
      "     |  with_retry(self, *, retry_if_exception_type: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,), wait_exponential_jitter: 'bool' = True, stop_after_attempt: 'int' = 3) -> 'Runnable[Input, Output]'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __class_getitem__(params) from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  __init_subclass__(*args, **kwargs) from pydantic.main.ModelMetaclass\n",
      "     |      This method is called when a class is subclassed.\n",
      "     |      \n",
      "     |      The default implementation does nothing. It may be\n",
      "     |      overridden to extend subclasses.\n",
      "    \n",
      "    class TextGen(langchain.llms.base.LLM)\n",
      "     |  TextGen(*, cache: Optional[bool] = None, verbose: bool = None, callbacks: Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager, NoneType] = None, callback_manager: Optional[langchain.callbacks.base.BaseCallbackManager] = None, tags: Optional[List[str]] = None, metadata: Optional[Dict[str, Any]] = None, model_url: str, preset: Optional[str] = None, max_new_tokens: Optional[int] = 250, do_sample: bool = True, temperature: Optional[float] = 1.3, top_p: Optional[float] = 0.1, typical_p: Optional[float] = 1, epsilon_cutoff: Optional[float] = 0, eta_cutoff: Optional[float] = 0, repetition_penalty: Optional[float] = 1.18, top_k: Optional[float] = 40, min_length: Optional[int] = 0, no_repeat_ngram_size: Optional[int] = 0, num_beams: Optional[int] = 1, penalty_alpha: Optional[float] = 0, length_penalty: Optional[float] = 1, early_stopping: bool = False, seed: int = -1, add_bos_token: bool = True, truncation_length: Optional[int] = 2048, ban_eos_token: bool = False, skip_special_tokens: bool = True, stopping_strings: Optional[List[str]] = [], streaming: bool = False) -> None\n",
      "     |  \n",
      "     |  text-generation-webui models.\n",
      "     |  \n",
      "     |  To use, you should have the text-generation-webui installed, a model loaded,\n",
      "     |  and --api added as a command-line option.\n",
      "     |  \n",
      "     |  Suggested installation, use one-click installer for your OS:\n",
      "     |  https://github.com/oobabooga/text-generation-webui#one-click-installers\n",
      "     |  \n",
      "     |  Parameters below taken from text-generation-webui api example:\n",
      "     |  https://github.com/oobabooga/text-generation-webui/blob/main/api-examples/api-example.py\n",
      "     |  \n",
      "     |  Example:\n",
      "     |      .. code-block:: python\n",
      "     |  \n",
      "     |          from langchain.llms import TextGen\n",
      "     |          llm = TextGen(model_url=\"http://localhost:8500\")\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      TextGen\n",
      "     |      langchain.llms.base.LLM\n",
      "     |      langchain.llms.base.BaseLLM\n",
      "     |      langchain.schema.language_model.BaseLanguageModel\n",
      "     |      langchain.load.serializable.Serializable\n",
      "     |      pydantic.main.BaseModel\n",
      "     |      pydantic.utils.Representation\n",
      "     |      langchain.schema.runnable.base.Runnable\n",
      "     |      typing.Generic\n",
      "     |      abc.ABC\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __json_encoder__ = pydantic_encoder(obj: Any) -> Any\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {'add_bos_token': <class 'bool'>, 'ban_eos_token': <...\n",
      "     |  \n",
      "     |  __class_vars__ = set()\n",
      "     |  \n",
      "     |  __config__ = <class 'langchain.llms.textgen.Config'>\n",
      "     |  \n",
      "     |  __custom_root_type__ = False\n",
      "     |  \n",
      "     |  __exclude_fields__ = {'callback_manager': True, 'callbacks': True, 'me...\n",
      "     |  \n",
      "     |  __fields__ = {'add_bos_token': ModelField(name='add_bos_token', type=b...\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "     |  \n",
      "     |  __include_fields__ = None\n",
      "     |  \n",
      "     |  __parameters__ = ()\n",
      "     |  \n",
      "     |  __post_root_validators__ = [(False, <function BaseLLM.raise_deprecatio...\n",
      "     |  \n",
      "     |  __pre_root_validators__ = []\n",
      "     |  \n",
      "     |  __private_attributes__ = {'_lc_kwargs': ModelPrivateAttr(default=Pydan...\n",
      "     |  \n",
      "     |  __schema_cache__ = {}\n",
      "     |  \n",
      "     |  __signature__ = <Signature (*, cache: Optional[bool] = None, ver...ist...\n",
      "     |  \n",
      "     |  __validators__ = {'verbose': [<pydantic.class_validators.Validator obj...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  __call__(self, prompt: 'str', stop: 'Optional[List[str]]' = None, callbacks: 'Callbacks' = None, *, tags: 'Optional[List[str]]' = None, metadata: 'Optional[Dict[str, Any]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Check Cache and run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  __str__(self) -> 'str'\n",
      "     |      Get a string representation of the object for printing.\n",
      "     |  \n",
      "     |  async abatch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      "     |      Default implementation of abatch, which calls ainvoke N times.\n",
      "     |      Subclasses should override this method if they can batch more efficiently.\n",
      "     |  \n",
      "     |  async agenerate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  async agenerate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Asynchronously pass a sequence of prompts and return model generations.\n",
      "     |      \n",
      "     |      This method should make use of batched calls for models that expose a batched\n",
      "     |      API.\n",
      "     |      \n",
      "     |      Use this method when you want to:\n",
      "     |          1. take advantage of batched calls,\n",
      "     |          2. need more output from the model than just the top generated value,\n",
      "     |          3. are building chains that are agnostic to the underlying language model\n",
      "     |              type (e.g., pure text completion models vs chat models).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      "     |              converted to match the format of any language model (string for pure\n",
      "     |              text generation models and BaseMessages for chat models).\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          callbacks: Callbacks to pass through. Used for executing additional\n",
      "     |              functionality, such as logging or streaming, throughout generation.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          An LLMResult, which contains a list of candidate Generations for each input\n",
      "     |              prompt and additional model provider-specific output.\n",
      "     |  \n",
      "     |  async ainvoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Default implementation of ainvoke, which calls invoke in a thread pool.\n",
      "     |      Subclasses should override this method if they can run asynchronously.\n",
      "     |  \n",
      "     |  async apredict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Asynchronously pass a string to the model and return a string prediction.\n",
      "     |      \n",
      "     |      Use this method when calling pure text generation models and only the top\n",
      "     |          candidate generation is needed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: String input to pass to the model.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a string.\n",
      "     |  \n",
      "     |  async apredict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      "     |      Asynchronously pass messages to the model and return a message prediction.\n",
      "     |      \n",
      "     |      Use this method when calling chat models and only the top\n",
      "     |          candidate generation is needed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: A sequence of chat messages corresponding to a single model input.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a message.\n",
      "     |  \n",
      "     |  async astream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'AsyncIterator[str]'\n",
      "     |      Default implementation of astream, which calls ainvoke.\n",
      "     |      Subclasses should override this method if they support streaming output.\n",
      "     |  \n",
      "     |  batch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      "     |      Default implementation of batch, which calls invoke N times.\n",
      "     |      Subclasses should override this method if they can batch more efficiently.\n",
      "     |  \n",
      "     |  dict(self, **kwargs: 'Any') -> 'Dict'\n",
      "     |      Return a dictionary of the LLM.\n",
      "     |  \n",
      "     |  generate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  generate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Pass a sequence of prompts to the model and return model generations.\n",
      "     |      \n",
      "     |      This method should make use of batched calls for models that expose a batched\n",
      "     |      API.\n",
      "     |      \n",
      "     |      Use this method when you want to:\n",
      "     |          1. take advantage of batched calls,\n",
      "     |          2. need more output from the model than just the top generated value,\n",
      "     |          3. are building chains that are agnostic to the underlying language model\n",
      "     |              type (e.g., pure text completion models vs chat models).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      "     |              converted to match the format of any language model (string for pure\n",
      "     |              text generation models and BaseMessages for chat models).\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          callbacks: Callbacks to pass through. Used for executing additional\n",
      "     |              functionality, such as logging or streaming, throughout generation.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          An LLMResult, which contains a list of candidate Generations for each input\n",
      "     |              prompt and additional model provider-specific output.\n",
      "     |  \n",
      "     |  invoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |  \n",
      "     |  predict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Pass a single string input to the model and return a string prediction.\n",
      "     |      \n",
      "     |       Use this method when passing in raw text. If you want to pass in specific\n",
      "     |          types of chat messages, use predict_messages.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: String input to pass to the model.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a string.\n",
      "     |  \n",
      "     |  predict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      "     |      Pass a message sequence to the model and return a message prediction.\n",
      "     |      \n",
      "     |      Use this method when passing in chat messages. If you want to pass in raw text,\n",
      "     |          use predict.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: A sequence of chat messages corresponding to a single model input.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a message.\n",
      "     |  \n",
      "     |  save(self, file_path: 'Union[Path, str]') -> 'None'\n",
      "     |      Save the LLM.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          file_path: Path to file to save the LLM to.\n",
      "     |      \n",
      "     |      Example:\n",
      "     |      .. code-block:: python\n",
      "     |      \n",
      "     |          llm.save(file_path=\"path/llm.yaml\")\n",
      "     |  \n",
      "     |  stream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'Iterator[str]'\n",
      "     |      Default implementation of stream, which calls invoke.\n",
      "     |      Subclasses should override this method if they support streaming output.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  raise_deprecation(values: 'Dict') -> 'Dict' from pydantic.main.ModelMetaclass\n",
      "     |      Raise deprecation warning if callback_manager is used.\n",
      "     |  \n",
      "     |  set_verbose(verbose: 'Optional[bool]') -> 'bool' from pydantic.main.ModelMetaclass\n",
      "     |      If verbose is None, set it.\n",
      "     |      \n",
      "     |      This allows users to pass in None as verbose to access the global setting.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  Config = <class 'langchain.llms.base.BaseLLM.Config'>\n",
      "     |      Configuration for this pydantic object.\n",
      "     |  \n",
      "     |  \n",
      "     |  __orig_bases__ = (langchain.schema.language_model.BaseLanguageModel[st...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.schema.language_model.BaseLanguageModel:\n",
      "     |  \n",
      "     |  get_num_tokens(self, text: 'str') -> 'int'\n",
      "     |      Get the number of tokens present in the text.\n",
      "     |      \n",
      "     |      Useful for checking if an input will fit in a model's context window.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: The string input to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The integer number of tokens in the text.\n",
      "     |  \n",
      "     |  get_num_tokens_from_messages(self, messages: 'List[BaseMessage]') -> 'int'\n",
      "     |      Get the number of tokens in the messages.\n",
      "     |      \n",
      "     |      Useful for checking if an input will fit in a model's context window.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: The message inputs to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The sum of the number of tokens across the messages.\n",
      "     |  \n",
      "     |  get_token_ids(self, text: 'str') -> 'List[int]'\n",
      "     |      Return the ordered ids of the tokens in a text.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: The string input to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          A list of ids corresponding to the tokens in the text, in order they occur\n",
      "     |              in the text.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from langchain.schema.language_model.BaseLanguageModel:\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.load.serializable.Serializable:\n",
      "     |  \n",
      "     |  __init__(self, **kwargs: Any) -> None\n",
      "     |      Create a new model by parsing and validating input data from keyword arguments.\n",
      "     |      \n",
      "     |      Raises ValidationError if the input data cannot be parsed to form a valid model.\n",
      "     |  \n",
      "     |  to_json(self) -> Union[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\n",
      "     |  \n",
      "     |  to_json_not_implemented(self) -> langchain.load.serializable.SerializedNotImplemented\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from langchain.load.serializable.Serializable:\n",
      "     |  \n",
      "     |  lc_attributes\n",
      "     |      Return a list of attribute names that should be included in the\n",
      "     |      serialized kwargs. These attributes must be accepted by the\n",
      "     |      constructor.\n",
      "     |  \n",
      "     |  lc_namespace\n",
      "     |      Return the namespace of the langchain object.\n",
      "     |      eg. [\"langchain\", \"llms\", \"openai\"]\n",
      "     |  \n",
      "     |  lc_secrets\n",
      "     |      Return a map of constructor argument names to secret ids.\n",
      "     |      eg. {\"openai_api_key\": \"OPENAI_API_KEY\"}\n",
      "     |  \n",
      "     |  lc_serializable\n",
      "     |      Return whether or not the class is serializable.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __eq__(self, other: Any) -> bool\n",
      "     |  \n",
      "     |  __getstate__(self) -> 'DictAny'\n",
      "     |  \n",
      "     |  __iter__(self) -> 'TupleGenerator'\n",
      "     |      so `dict(model)` works\n",
      "     |  \n",
      "     |  __repr_args__(self) -> 'ReprArgs'\n",
      "     |  \n",
      "     |  __setattr__(self, name, value)\n",
      "     |  \n",
      "     |  __setstate__(self, state: 'DictAny') -> None\n",
      "     |  \n",
      "     |  copy(self: 'Model', *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, update: Optional[ForwardRef('DictStrAny')] = None, deep: bool = False) -> 'Model'\n",
      "     |      Duplicate a model, optionally choose which fields to include, exclude and change.\n",
      "     |      \n",
      "     |      :param include: fields to include in new model\n",
      "     |      :param exclude: fields to exclude from new model, as with values this takes precedence over include\n",
      "     |      :param update: values to change/add in the new model. Note: the data is not validated before creating\n",
      "     |          the new model: you should trust this data\n",
      "     |      :param deep: set to `True` to make a deep copy of the model\n",
      "     |      :return: new model instance\n",
      "     |  \n",
      "     |  json(self, *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) -> 'unicode'\n",
      "     |      Generate a JSON representation of the model, `include` and `exclude` arguments as per `dict()`.\n",
      "     |      \n",
      "     |      `encoder` is an optional function to supply as `default` to json.dumps(), other arguments as per `json.dumps()`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __get_validators__() -> 'CallableGenerator' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  __try_update_forward_refs__(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      "     |      Same as update_forward_refs but will not raise exception\n",
      "     |      when forward references are not defined.\n",
      "     |  \n",
      "     |  construct(_fields_set: Optional[ForwardRef('SetStr')] = None, **values: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |      Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data.\n",
      "     |      Default values are respected, but no other validation is performed.\n",
      "     |      Behaves as if `Config.extra = 'allow'` was set since it adds all passed values\n",
      "     |  \n",
      "     |  from_orm(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_file(path: Union[str, pathlib.Path], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_obj(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_raw(b: Union[str, bytes], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema(by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}') -> 'DictStrAny' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema_json(*, by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}', **dumps_kwargs: Any) -> 'unicode' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  update_forward_refs(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      "     |      Try to update ForwardRefs on fields based on this Model, globalns and localns.\n",
      "     |  \n",
      "     |  validate(value: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __fields_set__\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.utils.Representation:\n",
      "     |  \n",
      "     |  __pretty__(self, fmt: Callable[[Any], Any], **kwargs: Any) -> Generator[Any, NoneType, NoneType]\n",
      "     |      Used by devtools (https://python-devtools.helpmanual.io/) to provide a human readable representations of objects\n",
      "     |  \n",
      "     |  __repr__(self) -> 'unicode'\n",
      "     |  \n",
      "     |  __repr_name__(self) -> 'unicode'\n",
      "     |      Name of the instance's class, used in __repr__.\n",
      "     |  \n",
      "     |  __repr_str__(self, join_str: 'unicode') -> 'unicode'\n",
      "     |  \n",
      "     |  __rich_repr__(self) -> 'RichReprResult'\n",
      "     |      Get fields for Rich library\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.schema.runnable.base.Runnable:\n",
      "     |  \n",
      "     |  __or__(self, other: 'Union[Runnable[Any, Other], Callable[[Any], Other], Mapping[str, Union[Runnable[Any, Other], Callable[[Any], Other], Any]]]') -> 'RunnableSequence[Input, Other]'\n",
      "     |      Return self|value.\n",
      "     |  \n",
      "     |  __ror__(self, other: 'Union[Runnable[Other, Any], Callable[[Any], Other], Mapping[str, Union[Runnable[Other, Any], Callable[[Other], Any], Any]]]') -> 'RunnableSequence[Other, Output]'\n",
      "     |      Return value|self.\n",
      "     |  \n",
      "     |  async atransform(self, input: 'AsyncIterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'AsyncIterator[Output]'\n",
      "     |      Default implementation of atransform, which buffers input and calls astream.\n",
      "     |      Subclasses should override this method if they can start producing output while\n",
      "     |      input is still being generated.\n",
      "     |  \n",
      "     |  bind(self, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      "     |      Bind arguments to a Runnable, returning a new Runnable.\n",
      "     |  \n",
      "     |  map(self) -> 'Runnable[List[Input], List[Output]]'\n",
      "     |      Return a new Runnable that maps a list of inputs to a list of outputs,\n",
      "     |      by calling invoke() with each input.\n",
      "     |  \n",
      "     |  transform(self, input: 'Iterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'Iterator[Output]'\n",
      "     |      Default implementation of transform, which buffers input and then calls stream.\n",
      "     |      Subclasses should override this method if they can start producing output while\n",
      "     |      input is still being generated.\n",
      "     |  \n",
      "     |  with_config(self, config: 'Optional[RunnableConfig]' = None, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      "     |      Bind config to a Runnable, returning a new Runnable.\n",
      "     |  \n",
      "     |  with_fallbacks(self, fallbacks: 'Sequence[Runnable[Input, Output]]', *, exceptions_to_handle: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,)) -> 'RunnableWithFallbacks[Input, Output]'\n",
      "     |  \n",
      "     |  with_retry(self, *, retry_if_exception_type: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,), wait_exponential_jitter: 'bool' = True, stop_after_attempt: 'int' = 3) -> 'Runnable[Input, Output]'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __class_getitem__(params) from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  __init_subclass__(*args, **kwargs) from pydantic.main.ModelMetaclass\n",
      "     |      This method is called when a class is subclassed.\n",
      "     |      \n",
      "     |      The default implementation does nothing. It may be\n",
      "     |      overridden to extend subclasses.\n",
      "    \n",
      "    class TitanTakeoff(langchain.llms.base.LLM)\n",
      "     |  TitanTakeoff(*, cache: Optional[bool] = None, verbose: bool = None, callbacks: Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager, NoneType] = None, callback_manager: Optional[langchain.callbacks.base.BaseCallbackManager] = None, tags: Optional[List[str]] = None, metadata: Optional[Dict[str, Any]] = None, base_url: str = 'http://localhost:8000', generate_max_length: int = 128, sampling_topk: int = 1, sampling_topp: float = 1.0, sampling_temperature: float = 1.0, repetition_penalty: float = 1.0, no_repeat_ngram_size: int = 0, streaming: bool = False) -> None\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      TitanTakeoff\n",
      "     |      langchain.llms.base.LLM\n",
      "     |      langchain.llms.base.BaseLLM\n",
      "     |      langchain.schema.language_model.BaseLanguageModel\n",
      "     |      langchain.load.serializable.Serializable\n",
      "     |      pydantic.main.BaseModel\n",
      "     |      pydantic.utils.Representation\n",
      "     |      langchain.schema.runnable.base.Runnable\n",
      "     |      typing.Generic\n",
      "     |      abc.ABC\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __json_encoder__ = pydantic_encoder(obj: Any) -> Any\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {'base_url': <class 'str'>, 'generate_max_length': <...\n",
      "     |  \n",
      "     |  __class_vars__ = set()\n",
      "     |  \n",
      "     |  __config__ = <class 'langchain.llms.titan_takeoff.Config'>\n",
      "     |  \n",
      "     |  __custom_root_type__ = False\n",
      "     |  \n",
      "     |  __exclude_fields__ = {'callback_manager': True, 'callbacks': True, 'me...\n",
      "     |  \n",
      "     |  __fields__ = {'base_url': ModelField(name='base_url', type=str, requir...\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "     |  \n",
      "     |  __include_fields__ = None\n",
      "     |  \n",
      "     |  __parameters__ = ()\n",
      "     |  \n",
      "     |  __post_root_validators__ = [(False, <function BaseLLM.raise_deprecatio...\n",
      "     |  \n",
      "     |  __pre_root_validators__ = []\n",
      "     |  \n",
      "     |  __private_attributes__ = {'_lc_kwargs': ModelPrivateAttr(default=Pydan...\n",
      "     |  \n",
      "     |  __schema_cache__ = {}\n",
      "     |  \n",
      "     |  __signature__ = <Signature (*, cache: Optional[bool] = None, ver..._si...\n",
      "     |  \n",
      "     |  __validators__ = {'verbose': [<pydantic.class_validators.Validator obj...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  __call__(self, prompt: 'str', stop: 'Optional[List[str]]' = None, callbacks: 'Callbacks' = None, *, tags: 'Optional[List[str]]' = None, metadata: 'Optional[Dict[str, Any]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Check Cache and run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  __str__(self) -> 'str'\n",
      "     |      Get a string representation of the object for printing.\n",
      "     |  \n",
      "     |  async abatch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      "     |      Default implementation of abatch, which calls ainvoke N times.\n",
      "     |      Subclasses should override this method if they can batch more efficiently.\n",
      "     |  \n",
      "     |  async agenerate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  async agenerate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Asynchronously pass a sequence of prompts and return model generations.\n",
      "     |      \n",
      "     |      This method should make use of batched calls for models that expose a batched\n",
      "     |      API.\n",
      "     |      \n",
      "     |      Use this method when you want to:\n",
      "     |          1. take advantage of batched calls,\n",
      "     |          2. need more output from the model than just the top generated value,\n",
      "     |          3. are building chains that are agnostic to the underlying language model\n",
      "     |              type (e.g., pure text completion models vs chat models).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      "     |              converted to match the format of any language model (string for pure\n",
      "     |              text generation models and BaseMessages for chat models).\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          callbacks: Callbacks to pass through. Used for executing additional\n",
      "     |              functionality, such as logging or streaming, throughout generation.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          An LLMResult, which contains a list of candidate Generations for each input\n",
      "     |              prompt and additional model provider-specific output.\n",
      "     |  \n",
      "     |  async ainvoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Default implementation of ainvoke, which calls invoke in a thread pool.\n",
      "     |      Subclasses should override this method if they can run asynchronously.\n",
      "     |  \n",
      "     |  async apredict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Asynchronously pass a string to the model and return a string prediction.\n",
      "     |      \n",
      "     |      Use this method when calling pure text generation models and only the top\n",
      "     |          candidate generation is needed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: String input to pass to the model.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a string.\n",
      "     |  \n",
      "     |  async apredict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      "     |      Asynchronously pass messages to the model and return a message prediction.\n",
      "     |      \n",
      "     |      Use this method when calling chat models and only the top\n",
      "     |          candidate generation is needed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: A sequence of chat messages corresponding to a single model input.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a message.\n",
      "     |  \n",
      "     |  async astream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'AsyncIterator[str]'\n",
      "     |      Default implementation of astream, which calls ainvoke.\n",
      "     |      Subclasses should override this method if they support streaming output.\n",
      "     |  \n",
      "     |  batch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      "     |      Default implementation of batch, which calls invoke N times.\n",
      "     |      Subclasses should override this method if they can batch more efficiently.\n",
      "     |  \n",
      "     |  dict(self, **kwargs: 'Any') -> 'Dict'\n",
      "     |      Return a dictionary of the LLM.\n",
      "     |  \n",
      "     |  generate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  generate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Pass a sequence of prompts to the model and return model generations.\n",
      "     |      \n",
      "     |      This method should make use of batched calls for models that expose a batched\n",
      "     |      API.\n",
      "     |      \n",
      "     |      Use this method when you want to:\n",
      "     |          1. take advantage of batched calls,\n",
      "     |          2. need more output from the model than just the top generated value,\n",
      "     |          3. are building chains that are agnostic to the underlying language model\n",
      "     |              type (e.g., pure text completion models vs chat models).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      "     |              converted to match the format of any language model (string for pure\n",
      "     |              text generation models and BaseMessages for chat models).\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          callbacks: Callbacks to pass through. Used for executing additional\n",
      "     |              functionality, such as logging or streaming, throughout generation.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          An LLMResult, which contains a list of candidate Generations for each input\n",
      "     |              prompt and additional model provider-specific output.\n",
      "     |  \n",
      "     |  invoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |  \n",
      "     |  predict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Pass a single string input to the model and return a string prediction.\n",
      "     |      \n",
      "     |       Use this method when passing in raw text. If you want to pass in specific\n",
      "     |          types of chat messages, use predict_messages.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: String input to pass to the model.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a string.\n",
      "     |  \n",
      "     |  predict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      "     |      Pass a message sequence to the model and return a message prediction.\n",
      "     |      \n",
      "     |      Use this method when passing in chat messages. If you want to pass in raw text,\n",
      "     |          use predict.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: A sequence of chat messages corresponding to a single model input.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a message.\n",
      "     |  \n",
      "     |  save(self, file_path: 'Union[Path, str]') -> 'None'\n",
      "     |      Save the LLM.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          file_path: Path to file to save the LLM to.\n",
      "     |      \n",
      "     |      Example:\n",
      "     |      .. code-block:: python\n",
      "     |      \n",
      "     |          llm.save(file_path=\"path/llm.yaml\")\n",
      "     |  \n",
      "     |  stream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'Iterator[str]'\n",
      "     |      Default implementation of stream, which calls invoke.\n",
      "     |      Subclasses should override this method if they support streaming output.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  raise_deprecation(values: 'Dict') -> 'Dict' from pydantic.main.ModelMetaclass\n",
      "     |      Raise deprecation warning if callback_manager is used.\n",
      "     |  \n",
      "     |  set_verbose(verbose: 'Optional[bool]') -> 'bool' from pydantic.main.ModelMetaclass\n",
      "     |      If verbose is None, set it.\n",
      "     |      \n",
      "     |      This allows users to pass in None as verbose to access the global setting.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  Config = <class 'langchain.llms.base.BaseLLM.Config'>\n",
      "     |      Configuration for this pydantic object.\n",
      "     |  \n",
      "     |  \n",
      "     |  __orig_bases__ = (langchain.schema.language_model.BaseLanguageModel[st...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.schema.language_model.BaseLanguageModel:\n",
      "     |  \n",
      "     |  get_num_tokens(self, text: 'str') -> 'int'\n",
      "     |      Get the number of tokens present in the text.\n",
      "     |      \n",
      "     |      Useful for checking if an input will fit in a model's context window.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: The string input to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The integer number of tokens in the text.\n",
      "     |  \n",
      "     |  get_num_tokens_from_messages(self, messages: 'List[BaseMessage]') -> 'int'\n",
      "     |      Get the number of tokens in the messages.\n",
      "     |      \n",
      "     |      Useful for checking if an input will fit in a model's context window.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: The message inputs to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The sum of the number of tokens across the messages.\n",
      "     |  \n",
      "     |  get_token_ids(self, text: 'str') -> 'List[int]'\n",
      "     |      Return the ordered ids of the tokens in a text.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: The string input to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          A list of ids corresponding to the tokens in the text, in order they occur\n",
      "     |              in the text.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from langchain.schema.language_model.BaseLanguageModel:\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.load.serializable.Serializable:\n",
      "     |  \n",
      "     |  __init__(self, **kwargs: Any) -> None\n",
      "     |      Create a new model by parsing and validating input data from keyword arguments.\n",
      "     |      \n",
      "     |      Raises ValidationError if the input data cannot be parsed to form a valid model.\n",
      "     |  \n",
      "     |  to_json(self) -> Union[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\n",
      "     |  \n",
      "     |  to_json_not_implemented(self) -> langchain.load.serializable.SerializedNotImplemented\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from langchain.load.serializable.Serializable:\n",
      "     |  \n",
      "     |  lc_attributes\n",
      "     |      Return a list of attribute names that should be included in the\n",
      "     |      serialized kwargs. These attributes must be accepted by the\n",
      "     |      constructor.\n",
      "     |  \n",
      "     |  lc_namespace\n",
      "     |      Return the namespace of the langchain object.\n",
      "     |      eg. [\"langchain\", \"llms\", \"openai\"]\n",
      "     |  \n",
      "     |  lc_secrets\n",
      "     |      Return a map of constructor argument names to secret ids.\n",
      "     |      eg. {\"openai_api_key\": \"OPENAI_API_KEY\"}\n",
      "     |  \n",
      "     |  lc_serializable\n",
      "     |      Return whether or not the class is serializable.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __eq__(self, other: Any) -> bool\n",
      "     |  \n",
      "     |  __getstate__(self) -> 'DictAny'\n",
      "     |  \n",
      "     |  __iter__(self) -> 'TupleGenerator'\n",
      "     |      so `dict(model)` works\n",
      "     |  \n",
      "     |  __repr_args__(self) -> 'ReprArgs'\n",
      "     |  \n",
      "     |  __setattr__(self, name, value)\n",
      "     |  \n",
      "     |  __setstate__(self, state: 'DictAny') -> None\n",
      "     |  \n",
      "     |  copy(self: 'Model', *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, update: Optional[ForwardRef('DictStrAny')] = None, deep: bool = False) -> 'Model'\n",
      "     |      Duplicate a model, optionally choose which fields to include, exclude and change.\n",
      "     |      \n",
      "     |      :param include: fields to include in new model\n",
      "     |      :param exclude: fields to exclude from new model, as with values this takes precedence over include\n",
      "     |      :param update: values to change/add in the new model. Note: the data is not validated before creating\n",
      "     |          the new model: you should trust this data\n",
      "     |      :param deep: set to `True` to make a deep copy of the model\n",
      "     |      :return: new model instance\n",
      "     |  \n",
      "     |  json(self, *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) -> 'unicode'\n",
      "     |      Generate a JSON representation of the model, `include` and `exclude` arguments as per `dict()`.\n",
      "     |      \n",
      "     |      `encoder` is an optional function to supply as `default` to json.dumps(), other arguments as per `json.dumps()`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __get_validators__() -> 'CallableGenerator' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  __try_update_forward_refs__(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      "     |      Same as update_forward_refs but will not raise exception\n",
      "     |      when forward references are not defined.\n",
      "     |  \n",
      "     |  construct(_fields_set: Optional[ForwardRef('SetStr')] = None, **values: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |      Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data.\n",
      "     |      Default values are respected, but no other validation is performed.\n",
      "     |      Behaves as if `Config.extra = 'allow'` was set since it adds all passed values\n",
      "     |  \n",
      "     |  from_orm(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_file(path: Union[str, pathlib.Path], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_obj(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_raw(b: Union[str, bytes], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema(by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}') -> 'DictStrAny' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema_json(*, by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}', **dumps_kwargs: Any) -> 'unicode' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  update_forward_refs(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      "     |      Try to update ForwardRefs on fields based on this Model, globalns and localns.\n",
      "     |  \n",
      "     |  validate(value: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __fields_set__\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.utils.Representation:\n",
      "     |  \n",
      "     |  __pretty__(self, fmt: Callable[[Any], Any], **kwargs: Any) -> Generator[Any, NoneType, NoneType]\n",
      "     |      Used by devtools (https://python-devtools.helpmanual.io/) to provide a human readable representations of objects\n",
      "     |  \n",
      "     |  __repr__(self) -> 'unicode'\n",
      "     |  \n",
      "     |  __repr_name__(self) -> 'unicode'\n",
      "     |      Name of the instance's class, used in __repr__.\n",
      "     |  \n",
      "     |  __repr_str__(self, join_str: 'unicode') -> 'unicode'\n",
      "     |  \n",
      "     |  __rich_repr__(self) -> 'RichReprResult'\n",
      "     |      Get fields for Rich library\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.schema.runnable.base.Runnable:\n",
      "     |  \n",
      "     |  __or__(self, other: 'Union[Runnable[Any, Other], Callable[[Any], Other], Mapping[str, Union[Runnable[Any, Other], Callable[[Any], Other], Any]]]') -> 'RunnableSequence[Input, Other]'\n",
      "     |      Return self|value.\n",
      "     |  \n",
      "     |  __ror__(self, other: 'Union[Runnable[Other, Any], Callable[[Any], Other], Mapping[str, Union[Runnable[Other, Any], Callable[[Other], Any], Any]]]') -> 'RunnableSequence[Other, Output]'\n",
      "     |      Return value|self.\n",
      "     |  \n",
      "     |  async atransform(self, input: 'AsyncIterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'AsyncIterator[Output]'\n",
      "     |      Default implementation of atransform, which buffers input and calls astream.\n",
      "     |      Subclasses should override this method if they can start producing output while\n",
      "     |      input is still being generated.\n",
      "     |  \n",
      "     |  bind(self, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      "     |      Bind arguments to a Runnable, returning a new Runnable.\n",
      "     |  \n",
      "     |  map(self) -> 'Runnable[List[Input], List[Output]]'\n",
      "     |      Return a new Runnable that maps a list of inputs to a list of outputs,\n",
      "     |      by calling invoke() with each input.\n",
      "     |  \n",
      "     |  transform(self, input: 'Iterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'Iterator[Output]'\n",
      "     |      Default implementation of transform, which buffers input and then calls stream.\n",
      "     |      Subclasses should override this method if they can start producing output while\n",
      "     |      input is still being generated.\n",
      "     |  \n",
      "     |  with_config(self, config: 'Optional[RunnableConfig]' = None, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      "     |      Bind config to a Runnable, returning a new Runnable.\n",
      "     |  \n",
      "     |  with_fallbacks(self, fallbacks: 'Sequence[Runnable[Input, Output]]', *, exceptions_to_handle: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,)) -> 'RunnableWithFallbacks[Input, Output]'\n",
      "     |  \n",
      "     |  with_retry(self, *, retry_if_exception_type: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,), wait_exponential_jitter: 'bool' = True, stop_after_attempt: 'int' = 3) -> 'Runnable[Input, Output]'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __class_getitem__(params) from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  __init_subclass__(*args, **kwargs) from pydantic.main.ModelMetaclass\n",
      "     |      This method is called when a class is subclassed.\n",
      "     |      \n",
      "     |      The default implementation does nothing. It may be\n",
      "     |      overridden to extend subclasses.\n",
      "    \n",
      "    class Tongyi(langchain.llms.base.LLM)\n",
      "     |  Tongyi(*, cache: Optional[bool] = None, verbose: bool = None, callbacks: Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager, NoneType] = None, callback_manager: Optional[langchain.callbacks.base.BaseCallbackManager] = None, tags: Optional[List[str]] = None, metadata: Optional[Dict[str, Any]] = None, client: Any = None, model_name: str = 'qwen-plus-v1', model_kwargs: Dict[str, Any] = None, top_p: float = 0.8, dashscope_api_key: Optional[str] = None, n: int = 1, streaming: bool = False, max_retries: int = 10, prefix_messages: List = None) -> None\n",
      "     |  \n",
      "     |  Tongyi Qwen large language models.\n",
      "     |  \n",
      "     |  To use, you should have the ``dashscope`` python package installed, and the\n",
      "     |  environment variable ``DASHSCOPE_API_KEY`` set with your API key, or pass\n",
      "     |  it as a named parameter to the constructor.\n",
      "     |  \n",
      "     |  Example:\n",
      "     |      .. code-block:: python\n",
      "     |  \n",
      "     |          from langchain.llms import Tongyi\n",
      "     |          Tongyi = tongyi()\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Tongyi\n",
      "     |      langchain.llms.base.LLM\n",
      "     |      langchain.llms.base.BaseLLM\n",
      "     |      langchain.schema.language_model.BaseLanguageModel\n",
      "     |      langchain.load.serializable.Serializable\n",
      "     |      pydantic.main.BaseModel\n",
      "     |      pydantic.utils.Representation\n",
      "     |      langchain.schema.runnable.base.Runnable\n",
      "     |      typing.Generic\n",
      "     |      abc.ABC\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  validate_environment(values: 'Dict') -> 'Dict' from pydantic.main.ModelMetaclass\n",
      "     |      Validate that api key and python package exists in environment.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __json_encoder__ = pydantic_encoder(obj: Any) -> Any\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  lc_secrets\n",
      "     |      Return a map of constructor argument names to secret ids.\n",
      "     |      eg. {\"openai_api_key\": \"OPENAI_API_KEY\"}\n",
      "     |  \n",
      "     |  lc_serializable\n",
      "     |      Return whether or not the class is serializable.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {'client': 'Any', 'dashscope_api_key': 'Optional[str...\n",
      "     |  \n",
      "     |  __class_vars__ = set()\n",
      "     |  \n",
      "     |  __config__ = <class 'langchain.llms.tongyi.Config'>\n",
      "     |  \n",
      "     |  __custom_root_type__ = False\n",
      "     |  \n",
      "     |  __exclude_fields__ = {'callback_manager': True, 'callbacks': True, 'me...\n",
      "     |  \n",
      "     |  __fields__ = {'cache': ModelField(name='cache', type=Optional[bool], r...\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "     |  \n",
      "     |  __include_fields__ = None\n",
      "     |  \n",
      "     |  __parameters__ = ()\n",
      "     |  \n",
      "     |  __post_root_validators__ = [(False, <function BaseLLM.raise_deprecatio...\n",
      "     |  \n",
      "     |  __pre_root_validators__ = []\n",
      "     |  \n",
      "     |  __private_attributes__ = {'_lc_kwargs': ModelPrivateAttr(default=Pydan...\n",
      "     |  \n",
      "     |  __schema_cache__ = {}\n",
      "     |  \n",
      "     |  __signature__ = <Signature (*, cache: Optional[bool] = None, ver... in...\n",
      "     |  \n",
      "     |  __validators__ = {'verbose': [<pydantic.class_validators.Validator obj...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  __call__(self, prompt: 'str', stop: 'Optional[List[str]]' = None, callbacks: 'Callbacks' = None, *, tags: 'Optional[List[str]]' = None, metadata: 'Optional[Dict[str, Any]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Check Cache and run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  __str__(self) -> 'str'\n",
      "     |      Get a string representation of the object for printing.\n",
      "     |  \n",
      "     |  async abatch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      "     |      Default implementation of abatch, which calls ainvoke N times.\n",
      "     |      Subclasses should override this method if they can batch more efficiently.\n",
      "     |  \n",
      "     |  async agenerate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  async agenerate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Asynchronously pass a sequence of prompts and return model generations.\n",
      "     |      \n",
      "     |      This method should make use of batched calls for models that expose a batched\n",
      "     |      API.\n",
      "     |      \n",
      "     |      Use this method when you want to:\n",
      "     |          1. take advantage of batched calls,\n",
      "     |          2. need more output from the model than just the top generated value,\n",
      "     |          3. are building chains that are agnostic to the underlying language model\n",
      "     |              type (e.g., pure text completion models vs chat models).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      "     |              converted to match the format of any language model (string for pure\n",
      "     |              text generation models and BaseMessages for chat models).\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          callbacks: Callbacks to pass through. Used for executing additional\n",
      "     |              functionality, such as logging or streaming, throughout generation.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          An LLMResult, which contains a list of candidate Generations for each input\n",
      "     |              prompt and additional model provider-specific output.\n",
      "     |  \n",
      "     |  async ainvoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Default implementation of ainvoke, which calls invoke in a thread pool.\n",
      "     |      Subclasses should override this method if they can run asynchronously.\n",
      "     |  \n",
      "     |  async apredict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Asynchronously pass a string to the model and return a string prediction.\n",
      "     |      \n",
      "     |      Use this method when calling pure text generation models and only the top\n",
      "     |          candidate generation is needed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: String input to pass to the model.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a string.\n",
      "     |  \n",
      "     |  async apredict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      "     |      Asynchronously pass messages to the model and return a message prediction.\n",
      "     |      \n",
      "     |      Use this method when calling chat models and only the top\n",
      "     |          candidate generation is needed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: A sequence of chat messages corresponding to a single model input.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a message.\n",
      "     |  \n",
      "     |  async astream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'AsyncIterator[str]'\n",
      "     |      Default implementation of astream, which calls ainvoke.\n",
      "     |      Subclasses should override this method if they support streaming output.\n",
      "     |  \n",
      "     |  batch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      "     |      Default implementation of batch, which calls invoke N times.\n",
      "     |      Subclasses should override this method if they can batch more efficiently.\n",
      "     |  \n",
      "     |  dict(self, **kwargs: 'Any') -> 'Dict'\n",
      "     |      Return a dictionary of the LLM.\n",
      "     |  \n",
      "     |  generate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  generate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Pass a sequence of prompts to the model and return model generations.\n",
      "     |      \n",
      "     |      This method should make use of batched calls for models that expose a batched\n",
      "     |      API.\n",
      "     |      \n",
      "     |      Use this method when you want to:\n",
      "     |          1. take advantage of batched calls,\n",
      "     |          2. need more output from the model than just the top generated value,\n",
      "     |          3. are building chains that are agnostic to the underlying language model\n",
      "     |              type (e.g., pure text completion models vs chat models).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      "     |              converted to match the format of any language model (string for pure\n",
      "     |              text generation models and BaseMessages for chat models).\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          callbacks: Callbacks to pass through. Used for executing additional\n",
      "     |              functionality, such as logging or streaming, throughout generation.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          An LLMResult, which contains a list of candidate Generations for each input\n",
      "     |              prompt and additional model provider-specific output.\n",
      "     |  \n",
      "     |  invoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |  \n",
      "     |  predict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Pass a single string input to the model and return a string prediction.\n",
      "     |      \n",
      "     |       Use this method when passing in raw text. If you want to pass in specific\n",
      "     |          types of chat messages, use predict_messages.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: String input to pass to the model.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a string.\n",
      "     |  \n",
      "     |  predict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      "     |      Pass a message sequence to the model and return a message prediction.\n",
      "     |      \n",
      "     |      Use this method when passing in chat messages. If you want to pass in raw text,\n",
      "     |          use predict.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: A sequence of chat messages corresponding to a single model input.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a message.\n",
      "     |  \n",
      "     |  save(self, file_path: 'Union[Path, str]') -> 'None'\n",
      "     |      Save the LLM.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          file_path: Path to file to save the LLM to.\n",
      "     |      \n",
      "     |      Example:\n",
      "     |      .. code-block:: python\n",
      "     |      \n",
      "     |          llm.save(file_path=\"path/llm.yaml\")\n",
      "     |  \n",
      "     |  stream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'Iterator[str]'\n",
      "     |      Default implementation of stream, which calls invoke.\n",
      "     |      Subclasses should override this method if they support streaming output.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  raise_deprecation(values: 'Dict') -> 'Dict' from pydantic.main.ModelMetaclass\n",
      "     |      Raise deprecation warning if callback_manager is used.\n",
      "     |  \n",
      "     |  set_verbose(verbose: 'Optional[bool]') -> 'bool' from pydantic.main.ModelMetaclass\n",
      "     |      If verbose is None, set it.\n",
      "     |      \n",
      "     |      This allows users to pass in None as verbose to access the global setting.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  Config = <class 'langchain.llms.base.BaseLLM.Config'>\n",
      "     |      Configuration for this pydantic object.\n",
      "     |  \n",
      "     |  \n",
      "     |  __orig_bases__ = (langchain.schema.language_model.BaseLanguageModel[st...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.schema.language_model.BaseLanguageModel:\n",
      "     |  \n",
      "     |  get_num_tokens(self, text: 'str') -> 'int'\n",
      "     |      Get the number of tokens present in the text.\n",
      "     |      \n",
      "     |      Useful for checking if an input will fit in a model's context window.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: The string input to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The integer number of tokens in the text.\n",
      "     |  \n",
      "     |  get_num_tokens_from_messages(self, messages: 'List[BaseMessage]') -> 'int'\n",
      "     |      Get the number of tokens in the messages.\n",
      "     |      \n",
      "     |      Useful for checking if an input will fit in a model's context window.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: The message inputs to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The sum of the number of tokens across the messages.\n",
      "     |  \n",
      "     |  get_token_ids(self, text: 'str') -> 'List[int]'\n",
      "     |      Return the ordered ids of the tokens in a text.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: The string input to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          A list of ids corresponding to the tokens in the text, in order they occur\n",
      "     |              in the text.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from langchain.schema.language_model.BaseLanguageModel:\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.load.serializable.Serializable:\n",
      "     |  \n",
      "     |  __init__(self, **kwargs: Any) -> None\n",
      "     |      Create a new model by parsing and validating input data from keyword arguments.\n",
      "     |      \n",
      "     |      Raises ValidationError if the input data cannot be parsed to form a valid model.\n",
      "     |  \n",
      "     |  to_json(self) -> Union[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\n",
      "     |  \n",
      "     |  to_json_not_implemented(self) -> langchain.load.serializable.SerializedNotImplemented\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from langchain.load.serializable.Serializable:\n",
      "     |  \n",
      "     |  lc_attributes\n",
      "     |      Return a list of attribute names that should be included in the\n",
      "     |      serialized kwargs. These attributes must be accepted by the\n",
      "     |      constructor.\n",
      "     |  \n",
      "     |  lc_namespace\n",
      "     |      Return the namespace of the langchain object.\n",
      "     |      eg. [\"langchain\", \"llms\", \"openai\"]\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __eq__(self, other: Any) -> bool\n",
      "     |  \n",
      "     |  __getstate__(self) -> 'DictAny'\n",
      "     |  \n",
      "     |  __iter__(self) -> 'TupleGenerator'\n",
      "     |      so `dict(model)` works\n",
      "     |  \n",
      "     |  __repr_args__(self) -> 'ReprArgs'\n",
      "     |  \n",
      "     |  __setattr__(self, name, value)\n",
      "     |  \n",
      "     |  __setstate__(self, state: 'DictAny') -> None\n",
      "     |  \n",
      "     |  copy(self: 'Model', *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, update: Optional[ForwardRef('DictStrAny')] = None, deep: bool = False) -> 'Model'\n",
      "     |      Duplicate a model, optionally choose which fields to include, exclude and change.\n",
      "     |      \n",
      "     |      :param include: fields to include in new model\n",
      "     |      :param exclude: fields to exclude from new model, as with values this takes precedence over include\n",
      "     |      :param update: values to change/add in the new model. Note: the data is not validated before creating\n",
      "     |          the new model: you should trust this data\n",
      "     |      :param deep: set to `True` to make a deep copy of the model\n",
      "     |      :return: new model instance\n",
      "     |  \n",
      "     |  json(self, *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) -> 'unicode'\n",
      "     |      Generate a JSON representation of the model, `include` and `exclude` arguments as per `dict()`.\n",
      "     |      \n",
      "     |      `encoder` is an optional function to supply as `default` to json.dumps(), other arguments as per `json.dumps()`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __get_validators__() -> 'CallableGenerator' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  __try_update_forward_refs__(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      "     |      Same as update_forward_refs but will not raise exception\n",
      "     |      when forward references are not defined.\n",
      "     |  \n",
      "     |  construct(_fields_set: Optional[ForwardRef('SetStr')] = None, **values: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |      Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data.\n",
      "     |      Default values are respected, but no other validation is performed.\n",
      "     |      Behaves as if `Config.extra = 'allow'` was set since it adds all passed values\n",
      "     |  \n",
      "     |  from_orm(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_file(path: Union[str, pathlib.Path], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_obj(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_raw(b: Union[str, bytes], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema(by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}') -> 'DictStrAny' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema_json(*, by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}', **dumps_kwargs: Any) -> 'unicode' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  update_forward_refs(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      "     |      Try to update ForwardRefs on fields based on this Model, globalns and localns.\n",
      "     |  \n",
      "     |  validate(value: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __fields_set__\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.utils.Representation:\n",
      "     |  \n",
      "     |  __pretty__(self, fmt: Callable[[Any], Any], **kwargs: Any) -> Generator[Any, NoneType, NoneType]\n",
      "     |      Used by devtools (https://python-devtools.helpmanual.io/) to provide a human readable representations of objects\n",
      "     |  \n",
      "     |  __repr__(self) -> 'unicode'\n",
      "     |  \n",
      "     |  __repr_name__(self) -> 'unicode'\n",
      "     |      Name of the instance's class, used in __repr__.\n",
      "     |  \n",
      "     |  __repr_str__(self, join_str: 'unicode') -> 'unicode'\n",
      "     |  \n",
      "     |  __rich_repr__(self) -> 'RichReprResult'\n",
      "     |      Get fields for Rich library\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.schema.runnable.base.Runnable:\n",
      "     |  \n",
      "     |  __or__(self, other: 'Union[Runnable[Any, Other], Callable[[Any], Other], Mapping[str, Union[Runnable[Any, Other], Callable[[Any], Other], Any]]]') -> 'RunnableSequence[Input, Other]'\n",
      "     |      Return self|value.\n",
      "     |  \n",
      "     |  __ror__(self, other: 'Union[Runnable[Other, Any], Callable[[Any], Other], Mapping[str, Union[Runnable[Other, Any], Callable[[Other], Any], Any]]]') -> 'RunnableSequence[Other, Output]'\n",
      "     |      Return value|self.\n",
      "     |  \n",
      "     |  async atransform(self, input: 'AsyncIterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'AsyncIterator[Output]'\n",
      "     |      Default implementation of atransform, which buffers input and calls astream.\n",
      "     |      Subclasses should override this method if they can start producing output while\n",
      "     |      input is still being generated.\n",
      "     |  \n",
      "     |  bind(self, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      "     |      Bind arguments to a Runnable, returning a new Runnable.\n",
      "     |  \n",
      "     |  map(self) -> 'Runnable[List[Input], List[Output]]'\n",
      "     |      Return a new Runnable that maps a list of inputs to a list of outputs,\n",
      "     |      by calling invoke() with each input.\n",
      "     |  \n",
      "     |  transform(self, input: 'Iterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'Iterator[Output]'\n",
      "     |      Default implementation of transform, which buffers input and then calls stream.\n",
      "     |      Subclasses should override this method if they can start producing output while\n",
      "     |      input is still being generated.\n",
      "     |  \n",
      "     |  with_config(self, config: 'Optional[RunnableConfig]' = None, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      "     |      Bind config to a Runnable, returning a new Runnable.\n",
      "     |  \n",
      "     |  with_fallbacks(self, fallbacks: 'Sequence[Runnable[Input, Output]]', *, exceptions_to_handle: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,)) -> 'RunnableWithFallbacks[Input, Output]'\n",
      "     |  \n",
      "     |  with_retry(self, *, retry_if_exception_type: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,), wait_exponential_jitter: 'bool' = True, stop_after_attempt: 'int' = 3) -> 'Runnable[Input, Output]'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __class_getitem__(params) from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  __init_subclass__(*args, **kwargs) from pydantic.main.ModelMetaclass\n",
      "     |      This method is called when a class is subclassed.\n",
      "     |      \n",
      "     |      The default implementation does nothing. It may be\n",
      "     |      overridden to extend subclasses.\n",
      "    \n",
      "    class VLLM(langchain.llms.base.BaseLLM)\n",
      "     |  VLLM(*, cache: Optional[bool] = None, verbose: bool = None, callbacks: Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager, NoneType] = None, callback_manager: Optional[langchain.callbacks.base.BaseCallbackManager] = None, tags: Optional[List[str]] = None, metadata: Optional[Dict[str, Any]] = None, model: str = '', tensor_parallel_size: Optional[int] = 1, trust_remote_code: Optional[bool] = False, n: int = 1, best_of: Optional[int] = None, presence_penalty: float = 0.0, frequency_penalty: float = 0.0, temperature: float = 1.0, top_p: float = 1.0, top_k: int = -1, use_beam_search: bool = False, stop: Optional[List[str]] = None, ignore_eos: bool = False, max_new_tokens: int = 512, logprobs: Optional[int] = None, dtype: str = 'auto', download_dir: Optional[str] = None, vllm_kwargs: Dict[str, Any] = None, client: Any = None) -> None\n",
      "     |  \n",
      "     |  VLLM language model.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      VLLM\n",
      "     |      langchain.llms.base.BaseLLM\n",
      "     |      langchain.schema.language_model.BaseLanguageModel\n",
      "     |      langchain.load.serializable.Serializable\n",
      "     |      pydantic.main.BaseModel\n",
      "     |      pydantic.utils.Representation\n",
      "     |      langchain.schema.runnable.base.Runnable\n",
      "     |      typing.Generic\n",
      "     |      abc.ABC\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  validate_environment(values: Dict) -> Dict from pydantic.main.ModelMetaclass\n",
      "     |      Validate that python package exists in environment.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __json_encoder__ = pydantic_encoder(obj: Any) -> Any\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {'best_of': typing.Optional[int], 'client': typing.A...\n",
      "     |  \n",
      "     |  __class_vars__ = set()\n",
      "     |  \n",
      "     |  __config__ = <class 'langchain.llms.vllm.Config'>\n",
      "     |  \n",
      "     |  __custom_root_type__ = False\n",
      "     |  \n",
      "     |  __exclude_fields__ = {'callback_manager': True, 'callbacks': True, 'me...\n",
      "     |  \n",
      "     |  __fields__ = {'best_of': ModelField(name='best_of', type=Optional[int]...\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "     |  \n",
      "     |  __include_fields__ = None\n",
      "     |  \n",
      "     |  __parameters__ = ()\n",
      "     |  \n",
      "     |  __post_root_validators__ = [(False, <function BaseLLM.raise_deprecatio...\n",
      "     |  \n",
      "     |  __pre_root_validators__ = []\n",
      "     |  \n",
      "     |  __private_attributes__ = {'_lc_kwargs': ModelPrivateAttr(default=Pydan...\n",
      "     |  \n",
      "     |  __schema_cache__ = {}\n",
      "     |  \n",
      "     |  __signature__ = <Signature (*, cache: Optional[bool] = None, ver...ct[...\n",
      "     |  \n",
      "     |  __validators__ = {'verbose': [<pydantic.class_validators.Validator obj...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  __call__(self, prompt: 'str', stop: 'Optional[List[str]]' = None, callbacks: 'Callbacks' = None, *, tags: 'Optional[List[str]]' = None, metadata: 'Optional[Dict[str, Any]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Check Cache and run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  __str__(self) -> 'str'\n",
      "     |      Get a string representation of the object for printing.\n",
      "     |  \n",
      "     |  async abatch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      "     |      Default implementation of abatch, which calls ainvoke N times.\n",
      "     |      Subclasses should override this method if they can batch more efficiently.\n",
      "     |  \n",
      "     |  async agenerate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  async agenerate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Asynchronously pass a sequence of prompts and return model generations.\n",
      "     |      \n",
      "     |      This method should make use of batched calls for models that expose a batched\n",
      "     |      API.\n",
      "     |      \n",
      "     |      Use this method when you want to:\n",
      "     |          1. take advantage of batched calls,\n",
      "     |          2. need more output from the model than just the top generated value,\n",
      "     |          3. are building chains that are agnostic to the underlying language model\n",
      "     |              type (e.g., pure text completion models vs chat models).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      "     |              converted to match the format of any language model (string for pure\n",
      "     |              text generation models and BaseMessages for chat models).\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          callbacks: Callbacks to pass through. Used for executing additional\n",
      "     |              functionality, such as logging or streaming, throughout generation.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          An LLMResult, which contains a list of candidate Generations for each input\n",
      "     |              prompt and additional model provider-specific output.\n",
      "     |  \n",
      "     |  async ainvoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Default implementation of ainvoke, which calls invoke in a thread pool.\n",
      "     |      Subclasses should override this method if they can run asynchronously.\n",
      "     |  \n",
      "     |  async apredict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Asynchronously pass a string to the model and return a string prediction.\n",
      "     |      \n",
      "     |      Use this method when calling pure text generation models and only the top\n",
      "     |          candidate generation is needed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: String input to pass to the model.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a string.\n",
      "     |  \n",
      "     |  async apredict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      "     |      Asynchronously pass messages to the model and return a message prediction.\n",
      "     |      \n",
      "     |      Use this method when calling chat models and only the top\n",
      "     |          candidate generation is needed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: A sequence of chat messages corresponding to a single model input.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a message.\n",
      "     |  \n",
      "     |  async astream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'AsyncIterator[str]'\n",
      "     |      Default implementation of astream, which calls ainvoke.\n",
      "     |      Subclasses should override this method if they support streaming output.\n",
      "     |  \n",
      "     |  batch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      "     |      Default implementation of batch, which calls invoke N times.\n",
      "     |      Subclasses should override this method if they can batch more efficiently.\n",
      "     |  \n",
      "     |  dict(self, **kwargs: 'Any') -> 'Dict'\n",
      "     |      Return a dictionary of the LLM.\n",
      "     |  \n",
      "     |  generate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  generate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Pass a sequence of prompts to the model and return model generations.\n",
      "     |      \n",
      "     |      This method should make use of batched calls for models that expose a batched\n",
      "     |      API.\n",
      "     |      \n",
      "     |      Use this method when you want to:\n",
      "     |          1. take advantage of batched calls,\n",
      "     |          2. need more output from the model than just the top generated value,\n",
      "     |          3. are building chains that are agnostic to the underlying language model\n",
      "     |              type (e.g., pure text completion models vs chat models).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      "     |              converted to match the format of any language model (string for pure\n",
      "     |              text generation models and BaseMessages for chat models).\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          callbacks: Callbacks to pass through. Used for executing additional\n",
      "     |              functionality, such as logging or streaming, throughout generation.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          An LLMResult, which contains a list of candidate Generations for each input\n",
      "     |              prompt and additional model provider-specific output.\n",
      "     |  \n",
      "     |  invoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |  \n",
      "     |  predict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Pass a single string input to the model and return a string prediction.\n",
      "     |      \n",
      "     |       Use this method when passing in raw text. If you want to pass in specific\n",
      "     |          types of chat messages, use predict_messages.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: String input to pass to the model.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a string.\n",
      "     |  \n",
      "     |  predict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      "     |      Pass a message sequence to the model and return a message prediction.\n",
      "     |      \n",
      "     |      Use this method when passing in chat messages. If you want to pass in raw text,\n",
      "     |          use predict.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: A sequence of chat messages corresponding to a single model input.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a message.\n",
      "     |  \n",
      "     |  save(self, file_path: 'Union[Path, str]') -> 'None'\n",
      "     |      Save the LLM.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          file_path: Path to file to save the LLM to.\n",
      "     |      \n",
      "     |      Example:\n",
      "     |      .. code-block:: python\n",
      "     |      \n",
      "     |          llm.save(file_path=\"path/llm.yaml\")\n",
      "     |  \n",
      "     |  stream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'Iterator[str]'\n",
      "     |      Default implementation of stream, which calls invoke.\n",
      "     |      Subclasses should override this method if they support streaming output.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  raise_deprecation(values: 'Dict') -> 'Dict' from pydantic.main.ModelMetaclass\n",
      "     |      Raise deprecation warning if callback_manager is used.\n",
      "     |  \n",
      "     |  set_verbose(verbose: 'Optional[bool]') -> 'bool' from pydantic.main.ModelMetaclass\n",
      "     |      If verbose is None, set it.\n",
      "     |      \n",
      "     |      This allows users to pass in None as verbose to access the global setting.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  Config = <class 'langchain.llms.base.BaseLLM.Config'>\n",
      "     |      Configuration for this pydantic object.\n",
      "     |  \n",
      "     |  \n",
      "     |  __orig_bases__ = (langchain.schema.language_model.BaseLanguageModel[st...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.schema.language_model.BaseLanguageModel:\n",
      "     |  \n",
      "     |  get_num_tokens(self, text: 'str') -> 'int'\n",
      "     |      Get the number of tokens present in the text.\n",
      "     |      \n",
      "     |      Useful for checking if an input will fit in a model's context window.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: The string input to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The integer number of tokens in the text.\n",
      "     |  \n",
      "     |  get_num_tokens_from_messages(self, messages: 'List[BaseMessage]') -> 'int'\n",
      "     |      Get the number of tokens in the messages.\n",
      "     |      \n",
      "     |      Useful for checking if an input will fit in a model's context window.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: The message inputs to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The sum of the number of tokens across the messages.\n",
      "     |  \n",
      "     |  get_token_ids(self, text: 'str') -> 'List[int]'\n",
      "     |      Return the ordered ids of the tokens in a text.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: The string input to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          A list of ids corresponding to the tokens in the text, in order they occur\n",
      "     |              in the text.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from langchain.schema.language_model.BaseLanguageModel:\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.load.serializable.Serializable:\n",
      "     |  \n",
      "     |  __init__(self, **kwargs: Any) -> None\n",
      "     |      Create a new model by parsing and validating input data from keyword arguments.\n",
      "     |      \n",
      "     |      Raises ValidationError if the input data cannot be parsed to form a valid model.\n",
      "     |  \n",
      "     |  to_json(self) -> Union[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\n",
      "     |  \n",
      "     |  to_json_not_implemented(self) -> langchain.load.serializable.SerializedNotImplemented\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from langchain.load.serializable.Serializable:\n",
      "     |  \n",
      "     |  lc_attributes\n",
      "     |      Return a list of attribute names that should be included in the\n",
      "     |      serialized kwargs. These attributes must be accepted by the\n",
      "     |      constructor.\n",
      "     |  \n",
      "     |  lc_namespace\n",
      "     |      Return the namespace of the langchain object.\n",
      "     |      eg. [\"langchain\", \"llms\", \"openai\"]\n",
      "     |  \n",
      "     |  lc_secrets\n",
      "     |      Return a map of constructor argument names to secret ids.\n",
      "     |      eg. {\"openai_api_key\": \"OPENAI_API_KEY\"}\n",
      "     |  \n",
      "     |  lc_serializable\n",
      "     |      Return whether or not the class is serializable.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __eq__(self, other: Any) -> bool\n",
      "     |  \n",
      "     |  __getstate__(self) -> 'DictAny'\n",
      "     |  \n",
      "     |  __iter__(self) -> 'TupleGenerator'\n",
      "     |      so `dict(model)` works\n",
      "     |  \n",
      "     |  __repr_args__(self) -> 'ReprArgs'\n",
      "     |  \n",
      "     |  __setattr__(self, name, value)\n",
      "     |  \n",
      "     |  __setstate__(self, state: 'DictAny') -> None\n",
      "     |  \n",
      "     |  copy(self: 'Model', *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, update: Optional[ForwardRef('DictStrAny')] = None, deep: bool = False) -> 'Model'\n",
      "     |      Duplicate a model, optionally choose which fields to include, exclude and change.\n",
      "     |      \n",
      "     |      :param include: fields to include in new model\n",
      "     |      :param exclude: fields to exclude from new model, as with values this takes precedence over include\n",
      "     |      :param update: values to change/add in the new model. Note: the data is not validated before creating\n",
      "     |          the new model: you should trust this data\n",
      "     |      :param deep: set to `True` to make a deep copy of the model\n",
      "     |      :return: new model instance\n",
      "     |  \n",
      "     |  json(self, *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) -> 'unicode'\n",
      "     |      Generate a JSON representation of the model, `include` and `exclude` arguments as per `dict()`.\n",
      "     |      \n",
      "     |      `encoder` is an optional function to supply as `default` to json.dumps(), other arguments as per `json.dumps()`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __get_validators__() -> 'CallableGenerator' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  __try_update_forward_refs__(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      "     |      Same as update_forward_refs but will not raise exception\n",
      "     |      when forward references are not defined.\n",
      "     |  \n",
      "     |  construct(_fields_set: Optional[ForwardRef('SetStr')] = None, **values: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |      Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data.\n",
      "     |      Default values are respected, but no other validation is performed.\n",
      "     |      Behaves as if `Config.extra = 'allow'` was set since it adds all passed values\n",
      "     |  \n",
      "     |  from_orm(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_file(path: Union[str, pathlib.Path], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_obj(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_raw(b: Union[str, bytes], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema(by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}') -> 'DictStrAny' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema_json(*, by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}', **dumps_kwargs: Any) -> 'unicode' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  update_forward_refs(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      "     |      Try to update ForwardRefs on fields based on this Model, globalns and localns.\n",
      "     |  \n",
      "     |  validate(value: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __fields_set__\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.utils.Representation:\n",
      "     |  \n",
      "     |  __pretty__(self, fmt: Callable[[Any], Any], **kwargs: Any) -> Generator[Any, NoneType, NoneType]\n",
      "     |      Used by devtools (https://python-devtools.helpmanual.io/) to provide a human readable representations of objects\n",
      "     |  \n",
      "     |  __repr__(self) -> 'unicode'\n",
      "     |  \n",
      "     |  __repr_name__(self) -> 'unicode'\n",
      "     |      Name of the instance's class, used in __repr__.\n",
      "     |  \n",
      "     |  __repr_str__(self, join_str: 'unicode') -> 'unicode'\n",
      "     |  \n",
      "     |  __rich_repr__(self) -> 'RichReprResult'\n",
      "     |      Get fields for Rich library\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.schema.runnable.base.Runnable:\n",
      "     |  \n",
      "     |  __or__(self, other: 'Union[Runnable[Any, Other], Callable[[Any], Other], Mapping[str, Union[Runnable[Any, Other], Callable[[Any], Other], Any]]]') -> 'RunnableSequence[Input, Other]'\n",
      "     |      Return self|value.\n",
      "     |  \n",
      "     |  __ror__(self, other: 'Union[Runnable[Other, Any], Callable[[Any], Other], Mapping[str, Union[Runnable[Other, Any], Callable[[Other], Any], Any]]]') -> 'RunnableSequence[Other, Output]'\n",
      "     |      Return value|self.\n",
      "     |  \n",
      "     |  async atransform(self, input: 'AsyncIterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'AsyncIterator[Output]'\n",
      "     |      Default implementation of atransform, which buffers input and calls astream.\n",
      "     |      Subclasses should override this method if they can start producing output while\n",
      "     |      input is still being generated.\n",
      "     |  \n",
      "     |  bind(self, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      "     |      Bind arguments to a Runnable, returning a new Runnable.\n",
      "     |  \n",
      "     |  map(self) -> 'Runnable[List[Input], List[Output]]'\n",
      "     |      Return a new Runnable that maps a list of inputs to a list of outputs,\n",
      "     |      by calling invoke() with each input.\n",
      "     |  \n",
      "     |  transform(self, input: 'Iterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'Iterator[Output]'\n",
      "     |      Default implementation of transform, which buffers input and then calls stream.\n",
      "     |      Subclasses should override this method if they can start producing output while\n",
      "     |      input is still being generated.\n",
      "     |  \n",
      "     |  with_config(self, config: 'Optional[RunnableConfig]' = None, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      "     |      Bind config to a Runnable, returning a new Runnable.\n",
      "     |  \n",
      "     |  with_fallbacks(self, fallbacks: 'Sequence[Runnable[Input, Output]]', *, exceptions_to_handle: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,)) -> 'RunnableWithFallbacks[Input, Output]'\n",
      "     |  \n",
      "     |  with_retry(self, *, retry_if_exception_type: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,), wait_exponential_jitter: 'bool' = True, stop_after_attempt: 'int' = 3) -> 'Runnable[Input, Output]'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __class_getitem__(params) from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  __init_subclass__(*args, **kwargs) from pydantic.main.ModelMetaclass\n",
      "     |      This method is called when a class is subclassed.\n",
      "     |      \n",
      "     |      The default implementation does nothing. It may be\n",
      "     |      overridden to extend subclasses.\n",
      "    \n",
      "    class VLLMOpenAI(langchain.llms.openai.BaseOpenAI)\n",
      "     |  VLLMOpenAI(*, cache: Optional[bool] = None, verbose: bool = None, callbacks: Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager, NoneType] = None, callback_manager: Optional[langchain.callbacks.base.BaseCallbackManager] = None, tags: Optional[List[str]] = None, metadata: Optional[Dict[str, Any]] = None, client: Any = None, model: str = 'text-davinci-003', temperature: float = 0.7, max_tokens: int = 256, top_p: float = 1, frequency_penalty: float = 0, presence_penalty: float = 0, n: int = 1, best_of: int = 1, model_kwargs: Dict[str, Any] = None, openai_api_key: Optional[str] = None, openai_api_base: Optional[str] = None, openai_organization: Optional[str] = None, openai_proxy: Optional[str] = None, batch_size: int = 20, request_timeout: Union[float, Tuple[float, float], NoneType] = None, logit_bias: Optional[Dict[str, float]] = None, max_retries: int = 6, streaming: bool = False, allowed_special: Union[Literal['all'], AbstractSet[str]] = set(), disallowed_special: Union[Literal['all'], Collection[str]] = 'all', tiktoken_model_name: Optional[str] = None) -> None\n",
      "     |  \n",
      "     |  vLLM OpenAI-compatible API client\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      VLLMOpenAI\n",
      "     |      langchain.llms.openai.BaseOpenAI\n",
      "     |      langchain.llms.base.BaseLLM\n",
      "     |      langchain.schema.language_model.BaseLanguageModel\n",
      "     |      langchain.load.serializable.Serializable\n",
      "     |      pydantic.main.BaseModel\n",
      "     |      pydantic.utils.Representation\n",
      "     |      langchain.schema.runnable.base.Runnable\n",
      "     |      typing.Generic\n",
      "     |      abc.ABC\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __json_encoder__ = pydantic_encoder(obj: Any) -> Any\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {}\n",
      "     |  \n",
      "     |  __class_vars__ = set()\n",
      "     |  \n",
      "     |  __config__ = <class 'langchain.llms.vllm.Config'>\n",
      "     |  \n",
      "     |  __custom_root_type__ = False\n",
      "     |  \n",
      "     |  __exclude_fields__ = {'callback_manager': True, 'callbacks': True, 'me...\n",
      "     |  \n",
      "     |  __fields__ = {'allowed_special': ModelField(name='allowed_special', ty...\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "     |  \n",
      "     |  __include_fields__ = None\n",
      "     |  \n",
      "     |  __parameters__ = ()\n",
      "     |  \n",
      "     |  __post_root_validators__ = [(False, <function BaseLLM.raise_deprecatio...\n",
      "     |  \n",
      "     |  __pre_root_validators__ = [<function BaseOpenAI.build_extra>]\n",
      "     |  \n",
      "     |  __private_attributes__ = {'_lc_kwargs': ModelPrivateAttr(default=Pydan...\n",
      "     |  \n",
      "     |  __schema_cache__ = {}\n",
      "     |  \n",
      "     |  __signature__ = <Signature (*, cache: Optional[bool] = None, ver...kto...\n",
      "     |  \n",
      "     |  __validators__ = {'verbose': [<pydantic.class_validators.Validator obj...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.llms.openai.BaseOpenAI:\n",
      "     |  \n",
      "     |  create_llm_result(self, choices: 'Any', prompts: 'List[str]', token_usage: 'Dict[str, int]') -> 'LLMResult'\n",
      "     |      Create the LLMResult from the choices and prompts.\n",
      "     |  \n",
      "     |  get_sub_prompts(self, params: 'Dict[str, Any]', prompts: 'List[str]', stop: 'Optional[List[str]]' = None) -> 'List[List[str]]'\n",
      "     |      Get the sub prompts for llm call.\n",
      "     |  \n",
      "     |  get_token_ids(self, text: 'str') -> 'List[int]'\n",
      "     |      Get the token IDs using the tiktoken package.\n",
      "     |  \n",
      "     |  max_tokens_for_prompt(self, prompt: 'str') -> 'int'\n",
      "     |      Calculate the maximum number of tokens possible to generate for a prompt.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompt: The prompt to pass into the model.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The maximum number of tokens to generate for a prompt.\n",
      "     |      \n",
      "     |      Example:\n",
      "     |          .. code-block:: python\n",
      "     |      \n",
      "     |              max_tokens = openai.max_token_for_prompt(\"Tell me a joke.\")\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from langchain.llms.openai.BaseOpenAI:\n",
      "     |  \n",
      "     |  build_extra(values: 'Dict[str, Any]') -> 'Dict[str, Any]' from pydantic.main.ModelMetaclass\n",
      "     |      Build extra kwargs from additional params that were passed in.\n",
      "     |  \n",
      "     |  validate_environment(values: 'Dict') -> 'Dict' from pydantic.main.ModelMetaclass\n",
      "     |      Validate that api key and python package exists in environment.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from langchain.llms.openai.BaseOpenAI:\n",
      "     |  \n",
      "     |  __new__(cls, **data: 'Any') -> 'Union[OpenAIChat, BaseOpenAI]'\n",
      "     |      Initialize the OpenAI object.\n",
      "     |  \n",
      "     |  modelname_to_contextsize(modelname: 'str') -> 'int'\n",
      "     |      Calculate the maximum number of tokens possible to generate for a model.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          modelname: The modelname we want to know the context size for.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The maximum context size\n",
      "     |      \n",
      "     |      Example:\n",
      "     |          .. code-block:: python\n",
      "     |      \n",
      "     |              max_tokens = openai.modelname_to_contextsize(\"text-davinci-003\")\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from langchain.llms.openai.BaseOpenAI:\n",
      "     |  \n",
      "     |  lc_secrets\n",
      "     |      Return a map of constructor argument names to secret ids.\n",
      "     |      eg. {\"openai_api_key\": \"OPENAI_API_KEY\"}\n",
      "     |  \n",
      "     |  lc_serializable\n",
      "     |      Return whether or not the class is serializable.\n",
      "     |  \n",
      "     |  max_context_size\n",
      "     |      Get max context size for this model.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from langchain.llms.openai.BaseOpenAI:\n",
      "     |  \n",
      "     |  Config = <class 'langchain.llms.openai.BaseOpenAI.Config'>\n",
      "     |      Configuration for this pydantic object.\n",
      "     |  \n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  __call__(self, prompt: 'str', stop: 'Optional[List[str]]' = None, callbacks: 'Callbacks' = None, *, tags: 'Optional[List[str]]' = None, metadata: 'Optional[Dict[str, Any]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Check Cache and run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  __str__(self) -> 'str'\n",
      "     |      Get a string representation of the object for printing.\n",
      "     |  \n",
      "     |  async abatch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      "     |      Default implementation of abatch, which calls ainvoke N times.\n",
      "     |      Subclasses should override this method if they can batch more efficiently.\n",
      "     |  \n",
      "     |  async agenerate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  async agenerate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Asynchronously pass a sequence of prompts and return model generations.\n",
      "     |      \n",
      "     |      This method should make use of batched calls for models that expose a batched\n",
      "     |      API.\n",
      "     |      \n",
      "     |      Use this method when you want to:\n",
      "     |          1. take advantage of batched calls,\n",
      "     |          2. need more output from the model than just the top generated value,\n",
      "     |          3. are building chains that are agnostic to the underlying language model\n",
      "     |              type (e.g., pure text completion models vs chat models).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      "     |              converted to match the format of any language model (string for pure\n",
      "     |              text generation models and BaseMessages for chat models).\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          callbacks: Callbacks to pass through. Used for executing additional\n",
      "     |              functionality, such as logging or streaming, throughout generation.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          An LLMResult, which contains a list of candidate Generations for each input\n",
      "     |              prompt and additional model provider-specific output.\n",
      "     |  \n",
      "     |  async ainvoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Default implementation of ainvoke, which calls invoke in a thread pool.\n",
      "     |      Subclasses should override this method if they can run asynchronously.\n",
      "     |  \n",
      "     |  async apredict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Asynchronously pass a string to the model and return a string prediction.\n",
      "     |      \n",
      "     |      Use this method when calling pure text generation models and only the top\n",
      "     |          candidate generation is needed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: String input to pass to the model.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a string.\n",
      "     |  \n",
      "     |  async apredict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      "     |      Asynchronously pass messages to the model and return a message prediction.\n",
      "     |      \n",
      "     |      Use this method when calling chat models and only the top\n",
      "     |          candidate generation is needed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: A sequence of chat messages corresponding to a single model input.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a message.\n",
      "     |  \n",
      "     |  async astream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'AsyncIterator[str]'\n",
      "     |      Default implementation of astream, which calls ainvoke.\n",
      "     |      Subclasses should override this method if they support streaming output.\n",
      "     |  \n",
      "     |  batch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      "     |      Default implementation of batch, which calls invoke N times.\n",
      "     |      Subclasses should override this method if they can batch more efficiently.\n",
      "     |  \n",
      "     |  dict(self, **kwargs: 'Any') -> 'Dict'\n",
      "     |      Return a dictionary of the LLM.\n",
      "     |  \n",
      "     |  generate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  generate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Pass a sequence of prompts to the model and return model generations.\n",
      "     |      \n",
      "     |      This method should make use of batched calls for models that expose a batched\n",
      "     |      API.\n",
      "     |      \n",
      "     |      Use this method when you want to:\n",
      "     |          1. take advantage of batched calls,\n",
      "     |          2. need more output from the model than just the top generated value,\n",
      "     |          3. are building chains that are agnostic to the underlying language model\n",
      "     |              type (e.g., pure text completion models vs chat models).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      "     |              converted to match the format of any language model (string for pure\n",
      "     |              text generation models and BaseMessages for chat models).\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          callbacks: Callbacks to pass through. Used for executing additional\n",
      "     |              functionality, such as logging or streaming, throughout generation.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          An LLMResult, which contains a list of candidate Generations for each input\n",
      "     |              prompt and additional model provider-specific output.\n",
      "     |  \n",
      "     |  invoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |  \n",
      "     |  predict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Pass a single string input to the model and return a string prediction.\n",
      "     |      \n",
      "     |       Use this method when passing in raw text. If you want to pass in specific\n",
      "     |          types of chat messages, use predict_messages.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: String input to pass to the model.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a string.\n",
      "     |  \n",
      "     |  predict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      "     |      Pass a message sequence to the model and return a message prediction.\n",
      "     |      \n",
      "     |      Use this method when passing in chat messages. If you want to pass in raw text,\n",
      "     |          use predict.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: A sequence of chat messages corresponding to a single model input.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a message.\n",
      "     |  \n",
      "     |  save(self, file_path: 'Union[Path, str]') -> 'None'\n",
      "     |      Save the LLM.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          file_path: Path to file to save the LLM to.\n",
      "     |      \n",
      "     |      Example:\n",
      "     |      .. code-block:: python\n",
      "     |      \n",
      "     |          llm.save(file_path=\"path/llm.yaml\")\n",
      "     |  \n",
      "     |  stream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'Iterator[str]'\n",
      "     |      Default implementation of stream, which calls invoke.\n",
      "     |      Subclasses should override this method if they support streaming output.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  raise_deprecation(values: 'Dict') -> 'Dict' from pydantic.main.ModelMetaclass\n",
      "     |      Raise deprecation warning if callback_manager is used.\n",
      "     |  \n",
      "     |  set_verbose(verbose: 'Optional[bool]') -> 'bool' from pydantic.main.ModelMetaclass\n",
      "     |      If verbose is None, set it.\n",
      "     |      \n",
      "     |      This allows users to pass in None as verbose to access the global setting.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  __orig_bases__ = (langchain.schema.language_model.BaseLanguageModel[st...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.schema.language_model.BaseLanguageModel:\n",
      "     |  \n",
      "     |  get_num_tokens(self, text: 'str') -> 'int'\n",
      "     |      Get the number of tokens present in the text.\n",
      "     |      \n",
      "     |      Useful for checking if an input will fit in a model's context window.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: The string input to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The integer number of tokens in the text.\n",
      "     |  \n",
      "     |  get_num_tokens_from_messages(self, messages: 'List[BaseMessage]') -> 'int'\n",
      "     |      Get the number of tokens in the messages.\n",
      "     |      \n",
      "     |      Useful for checking if an input will fit in a model's context window.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: The message inputs to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The sum of the number of tokens across the messages.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from langchain.schema.language_model.BaseLanguageModel:\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.load.serializable.Serializable:\n",
      "     |  \n",
      "     |  __init__(self, **kwargs: Any) -> None\n",
      "     |      Create a new model by parsing and validating input data from keyword arguments.\n",
      "     |      \n",
      "     |      Raises ValidationError if the input data cannot be parsed to form a valid model.\n",
      "     |  \n",
      "     |  to_json(self) -> Union[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\n",
      "     |  \n",
      "     |  to_json_not_implemented(self) -> langchain.load.serializable.SerializedNotImplemented\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from langchain.load.serializable.Serializable:\n",
      "     |  \n",
      "     |  lc_attributes\n",
      "     |      Return a list of attribute names that should be included in the\n",
      "     |      serialized kwargs. These attributes must be accepted by the\n",
      "     |      constructor.\n",
      "     |  \n",
      "     |  lc_namespace\n",
      "     |      Return the namespace of the langchain object.\n",
      "     |      eg. [\"langchain\", \"llms\", \"openai\"]\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __eq__(self, other: Any) -> bool\n",
      "     |  \n",
      "     |  __getstate__(self) -> 'DictAny'\n",
      "     |  \n",
      "     |  __iter__(self) -> 'TupleGenerator'\n",
      "     |      so `dict(model)` works\n",
      "     |  \n",
      "     |  __repr_args__(self) -> 'ReprArgs'\n",
      "     |  \n",
      "     |  __setattr__(self, name, value)\n",
      "     |  \n",
      "     |  __setstate__(self, state: 'DictAny') -> None\n",
      "     |  \n",
      "     |  copy(self: 'Model', *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, update: Optional[ForwardRef('DictStrAny')] = None, deep: bool = False) -> 'Model'\n",
      "     |      Duplicate a model, optionally choose which fields to include, exclude and change.\n",
      "     |      \n",
      "     |      :param include: fields to include in new model\n",
      "     |      :param exclude: fields to exclude from new model, as with values this takes precedence over include\n",
      "     |      :param update: values to change/add in the new model. Note: the data is not validated before creating\n",
      "     |          the new model: you should trust this data\n",
      "     |      :param deep: set to `True` to make a deep copy of the model\n",
      "     |      :return: new model instance\n",
      "     |  \n",
      "     |  json(self, *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) -> 'unicode'\n",
      "     |      Generate a JSON representation of the model, `include` and `exclude` arguments as per `dict()`.\n",
      "     |      \n",
      "     |      `encoder` is an optional function to supply as `default` to json.dumps(), other arguments as per `json.dumps()`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __get_validators__() -> 'CallableGenerator' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  __try_update_forward_refs__(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      "     |      Same as update_forward_refs but will not raise exception\n",
      "     |      when forward references are not defined.\n",
      "     |  \n",
      "     |  construct(_fields_set: Optional[ForwardRef('SetStr')] = None, **values: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |      Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data.\n",
      "     |      Default values are respected, but no other validation is performed.\n",
      "     |      Behaves as if `Config.extra = 'allow'` was set since it adds all passed values\n",
      "     |  \n",
      "     |  from_orm(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_file(path: Union[str, pathlib.Path], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_obj(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_raw(b: Union[str, bytes], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema(by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}') -> 'DictStrAny' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema_json(*, by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}', **dumps_kwargs: Any) -> 'unicode' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  update_forward_refs(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      "     |      Try to update ForwardRefs on fields based on this Model, globalns and localns.\n",
      "     |  \n",
      "     |  validate(value: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __fields_set__\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.utils.Representation:\n",
      "     |  \n",
      "     |  __pretty__(self, fmt: Callable[[Any], Any], **kwargs: Any) -> Generator[Any, NoneType, NoneType]\n",
      "     |      Used by devtools (https://python-devtools.helpmanual.io/) to provide a human readable representations of objects\n",
      "     |  \n",
      "     |  __repr__(self) -> 'unicode'\n",
      "     |  \n",
      "     |  __repr_name__(self) -> 'unicode'\n",
      "     |      Name of the instance's class, used in __repr__.\n",
      "     |  \n",
      "     |  __repr_str__(self, join_str: 'unicode') -> 'unicode'\n",
      "     |  \n",
      "     |  __rich_repr__(self) -> 'RichReprResult'\n",
      "     |      Get fields for Rich library\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.schema.runnable.base.Runnable:\n",
      "     |  \n",
      "     |  __or__(self, other: 'Union[Runnable[Any, Other], Callable[[Any], Other], Mapping[str, Union[Runnable[Any, Other], Callable[[Any], Other], Any]]]') -> 'RunnableSequence[Input, Other]'\n",
      "     |      Return self|value.\n",
      "     |  \n",
      "     |  __ror__(self, other: 'Union[Runnable[Other, Any], Callable[[Any], Other], Mapping[str, Union[Runnable[Other, Any], Callable[[Other], Any], Any]]]') -> 'RunnableSequence[Other, Output]'\n",
      "     |      Return value|self.\n",
      "     |  \n",
      "     |  async atransform(self, input: 'AsyncIterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'AsyncIterator[Output]'\n",
      "     |      Default implementation of atransform, which buffers input and calls astream.\n",
      "     |      Subclasses should override this method if they can start producing output while\n",
      "     |      input is still being generated.\n",
      "     |  \n",
      "     |  bind(self, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      "     |      Bind arguments to a Runnable, returning a new Runnable.\n",
      "     |  \n",
      "     |  map(self) -> 'Runnable[List[Input], List[Output]]'\n",
      "     |      Return a new Runnable that maps a list of inputs to a list of outputs,\n",
      "     |      by calling invoke() with each input.\n",
      "     |  \n",
      "     |  transform(self, input: 'Iterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'Iterator[Output]'\n",
      "     |      Default implementation of transform, which buffers input and then calls stream.\n",
      "     |      Subclasses should override this method if they can start producing output while\n",
      "     |      input is still being generated.\n",
      "     |  \n",
      "     |  with_config(self, config: 'Optional[RunnableConfig]' = None, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      "     |      Bind config to a Runnable, returning a new Runnable.\n",
      "     |  \n",
      "     |  with_fallbacks(self, fallbacks: 'Sequence[Runnable[Input, Output]]', *, exceptions_to_handle: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,)) -> 'RunnableWithFallbacks[Input, Output]'\n",
      "     |  \n",
      "     |  with_retry(self, *, retry_if_exception_type: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,), wait_exponential_jitter: 'bool' = True, stop_after_attempt: 'int' = 3) -> 'Runnable[Input, Output]'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __class_getitem__(params) from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  __init_subclass__(*args, **kwargs) from pydantic.main.ModelMetaclass\n",
      "     |      This method is called when a class is subclassed.\n",
      "     |      \n",
      "     |      The default implementation does nothing. It may be\n",
      "     |      overridden to extend subclasses.\n",
      "    \n",
      "    class VertexAI(_VertexAICommon, langchain.llms.base.LLM)\n",
      "     |  VertexAI(*, cache: Optional[bool] = None, verbose: bool = None, callbacks: Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager, NoneType] = None, callback_manager: Optional[langchain.callbacks.base.BaseCallbackManager] = None, tags: Optional[List[str]] = None, metadata: Optional[Dict[str, Any]] = None, project: Optional[str] = None, location: str = 'us-central1', request_parallelism: int = 5, max_retries: int = 6, stop: Optional[List[str]] = None, model_name: str = 'text-bison', client: ForwardRef(\"'_LanguageModel'\") = None, temperature: float = 0.0, max_output_tokens: int = 128, top_p: float = 0.95, top_k: int = 40, credentials: Any = None, tuned_model_name: Optional[str] = None) -> None\n",
      "     |  \n",
      "     |  Google Vertex AI large language models.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      VertexAI\n",
      "     |      _VertexAICommon\n",
      "     |      _VertexAIBase\n",
      "     |      langchain.llms.base.LLM\n",
      "     |      langchain.llms.base.BaseLLM\n",
      "     |      langchain.schema.language_model.BaseLanguageModel\n",
      "     |      langchain.load.serializable.Serializable\n",
      "     |      pydantic.main.BaseModel\n",
      "     |      pydantic.utils.Representation\n",
      "     |      langchain.schema.runnable.base.Runnable\n",
      "     |      typing.Generic\n",
      "     |      abc.ABC\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  validate_environment(values: 'Dict') -> 'Dict' from pydantic.main.ModelMetaclass\n",
      "     |      Validate that the python package exists in environment.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __json_encoder__ = pydantic_encoder(obj: Any) -> Any\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {'model_name': 'str', 'tuned_model_name': 'Optional[...\n",
      "     |  \n",
      "     |  __class_vars__ = {'task_executor'}\n",
      "     |  \n",
      "     |  __config__ = <class 'langchain.llms.vertexai.Config'>\n",
      "     |  \n",
      "     |  __custom_root_type__ = False\n",
      "     |  \n",
      "     |  __exclude_fields__ = {'callback_manager': True, 'callbacks': True, 'me...\n",
      "     |  \n",
      "     |  __fields__ = {'cache': ModelField(name='cache', type=Optional[bool], r...\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "     |  \n",
      "     |  __include_fields__ = None\n",
      "     |  \n",
      "     |  __parameters__ = ()\n",
      "     |  \n",
      "     |  __post_root_validators__ = [(False, <function BaseLLM.raise_deprecatio...\n",
      "     |  \n",
      "     |  __pre_root_validators__ = []\n",
      "     |  \n",
      "     |  __private_attributes__ = {'_lc_kwargs': ModelPrivateAttr(default=Pydan...\n",
      "     |  \n",
      "     |  __schema_cache__ = {}\n",
      "     |  \n",
      "     |  __signature__ = <Signature (*, cache: Optional[bool] = None, ver... tu...\n",
      "     |  \n",
      "     |  __validators__ = {'verbose': [<pydantic.class_validators.Validator obj...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from _VertexAICommon:\n",
      "     |  \n",
      "     |  is_codey_model\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from _VertexAIBase:\n",
      "     |  \n",
      "     |  task_executor = None\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  __call__(self, prompt: 'str', stop: 'Optional[List[str]]' = None, callbacks: 'Callbacks' = None, *, tags: 'Optional[List[str]]' = None, metadata: 'Optional[Dict[str, Any]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Check Cache and run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  __str__(self) -> 'str'\n",
      "     |      Get a string representation of the object for printing.\n",
      "     |  \n",
      "     |  async abatch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      "     |      Default implementation of abatch, which calls ainvoke N times.\n",
      "     |      Subclasses should override this method if they can batch more efficiently.\n",
      "     |  \n",
      "     |  async agenerate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  async agenerate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Asynchronously pass a sequence of prompts and return model generations.\n",
      "     |      \n",
      "     |      This method should make use of batched calls for models that expose a batched\n",
      "     |      API.\n",
      "     |      \n",
      "     |      Use this method when you want to:\n",
      "     |          1. take advantage of batched calls,\n",
      "     |          2. need more output from the model than just the top generated value,\n",
      "     |          3. are building chains that are agnostic to the underlying language model\n",
      "     |              type (e.g., pure text completion models vs chat models).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      "     |              converted to match the format of any language model (string for pure\n",
      "     |              text generation models and BaseMessages for chat models).\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          callbacks: Callbacks to pass through. Used for executing additional\n",
      "     |              functionality, such as logging or streaming, throughout generation.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          An LLMResult, which contains a list of candidate Generations for each input\n",
      "     |              prompt and additional model provider-specific output.\n",
      "     |  \n",
      "     |  async ainvoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Default implementation of ainvoke, which calls invoke in a thread pool.\n",
      "     |      Subclasses should override this method if they can run asynchronously.\n",
      "     |  \n",
      "     |  async apredict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Asynchronously pass a string to the model and return a string prediction.\n",
      "     |      \n",
      "     |      Use this method when calling pure text generation models and only the top\n",
      "     |          candidate generation is needed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: String input to pass to the model.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a string.\n",
      "     |  \n",
      "     |  async apredict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      "     |      Asynchronously pass messages to the model and return a message prediction.\n",
      "     |      \n",
      "     |      Use this method when calling chat models and only the top\n",
      "     |          candidate generation is needed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: A sequence of chat messages corresponding to a single model input.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a message.\n",
      "     |  \n",
      "     |  async astream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'AsyncIterator[str]'\n",
      "     |      Default implementation of astream, which calls ainvoke.\n",
      "     |      Subclasses should override this method if they support streaming output.\n",
      "     |  \n",
      "     |  batch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      "     |      Default implementation of batch, which calls invoke N times.\n",
      "     |      Subclasses should override this method if they can batch more efficiently.\n",
      "     |  \n",
      "     |  dict(self, **kwargs: 'Any') -> 'Dict'\n",
      "     |      Return a dictionary of the LLM.\n",
      "     |  \n",
      "     |  generate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  generate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Pass a sequence of prompts to the model and return model generations.\n",
      "     |      \n",
      "     |      This method should make use of batched calls for models that expose a batched\n",
      "     |      API.\n",
      "     |      \n",
      "     |      Use this method when you want to:\n",
      "     |          1. take advantage of batched calls,\n",
      "     |          2. need more output from the model than just the top generated value,\n",
      "     |          3. are building chains that are agnostic to the underlying language model\n",
      "     |              type (e.g., pure text completion models vs chat models).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      "     |              converted to match the format of any language model (string for pure\n",
      "     |              text generation models and BaseMessages for chat models).\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          callbacks: Callbacks to pass through. Used for executing additional\n",
      "     |              functionality, such as logging or streaming, throughout generation.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          An LLMResult, which contains a list of candidate Generations for each input\n",
      "     |              prompt and additional model provider-specific output.\n",
      "     |  \n",
      "     |  invoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |  \n",
      "     |  predict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Pass a single string input to the model and return a string prediction.\n",
      "     |      \n",
      "     |       Use this method when passing in raw text. If you want to pass in specific\n",
      "     |          types of chat messages, use predict_messages.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: String input to pass to the model.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a string.\n",
      "     |  \n",
      "     |  predict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      "     |      Pass a message sequence to the model and return a message prediction.\n",
      "     |      \n",
      "     |      Use this method when passing in chat messages. If you want to pass in raw text,\n",
      "     |          use predict.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: A sequence of chat messages corresponding to a single model input.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a message.\n",
      "     |  \n",
      "     |  save(self, file_path: 'Union[Path, str]') -> 'None'\n",
      "     |      Save the LLM.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          file_path: Path to file to save the LLM to.\n",
      "     |      \n",
      "     |      Example:\n",
      "     |      .. code-block:: python\n",
      "     |      \n",
      "     |          llm.save(file_path=\"path/llm.yaml\")\n",
      "     |  \n",
      "     |  stream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'Iterator[str]'\n",
      "     |      Default implementation of stream, which calls invoke.\n",
      "     |      Subclasses should override this method if they support streaming output.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  raise_deprecation(values: 'Dict') -> 'Dict' from pydantic.main.ModelMetaclass\n",
      "     |      Raise deprecation warning if callback_manager is used.\n",
      "     |  \n",
      "     |  set_verbose(verbose: 'Optional[bool]') -> 'bool' from pydantic.main.ModelMetaclass\n",
      "     |      If verbose is None, set it.\n",
      "     |      \n",
      "     |      This allows users to pass in None as verbose to access the global setting.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  Config = <class 'langchain.llms.base.BaseLLM.Config'>\n",
      "     |      Configuration for this pydantic object.\n",
      "     |  \n",
      "     |  \n",
      "     |  __orig_bases__ = (langchain.schema.language_model.BaseLanguageModel[st...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.schema.language_model.BaseLanguageModel:\n",
      "     |  \n",
      "     |  get_num_tokens(self, text: 'str') -> 'int'\n",
      "     |      Get the number of tokens present in the text.\n",
      "     |      \n",
      "     |      Useful for checking if an input will fit in a model's context window.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: The string input to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The integer number of tokens in the text.\n",
      "     |  \n",
      "     |  get_num_tokens_from_messages(self, messages: 'List[BaseMessage]') -> 'int'\n",
      "     |      Get the number of tokens in the messages.\n",
      "     |      \n",
      "     |      Useful for checking if an input will fit in a model's context window.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: The message inputs to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The sum of the number of tokens across the messages.\n",
      "     |  \n",
      "     |  get_token_ids(self, text: 'str') -> 'List[int]'\n",
      "     |      Return the ordered ids of the tokens in a text.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: The string input to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          A list of ids corresponding to the tokens in the text, in order they occur\n",
      "     |              in the text.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from langchain.schema.language_model.BaseLanguageModel:\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.load.serializable.Serializable:\n",
      "     |  \n",
      "     |  __init__(self, **kwargs: Any) -> None\n",
      "     |      Create a new model by parsing and validating input data from keyword arguments.\n",
      "     |      \n",
      "     |      Raises ValidationError if the input data cannot be parsed to form a valid model.\n",
      "     |  \n",
      "     |  to_json(self) -> Union[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\n",
      "     |  \n",
      "     |  to_json_not_implemented(self) -> langchain.load.serializable.SerializedNotImplemented\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from langchain.load.serializable.Serializable:\n",
      "     |  \n",
      "     |  lc_attributes\n",
      "     |      Return a list of attribute names that should be included in the\n",
      "     |      serialized kwargs. These attributes must be accepted by the\n",
      "     |      constructor.\n",
      "     |  \n",
      "     |  lc_namespace\n",
      "     |      Return the namespace of the langchain object.\n",
      "     |      eg. [\"langchain\", \"llms\", \"openai\"]\n",
      "     |  \n",
      "     |  lc_secrets\n",
      "     |      Return a map of constructor argument names to secret ids.\n",
      "     |      eg. {\"openai_api_key\": \"OPENAI_API_KEY\"}\n",
      "     |  \n",
      "     |  lc_serializable\n",
      "     |      Return whether or not the class is serializable.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __eq__(self, other: Any) -> bool\n",
      "     |  \n",
      "     |  __getstate__(self) -> 'DictAny'\n",
      "     |  \n",
      "     |  __iter__(self) -> 'TupleGenerator'\n",
      "     |      so `dict(model)` works\n",
      "     |  \n",
      "     |  __repr_args__(self) -> 'ReprArgs'\n",
      "     |  \n",
      "     |  __setattr__(self, name, value)\n",
      "     |  \n",
      "     |  __setstate__(self, state: 'DictAny') -> None\n",
      "     |  \n",
      "     |  copy(self: 'Model', *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, update: Optional[ForwardRef('DictStrAny')] = None, deep: bool = False) -> 'Model'\n",
      "     |      Duplicate a model, optionally choose which fields to include, exclude and change.\n",
      "     |      \n",
      "     |      :param include: fields to include in new model\n",
      "     |      :param exclude: fields to exclude from new model, as with values this takes precedence over include\n",
      "     |      :param update: values to change/add in the new model. Note: the data is not validated before creating\n",
      "     |          the new model: you should trust this data\n",
      "     |      :param deep: set to `True` to make a deep copy of the model\n",
      "     |      :return: new model instance\n",
      "     |  \n",
      "     |  json(self, *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) -> 'unicode'\n",
      "     |      Generate a JSON representation of the model, `include` and `exclude` arguments as per `dict()`.\n",
      "     |      \n",
      "     |      `encoder` is an optional function to supply as `default` to json.dumps(), other arguments as per `json.dumps()`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __get_validators__() -> 'CallableGenerator' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  __try_update_forward_refs__(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      "     |      Same as update_forward_refs but will not raise exception\n",
      "     |      when forward references are not defined.\n",
      "     |  \n",
      "     |  construct(_fields_set: Optional[ForwardRef('SetStr')] = None, **values: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |      Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data.\n",
      "     |      Default values are respected, but no other validation is performed.\n",
      "     |      Behaves as if `Config.extra = 'allow'` was set since it adds all passed values\n",
      "     |  \n",
      "     |  from_orm(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_file(path: Union[str, pathlib.Path], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_obj(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_raw(b: Union[str, bytes], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema(by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}') -> 'DictStrAny' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema_json(*, by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}', **dumps_kwargs: Any) -> 'unicode' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  update_forward_refs(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      "     |      Try to update ForwardRefs on fields based on this Model, globalns and localns.\n",
      "     |  \n",
      "     |  validate(value: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __fields_set__\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.utils.Representation:\n",
      "     |  \n",
      "     |  __pretty__(self, fmt: Callable[[Any], Any], **kwargs: Any) -> Generator[Any, NoneType, NoneType]\n",
      "     |      Used by devtools (https://python-devtools.helpmanual.io/) to provide a human readable representations of objects\n",
      "     |  \n",
      "     |  __repr__(self) -> 'unicode'\n",
      "     |  \n",
      "     |  __repr_name__(self) -> 'unicode'\n",
      "     |      Name of the instance's class, used in __repr__.\n",
      "     |  \n",
      "     |  __repr_str__(self, join_str: 'unicode') -> 'unicode'\n",
      "     |  \n",
      "     |  __rich_repr__(self) -> 'RichReprResult'\n",
      "     |      Get fields for Rich library\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.schema.runnable.base.Runnable:\n",
      "     |  \n",
      "     |  __or__(self, other: 'Union[Runnable[Any, Other], Callable[[Any], Other], Mapping[str, Union[Runnable[Any, Other], Callable[[Any], Other], Any]]]') -> 'RunnableSequence[Input, Other]'\n",
      "     |      Return self|value.\n",
      "     |  \n",
      "     |  __ror__(self, other: 'Union[Runnable[Other, Any], Callable[[Any], Other], Mapping[str, Union[Runnable[Other, Any], Callable[[Other], Any], Any]]]') -> 'RunnableSequence[Other, Output]'\n",
      "     |      Return value|self.\n",
      "     |  \n",
      "     |  async atransform(self, input: 'AsyncIterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'AsyncIterator[Output]'\n",
      "     |      Default implementation of atransform, which buffers input and calls astream.\n",
      "     |      Subclasses should override this method if they can start producing output while\n",
      "     |      input is still being generated.\n",
      "     |  \n",
      "     |  bind(self, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      "     |      Bind arguments to a Runnable, returning a new Runnable.\n",
      "     |  \n",
      "     |  map(self) -> 'Runnable[List[Input], List[Output]]'\n",
      "     |      Return a new Runnable that maps a list of inputs to a list of outputs,\n",
      "     |      by calling invoke() with each input.\n",
      "     |  \n",
      "     |  transform(self, input: 'Iterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'Iterator[Output]'\n",
      "     |      Default implementation of transform, which buffers input and then calls stream.\n",
      "     |      Subclasses should override this method if they can start producing output while\n",
      "     |      input is still being generated.\n",
      "     |  \n",
      "     |  with_config(self, config: 'Optional[RunnableConfig]' = None, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      "     |      Bind config to a Runnable, returning a new Runnable.\n",
      "     |  \n",
      "     |  with_fallbacks(self, fallbacks: 'Sequence[Runnable[Input, Output]]', *, exceptions_to_handle: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,)) -> 'RunnableWithFallbacks[Input, Output]'\n",
      "     |  \n",
      "     |  with_retry(self, *, retry_if_exception_type: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,), wait_exponential_jitter: 'bool' = True, stop_after_attempt: 'int' = 3) -> 'Runnable[Input, Output]'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __class_getitem__(params) from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  __init_subclass__(*args, **kwargs) from pydantic.main.ModelMetaclass\n",
      "     |      This method is called when a class is subclassed.\n",
      "     |      \n",
      "     |      The default implementation does nothing. It may be\n",
      "     |      overridden to extend subclasses.\n",
      "    \n",
      "    class VertexAIModelGarden(_VertexAIBase, langchain.llms.base.LLM)\n",
      "     |  VertexAIModelGarden(*, cache: Optional[bool] = None, verbose: bool = None, callbacks: Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager, NoneType] = None, callback_manager: Optional[langchain.callbacks.base.BaseCallbackManager] = None, tags: Optional[List[str]] = None, metadata: Optional[Dict[str, Any]] = None, project: Optional[str] = None, location: str = 'us-central1', request_parallelism: int = 5, max_retries: int = 6, stop: Optional[List[str]] = None, model_name: Optional[str] = None, client: ForwardRef(\"'PredictionServiceClient'\") = None, endpoint_id: str, allowed_model_args: Optional[List[str]] = None, prompt_arg: str = 'prompt', result_arg: str = 'generated_text') -> None\n",
      "     |  \n",
      "     |  Large language models served from Vertex AI Model Garden.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      VertexAIModelGarden\n",
      "     |      _VertexAIBase\n",
      "     |      langchain.llms.base.LLM\n",
      "     |      langchain.llms.base.BaseLLM\n",
      "     |      langchain.schema.language_model.BaseLanguageModel\n",
      "     |      langchain.load.serializable.Serializable\n",
      "     |      pydantic.main.BaseModel\n",
      "     |      pydantic.utils.Representation\n",
      "     |      langchain.schema.runnable.base.Runnable\n",
      "     |      typing.Generic\n",
      "     |      abc.ABC\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  validate_environment(values: 'Dict') -> 'Dict' from pydantic.main.ModelMetaclass\n",
      "     |      Validate that the python package exists in environment.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __json_encoder__ = pydantic_encoder(obj: Any) -> Any\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {'allowed_model_args': 'Optional[List[str]]', 'clien...\n",
      "     |  \n",
      "     |  __class_vars__ = {'task_executor'}\n",
      "     |  \n",
      "     |  __config__ = <class 'langchain.llms.vertexai.Config'>\n",
      "     |  \n",
      "     |  __custom_root_type__ = False\n",
      "     |  \n",
      "     |  __exclude_fields__ = {'callback_manager': True, 'callbacks': True, 'me...\n",
      "     |  \n",
      "     |  __fields__ = {'allowed_model_args': ModelField(name='allowed_model_arg...\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "     |  \n",
      "     |  __include_fields__ = None\n",
      "     |  \n",
      "     |  __parameters__ = ()\n",
      "     |  \n",
      "     |  __post_root_validators__ = [(False, <function BaseLLM.raise_deprecatio...\n",
      "     |  \n",
      "     |  __pre_root_validators__ = []\n",
      "     |  \n",
      "     |  __private_attributes__ = {'_lc_kwargs': ModelPrivateAttr(default=Pydan...\n",
      "     |  \n",
      "     |  __schema_cache__ = {}\n",
      "     |  \n",
      "     |  __signature__ = <Signature (*, cache: Optional[bool] = None, ver...pt'...\n",
      "     |  \n",
      "     |  __validators__ = {'verbose': [<pydantic.class_validators.Validator obj...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from _VertexAIBase:\n",
      "     |  \n",
      "     |  task_executor = None\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  __call__(self, prompt: 'str', stop: 'Optional[List[str]]' = None, callbacks: 'Callbacks' = None, *, tags: 'Optional[List[str]]' = None, metadata: 'Optional[Dict[str, Any]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Check Cache and run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  __str__(self) -> 'str'\n",
      "     |      Get a string representation of the object for printing.\n",
      "     |  \n",
      "     |  async abatch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      "     |      Default implementation of abatch, which calls ainvoke N times.\n",
      "     |      Subclasses should override this method if they can batch more efficiently.\n",
      "     |  \n",
      "     |  async agenerate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  async agenerate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Asynchronously pass a sequence of prompts and return model generations.\n",
      "     |      \n",
      "     |      This method should make use of batched calls for models that expose a batched\n",
      "     |      API.\n",
      "     |      \n",
      "     |      Use this method when you want to:\n",
      "     |          1. take advantage of batched calls,\n",
      "     |          2. need more output from the model than just the top generated value,\n",
      "     |          3. are building chains that are agnostic to the underlying language model\n",
      "     |              type (e.g., pure text completion models vs chat models).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      "     |              converted to match the format of any language model (string for pure\n",
      "     |              text generation models and BaseMessages for chat models).\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          callbacks: Callbacks to pass through. Used for executing additional\n",
      "     |              functionality, such as logging or streaming, throughout generation.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          An LLMResult, which contains a list of candidate Generations for each input\n",
      "     |              prompt and additional model provider-specific output.\n",
      "     |  \n",
      "     |  async ainvoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Default implementation of ainvoke, which calls invoke in a thread pool.\n",
      "     |      Subclasses should override this method if they can run asynchronously.\n",
      "     |  \n",
      "     |  async apredict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Asynchronously pass a string to the model and return a string prediction.\n",
      "     |      \n",
      "     |      Use this method when calling pure text generation models and only the top\n",
      "     |          candidate generation is needed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: String input to pass to the model.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a string.\n",
      "     |  \n",
      "     |  async apredict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      "     |      Asynchronously pass messages to the model and return a message prediction.\n",
      "     |      \n",
      "     |      Use this method when calling chat models and only the top\n",
      "     |          candidate generation is needed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: A sequence of chat messages corresponding to a single model input.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a message.\n",
      "     |  \n",
      "     |  async astream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'AsyncIterator[str]'\n",
      "     |      Default implementation of astream, which calls ainvoke.\n",
      "     |      Subclasses should override this method if they support streaming output.\n",
      "     |  \n",
      "     |  batch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      "     |      Default implementation of batch, which calls invoke N times.\n",
      "     |      Subclasses should override this method if they can batch more efficiently.\n",
      "     |  \n",
      "     |  dict(self, **kwargs: 'Any') -> 'Dict'\n",
      "     |      Return a dictionary of the LLM.\n",
      "     |  \n",
      "     |  generate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  generate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Pass a sequence of prompts to the model and return model generations.\n",
      "     |      \n",
      "     |      This method should make use of batched calls for models that expose a batched\n",
      "     |      API.\n",
      "     |      \n",
      "     |      Use this method when you want to:\n",
      "     |          1. take advantage of batched calls,\n",
      "     |          2. need more output from the model than just the top generated value,\n",
      "     |          3. are building chains that are agnostic to the underlying language model\n",
      "     |              type (e.g., pure text completion models vs chat models).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      "     |              converted to match the format of any language model (string for pure\n",
      "     |              text generation models and BaseMessages for chat models).\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          callbacks: Callbacks to pass through. Used for executing additional\n",
      "     |              functionality, such as logging or streaming, throughout generation.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          An LLMResult, which contains a list of candidate Generations for each input\n",
      "     |              prompt and additional model provider-specific output.\n",
      "     |  \n",
      "     |  invoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |  \n",
      "     |  predict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Pass a single string input to the model and return a string prediction.\n",
      "     |      \n",
      "     |       Use this method when passing in raw text. If you want to pass in specific\n",
      "     |          types of chat messages, use predict_messages.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: String input to pass to the model.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a string.\n",
      "     |  \n",
      "     |  predict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      "     |      Pass a message sequence to the model and return a message prediction.\n",
      "     |      \n",
      "     |      Use this method when passing in chat messages. If you want to pass in raw text,\n",
      "     |          use predict.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: A sequence of chat messages corresponding to a single model input.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a message.\n",
      "     |  \n",
      "     |  save(self, file_path: 'Union[Path, str]') -> 'None'\n",
      "     |      Save the LLM.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          file_path: Path to file to save the LLM to.\n",
      "     |      \n",
      "     |      Example:\n",
      "     |      .. code-block:: python\n",
      "     |      \n",
      "     |          llm.save(file_path=\"path/llm.yaml\")\n",
      "     |  \n",
      "     |  stream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'Iterator[str]'\n",
      "     |      Default implementation of stream, which calls invoke.\n",
      "     |      Subclasses should override this method if they support streaming output.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  raise_deprecation(values: 'Dict') -> 'Dict' from pydantic.main.ModelMetaclass\n",
      "     |      Raise deprecation warning if callback_manager is used.\n",
      "     |  \n",
      "     |  set_verbose(verbose: 'Optional[bool]') -> 'bool' from pydantic.main.ModelMetaclass\n",
      "     |      If verbose is None, set it.\n",
      "     |      \n",
      "     |      This allows users to pass in None as verbose to access the global setting.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  Config = <class 'langchain.llms.base.BaseLLM.Config'>\n",
      "     |      Configuration for this pydantic object.\n",
      "     |  \n",
      "     |  \n",
      "     |  __orig_bases__ = (langchain.schema.language_model.BaseLanguageModel[st...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.schema.language_model.BaseLanguageModel:\n",
      "     |  \n",
      "     |  get_num_tokens(self, text: 'str') -> 'int'\n",
      "     |      Get the number of tokens present in the text.\n",
      "     |      \n",
      "     |      Useful for checking if an input will fit in a model's context window.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: The string input to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The integer number of tokens in the text.\n",
      "     |  \n",
      "     |  get_num_tokens_from_messages(self, messages: 'List[BaseMessage]') -> 'int'\n",
      "     |      Get the number of tokens in the messages.\n",
      "     |      \n",
      "     |      Useful for checking if an input will fit in a model's context window.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: The message inputs to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The sum of the number of tokens across the messages.\n",
      "     |  \n",
      "     |  get_token_ids(self, text: 'str') -> 'List[int]'\n",
      "     |      Return the ordered ids of the tokens in a text.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: The string input to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          A list of ids corresponding to the tokens in the text, in order they occur\n",
      "     |              in the text.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from langchain.schema.language_model.BaseLanguageModel:\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.load.serializable.Serializable:\n",
      "     |  \n",
      "     |  __init__(self, **kwargs: Any) -> None\n",
      "     |      Create a new model by parsing and validating input data from keyword arguments.\n",
      "     |      \n",
      "     |      Raises ValidationError if the input data cannot be parsed to form a valid model.\n",
      "     |  \n",
      "     |  to_json(self) -> Union[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\n",
      "     |  \n",
      "     |  to_json_not_implemented(self) -> langchain.load.serializable.SerializedNotImplemented\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from langchain.load.serializable.Serializable:\n",
      "     |  \n",
      "     |  lc_attributes\n",
      "     |      Return a list of attribute names that should be included in the\n",
      "     |      serialized kwargs. These attributes must be accepted by the\n",
      "     |      constructor.\n",
      "     |  \n",
      "     |  lc_namespace\n",
      "     |      Return the namespace of the langchain object.\n",
      "     |      eg. [\"langchain\", \"llms\", \"openai\"]\n",
      "     |  \n",
      "     |  lc_secrets\n",
      "     |      Return a map of constructor argument names to secret ids.\n",
      "     |      eg. {\"openai_api_key\": \"OPENAI_API_KEY\"}\n",
      "     |  \n",
      "     |  lc_serializable\n",
      "     |      Return whether or not the class is serializable.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __eq__(self, other: Any) -> bool\n",
      "     |  \n",
      "     |  __getstate__(self) -> 'DictAny'\n",
      "     |  \n",
      "     |  __iter__(self) -> 'TupleGenerator'\n",
      "     |      so `dict(model)` works\n",
      "     |  \n",
      "     |  __repr_args__(self) -> 'ReprArgs'\n",
      "     |  \n",
      "     |  __setattr__(self, name, value)\n",
      "     |  \n",
      "     |  __setstate__(self, state: 'DictAny') -> None\n",
      "     |  \n",
      "     |  copy(self: 'Model', *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, update: Optional[ForwardRef('DictStrAny')] = None, deep: bool = False) -> 'Model'\n",
      "     |      Duplicate a model, optionally choose which fields to include, exclude and change.\n",
      "     |      \n",
      "     |      :param include: fields to include in new model\n",
      "     |      :param exclude: fields to exclude from new model, as with values this takes precedence over include\n",
      "     |      :param update: values to change/add in the new model. Note: the data is not validated before creating\n",
      "     |          the new model: you should trust this data\n",
      "     |      :param deep: set to `True` to make a deep copy of the model\n",
      "     |      :return: new model instance\n",
      "     |  \n",
      "     |  json(self, *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) -> 'unicode'\n",
      "     |      Generate a JSON representation of the model, `include` and `exclude` arguments as per `dict()`.\n",
      "     |      \n",
      "     |      `encoder` is an optional function to supply as `default` to json.dumps(), other arguments as per `json.dumps()`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __get_validators__() -> 'CallableGenerator' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  __try_update_forward_refs__(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      "     |      Same as update_forward_refs but will not raise exception\n",
      "     |      when forward references are not defined.\n",
      "     |  \n",
      "     |  construct(_fields_set: Optional[ForwardRef('SetStr')] = None, **values: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |      Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data.\n",
      "     |      Default values are respected, but no other validation is performed.\n",
      "     |      Behaves as if `Config.extra = 'allow'` was set since it adds all passed values\n",
      "     |  \n",
      "     |  from_orm(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_file(path: Union[str, pathlib.Path], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_obj(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_raw(b: Union[str, bytes], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema(by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}') -> 'DictStrAny' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema_json(*, by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}', **dumps_kwargs: Any) -> 'unicode' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  update_forward_refs(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      "     |      Try to update ForwardRefs on fields based on this Model, globalns and localns.\n",
      "     |  \n",
      "     |  validate(value: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __fields_set__\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.utils.Representation:\n",
      "     |  \n",
      "     |  __pretty__(self, fmt: Callable[[Any], Any], **kwargs: Any) -> Generator[Any, NoneType, NoneType]\n",
      "     |      Used by devtools (https://python-devtools.helpmanual.io/) to provide a human readable representations of objects\n",
      "     |  \n",
      "     |  __repr__(self) -> 'unicode'\n",
      "     |  \n",
      "     |  __repr_name__(self) -> 'unicode'\n",
      "     |      Name of the instance's class, used in __repr__.\n",
      "     |  \n",
      "     |  __repr_str__(self, join_str: 'unicode') -> 'unicode'\n",
      "     |  \n",
      "     |  __rich_repr__(self) -> 'RichReprResult'\n",
      "     |      Get fields for Rich library\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.schema.runnable.base.Runnable:\n",
      "     |  \n",
      "     |  __or__(self, other: 'Union[Runnable[Any, Other], Callable[[Any], Other], Mapping[str, Union[Runnable[Any, Other], Callable[[Any], Other], Any]]]') -> 'RunnableSequence[Input, Other]'\n",
      "     |      Return self|value.\n",
      "     |  \n",
      "     |  __ror__(self, other: 'Union[Runnable[Other, Any], Callable[[Any], Other], Mapping[str, Union[Runnable[Other, Any], Callable[[Other], Any], Any]]]') -> 'RunnableSequence[Other, Output]'\n",
      "     |      Return value|self.\n",
      "     |  \n",
      "     |  async atransform(self, input: 'AsyncIterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'AsyncIterator[Output]'\n",
      "     |      Default implementation of atransform, which buffers input and calls astream.\n",
      "     |      Subclasses should override this method if they can start producing output while\n",
      "     |      input is still being generated.\n",
      "     |  \n",
      "     |  bind(self, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      "     |      Bind arguments to a Runnable, returning a new Runnable.\n",
      "     |  \n",
      "     |  map(self) -> 'Runnable[List[Input], List[Output]]'\n",
      "     |      Return a new Runnable that maps a list of inputs to a list of outputs,\n",
      "     |      by calling invoke() with each input.\n",
      "     |  \n",
      "     |  transform(self, input: 'Iterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'Iterator[Output]'\n",
      "     |      Default implementation of transform, which buffers input and then calls stream.\n",
      "     |      Subclasses should override this method if they can start producing output while\n",
      "     |      input is still being generated.\n",
      "     |  \n",
      "     |  with_config(self, config: 'Optional[RunnableConfig]' = None, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      "     |      Bind config to a Runnable, returning a new Runnable.\n",
      "     |  \n",
      "     |  with_fallbacks(self, fallbacks: 'Sequence[Runnable[Input, Output]]', *, exceptions_to_handle: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,)) -> 'RunnableWithFallbacks[Input, Output]'\n",
      "     |  \n",
      "     |  with_retry(self, *, retry_if_exception_type: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,), wait_exponential_jitter: 'bool' = True, stop_after_attempt: 'int' = 3) -> 'Runnable[Input, Output]'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __class_getitem__(params) from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  __init_subclass__(*args, **kwargs) from pydantic.main.ModelMetaclass\n",
      "     |      This method is called when a class is subclassed.\n",
      "     |      \n",
      "     |      The default implementation does nothing. It may be\n",
      "     |      overridden to extend subclasses.\n",
      "    \n",
      "    class Writer(langchain.llms.base.LLM)\n",
      "     |  Writer(*, cache: Optional[bool] = None, verbose: bool = None, callbacks: Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager, NoneType] = None, callback_manager: Optional[langchain.callbacks.base.BaseCallbackManager] = None, tags: Optional[List[str]] = None, metadata: Optional[Dict[str, Any]] = None, writer_org_id: Optional[str] = None, model_id: str = 'palmyra-instruct', min_tokens: Optional[int] = None, max_tokens: Optional[int] = None, temperature: Optional[float] = None, top_p: Optional[float] = None, stop: Optional[List[str]] = None, presence_penalty: Optional[float] = None, repetition_penalty: Optional[float] = None, best_of: Optional[int] = None, logprobs: bool = False, n: Optional[int] = None, writer_api_key: Optional[str] = None, base_url: Optional[str] = None) -> None\n",
      "     |  \n",
      "     |  Writer large language models.\n",
      "     |  \n",
      "     |  To use, you should have the environment variable ``WRITER_API_KEY`` and\n",
      "     |  ``WRITER_ORG_ID`` set with your API key and organization ID respectively.\n",
      "     |  \n",
      "     |  Example:\n",
      "     |      .. code-block:: python\n",
      "     |  \n",
      "     |          from langchain import Writer\n",
      "     |          writer = Writer(model_id=\"palmyra-base\")\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Writer\n",
      "     |      langchain.llms.base.LLM\n",
      "     |      langchain.llms.base.BaseLLM\n",
      "     |      langchain.schema.language_model.BaseLanguageModel\n",
      "     |      langchain.load.serializable.Serializable\n",
      "     |      pydantic.main.BaseModel\n",
      "     |      pydantic.utils.Representation\n",
      "     |      langchain.schema.runnable.base.Runnable\n",
      "     |      typing.Generic\n",
      "     |      abc.ABC\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  validate_environment(values: Dict) -> Dict from pydantic.main.ModelMetaclass\n",
      "     |      Validate that api key and organization id exist in environment.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __json_encoder__ = pydantic_encoder(obj: Any) -> Any\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  Config = <class 'langchain.llms.writer.Writer.Config'>\n",
      "     |      Configuration for this pydantic object.\n",
      "     |  \n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {'base_url': typing.Optional[str], 'best_of': typing...\n",
      "     |  \n",
      "     |  __class_vars__ = set()\n",
      "     |  \n",
      "     |  __config__ = <class 'langchain.llms.writer.Config'>\n",
      "     |  \n",
      "     |  __custom_root_type__ = False\n",
      "     |  \n",
      "     |  __exclude_fields__ = {'callback_manager': True, 'callbacks': True, 'me...\n",
      "     |  \n",
      "     |  __fields__ = {'base_url': ModelField(name='base_url', type=Optional[st...\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "     |  \n",
      "     |  __include_fields__ = None\n",
      "     |  \n",
      "     |  __parameters__ = ()\n",
      "     |  \n",
      "     |  __post_root_validators__ = [(False, <function BaseLLM.raise_deprecatio...\n",
      "     |  \n",
      "     |  __pre_root_validators__ = []\n",
      "     |  \n",
      "     |  __private_attributes__ = {'_lc_kwargs': ModelPrivateAttr(default=Pydan...\n",
      "     |  \n",
      "     |  __schema_cache__ = {}\n",
      "     |  \n",
      "     |  __signature__ = <Signature (*, cache: Optional[bool] = None, ver... = ...\n",
      "     |  \n",
      "     |  __validators__ = {'verbose': [<pydantic.class_validators.Validator obj...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  __call__(self, prompt: 'str', stop: 'Optional[List[str]]' = None, callbacks: 'Callbacks' = None, *, tags: 'Optional[List[str]]' = None, metadata: 'Optional[Dict[str, Any]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Check Cache and run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  __str__(self) -> 'str'\n",
      "     |      Get a string representation of the object for printing.\n",
      "     |  \n",
      "     |  async abatch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      "     |      Default implementation of abatch, which calls ainvoke N times.\n",
      "     |      Subclasses should override this method if they can batch more efficiently.\n",
      "     |  \n",
      "     |  async agenerate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  async agenerate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Asynchronously pass a sequence of prompts and return model generations.\n",
      "     |      \n",
      "     |      This method should make use of batched calls for models that expose a batched\n",
      "     |      API.\n",
      "     |      \n",
      "     |      Use this method when you want to:\n",
      "     |          1. take advantage of batched calls,\n",
      "     |          2. need more output from the model than just the top generated value,\n",
      "     |          3. are building chains that are agnostic to the underlying language model\n",
      "     |              type (e.g., pure text completion models vs chat models).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      "     |              converted to match the format of any language model (string for pure\n",
      "     |              text generation models and BaseMessages for chat models).\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          callbacks: Callbacks to pass through. Used for executing additional\n",
      "     |              functionality, such as logging or streaming, throughout generation.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          An LLMResult, which contains a list of candidate Generations for each input\n",
      "     |              prompt and additional model provider-specific output.\n",
      "     |  \n",
      "     |  async ainvoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Default implementation of ainvoke, which calls invoke in a thread pool.\n",
      "     |      Subclasses should override this method if they can run asynchronously.\n",
      "     |  \n",
      "     |  async apredict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Asynchronously pass a string to the model and return a string prediction.\n",
      "     |      \n",
      "     |      Use this method when calling pure text generation models and only the top\n",
      "     |          candidate generation is needed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: String input to pass to the model.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a string.\n",
      "     |  \n",
      "     |  async apredict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      "     |      Asynchronously pass messages to the model and return a message prediction.\n",
      "     |      \n",
      "     |      Use this method when calling chat models and only the top\n",
      "     |          candidate generation is needed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: A sequence of chat messages corresponding to a single model input.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a message.\n",
      "     |  \n",
      "     |  async astream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'AsyncIterator[str]'\n",
      "     |      Default implementation of astream, which calls ainvoke.\n",
      "     |      Subclasses should override this method if they support streaming output.\n",
      "     |  \n",
      "     |  batch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      "     |      Default implementation of batch, which calls invoke N times.\n",
      "     |      Subclasses should override this method if they can batch more efficiently.\n",
      "     |  \n",
      "     |  dict(self, **kwargs: 'Any') -> 'Dict'\n",
      "     |      Return a dictionary of the LLM.\n",
      "     |  \n",
      "     |  generate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  generate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Pass a sequence of prompts to the model and return model generations.\n",
      "     |      \n",
      "     |      This method should make use of batched calls for models that expose a batched\n",
      "     |      API.\n",
      "     |      \n",
      "     |      Use this method when you want to:\n",
      "     |          1. take advantage of batched calls,\n",
      "     |          2. need more output from the model than just the top generated value,\n",
      "     |          3. are building chains that are agnostic to the underlying language model\n",
      "     |              type (e.g., pure text completion models vs chat models).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      "     |              converted to match the format of any language model (string for pure\n",
      "     |              text generation models and BaseMessages for chat models).\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          callbacks: Callbacks to pass through. Used for executing additional\n",
      "     |              functionality, such as logging or streaming, throughout generation.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          An LLMResult, which contains a list of candidate Generations for each input\n",
      "     |              prompt and additional model provider-specific output.\n",
      "     |  \n",
      "     |  invoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |  \n",
      "     |  predict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Pass a single string input to the model and return a string prediction.\n",
      "     |      \n",
      "     |       Use this method when passing in raw text. If you want to pass in specific\n",
      "     |          types of chat messages, use predict_messages.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: String input to pass to the model.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a string.\n",
      "     |  \n",
      "     |  predict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      "     |      Pass a message sequence to the model and return a message prediction.\n",
      "     |      \n",
      "     |      Use this method when passing in chat messages. If you want to pass in raw text,\n",
      "     |          use predict.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: A sequence of chat messages corresponding to a single model input.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a message.\n",
      "     |  \n",
      "     |  save(self, file_path: 'Union[Path, str]') -> 'None'\n",
      "     |      Save the LLM.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          file_path: Path to file to save the LLM to.\n",
      "     |      \n",
      "     |      Example:\n",
      "     |      .. code-block:: python\n",
      "     |      \n",
      "     |          llm.save(file_path=\"path/llm.yaml\")\n",
      "     |  \n",
      "     |  stream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'Iterator[str]'\n",
      "     |      Default implementation of stream, which calls invoke.\n",
      "     |      Subclasses should override this method if they support streaming output.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  raise_deprecation(values: 'Dict') -> 'Dict' from pydantic.main.ModelMetaclass\n",
      "     |      Raise deprecation warning if callback_manager is used.\n",
      "     |  \n",
      "     |  set_verbose(verbose: 'Optional[bool]') -> 'bool' from pydantic.main.ModelMetaclass\n",
      "     |      If verbose is None, set it.\n",
      "     |      \n",
      "     |      This allows users to pass in None as verbose to access the global setting.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  __orig_bases__ = (langchain.schema.language_model.BaseLanguageModel[st...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.schema.language_model.BaseLanguageModel:\n",
      "     |  \n",
      "     |  get_num_tokens(self, text: 'str') -> 'int'\n",
      "     |      Get the number of tokens present in the text.\n",
      "     |      \n",
      "     |      Useful for checking if an input will fit in a model's context window.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: The string input to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The integer number of tokens in the text.\n",
      "     |  \n",
      "     |  get_num_tokens_from_messages(self, messages: 'List[BaseMessage]') -> 'int'\n",
      "     |      Get the number of tokens in the messages.\n",
      "     |      \n",
      "     |      Useful for checking if an input will fit in a model's context window.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: The message inputs to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The sum of the number of tokens across the messages.\n",
      "     |  \n",
      "     |  get_token_ids(self, text: 'str') -> 'List[int]'\n",
      "     |      Return the ordered ids of the tokens in a text.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: The string input to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          A list of ids corresponding to the tokens in the text, in order they occur\n",
      "     |              in the text.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from langchain.schema.language_model.BaseLanguageModel:\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.load.serializable.Serializable:\n",
      "     |  \n",
      "     |  __init__(self, **kwargs: Any) -> None\n",
      "     |      Create a new model by parsing and validating input data from keyword arguments.\n",
      "     |      \n",
      "     |      Raises ValidationError if the input data cannot be parsed to form a valid model.\n",
      "     |  \n",
      "     |  to_json(self) -> Union[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\n",
      "     |  \n",
      "     |  to_json_not_implemented(self) -> langchain.load.serializable.SerializedNotImplemented\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from langchain.load.serializable.Serializable:\n",
      "     |  \n",
      "     |  lc_attributes\n",
      "     |      Return a list of attribute names that should be included in the\n",
      "     |      serialized kwargs. These attributes must be accepted by the\n",
      "     |      constructor.\n",
      "     |  \n",
      "     |  lc_namespace\n",
      "     |      Return the namespace of the langchain object.\n",
      "     |      eg. [\"langchain\", \"llms\", \"openai\"]\n",
      "     |  \n",
      "     |  lc_secrets\n",
      "     |      Return a map of constructor argument names to secret ids.\n",
      "     |      eg. {\"openai_api_key\": \"OPENAI_API_KEY\"}\n",
      "     |  \n",
      "     |  lc_serializable\n",
      "     |      Return whether or not the class is serializable.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __eq__(self, other: Any) -> bool\n",
      "     |  \n",
      "     |  __getstate__(self) -> 'DictAny'\n",
      "     |  \n",
      "     |  __iter__(self) -> 'TupleGenerator'\n",
      "     |      so `dict(model)` works\n",
      "     |  \n",
      "     |  __repr_args__(self) -> 'ReprArgs'\n",
      "     |  \n",
      "     |  __setattr__(self, name, value)\n",
      "     |  \n",
      "     |  __setstate__(self, state: 'DictAny') -> None\n",
      "     |  \n",
      "     |  copy(self: 'Model', *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, update: Optional[ForwardRef('DictStrAny')] = None, deep: bool = False) -> 'Model'\n",
      "     |      Duplicate a model, optionally choose which fields to include, exclude and change.\n",
      "     |      \n",
      "     |      :param include: fields to include in new model\n",
      "     |      :param exclude: fields to exclude from new model, as with values this takes precedence over include\n",
      "     |      :param update: values to change/add in the new model. Note: the data is not validated before creating\n",
      "     |          the new model: you should trust this data\n",
      "     |      :param deep: set to `True` to make a deep copy of the model\n",
      "     |      :return: new model instance\n",
      "     |  \n",
      "     |  json(self, *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) -> 'unicode'\n",
      "     |      Generate a JSON representation of the model, `include` and `exclude` arguments as per `dict()`.\n",
      "     |      \n",
      "     |      `encoder` is an optional function to supply as `default` to json.dumps(), other arguments as per `json.dumps()`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __get_validators__() -> 'CallableGenerator' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  __try_update_forward_refs__(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      "     |      Same as update_forward_refs but will not raise exception\n",
      "     |      when forward references are not defined.\n",
      "     |  \n",
      "     |  construct(_fields_set: Optional[ForwardRef('SetStr')] = None, **values: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |      Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data.\n",
      "     |      Default values are respected, but no other validation is performed.\n",
      "     |      Behaves as if `Config.extra = 'allow'` was set since it adds all passed values\n",
      "     |  \n",
      "     |  from_orm(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_file(path: Union[str, pathlib.Path], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_obj(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_raw(b: Union[str, bytes], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema(by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}') -> 'DictStrAny' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema_json(*, by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}', **dumps_kwargs: Any) -> 'unicode' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  update_forward_refs(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      "     |      Try to update ForwardRefs on fields based on this Model, globalns and localns.\n",
      "     |  \n",
      "     |  validate(value: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __fields_set__\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.utils.Representation:\n",
      "     |  \n",
      "     |  __pretty__(self, fmt: Callable[[Any], Any], **kwargs: Any) -> Generator[Any, NoneType, NoneType]\n",
      "     |      Used by devtools (https://python-devtools.helpmanual.io/) to provide a human readable representations of objects\n",
      "     |  \n",
      "     |  __repr__(self) -> 'unicode'\n",
      "     |  \n",
      "     |  __repr_name__(self) -> 'unicode'\n",
      "     |      Name of the instance's class, used in __repr__.\n",
      "     |  \n",
      "     |  __repr_str__(self, join_str: 'unicode') -> 'unicode'\n",
      "     |  \n",
      "     |  __rich_repr__(self) -> 'RichReprResult'\n",
      "     |      Get fields for Rich library\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.schema.runnable.base.Runnable:\n",
      "     |  \n",
      "     |  __or__(self, other: 'Union[Runnable[Any, Other], Callable[[Any], Other], Mapping[str, Union[Runnable[Any, Other], Callable[[Any], Other], Any]]]') -> 'RunnableSequence[Input, Other]'\n",
      "     |      Return self|value.\n",
      "     |  \n",
      "     |  __ror__(self, other: 'Union[Runnable[Other, Any], Callable[[Any], Other], Mapping[str, Union[Runnable[Other, Any], Callable[[Other], Any], Any]]]') -> 'RunnableSequence[Other, Output]'\n",
      "     |      Return value|self.\n",
      "     |  \n",
      "     |  async atransform(self, input: 'AsyncIterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'AsyncIterator[Output]'\n",
      "     |      Default implementation of atransform, which buffers input and calls astream.\n",
      "     |      Subclasses should override this method if they can start producing output while\n",
      "     |      input is still being generated.\n",
      "     |  \n",
      "     |  bind(self, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      "     |      Bind arguments to a Runnable, returning a new Runnable.\n",
      "     |  \n",
      "     |  map(self) -> 'Runnable[List[Input], List[Output]]'\n",
      "     |      Return a new Runnable that maps a list of inputs to a list of outputs,\n",
      "     |      by calling invoke() with each input.\n",
      "     |  \n",
      "     |  transform(self, input: 'Iterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'Iterator[Output]'\n",
      "     |      Default implementation of transform, which buffers input and then calls stream.\n",
      "     |      Subclasses should override this method if they can start producing output while\n",
      "     |      input is still being generated.\n",
      "     |  \n",
      "     |  with_config(self, config: 'Optional[RunnableConfig]' = None, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      "     |      Bind config to a Runnable, returning a new Runnable.\n",
      "     |  \n",
      "     |  with_fallbacks(self, fallbacks: 'Sequence[Runnable[Input, Output]]', *, exceptions_to_handle: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,)) -> 'RunnableWithFallbacks[Input, Output]'\n",
      "     |  \n",
      "     |  with_retry(self, *, retry_if_exception_type: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,), wait_exponential_jitter: 'bool' = True, stop_after_attempt: 'int' = 3) -> 'Runnable[Input, Output]'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __class_getitem__(params) from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  __init_subclass__(*args, **kwargs) from pydantic.main.ModelMetaclass\n",
      "     |      This method is called when a class is subclassed.\n",
      "     |      \n",
      "     |      The default implementation does nothing. It may be\n",
      "     |      overridden to extend subclasses.\n",
      "    \n",
      "    class Xinference(langchain.llms.base.LLM)\n",
      "     |  Xinference(server_url: Optional[str] = None, model_uid: Optional[str] = None) -> None\n",
      "     |  \n",
      "     |  Wrapper for accessing Xinference's large-scale model inference service.\n",
      "     |  To use, you should have the xinference library installed:\n",
      "     |  .. code-block:: bash\n",
      "     |  \n",
      "     |      pip install \"xinference[all]\"\n",
      "     |  \n",
      "     |  Check out: https://github.com/xorbitsai/inference\n",
      "     |  To run, you need to start a Xinference supervisor on one server and Xinference workers on the other servers\n",
      "     |  Example:\n",
      "     |      To start a local instance of Xinference, run\n",
      "     |       .. code-block:: bash\n",
      "     |  \n",
      "     |          $ xinference\n",
      "     |  \n",
      "     |      You can also deploy Xinference in a distributed cluster. Here are the steps:\n",
      "     |      Starting the supervisor:\n",
      "     |      .. code-block:: bash\n",
      "     |  \n",
      "     |          $ xinference-supervisor\n",
      "     |  \n",
      "     |      Starting the worker:\n",
      "     |      .. code-block:: bash\n",
      "     |  \n",
      "     |          $ xinference-worker\n",
      "     |  \n",
      "     |  Then, launch a model using command line interface (CLI).\n",
      "     |  \n",
      "     |  Example:\n",
      "     |  .. code-block:: bash\n",
      "     |  \n",
      "     |          $ xinference launch -n orca -s 3 -q q4_0\n",
      "     |  \n",
      "     |  It will return a model UID. Then, you can use Xinference with LangChain.\n",
      "     |  \n",
      "     |  Example:\n",
      "     |      .. code-block:: python\n",
      "     |  \n",
      "     |          from langchain.llms import Xinference\n",
      "     |  \n",
      "     |          llm = Xinference(\n",
      "     |              server_url=\"http://0.0.0.0:9997\",\n",
      "     |              model_uid = {model_uid} # replace model_uid with the model UID return from launching the model\n",
      "     |          )\n",
      "     |  \n",
      "     |          llm(\n",
      "     |              prompt=\"Q: where can we visit in the capital of France? A:\",\n",
      "     |              generate_config={\"max_tokens\": 1024, \"stream\": True},\n",
      "     |          )\n",
      "     |  \n",
      "     |  To view all the supported builtin models, run:\n",
      "     |  .. code-block:: bash\n",
      "     |      $ xinference list --all\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Xinference\n",
      "     |      langchain.llms.base.LLM\n",
      "     |      langchain.llms.base.BaseLLM\n",
      "     |      langchain.schema.language_model.BaseLanguageModel\n",
      "     |      langchain.load.serializable.Serializable\n",
      "     |      pydantic.main.BaseModel\n",
      "     |      pydantic.utils.Representation\n",
      "     |      langchain.schema.runnable.base.Runnable\n",
      "     |      typing.Generic\n",
      "     |      abc.ABC\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, server_url: Optional[str] = None, model_uid: Optional[str] = None)\n",
      "     |      Create a new model by parsing and validating input data from keyword arguments.\n",
      "     |      \n",
      "     |      Raises ValidationError if the input data cannot be parsed to form a valid model.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __json_encoder__ = pydantic_encoder(obj: Any) -> Any\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {'client': typing.Any, 'model_uid': typing.Optional[...\n",
      "     |  \n",
      "     |  __class_vars__ = set()\n",
      "     |  \n",
      "     |  __config__ = <class 'langchain.llms.xinference.Config'>\n",
      "     |  \n",
      "     |  __custom_root_type__ = False\n",
      "     |  \n",
      "     |  __exclude_fields__ = {'callback_manager': True, 'callbacks': True, 'me...\n",
      "     |  \n",
      "     |  __fields__ = {'cache': ModelField(name='cache', type=Optional[bool], r...\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "     |  \n",
      "     |  __include_fields__ = None\n",
      "     |  \n",
      "     |  __parameters__ = ()\n",
      "     |  \n",
      "     |  __post_root_validators__ = [(False, <function BaseLLM.raise_deprecatio...\n",
      "     |  \n",
      "     |  __pre_root_validators__ = []\n",
      "     |  \n",
      "     |  __private_attributes__ = {'_lc_kwargs': ModelPrivateAttr(default=Pydan...\n",
      "     |  \n",
      "     |  __schema_cache__ = {}\n",
      "     |  \n",
      "     |  __signature__ = <Signature (server_url: Optional[str] = None, model_ui...\n",
      "     |  \n",
      "     |  __validators__ = {'verbose': [<pydantic.class_validators.Validator obj...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  __call__(self, prompt: 'str', stop: 'Optional[List[str]]' = None, callbacks: 'Callbacks' = None, *, tags: 'Optional[List[str]]' = None, metadata: 'Optional[Dict[str, Any]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Check Cache and run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  __str__(self) -> 'str'\n",
      "     |      Get a string representation of the object for printing.\n",
      "     |  \n",
      "     |  async abatch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      "     |      Default implementation of abatch, which calls ainvoke N times.\n",
      "     |      Subclasses should override this method if they can batch more efficiently.\n",
      "     |  \n",
      "     |  async agenerate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  async agenerate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Asynchronously pass a sequence of prompts and return model generations.\n",
      "     |      \n",
      "     |      This method should make use of batched calls for models that expose a batched\n",
      "     |      API.\n",
      "     |      \n",
      "     |      Use this method when you want to:\n",
      "     |          1. take advantage of batched calls,\n",
      "     |          2. need more output from the model than just the top generated value,\n",
      "     |          3. are building chains that are agnostic to the underlying language model\n",
      "     |              type (e.g., pure text completion models vs chat models).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      "     |              converted to match the format of any language model (string for pure\n",
      "     |              text generation models and BaseMessages for chat models).\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          callbacks: Callbacks to pass through. Used for executing additional\n",
      "     |              functionality, such as logging or streaming, throughout generation.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          An LLMResult, which contains a list of candidate Generations for each input\n",
      "     |              prompt and additional model provider-specific output.\n",
      "     |  \n",
      "     |  async ainvoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Default implementation of ainvoke, which calls invoke in a thread pool.\n",
      "     |      Subclasses should override this method if they can run asynchronously.\n",
      "     |  \n",
      "     |  async apredict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Asynchronously pass a string to the model and return a string prediction.\n",
      "     |      \n",
      "     |      Use this method when calling pure text generation models and only the top\n",
      "     |          candidate generation is needed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: String input to pass to the model.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a string.\n",
      "     |  \n",
      "     |  async apredict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      "     |      Asynchronously pass messages to the model and return a message prediction.\n",
      "     |      \n",
      "     |      Use this method when calling chat models and only the top\n",
      "     |          candidate generation is needed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: A sequence of chat messages corresponding to a single model input.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a message.\n",
      "     |  \n",
      "     |  async astream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'AsyncIterator[str]'\n",
      "     |      Default implementation of astream, which calls ainvoke.\n",
      "     |      Subclasses should override this method if they support streaming output.\n",
      "     |  \n",
      "     |  batch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      "     |      Default implementation of batch, which calls invoke N times.\n",
      "     |      Subclasses should override this method if they can batch more efficiently.\n",
      "     |  \n",
      "     |  dict(self, **kwargs: 'Any') -> 'Dict'\n",
      "     |      Return a dictionary of the LLM.\n",
      "     |  \n",
      "     |  generate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Run the LLM on the given prompt and input.\n",
      "     |  \n",
      "     |  generate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      "     |      Pass a sequence of prompts to the model and return model generations.\n",
      "     |      \n",
      "     |      This method should make use of batched calls for models that expose a batched\n",
      "     |      API.\n",
      "     |      \n",
      "     |      Use this method when you want to:\n",
      "     |          1. take advantage of batched calls,\n",
      "     |          2. need more output from the model than just the top generated value,\n",
      "     |          3. are building chains that are agnostic to the underlying language model\n",
      "     |              type (e.g., pure text completion models vs chat models).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      "     |              converted to match the format of any language model (string for pure\n",
      "     |              text generation models and BaseMessages for chat models).\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          callbacks: Callbacks to pass through. Used for executing additional\n",
      "     |              functionality, such as logging or streaming, throughout generation.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          An LLMResult, which contains a list of candidate Generations for each input\n",
      "     |              prompt and additional model provider-specific output.\n",
      "     |  \n",
      "     |  invoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |  \n",
      "     |  predict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      "     |      Pass a single string input to the model and return a string prediction.\n",
      "     |      \n",
      "     |       Use this method when passing in raw text. If you want to pass in specific\n",
      "     |          types of chat messages, use predict_messages.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: String input to pass to the model.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a string.\n",
      "     |  \n",
      "     |  predict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      "     |      Pass a message sequence to the model and return a message prediction.\n",
      "     |      \n",
      "     |      Use this method when passing in chat messages. If you want to pass in raw text,\n",
      "     |          use predict.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: A sequence of chat messages corresponding to a single model input.\n",
      "     |          stop: Stop words to use when generating. Model output is cut off at the\n",
      "     |              first occurrence of any of these substrings.\n",
      "     |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      "     |              to the model provider API call.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Top model prediction as a message.\n",
      "     |  \n",
      "     |  save(self, file_path: 'Union[Path, str]') -> 'None'\n",
      "     |      Save the LLM.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          file_path: Path to file to save the LLM to.\n",
      "     |      \n",
      "     |      Example:\n",
      "     |      .. code-block:: python\n",
      "     |      \n",
      "     |          llm.save(file_path=\"path/llm.yaml\")\n",
      "     |  \n",
      "     |  stream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'Iterator[str]'\n",
      "     |      Default implementation of stream, which calls invoke.\n",
      "     |      Subclasses should override this method if they support streaming output.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  raise_deprecation(values: 'Dict') -> 'Dict' from pydantic.main.ModelMetaclass\n",
      "     |      Raise deprecation warning if callback_manager is used.\n",
      "     |  \n",
      "     |  set_verbose(verbose: 'Optional[bool]') -> 'bool' from pydantic.main.ModelMetaclass\n",
      "     |      If verbose is None, set it.\n",
      "     |      \n",
      "     |      This allows users to pass in None as verbose to access the global setting.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from langchain.llms.base.BaseLLM:\n",
      "     |  \n",
      "     |  Config = <class 'langchain.llms.base.BaseLLM.Config'>\n",
      "     |      Configuration for this pydantic object.\n",
      "     |  \n",
      "     |  \n",
      "     |  __orig_bases__ = (langchain.schema.language_model.BaseLanguageModel[st...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.schema.language_model.BaseLanguageModel:\n",
      "     |  \n",
      "     |  get_num_tokens(self, text: 'str') -> 'int'\n",
      "     |      Get the number of tokens present in the text.\n",
      "     |      \n",
      "     |      Useful for checking if an input will fit in a model's context window.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: The string input to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The integer number of tokens in the text.\n",
      "     |  \n",
      "     |  get_num_tokens_from_messages(self, messages: 'List[BaseMessage]') -> 'int'\n",
      "     |      Get the number of tokens in the messages.\n",
      "     |      \n",
      "     |      Useful for checking if an input will fit in a model's context window.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          messages: The message inputs to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The sum of the number of tokens across the messages.\n",
      "     |  \n",
      "     |  get_token_ids(self, text: 'str') -> 'List[int]'\n",
      "     |      Return the ordered ids of the tokens in a text.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          text: The string input to tokenize.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          A list of ids corresponding to the tokens in the text, in order they occur\n",
      "     |              in the text.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from langchain.schema.language_model.BaseLanguageModel:\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.load.serializable.Serializable:\n",
      "     |  \n",
      "     |  to_json(self) -> Union[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\n",
      "     |  \n",
      "     |  to_json_not_implemented(self) -> langchain.load.serializable.SerializedNotImplemented\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from langchain.load.serializable.Serializable:\n",
      "     |  \n",
      "     |  lc_attributes\n",
      "     |      Return a list of attribute names that should be included in the\n",
      "     |      serialized kwargs. These attributes must be accepted by the\n",
      "     |      constructor.\n",
      "     |  \n",
      "     |  lc_namespace\n",
      "     |      Return the namespace of the langchain object.\n",
      "     |      eg. [\"langchain\", \"llms\", \"openai\"]\n",
      "     |  \n",
      "     |  lc_secrets\n",
      "     |      Return a map of constructor argument names to secret ids.\n",
      "     |      eg. {\"openai_api_key\": \"OPENAI_API_KEY\"}\n",
      "     |  \n",
      "     |  lc_serializable\n",
      "     |      Return whether or not the class is serializable.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __eq__(self, other: Any) -> bool\n",
      "     |  \n",
      "     |  __getstate__(self) -> 'DictAny'\n",
      "     |  \n",
      "     |  __iter__(self) -> 'TupleGenerator'\n",
      "     |      so `dict(model)` works\n",
      "     |  \n",
      "     |  __repr_args__(self) -> 'ReprArgs'\n",
      "     |  \n",
      "     |  __setattr__(self, name, value)\n",
      "     |  \n",
      "     |  __setstate__(self, state: 'DictAny') -> None\n",
      "     |  \n",
      "     |  copy(self: 'Model', *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, update: Optional[ForwardRef('DictStrAny')] = None, deep: bool = False) -> 'Model'\n",
      "     |      Duplicate a model, optionally choose which fields to include, exclude and change.\n",
      "     |      \n",
      "     |      :param include: fields to include in new model\n",
      "     |      :param exclude: fields to exclude from new model, as with values this takes precedence over include\n",
      "     |      :param update: values to change/add in the new model. Note: the data is not validated before creating\n",
      "     |          the new model: you should trust this data\n",
      "     |      :param deep: set to `True` to make a deep copy of the model\n",
      "     |      :return: new model instance\n",
      "     |  \n",
      "     |  json(self, *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) -> 'unicode'\n",
      "     |      Generate a JSON representation of the model, `include` and `exclude` arguments as per `dict()`.\n",
      "     |      \n",
      "     |      `encoder` is an optional function to supply as `default` to json.dumps(), other arguments as per `json.dumps()`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __get_validators__() -> 'CallableGenerator' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  __try_update_forward_refs__(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      "     |      Same as update_forward_refs but will not raise exception\n",
      "     |      when forward references are not defined.\n",
      "     |  \n",
      "     |  construct(_fields_set: Optional[ForwardRef('SetStr')] = None, **values: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |      Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data.\n",
      "     |      Default values are respected, but no other validation is performed.\n",
      "     |      Behaves as if `Config.extra = 'allow'` was set since it adds all passed values\n",
      "     |  \n",
      "     |  from_orm(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_file(path: Union[str, pathlib.Path], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_obj(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  parse_raw(b: Union[str, bytes], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema(by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}') -> 'DictStrAny' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  schema_json(*, by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}', **dumps_kwargs: Any) -> 'unicode' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  update_forward_refs(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      "     |      Try to update ForwardRefs on fields based on this Model, globalns and localns.\n",
      "     |  \n",
      "     |  validate(value: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from pydantic.main.BaseModel:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __fields_set__\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.utils.Representation:\n",
      "     |  \n",
      "     |  __pretty__(self, fmt: Callable[[Any], Any], **kwargs: Any) -> Generator[Any, NoneType, NoneType]\n",
      "     |      Used by devtools (https://python-devtools.helpmanual.io/) to provide a human readable representations of objects\n",
      "     |  \n",
      "     |  __repr__(self) -> 'unicode'\n",
      "     |  \n",
      "     |  __repr_name__(self) -> 'unicode'\n",
      "     |      Name of the instance's class, used in __repr__.\n",
      "     |  \n",
      "     |  __repr_str__(self, join_str: 'unicode') -> 'unicode'\n",
      "     |  \n",
      "     |  __rich_repr__(self) -> 'RichReprResult'\n",
      "     |      Get fields for Rich library\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from langchain.schema.runnable.base.Runnable:\n",
      "     |  \n",
      "     |  __or__(self, other: 'Union[Runnable[Any, Other], Callable[[Any], Other], Mapping[str, Union[Runnable[Any, Other], Callable[[Any], Other], Any]]]') -> 'RunnableSequence[Input, Other]'\n",
      "     |      Return self|value.\n",
      "     |  \n",
      "     |  __ror__(self, other: 'Union[Runnable[Other, Any], Callable[[Any], Other], Mapping[str, Union[Runnable[Other, Any], Callable[[Other], Any], Any]]]') -> 'RunnableSequence[Other, Output]'\n",
      "     |      Return value|self.\n",
      "     |  \n",
      "     |  async atransform(self, input: 'AsyncIterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'AsyncIterator[Output]'\n",
      "     |      Default implementation of atransform, which buffers input and calls astream.\n",
      "     |      Subclasses should override this method if they can start producing output while\n",
      "     |      input is still being generated.\n",
      "     |  \n",
      "     |  bind(self, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      "     |      Bind arguments to a Runnable, returning a new Runnable.\n",
      "     |  \n",
      "     |  map(self) -> 'Runnable[List[Input], List[Output]]'\n",
      "     |      Return a new Runnable that maps a list of inputs to a list of outputs,\n",
      "     |      by calling invoke() with each input.\n",
      "     |  \n",
      "     |  transform(self, input: 'Iterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'Iterator[Output]'\n",
      "     |      Default implementation of transform, which buffers input and then calls stream.\n",
      "     |      Subclasses should override this method if they can start producing output while\n",
      "     |      input is still being generated.\n",
      "     |  \n",
      "     |  with_config(self, config: 'Optional[RunnableConfig]' = None, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      "     |      Bind config to a Runnable, returning a new Runnable.\n",
      "     |  \n",
      "     |  with_fallbacks(self, fallbacks: 'Sequence[Runnable[Input, Output]]', *, exceptions_to_handle: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,)) -> 'RunnableWithFallbacks[Input, Output]'\n",
      "     |  \n",
      "     |  with_retry(self, *, retry_if_exception_type: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,), wait_exponential_jitter: 'bool' = True, stop_after_attempt: 'int' = 3) -> 'Runnable[Input, Output]'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __class_getitem__(params) from pydantic.main.ModelMetaclass\n",
      "     |  \n",
      "     |  __init_subclass__(*args, **kwargs) from pydantic.main.ModelMetaclass\n",
      "     |      This method is called when a class is subclassed.\n",
      "     |      \n",
      "     |      The default implementation does nothing. It may be\n",
      "     |      overridden to extend subclasses.\n",
      "\n",
      "DATA\n",
      "    __all__ = ['AI21', 'AlephAlpha', 'AmazonAPIGateway', 'Anthropic', 'Any...\n",
      "    __annotations__ = {'type_to_cls_dict': typing.Dict[str, typing.Type[la...\n",
      "\n",
      "FILE\n",
      "    c:\\users\\ujjwal\\miniconda3\\envs\\pytorch2\\lib\\site-packages\\langchain\\llms\\__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help('langchain.llms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import Tool\n",
    "from langchain.chains.conversation.memory import ConversationBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.agents import initialize_agent\n",
    "\n",
    "from llama_index import VectorStoreIndex, SimpleDirectoryReader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "stat: path should be string, bytes, os.PathLike or integer, not list",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[70], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m doc \u001b[39m=\u001b[39m SimpleDirectoryReader(documents)\u001b[39m.\u001b[39mload_data()\n",
      "File \u001b[1;32mc:\\Users\\Ujjwal\\miniconda3\\envs\\pytorch2\\lib\\site-packages\\llama_index\\readers\\file\\base.py:108\u001b[0m, in \u001b[0;36mSimpleDirectoryReader.__init__\u001b[1;34m(self, input_dir, input_files, exclude, exclude_hidden, errors, recursive, encoding, filename_as_id, required_exts, file_extractor, num_files_limit, file_metadata)\u001b[0m\n\u001b[0;32m    106\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_files\u001b[39m.\u001b[39mappend(input_file)\n\u001b[0;32m    107\u001b[0m \u001b[39melif\u001b[39;00m input_dir:\n\u001b[1;32m--> 108\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49misdir(input_dir):\n\u001b[0;32m    109\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mDirectory \u001b[39m\u001b[39m{\u001b[39;00minput_dir\u001b[39m}\u001b[39;00m\u001b[39m does not exist.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    110\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_dir \u001b[39m=\u001b[39m Path(input_dir)\n",
      "File \u001b[1;32mc:\\Users\\Ujjwal\\miniconda3\\envs\\pytorch2\\lib\\genericpath.py:42\u001b[0m, in \u001b[0;36misdir\u001b[1;34m(s)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Return true if the pathname refers to an existing directory.\"\"\"\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 42\u001b[0m     st \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39;49mstat(s)\n\u001b[0;32m     43\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mOSError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[0;32m     44\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: stat: path should be string, bytes, os.PathLike or integer, not list"
     ]
    }
   ],
   "source": [
    "doc = SimpleDirectoryReader(documents).load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******\n",
      "Could not load OpenAI model. Using default LlamaCPP=llama2-13b-chat. If you intended to use OpenAI, please check your OPENAI_API_KEY.\n",
      "Original error:\n",
      "No API key found for OpenAI.\n",
      "Please set either the OPENAI_API_KEY environment variable or openai.api_key prior to initialization.\n",
      "API keys can be found or created at https://platform.openai.com/account/api-keys\n",
      "\n",
      "******\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******\n",
      "Could not load OpenAIEmbedding. Using HuggingFaceBgeEmbeddings with model_name=BAAI/bge-small-en. If you intended to use OpenAI, please check your OPENAI_API_KEY.\n",
      "Original error:\n",
      "No API key found for OpenAI.\n",
      "Please set either the OPENAI_API_KEY environment variable or openai.api_key prior to initialization.\n",
      "API keys can be found or created at https://platform.openai.com/account/api-keys\n",
      "\n",
      "******\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'get_doc_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[69], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m index \u001b[39m=\u001b[39m VectorStoreIndex\u001b[39m.\u001b[39;49mfrom_documents(documents\u001b[39m=\u001b[39;49mdocuments)\n\u001b[0;32m      3\u001b[0m tools \u001b[39m=\u001b[39m [\n\u001b[0;32m      4\u001b[0m     Tool(\n\u001b[0;32m      5\u001b[0m         name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mLlamaIndex\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      9\u001b[0m     ),\n\u001b[0;32m     10\u001b[0m ]\n",
      "File \u001b[1;32mc:\\Users\\Ujjwal\\miniconda3\\envs\\pytorch2\\lib\\site-packages\\llama_index\\indices\\base.py:97\u001b[0m, in \u001b[0;36mBaseIndex.from_documents\u001b[1;34m(cls, documents, storage_context, service_context, show_progress, **kwargs)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[39mwith\u001b[39;00m service_context\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mas_trace(\u001b[39m\"\u001b[39m\u001b[39mindex_construction\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m     96\u001b[0m     \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m documents:\n\u001b[1;32m---> 97\u001b[0m         docstore\u001b[39m.\u001b[39mset_document_hash(doc\u001b[39m.\u001b[39;49mget_doc_id(), doc\u001b[39m.\u001b[39mhash)\n\u001b[0;32m     98\u001b[0m     nodes \u001b[39m=\u001b[39m service_context\u001b[39m.\u001b[39mnode_parser\u001b[39m.\u001b[39mget_nodes_from_documents(\n\u001b[0;32m     99\u001b[0m         documents, show_progress\u001b[39m=\u001b[39mshow_progress\n\u001b[0;32m    100\u001b[0m     )\n\u001b[0;32m    102\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m(\n\u001b[0;32m    103\u001b[0m         nodes\u001b[39m=\u001b[39mnodes,\n\u001b[0;32m    104\u001b[0m         storage_context\u001b[39m=\u001b[39mstorage_context,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    107\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m    108\u001b[0m     )\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'get_doc_id'"
     ]
    }
   ],
   "source": [
    "index = VectorStoreIndex.from_documents(documents=documents)\n",
    "\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"LlamaIndex\",\n",
    "        func=lambda q: str(index.as_query_engine().query(q)),\n",
    "        description=\"useful for when you want to answer questions about the author. The input to this tool should be a complete english sentence.\",\n",
    "        return_direct=True,\n",
    "    ),\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, List, Mapping, Optional\n",
    "\n",
    "from langchain.callbacks.manager import CallbackManagerForLLMRun\n",
    "from langchain.llms.base import LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LLM' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[75], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mCustomLLM\u001b[39;00m(LLM):\n\u001b[0;32m      2\u001b[0m     n: \u001b[39mint\u001b[39m\n\u001b[0;32m      4\u001b[0m     \u001b[39m@property\u001b[39m\n\u001b[0;32m      5\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m_llm_type\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'LLM' is not defined"
     ]
    }
   ],
   "source": [
    "class CustomLLM(LLM):\n",
    "    n: int\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"custom\"\n",
    "\n",
    "    def _call(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> str:\n",
    "        if stop is not None:\n",
    "            raise ValueError(\"stop kwargs are not permitted.\")\n",
    "        return prompt[: self.n]\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self) -> Mapping[str, Any]:\n",
    "        \"\"\"Get the identifying parameters.\"\"\"\n",
    "        return {\"n\": self.n}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
